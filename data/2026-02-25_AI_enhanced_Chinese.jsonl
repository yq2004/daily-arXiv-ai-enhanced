{"id": "2602.20507", "categories": ["cs.IR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.20507", "abs": "https://arxiv.org/abs/2602.20507", "authors": ["William Anthony Mason"], "title": "Indaleko: The Unified Personal Index", "comment": "PhD dissertation, University of British Columbia, August 2025. 287 pages", "summary": "Personal information retrieval fails when systems ignore how human memory works. While existing platforms force keyword searches across isolated silos, humans naturally recall through episodic cues like when, where, and in what context information was encountered. This dissertation presents the Unified Personal Index (UPI), a memory-aligned architecture that bridges this fundamental gap. The Indaleko prototype demonstrates the UPI's feasibility on a 31-million file dataset spanning 160TB across eight storage platforms. By integrating temporal, spatial, and activity metadata into a unified graph database, Indaleko enables natural language queries like \"photos near the conference venue last spring\" that existing systems cannot process. The implementation achieves sub-second query responses through memory anchor indexing, eliminates cross-platform search fragmentation, and maintains perfect precision for well-specified memory patterns. Evaluation against commercial systems (Google Drive, OneDrive, Dropbox, Windows Search) reveals that all fail on memory-based queries, returning overwhelming result sets without contextual filtering. In contrast, Indaleko successfully processes multi-dimensional queries combining time, location, and activity patterns. The extensible architecture supports rapid integration of new data sources (10 minutes to 10 hours per provider) while preserving privacy through UUID-based semantic decoupling. The UPI's architectural synthesis bridges cognitive theory with distributed systems design, as demonstrated through the Indaleko prototype and rigorous evaluation. This work transforms personal information retrieval from keyword matching to memory-aligned finding, providing immediate benefits for existing data while establishing foundations for future context-aware systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0e\u4eba\u7c7b\u8bb0\u5fc6\u673a\u5236\u5bf9\u9f50\u7684\u4e2a\u4eba\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u67b6\u6784\u2014\u2014\u7edf\u4e00\u4e2a\u4eba\u7d22\u5f15\uff08UPI\uff09\uff0c\u901a\u8fc7\u6574\u5408\u65f6\u95f4\u3001\u7a7a\u95f4\u548c\u6d3b\u52a8\u5143\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u81ea\u7136\u8bed\u8a00\u7684\u591a\u7ef4\u5ea6\u67e5\u8be2\u3002", "motivation": "\u73b0\u6709\u4e2a\u4eba\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u5b58\u5728\u6839\u672c\u7f3a\u9677\uff0c\u5b83\u4eec\u5ffd\u7565\u4e86\u4eba\u7c7b\u8bb0\u5fc6\u7684\u5de5\u4f5c\u65b9\u5f0f\u3002\u5f53\u524d\u5e73\u53f0\u5f3a\u5236\u7528\u6237\u4f7f\u7528\u5173\u952e\u8bcd\u641c\u7d22\u5b64\u7acb\u7684\u6570\u636e\u5b64\u5c9b\uff0c\u800c\u4eba\u7c7b\u81ea\u7136\u56de\u5fc6\u662f\u901a\u8fc7\u60c5\u5883\u7ebf\u7d22\uff08\u4f55\u65f6\u3001\u4f55\u5730\u3001\u5728\u4ec0\u4e48\u60c5\u5883\u4e0b\u9047\u5230\u4fe1\u606f\uff09\u8fdb\u884c\u7684\u3002", "method": "\u63d0\u51fa\u4e86\u7edf\u4e00\u4e2a\u4eba\u7d22\u5f15\uff08UPI\uff09\u67b6\u6784\uff0c\u901a\u8fc7\u5c06\u65f6\u95f4\u3001\u7a7a\u95f4\u548c\u6d3b\u52a8\u5143\u6570\u636e\u6574\u5408\u5230\u7edf\u4e00\u7684\u56fe\u6570\u636e\u5e93\u4e2d\uff0c\u6784\u5efa\u4e86Indaleko\u539f\u578b\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u4f7f\u7528\u5185\u5b58\u951a\u70b9\u7d22\u5f15\u5b9e\u73b0\u4e9a\u79d2\u7ea7\u67e5\u8be2\u54cd\u5e94\uff0c\u901a\u8fc7UUID\u8bed\u4e49\u89e3\u8026\u4fdd\u62a4\u9690\u79c1\uff0c\u5e76\u652f\u6301\u5feb\u901f\u96c6\u6210\u65b0\u6570\u636e\u6e90\u3002", "result": "Indaleko\u539f\u578b\u572831\u767e\u4e07\u6587\u4ef6\u3001160TB\u8de88\u4e2a\u5b58\u50a8\u5e73\u53f0\u7684\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027\u3002\u76f8\u6bd4Google Drive\u3001OneDrive\u3001Dropbox\u548cWindows Search\u7b49\u5546\u4e1a\u7cfb\u7edf\uff0cIndaleko\u80fd\u6210\u529f\u5904\u7406\u7ed3\u5408\u65f6\u95f4\u3001\u4f4d\u7f6e\u548c\u6d3b\u52a8\u6a21\u5f0f\u7684\u591a\u7ef4\u5ea6\u67e5\u8be2\uff0c\u800c\u73b0\u6709\u7cfb\u7edf\u5728\u8bb0\u5fc6\u5f0f\u67e5\u8be2\u4e0a\u5b8c\u5168\u5931\u8d25\u3002\u7cfb\u7edf\u5b9e\u73b0\u4e86\u4e9a\u79d2\u7ea7\u67e5\u8be2\u54cd\u5e94\uff0c\u5bf9\u660e\u786e\u8bb0\u5fc6\u6a21\u5f0f\u4fdd\u6301\u5b8c\u7f8e\u7cbe\u786e\u5ea6\u3002", "conclusion": "UPI\u67b6\u6784\u5f25\u5408\u4e86\u8ba4\u77e5\u7406\u8bba\u4e0e\u5206\u5e03\u5f0f\u7cfb\u7edf\u8bbe\u8ba1\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u5c06\u4e2a\u4eba\u4fe1\u606f\u68c0\u7d22\u4ece\u5173\u952e\u8bcd\u5339\u914d\u8f6c\u53d8\u4e3a\u8bb0\u5fc6\u5bf9\u9f50\u7684\u67e5\u627e\u3002\u8fd9\u4e0d\u4ec5\u4e3a\u73b0\u6709\u6570\u636e\u63d0\u4f9b\u4e86\u5373\u65f6\u6548\u76ca\uff0c\u8fd8\u4e3a\u672a\u6765\u60c5\u5883\u611f\u77e5\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.20676", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20676", "abs": "https://arxiv.org/abs/2602.20676", "authors": ["Shuzhi Cao", "Rong Chen", "Ailong He", "Shuguang Han", "Jufeng Chen"], "title": "PRECTR-V2:Unified Relevance-CTR Framework with Cross-User Preference Mining, Exposure Bias Correction, and LLM-Distilled Encoder Optimization", "comment": "arXiv admin note: text overlap with arXiv:2503.18395", "summary": "In search systems, effectively coordinating the two core objectives of search relevance matching and click-through rate (CTR) prediction is crucial for discovering users' interests and enhancing platform revenue. In our prior work PRECTR, we proposed a unified framework to integrate these two subtasks,thereby eliminating their inconsistency and leading to mutual benefit.However, our previous work still faces three main challenges. First, low-active users and new users have limited search behavioral data, making it difficult to achieve effective personalized relevance preference modeling. Second, training data for ranking models predominantly come from high-relevance exposures, creating a distribution mismatch with the broader candidate space in coarse-ranking, leading to generalization bias. Third, due to the latency constraint, the original model employs an Emb+MLP architecture with a frozen BERT encoder, which prevents joint optimization and creates misalignment between representation learning and CTR fine-tuning. To solve these issues, we further reinforce our method and propose PRECTR-V2. Specifically, we mitigate the low-activity users' sparse behavior problem by mining global relevance preferences under the specific query, which facilitates effective personalized relevance modeling for cold-start scenarios. Subsequently, we construct hard negative samples through embedding noise injection and relevance label reconstruction, and optimize their relative ranking against positive samples via pairwise loss, thereby correcting exposure bias. Finally, we pretrain a lightweight transformer-based encoder via knowledge distillation from LLM and SFT on the text relevance classification task. This encoder replaces the frozen BERT module, enabling better adaptation to CTR fine-tuning and advancing beyond the traditional Emb+MLP paradigm.", "AI": {"tldr": "PRECTR-V2\uff1a\u4e00\u4e2a\u6539\u8fdb\u7684\u641c\u7d22\u7cfb\u7edf\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u6316\u6398\u5168\u5c40\u76f8\u5173\u6027\u504f\u597d\u3001\u6784\u5efa\u56f0\u96be\u8d1f\u6837\u672c\u548c\u8f7b\u91cf\u7ea7Transformer\u7f16\u7801\u5668\uff0c\u89e3\u51b3\u51b7\u542f\u52a8\u7528\u6237\u5efa\u6a21\u3001\u66dd\u5149\u504f\u5dee\u548c\u6a21\u578b\u67b6\u6784\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3PRECTR\u6846\u67b6\u9762\u4e34\u7684\u4e09\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1\uff09\u4f4e\u6d3b\u8dc3\u7528\u6237\u548c\u65b0\u7528\u6237\u641c\u7d22\u884c\u4e3a\u6570\u636e\u6709\u9650\uff0c\u96be\u4ee5\u5b9e\u73b0\u4e2a\u6027\u5316\u76f8\u5173\u6027\u504f\u597d\u5efa\u6a21\uff1b2\uff09\u8bad\u7ec3\u6570\u636e\u4e3b\u8981\u6765\u81ea\u9ad8\u76f8\u5173\u6027\u66dd\u5149\uff0c\u4e0e\u7c97\u6392\u9636\u6bb5\u5019\u9009\u7a7a\u95f4\u5b58\u5728\u5206\u5e03\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u6cdb\u5316\u504f\u5dee\uff1b3\uff09\u5ef6\u8fdf\u9650\u5236\u4e0b\u4f7f\u7528\u51bb\u7ed3BERT\u7f16\u7801\u5668\u7684Emb+MLP\u67b6\u6784\uff0c\u963b\u788d\u8054\u5408\u4f18\u5316\uff0c\u9020\u6210\u8868\u5f81\u5b66\u4e60\u4e0eCTR\u5fae\u8c03\u4e0d\u5bf9\u9f50\u3002", "method": "1\uff09\u901a\u8fc7\u6316\u6398\u7279\u5b9a\u67e5\u8be2\u4e0b\u7684\u5168\u5c40\u76f8\u5173\u6027\u504f\u597d\uff0c\u7f13\u89e3\u4f4e\u6d3b\u8dc3\u7528\u6237\u7a00\u758f\u884c\u4e3a\u95ee\u9898\uff1b2\uff09\u901a\u8fc7\u5d4c\u5165\u566a\u58f0\u6ce8\u5165\u548c\u76f8\u5173\u6027\u6807\u7b7e\u91cd\u6784\u6784\u5efa\u56f0\u96be\u8d1f\u6837\u672c\uff0c\u4f7f\u7528\u6210\u5bf9\u635f\u5931\u4f18\u5316\u5176\u4e0e\u6b63\u6837\u672c\u7684\u76f8\u5bf9\u6392\u5e8f\uff1b3\uff09\u901a\u8fc7LLM\u77e5\u8bc6\u84b8\u998f\u548c\u5728\u6587\u672c\u76f8\u5173\u6027\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684SFT\u9884\u8bad\u7ec3\u8f7b\u91cf\u7ea7Transformer\u7f16\u7801\u5668\uff0c\u66ff\u4ee3\u51bb\u7ed3BERT\u6a21\u5757\u3002", "result": "\u63d0\u51fa\u7684PRECTR-V2\u65b9\u6cd5\u80fd\u591f\uff1a1\uff09\u6709\u6548\u5904\u7406\u51b7\u542f\u52a8\u573a\u666f\u4e0b\u7684\u4e2a\u6027\u5316\u76f8\u5173\u6027\u5efa\u6a21\uff1b2\uff09\u7ea0\u6b63\u66dd\u5149\u504f\u5dee\uff1b3\uff09\u5b9e\u73b0\u8868\u5f81\u5b66\u4e60\u4e0eCTR\u5fae\u8c03\u7684\u66f4\u597d\u5bf9\u9f50\uff0c\u8d85\u8d8a\u4f20\u7edfEmb+MLP\u8303\u5f0f\u3002", "conclusion": "PRECTR-V2\u901a\u8fc7\u4e09\u65b9\u9762\u6539\u8fdb\u89e3\u51b3\u4e86\u524d\u4ee3\u6846\u67b6\u7684\u4e3b\u8981\u9650\u5236\uff0c\u4e3a\u641c\u7d22\u7cfb\u7edf\u4e2d\u7684\u76f8\u5173\u6027\u5339\u914d\u548cCTR\u9884\u6d4b\u534f\u8c03\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7279\u522b\u662f\u5728\u51b7\u542f\u52a8\u7528\u6237\u5904\u7406\u3001\u5206\u5e03\u5339\u914d\u548c\u6a21\u578b\u67b6\u6784\u4f18\u5316\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\u3002"}}
{"id": "2602.20704", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.20704", "abs": "https://arxiv.org/abs/2602.20704", "authors": ["Zesheng Wang", "Longfei Xu", "Weidong Deng", "Huimin Yan", "Kaikui Liu", "Xiangxiang Chu"], "title": "IntRR: A Framework for Integrating SID Redistribution and Length Reduction", "comment": null, "summary": "Generative Recommendation (GR) has emerged as a transformative paradigm that reformulates the traditional cascade ranking system into a sequence-to-item generation task, facilitated by the use of discrete Semantic IDs (SIDs). However, current SIDs are suboptimal as the indexing objectives (Stage 1) are misaligned with the actual recommendation goals (Stage 2). Since these identifiers remain static (Stage 2), the backbone model lacks the flexibility to adapt them to the evolving complexities of user interactions. Furthermore, the prevailing strategy of flattening hierarchical SIDs into token sequences leads to sequence length inflation, resulting in prohibitive computational overhead and inference latency. To address these challenges, we propose IntRR, a novel framework that integrates objective-aligned SID Redistribution and structural Length Reduction. By leveraging item-specific Unique IDs (UIDs) as collaborative anchors, this approach dynamically redistributes semantic weights across hierarchical codebook layers. Concurrently, IntRR handles the SID hierarchy recursively, eliminating the need to flatten sequences. This ensures a fixed cost of one token per item. Extensive experiments on benchmark datasets demonstrate that IntRR yields substantial improvements over representative generative baselines, achieving superior performance in both recommendation accuracy and efficiency.", "AI": {"tldr": "IntRR\u6846\u67b6\u901a\u8fc7\u76ee\u6807\u5bf9\u9f50\u7684\u8bed\u4e49ID\u91cd\u5206\u914d\u548c\u7ed3\u6784\u957f\u5ea6\u7f29\u51cf\uff0c\u89e3\u51b3\u4e86\u751f\u6210\u5f0f\u63a8\u8350\u4e2d\u8bed\u4e49ID\u4e0e\u63a8\u8350\u76ee\u6807\u4e0d\u5bf9\u9f50\u3001\u5e8f\u5217\u957f\u5ea6\u81a8\u80c0\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u5f0f\u63a8\u8350\u4e2d\u7684\u8bed\u4e49ID\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u7d22\u5f15\u76ee\u6807\uff08\u7b2c\u4e00\u9636\u6bb5\uff09\u4e0e\u5b9e\u9645\u63a8\u8350\u76ee\u6807\uff08\u7b2c\u4e8c\u9636\u6bb5\uff09\u4e0d\u5bf9\u9f50\uff0c\u4e14\u8bed\u4e49ID\u5728\u7b2c\u4e8c\u9636\u6bb5\u4fdd\u6301\u9759\u6001\uff0c\u65e0\u6cd5\u9002\u5e94\u7528\u6237\u4ea4\u4e92\u7684\u590d\u6742\u6027\uff1b2\uff09\u5c06\u5206\u5c42\u8bed\u4e49ID\u5c55\u5e73\u4e3a\u4ee4\u724c\u5e8f\u5217\u5bfc\u81f4\u5e8f\u5217\u957f\u5ea6\u81a8\u80c0\uff0c\u5e26\u6765\u9ad8\u6602\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u63a8\u7406\u5ef6\u8fdf\u3002", "method": "\u63d0\u51faIntRR\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u5229\u7528\u7269\u54c1\u7279\u5b9a\u552f\u4e00ID\u4f5c\u4e3a\u534f\u4f5c\u951a\u70b9\uff0c\u5728\u5206\u5c42\u7801\u672c\u5c42\u95f4\u52a8\u6001\u91cd\u65b0\u5206\u914d\u8bed\u4e49\u6743\u91cd\uff0c\u5b9e\u73b0\u76ee\u6807\u5bf9\u9f50\u7684\u8bed\u4e49ID\u91cd\u5206\u914d\uff1b2\uff09\u9012\u5f52\u5904\u7406\u8bed\u4e49ID\u5c42\u6b21\u7ed3\u6784\uff0c\u907f\u514d\u5e8f\u5217\u5c55\u5e73\uff0c\u786e\u4fdd\u6bcf\u4e2a\u7269\u54c1\u4ec5\u9700\u4e00\u4e2a\u4ee4\u724c\u7684\u56fa\u5b9a\u6210\u672c\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cIntRR\u76f8\u6bd4\u4ee3\u8868\u6027\u751f\u6210\u5f0f\u57fa\u7ebf\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\uff0c\u5728\u63a8\u8350\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u5b9e\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "IntRR\u901a\u8fc7\u52a8\u6001\u8bed\u4e49ID\u91cd\u5206\u914d\u548c\u9012\u5f52\u5c42\u6b21\u5904\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u751f\u6210\u5f0f\u63a8\u8350\u4e2d\u8bed\u4e49ID\u4e0d\u5bf9\u9f50\u548c\u5e8f\u5217\u957f\u5ea6\u81a8\u80c0\u7684\u95ee\u9898\uff0c\u4e3a\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20735", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20735", "abs": "https://arxiv.org/abs/2602.20735", "authors": ["Kun Ran", "Marwah Alaofi", "Danula Hettiachchi", "Chenglong Ma", "Khoi Nguyen Dinh Anh", "Khoi Vo Nguyen", "Sachin Pathiyan Cherumanal", "Lida Rashidi", "Falk Scholer", "Damiano Spina", "Shuoqi Sun", "Oleg Zendel"], "title": "RMIT-ADM+S at the MMU-RAG NeurIPS 2025 Competition", "comment": "MMU-RAG NeurIPS 2025 winning system", "summary": "This paper presents the award-winning RMIT-ADM+S system for the Text-to-Text\n  track of the NeurIPS~2025 MMU-RAG Competition. We introduce Routing-to-RAG\n  (R2RAG), a research-focused retrieval-augmented generation (RAG)\n  architecture composed of lightweight components that dynamically adapt the\n  retrieval strategy based on inferred query complexity and evidence\n  sufficiency. The system uses smaller LLMs, enabling operation on a single\n  consumer-grade GPU while supporting complex research tasks. It builds on the\n  G-RAG system, winner of the ACM~SIGIR~2025 LiveRAG Challenge, and extends it\n  with modules informed by qualitative review of outputs. R2RAG won the Best\n  Dynamic Evaluation award in the Open Source category, demonstrating high\n  effectiveness with careful design and efficient use of resources.", "AI": {"tldr": "\u63d0\u51faR2RAG\u7cfb\u7edf\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7ec4\u4ef6\u52a8\u6001\u8c03\u6574\u68c0\u7d22\u7b56\u7565\uff0c\u57fa\u4e8e\u67e5\u8be2\u590d\u6742\u6027\u548c\u8bc1\u636e\u5145\u5206\u6027\u8fdb\u884c\u81ea\u9002\u5e94\uff0c\u5728\u5355\u4e2a\u6d88\u8d39\u7ea7GPU\u4e0a\u9ad8\u6548\u8fd0\u884c\u5e76\u8d62\u5f97\u7ade\u8d5b\u5956\u9879\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfRAG\u7cfb\u7edf\u5728\u590d\u6742\u7814\u7a76\u4efb\u52a1\u4e2d\u68c0\u7d22\u7b56\u7565\u56fa\u5b9a\u7684\u5c40\u9650\u6027\uff0c\u9700\u8981\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u5b9e\u73b0\u9ad8\u6548\u4e14\u81ea\u9002\u5e94\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3002", "method": "\u57fa\u4e8eG-RAG\u7cfb\u7edf\u6269\u5c55\uff0c\u5f15\u5165\u8f7b\u91cf\u7ea7\u7ec4\u4ef6\u6784\u5efaR2RAG\u67b6\u6784\uff0c\u901a\u8fc7\u63a8\u65ad\u67e5\u8be2\u590d\u6742\u6027\u548c\u8bc1\u636e\u5145\u5206\u6027\u52a8\u6001\u8c03\u6574\u68c0\u7d22\u7b56\u7565\uff0c\u5e76\u96c6\u6210\u57fa\u4e8e\u8f93\u51fa\u8d28\u91cf\u5206\u6790\u7684\u6a21\u5757\u3002", "result": "R2RAG\u5728NeurIPS 2025 MMU-RAG\u7ade\u8d5b\u7684Text-to-Text\u8d5b\u9053\u4e2d\u83b7\u5956\uff0c\u83b7\u5f97\u5f00\u6e90\u7c7b\u522b\u6700\u4f73\u52a8\u6001\u8bc4\u4f30\u5956\uff0c\u8bc1\u660e\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u9ad8\u6548\u5904\u7406\u590d\u6742\u7814\u7a76\u4efb\u52a1\u7684\u80fd\u529b\u3002", "conclusion": "R2RAG\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u548c\u9ad8\u6548\u8d44\u6e90\u5229\u7528\uff0c\u5c55\u793a\u4e86\u8f7b\u91cf\u7ea7\u81ea\u9002\u5e94RAG\u67b6\u6784\u5728\u590d\u6742\u7814\u7a76\u4efb\u52a1\u4e2d\u7684\u9ad8\u6709\u6548\u6027\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2602.20162", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20162", "abs": "https://arxiv.org/abs/2602.20162", "authors": ["Yutao Sun", "Mingshuai Chen", "Tiancheng Zhao", "Phillip Miao", "Zilun Zhang", "Haozhan Shen", "Ruizhe Zhu", "Jianwei Yin"], "title": "Talking to Yourself: Defying Forgetting in Large Language Models", "comment": null, "summary": "Catastrophic forgetting remains a major challenge when fine-tuning large language models (LLMs) on narrow, task-specific data, often degrading their general knowledge and reasoning abilities. We propose SA-SFT, a lightweight self-augmentation routine in which an LLM generates self-dialogues prior to fine-tuning, and the resulting self-authored data are mixed with task data without modifying optimization or training schedules.\n  Despite requiring no external data or additional tuning, SA-SFT consistently mitigates catastrophic forgetting while improving in-domain performance. Across 50 evaluation scenarios, it maintains performance comparable to the original model and achieves the best results in 40 cases, outperforming common baselines such as layer freezing and external data mixing. Guided by these empirical findings, we further present a theoretical analysis suggesting that forgetting can partly stem from style-induced parameter drift, and that self-alignment through self-generated data provides an effective means to counteract this effect. Overall, our results indicate that self-augmentation offers a simple and effective mechanism for robust LLM adaptation without incurring catastrophic forgetting.", "AI": {"tldr": "\u63d0\u51faSA-SFT\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u751f\u6210\u5bf9\u8bdd\u6570\u636e\u4e0e\u4efb\u52a1\u6570\u636e\u6df7\u5408\u8bad\u7ec3\uff0c\u6709\u6548\u7f13\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u65f6\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u65e0\u9700\u5916\u90e8\u6570\u636e\u6216\u989d\u5916\u8c03\u4f18\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7279\u5b9a\u4efb\u52a1\u6570\u636e\u4e0a\u5fae\u8c03\u65f6\u5bb9\u6613\u51fa\u73b0\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5bfc\u81f4\u901a\u7528\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u4e0b\u964d\uff0c\u9700\u8981\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u7f13\u89e3\u65b9\u6cd5\u3002", "method": "SA-SFT\u65b9\u6cd5\uff1a\u5728\u5fae\u8c03\u524d\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u81ea\u5bf9\u8bdd\u6570\u636e\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u81ea\u751f\u6210\u6570\u636e\u4e0e\u4efb\u52a1\u6570\u636e\u6df7\u5408\uff0c\u4e0d\u6539\u53d8\u4f18\u5316\u6216\u8bad\u7ec3\u8ba1\u5212\uff0c\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u81ea\u589e\u5f3a\u3002", "result": "\u572850\u4e2a\u8bc4\u4f30\u573a\u666f\u4e2d\uff0cSA-SFT\u4fdd\u6301\u4e86\u4e0e\u539f\u59cb\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u572840\u4e2a\u6848\u4f8b\u4e2d\u53d6\u5f97\u6700\u4f73\u7ed3\u679c\uff0c\u4f18\u4e8e\u5c42\u51bb\u7ed3\u548c\u5916\u90e8\u6570\u636e\u6df7\u5408\u7b49\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u81ea\u751f\u6210\u6570\u636e\u63d0\u4f9b\u7684\u81ea\u5bf9\u9f50\u80fd\u6709\u6548\u5bf9\u6297\u98ce\u683c\u8bf1\u5bfc\u7684\u53c2\u6570\u6f02\u79fb\uff0cSA-SFT\u4e3a\u9c81\u68d2\u7684LLM\u9002\u5e94\u63d0\u4f9b\u4e86\u7b80\u5355\u6709\u6548\u7684\u673a\u5236\uff0c\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\u3002"}}
{"id": "2602.20800", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.20800", "abs": "https://arxiv.org/abs/2602.20800", "authors": ["Dalia Nahhas", "Xiaohao Cai", "Imran Razzak", "Shoaib Jameel"], "title": "Mitigating Preference Leakage via Strict Estimator Separation for Normative Generative Ranking", "comment": null, "summary": "In Generative Information Retrieval (GenIR), the bottleneck has shifted from generation to the selection of candidates, particularly for normative criteria such as cultural relevance. Current LLM-as-a-Judge evaluations often suffer from circularity and preference leakage, where overlapping supervision and evaluation models inflate performance. We address this by formalising cultural relevance as a within-query ranking task and introducing a leakage-free two-judge framework that strictly separates supervision (Judge B) from evaluation (Judge A). On a new benchmark of 33,052 (NGR-33k) culturally grounded stories, we find that while classical baselines yield only modest gains, a dense bi-encoder distilled from a Judge-B-supervised Cross-Encoder is highly effective. Although the Cross-Encoder provides a strong supervision signal for distillation, the distilled BGE-M3 model substantially outperforms it under leakage-free Judge~A evaluation. We validate our framework on the human-curated Moral Stories dataset, showing strong alignment with human norms. Our results demonstrate that rigorous evaluator separation is a prerequisite for credible GenIR evaluation, proving that subtle cultural preferences can be distilled into efficient rankers without leakage.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u6cc4\u6f0f\u7684\u53cc\u8bc4\u4f30\u6846\u67b6\u6765\u89e3\u51b3\u751f\u6210\u5f0f\u4fe1\u606f\u68c0\u7d22\u4e2d\u7684\u6587\u5316\u76f8\u5173\u6027\u8bc4\u4f30\u95ee\u9898\uff0c\u901a\u8fc7\u4e25\u683c\u5206\u79bb\u76d1\u7763\u548c\u8bc4\u4f30\u6a21\u578b\u6765\u907f\u514d\u504f\u597d\u6cc4\u6f0f\uff0c\u5e76\u5728\u65b0\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u751f\u6210\u5f0f\u4fe1\u606f\u68c0\u7d22\u4e2d\uff0c\u5019\u9009\u9009\u62e9\u6210\u4e3a\u74f6\u9888\uff0c\u7279\u522b\u662f\u5728\u6587\u5316\u76f8\u5173\u6027\u7b49\u89c4\u8303\u6027\u6807\u51c6\u65b9\u9762\u3002\u5f53\u524d\u7684LLM-as-a-Judge\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u5faa\u73af\u6027\u548c\u504f\u597d\u6cc4\u6f0f\u95ee\u9898\uff0c\u5373\u76d1\u7763\u548c\u8bc4\u4f30\u6a21\u578b\u91cd\u53e0\u5bfc\u81f4\u6027\u80fd\u865a\u9ad8\u3002", "method": "1. \u5c06\u6587\u5316\u76f8\u5173\u6027\u5f62\u5f0f\u5316\u4e3a\u67e5\u8be2\u5185\u6392\u5e8f\u4efb\u52a1\uff1b2. \u5f15\u5165\u65e0\u6cc4\u6f0f\u7684\u53cc\u8bc4\u4f30\u6846\u67b6\uff0c\u4e25\u683c\u5206\u79bb\u76d1\u7763\u6a21\u578b\uff08Judge B\uff09\u548c\u8bc4\u4f30\u6a21\u578b\uff08Judge A\uff09\uff1b3. \u5728\u65b0\u57fa\u51c6NGR-33k\uff0833,052\u4e2a\u6587\u5316\u76f8\u5173\u6545\u4e8b\uff09\u4e0a\u6d4b\u8bd5\uff1b4. \u4f7f\u7528Judge B\u76d1\u7763\u7684\u4ea4\u53c9\u7f16\u7801\u5668\u84b8\u998f\u51fa\u5bc6\u96c6\u53cc\u7f16\u7801\u5668\u6a21\u578b\uff1b5. \u5728\u4eba\u7c7b\u6807\u6ce8\u7684Moral Stories\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6846\u67b6\u3002", "result": "1. \u7ecf\u5178\u57fa\u7ebf\u65b9\u6cd5\u4ec5\u83b7\u5f97\u9002\u5ea6\u63d0\u5347\uff1b2. \u4eceJudge B\u76d1\u7763\u7684\u4ea4\u53c9\u7f16\u7801\u5668\u84b8\u998f\u51fa\u7684\u5bc6\u96c6\u53cc\u7f16\u7801\u5668\uff08BGE-M3\uff09\u975e\u5e38\u6709\u6548\uff1b3. \u867d\u7136\u4ea4\u53c9\u7f16\u7801\u5668\u4e3a\u84b8\u998f\u63d0\u4f9b\u4e86\u5f3a\u76d1\u7763\u4fe1\u53f7\uff0c\u4f46\u5728\u65e0\u6cc4\u6f0f\u7684Judge A\u8bc4\u4f30\u4e0b\uff0c\u84b8\u998f\u51fa\u7684BGE-M3\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u4ea4\u53c9\u7f16\u7801\u5668\uff1b4. \u5728Moral Stories\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u6846\u67b6\u4e0e\u4eba\u7c7b\u89c4\u8303\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u4e25\u683c\u7684\u8bc4\u4f30\u8005\u5206\u79bb\u662f\u53ef\u4fe1\u7684\u751f\u6210\u5f0f\u4fe1\u606f\u68c0\u7d22\u8bc4\u4f30\u7684\u524d\u63d0\u6761\u4ef6\uff0c\u5fae\u5999\u7684\u6587\u5316\u504f\u597d\u53ef\u4ee5\u5728\u65e0\u6cc4\u6f0f\u7684\u60c5\u51b5\u4e0b\u84b8\u998f\u5230\u9ad8\u6548\u7684\u6392\u5e8f\u5668\u4e2d\u3002"}}
{"id": "2602.20164", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20164", "abs": "https://arxiv.org/abs/2602.20164", "authors": ["Sachin Gopal Wani", "Eric Page", "Ajay Dholakia", "David Ellison"], "title": "Benchmarking Distilled Language Models: Performance and Efficiency in Resource-Constrained Settings", "comment": "16 pages, 5 figures, accepted at the the 2025 TPCTC Conference", "summary": "Knowledge distillation offers a transformative pathway to developing powerful, yet efficient, small language models (SLMs) suitable for resource-constrained environments. In this paper, we benchmark the performance and computational cost of distilled models against their vanilla and proprietary counterparts, providing a quantitative analysis of their efficiency. Our results demonstrate that distillation creates a superior performance-tocompute curve. We find that creating a distilled 8B model is over 2,000 times more compute-efficient than training its vanilla counterpart, while achieving reasoning capabilities on par with, or even exceeding, standard models ten times its size. These findings validate distillation not just as a compression technique, but as a primary strategy for building state-of-the-art, accessible AI", "AI": {"tldr": "\u77e5\u8bc6\u84b8\u998f\u80fd\u521b\u5efa\u6bd4\u4f20\u7edf\u8bad\u7ec3\u66f4\u9ad8\u6548\u7684\u5c0f\u8bed\u8a00\u6a21\u578b\uff0c8B\u84b8\u998f\u6a21\u578b\u6bd4\u666e\u901a\u8bad\u7ec3\u8ba1\u7b97\u6548\u7387\u9ad82000\u500d\u4ee5\u4e0a\uff0c\u6027\u80fd\u5ab2\u7f8e\u592710\u500d\u7684\u6a21\u578b", "motivation": "\u7814\u7a76\u77e5\u8bc6\u84b8\u998f\u5728\u5f00\u53d1\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u9ad8\u6548\u5c0f\u8bed\u8a00\u6a21\u578b\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u91cf\u5316\u5206\u6790\u6bd4\u8f83\u84b8\u998f\u6a21\u578b\u4e0e\u666e\u901a\u6a21\u578b\u3001\u4e13\u6709\u6a21\u578b\u7684\u6027\u80fd\u548c\u8ba1\u7b97\u6210\u672c", "method": "\u5bf9\u84b8\u998f\u6a21\u578b\u3001\u666e\u901a\u6a21\u578b\u548c\u4e13\u6709\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5206\u6790\u5b83\u4eec\u7684\u6027\u80fd\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u63d0\u4f9b\u5b9a\u91cf\u6548\u7387\u5206\u6790", "result": "\u84b8\u998f\u521b\u5efa\u4e86\u66f4\u4f18\u7684\u6027\u80fd-\u8ba1\u7b97\u66f2\u7ebf\uff0c8B\u84b8\u998f\u6a21\u578b\u6bd4\u666e\u901a\u8bad\u7ec3\u8ba1\u7b97\u6548\u7387\u9ad82000\u500d\u4ee5\u4e0a\uff0c\u63a8\u7406\u80fd\u529b\u4e0e\u6807\u51c6\u592710\u500d\u7684\u6a21\u578b\u76f8\u5f53\u751a\u81f3\u66f4\u597d", "conclusion": "\u84b8\u998f\u4e0d\u4ec5\u662f\u538b\u7f29\u6280\u672f\uff0c\u66f4\u662f\u6784\u5efa\u5148\u8fdb\u3001\u53ef\u8bbf\u95eeAI\u7684\u4e3b\u8981\u7b56\u7565\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5b9e\u7528\u4ef7\u503c"}}
{"id": "2602.20877", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20877", "abs": "https://arxiv.org/abs/2602.20877", "authors": ["Jiwoo Kang", "Yeon-Chang Lee"], "title": "E-MMKGR: A Unified Multimodal Knowledge Graph Framework for E-commerce Applications", "comment": null, "summary": "Multimodal recommender systems (MMRSs) enhance collaborative filtering by leveraging item-side modalities, but their reliance on a fixed set of modalities and task-specific objectives limits both modality extensibility and task generalization. We propose E-MMKGR, a framework that constructs an e-commerce-specific Multimodal Knowledge Graph E-MMKG and learns unified item representations through GNN-based propagation and KG-oriented optimization. These representations provide a shared semantic foundation applicable to diverse tasks. Experiments on real-world Amazon datasets show improvements of up to 10.18% in Recall@10 for recommendation and up to 21.72% over vector-based retrieval for product search, demonstrating the effectiveness and extensibility of our approach.", "AI": {"tldr": "E-MMKGR\u6846\u67b6\u901a\u8fc7\u6784\u5efa\u7535\u5b50\u5546\u52a1\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\uff0c\u5b66\u4e60\u7edf\u4e00\u7269\u54c1\u8868\u793a\uff0c\u63d0\u5347\u63a8\u8350\u548c\u641c\u7d22\u4efb\u52a1\u6027\u80fd", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u4f9d\u8d56\u56fa\u5b9a\u6a21\u6001\u96c6\u548c\u4efb\u52a1\u7279\u5b9a\u76ee\u6807\uff0c\u9650\u5236\u4e86\u6a21\u6001\u6269\u5c55\u6027\u548c\u4efb\u52a1\u6cdb\u5316\u80fd\u529b", "method": "\u6784\u5efa\u7535\u5b50\u5546\u52a1\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31E-MMKG\uff0c\u901a\u8fc7GNN\u4f20\u64ad\u548c\u77e5\u8bc6\u56fe\u8c31\u4f18\u5316\u5b66\u4e60\u7edf\u4e00\u7269\u54c1\u8868\u793a", "result": "\u5728Amazon\u6570\u636e\u96c6\u4e0a\uff0c\u63a8\u8350\u4efb\u52a1Recall@10\u63d0\u534710.18%\uff0c\u4ea7\u54c1\u641c\u7d22\u6bd4\u5411\u91cf\u68c0\u7d22\u63d0\u534721.72%", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7edf\u4e00\u8bed\u4e49\u8868\u793a\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u6001\u6269\u5c55\u548c\u4efb\u52a1\u6cdb\u5316\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2602.20166", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20166", "abs": "https://arxiv.org/abs/2602.20166", "authors": ["Yongda Yu", "Lei Zhang", "Xinxin Guo", "Minghui Yu", "Zhengqi Zhuang", "Guoping Rong", "Haifeng Shen", "Zhengfeng Li", "Boge Wang", "Guoan Zhang", "Bangyu Xiang", "Xiaobin Xu"], "title": "ConceptRM: The Quest to Mitigate Alert Fatigue through Consensus-Based Purity-Driven Data Cleaning for Reflection Modelling", "comment": null, "summary": "In many applications involving intelligent agents, the overwhelming volume of alerts (mostly false) generated by the agents may desensitize users and cause them to overlook critical issues, leading to the so-called ''alert fatigue''. A common strategy is to train a reflection model as a filter to intercept false alerts with labelled data collected from user verification feedback. However, a key challenge is the noisy nature of such data as it is often collected in production environments. As cleaning noise via manual annotation incurs high costs, this paper proposes a novel method ConceptRM for constructing a high-quality corpus to train a reflection model capable of effectively intercepting false alerts. With only a small amount of expert annotations as anchors, ConceptRM creates perturbed datasets with varying noise ratios and utilizes co-teaching to train multiple distinct models for collaborative learning. By analyzing the consensus decisions of these models, it effectively identifies reliable negative samples from a noisy dataset. Experimental results demonstrate that ConceptRM significantly enhances the interception of false alerts with minimal annotation cost, outperforming several state-of-the-art LLM baselines by up to 53.31% on in-domain datasets and 41.67% on out-of-domain datasets.", "AI": {"tldr": "\u63d0\u51faConceptRM\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c11\u91cf\u4e13\u5bb6\u6807\u6ce8\u4f5c\u4e3a\u951a\u70b9\uff0c\u5229\u7528\u534f\u540c\u5b66\u4e60\u4ece\u566a\u58f0\u6570\u636e\u4e2d\u6784\u5efa\u9ad8\u8d28\u91cf\u8bed\u6599\u5e93\uff0c\u6709\u6548\u62e6\u622a\u865a\u5047\u8b66\u62a5", "motivation": "\u667a\u80fd\u4ee3\u7406\u4ea7\u751f\u7684\u6d77\u91cf\u8b66\u62a5\uff08\u591a\u4e3a\u865a\u5047\uff09\u5bfc\u81f4\u7528\u6237\u4ea7\u751f\"\u8b66\u62a5\u75b2\u52b3\"\uff0c\u5bb9\u6613\u5ffd\u7565\u5173\u952e\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7528\u6237\u53cd\u9988\u6570\u636e\u8bad\u7ec3\u53cd\u5c04\u6a21\u578b\uff0c\u4f46\u8fd9\u4e9b\u6570\u636e\u566a\u58f0\u5927\uff0c\u4eba\u5de5\u6e05\u7406\u6210\u672c\u9ad8", "method": "ConceptRM\u65b9\u6cd5\uff1a1) \u4f7f\u7528\u5c11\u91cf\u4e13\u5bb6\u6807\u6ce8\u4f5c\u4e3a\u951a\u70b9\uff1b2) \u521b\u5efa\u5177\u6709\u4e0d\u540c\u566a\u58f0\u6bd4\u4f8b\u7684\u6270\u52a8\u6570\u636e\u96c6\uff1b3) \u91c7\u7528\u534f\u540c\u6559\u5b66\u8bad\u7ec3\u591a\u4e2a\u4e0d\u540c\u6a21\u578b\u8fdb\u884c\u534f\u4f5c\u5b66\u4e60\uff1b4) \u901a\u8fc7\u5206\u6790\u6a21\u578b\u5171\u8bc6\u51b3\u7b56\u4ece\u566a\u58f0\u6570\u636e\u4e2d\u8bc6\u522b\u53ef\u9760\u8d1f\u6837\u672c", "result": "\u5b9e\u9a8c\u8868\u660eConceptRM\u663e\u8457\u63d0\u9ad8\u4e86\u865a\u5047\u8b66\u62a5\u62e6\u622a\u80fd\u529b\uff0c\u5728\u57df\u5185\u6570\u636e\u96c6\u4e0a\u6bd4\u6700\u5148\u8fdb\u7684LLM\u57fa\u7ebf\u9ad8\u51fa53.31%\uff0c\u5728\u57df\u5916\u6570\u636e\u96c6\u4e0a\u9ad8\u51fa41.67%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6781\u4f4e\u7684\u6807\u6ce8\u6210\u672c", "conclusion": "ConceptRM\u901a\u8fc7\u534f\u540c\u5b66\u4e60\u4ece\u566a\u58f0\u6570\u636e\u4e2d\u6784\u5efa\u9ad8\u8d28\u91cf\u8bad\u7ec3\u8bed\u6599\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8b66\u62a5\u75b2\u52b3\u95ee\u9898\uff0c\u5728\u6700\u5c0f\u5316\u6807\u6ce8\u6210\u672c\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u865a\u5047\u8b66\u62a5\u62e6\u622a\u6027\u80fd"}}
{"id": "2602.20986", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.20986", "abs": "https://arxiv.org/abs/2602.20986", "authors": ["Thibault Formal", "Maxime Louis", "Herv\u00e9 D\u00e9jean", "St\u00e9phane Clinchant"], "title": "Naver Labs Europe @ WSDM CUP | Multilingual Retrieval", "comment": "Report paper of our submission to the WSDM Cup 2026", "summary": "This report presents our participation to the WSDM Cup 2026 shared task on multilingual document retrieval from English queries. The task provides a challenging benchmark for cross-lingual generalization. It also provides a natural testbed for evaluating SPLARE, our recently proposed learned sparse retrieval model, which produces generalizable sparse latent representations and is particularly well suited to multilingual retrieval settings.\n  We evaluate five progressively enhanced runs, starting from a SPLARE-7B model and incorporating lightweight improvements, including reranking with Qwen3-Reranker-4B and simple score fusion strategies. Our results demonstrate the strength of SPLARE compared to state-of-the-art dense baselines such as Qwen3-8B-Embed. More broadly, our submission highlights the continued relevance and competitiveness of learned sparse retrieval models beyond English-centric scenarios.", "AI": {"tldr": "\u672c\u7814\u7a76\u5728WSDM Cup 2026\u591a\u8bed\u8a00\u6587\u6863\u68c0\u7d22\u4efb\u52a1\u4e2d\u8bc4\u4f30\u4e86SPLARE\u7a00\u758f\u68c0\u7d22\u6a21\u578b\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u6539\u8fdb\uff08\u5305\u62ec\u91cd\u6392\u5e8f\u548c\u5206\u6570\u878d\u5408\uff09\u53d6\u5f97\u4e86\u4f18\u4e8e\u5bc6\u96c6\u57fa\u7ebf\u7684\u7ed3\u679c\u3002", "motivation": "WSDM Cup 2026\u7684\u591a\u8bed\u8a00\u6587\u6863\u68c0\u7d22\u4efb\u52a1\u4e3a\u8de8\u8bed\u8a00\u6cdb\u5316\u63d0\u4f9b\u4e86\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\uff0c\u540c\u65f6\u4e5f\u662f\u8bc4\u4f30SPLARE\u7a00\u758f\u68c0\u7d22\u6a21\u578b\u5728\u975e\u82f1\u8bed\u573a\u666f\u4e0b\u6027\u80fd\u7684\u7406\u60f3\u6d4b\u8bd5\u5e73\u53f0\u3002", "method": "\u4eceSPLARE-7B\u6a21\u578b\u5f00\u59cb\uff0c\u9010\u6b65\u6dfb\u52a0\u8f7b\u91cf\u7ea7\u6539\u8fdb\uff1a\u4f7f\u7528Qwen3-Reranker-4B\u8fdb\u884c\u91cd\u6392\u5e8f\uff0c\u5e76\u91c7\u7528\u7b80\u5355\u7684\u5206\u6570\u878d\u5408\u7b56\u7565\uff0c\u5171\u8bc4\u4f30\u4e86\u4e94\u4e2a\u6e10\u8fdb\u589e\u5f3a\u7684\u8fd0\u884c\u7248\u672c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSPLARE\u6a21\u578b\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u5bc6\u96c6\u57fa\u7ebf\uff08\u5982Qwen3-8B-Embed\uff09\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u7a00\u758f\u68c0\u7d22\u6a21\u578b\u5728\u591a\u8bed\u8a00\u68c0\u7d22\u573a\u666f\u4e2d\u7684\u7ade\u4e89\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u5b66\u4e60\u578b\u7a00\u758f\u68c0\u7d22\u6a21\u578b\uff08\u7279\u522b\u662fSPLARE\uff09\u5728\u8d85\u8d8a\u82f1\u8bed\u4e2d\u5fc3\u573a\u666f\u7684\u591a\u8bed\u8a00\u68c0\u7d22\u4efb\u52a1\u4e2d\u4ecd\u7136\u5177\u6709\u76f8\u5173\u6027\u548c\u7ade\u4e89\u529b\uff0c\u4e3a\u8de8\u8bed\u8a00\u68c0\u7d22\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20294", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.20294", "abs": "https://arxiv.org/abs/2602.20294", "authors": ["Yu Li", "Pranav Narayanan Venkit", "Yada Pruksachatkun", "Chien-Sheng Wu"], "title": "InterviewSim: A Scalable Framework for Interview-Grounded Personality Simulation", "comment": null, "summary": "Simulating real personalities with large language models requires grounding generation in authentic personal data. Existing evaluation approaches rely on demographic surveys, personality questionnaires, or short AI-led interviews as proxies, but lack direct assessment against what individuals actually said. We address this gap with an interview-grounded evaluation framework for personality simulation at a large scale. We extract over 671,000 question-answer pairs from 23,000 verified interview transcripts across 1,000 public personalities, each with an average of 11.5 hours of interview content. We propose a multi-dimensional evaluation framework with four complementary metrics measuring content similarity, factual consistency, personality alignment, and factual knowledge retention. Through systematic comparison, we demonstrate that methods grounded in real interview data substantially outperform those relying solely on biographical profiles or the model's parametric knowledge. We further reveal a trade-off in how interview data is best utilized: retrieval-augmented methods excel at capturing personality style and response quality, while chronological-based methods better preserve factual consistency and knowledge retention. Our evaluation framework enables principled method selection based on application requirements, and our empirical findings provide actionable insights for advancing personality simulation research.", "AI": {"tldr": "\u57fa\u4e8e\u771f\u5b9e\u8bbf\u8c08\u6570\u636e\u7684\u4eba\u683c\u6a21\u62df\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u8bbf\u8c08\u6570\u636e\uff0823,000\u4e2a\u8bbf\u8c08\uff0c67.1\u4e07\u95ee\u7b54\u5bf9\uff09\u63d0\u51fa\u56db\u4e2a\u8bc4\u4f30\u6307\u6807\uff0c\u53d1\u73b0\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u68c0\u7d22\u589e\u5f3a\u4e0e\u65f6\u95f4\u987a\u5e8f\u65b9\u6cd5\u7684\u4e0d\u540c\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u683c\u6a21\u62df\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u4eba\u53e3\u7edf\u8ba1\u8c03\u67e5\u3001\u4eba\u683c\u95ee\u5377\u6216\u7b80\u77ed\u7684AI\u8bbf\u8c08\u4f5c\u4e3a\u4ee3\u7406\uff0c\u7f3a\u4e4f\u5bf9\u4e2a\u4f53\u5b9e\u9645\u8a00\u8bba\u7684\u76f4\u63a5\u8bc4\u4f30\uff0c\u9700\u8981\u5efa\u7acb\u57fa\u4e8e\u771f\u5b9e\u8bbf\u8c08\u6570\u636e\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u4ece1,000\u4e2a\u516c\u4f17\u4eba\u7269\u768423,000\u4e2a\u9a8c\u8bc1\u8bbf\u8c08\u8f6c\u5f55\u4e2d\u63d0\u53d667.1\u4e07\u95ee\u7b54\u5bf9\uff0c\u63d0\u51fa\u5305\u542b\u5185\u5bb9\u76f8\u4f3c\u6027\u3001\u4e8b\u5b9e\u4e00\u81f4\u6027\u3001\u4eba\u683c\u5bf9\u9f50\u548c\u4e8b\u5b9e\u77e5\u8bc6\u4fdd\u7559\u56db\u4e2a\u7ef4\u5ea6\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7cfb\u7edf\u6bd4\u8f83\u4e0d\u540c\u6570\u636e\u5229\u7528\u65b9\u6cd5\u3002", "result": "\u57fa\u4e8e\u771f\u5b9e\u8bbf\u8c08\u6570\u636e\u7684\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u4f20\u8bb0\u8d44\u6599\u6216\u6a21\u578b\u53c2\u6570\u77e5\u8bc6\u7684\u65b9\u6cd5\uff1b\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u5728\u6355\u6349\u4eba\u683c\u98ce\u683c\u548c\u56de\u7b54\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u800c\u65f6\u95f4\u987a\u5e8f\u65b9\u6cd5\u5728\u4fdd\u6301\u4e8b\u5b9e\u4e00\u81f4\u6027\u548c\u77e5\u8bc6\u4fdd\u7559\u65b9\u9762\u66f4\u4f18\u3002", "conclusion": "\u8be5\u8bc4\u4f30\u6846\u67b6\u652f\u6301\u57fa\u4e8e\u5e94\u7528\u9700\u6c42\u7684\u539f\u5219\u6027\u65b9\u6cd5\u9009\u62e9\uff0c\u4e3a\u63a8\u8fdb\u4eba\u683c\u6a21\u62df\u7814\u7a76\u63d0\u4f9b\u5b9e\u8bc1\u89c1\u89e3\u548c\u53ef\u884c\u6307\u5bfc\u3002"}}
{"id": "2602.20995", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20995", "abs": "https://arxiv.org/abs/2602.20995", "authors": ["Junyu Bi", "Xinting Niu", "Daixuan Cheng", "Kun Yuan", "Tao Wang", "Binbin Cao", "Jian Wu", "Yuning Jiang"], "title": "Generative Pseudo-Labeling for Pre-Ranking with LLMs", "comment": null, "summary": "Pre-ranking is a critical stage in industrial recommendation systems, tasked with efficiently scoring thousands of recalled items for downstream ranking. A key challenge is the train-serving discrepancy: pre-ranking models are trained only on exposed interactions, yet must score all recalled candidates -- including unexposed items -- during online serving. This mismatch not only induces severe sample selection bias but also degrades generalization, especially for long-tail content. Existing debiasing approaches typically rely on heuristics (e.g., negative sampling) or distillation from biased rankers, which either mislabel plausible unexposed items as negatives or propagate exposure bias into pseudo-labels. In this work, we propose Generative Pseudo-Labeling (GPL), a framework that leverages large language models (LLMs) to generate unbiased, content-aware pseudo-labels for unexposed items, explicitly aligning the training distribution with the online serving space. By offline generating user-specific interest anchors and matching them with candidates in a frozen semantic space, GPL provides high-quality supervision without adding online latency. Deployed in a large-scale production system, GPL improves click-through rate by 3.07%, while significantly enhancing recommendation diversity and long-tail item discovery.", "AI": {"tldr": "\u63d0\u51faGPL\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u672a\u66dd\u5149\u7269\u54c1\u751f\u6210\u65e0\u504f\u4f2a\u6807\u7b7e\uff0c\u89e3\u51b3\u63a8\u8350\u7cfb\u7edf\u9884\u6392\u5e8f\u9636\u6bb5\u7684\u8bad\u7ec3-\u670d\u52a1\u504f\u5dee\u95ee\u9898", "motivation": "\u5de5\u4e1a\u63a8\u8350\u7cfb\u7edf\u4e2d\u9884\u6392\u5e8f\u9636\u6bb5\u5b58\u5728\u8bad\u7ec3-\u670d\u52a1\u504f\u5dee\u95ee\u9898\uff1a\u6a21\u578b\u53ea\u5728\u66dd\u5149\u4ea4\u4e92\u4e0a\u8bad\u7ec3\uff0c\u4f46\u7ebf\u4e0a\u670d\u52a1\u65f6\u9700\u8981\u4e3a\u6240\u6709\u53ec\u56de\u7269\u54c1\u8bc4\u5206\uff0c\u5305\u62ec\u672a\u66dd\u5149\u7269\u54c1\u3002\u8fd9\u5bfc\u81f4\u6837\u672c\u9009\u62e9\u504f\u5dee\u548c\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\uff0c\u7279\u522b\u662f\u5bf9\u957f\u5c3e\u5185\u5bb9\u5f71\u54cd\u66f4\u5927\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u8bef\u6807\u8d1f\u6837\u672c\u6216\u4f20\u64ad\u66dd\u5149\u504f\u5dee\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u751f\u6210\u5f0f\u4f2a\u6807\u7b7e\uff08GPL\uff09\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u672a\u66dd\u5149\u7269\u54c1\u751f\u6210\u65e0\u504f\u3001\u5185\u5bb9\u611f\u77e5\u7684\u4f2a\u6807\u7b7e\u3002\u901a\u8fc7\u79bb\u7ebf\u751f\u6210\u7528\u6237\u7279\u5b9a\u5174\u8da3\u951a\u70b9\uff0c\u5e76\u5728\u51bb\u7ed3\u7684\u8bed\u4e49\u7a7a\u95f4\u4e2d\u4e0e\u5019\u9009\u7269\u54c1\u5339\u914d\uff0c\u4ece\u800c\u5bf9\u9f50\u8bad\u7ec3\u5206\u5e03\u4e0e\u7ebf\u4e0a\u670d\u52a1\u7a7a\u95f4\u3002", "result": "\u5728\u5927\u89c4\u6a21\u751f\u4ea7\u7cfb\u7edf\u4e2d\u90e8\u7f72\uff0c\u70b9\u51fb\u7387\u63d0\u53473.07%\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u591a\u6837\u6027\u548c\u957f\u5c3e\u7269\u54c1\u53d1\u73b0\u80fd\u529b\u3002", "conclusion": "GPL\u6846\u67b6\u901a\u8fc7\u5229\u7528LLMs\u751f\u6210\u65e0\u504f\u4f2a\u6807\u7b7e\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9884\u6392\u5e8f\u9636\u6bb5\u7684\u8bad\u7ec3-\u670d\u52a1\u504f\u5dee\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u7ebf\u4e0a\u670d\u52a1\u6548\u7387\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u6548\u679c\u548c\u591a\u6837\u6027\u3002"}}
{"id": "2602.20300", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20300", "abs": "https://arxiv.org/abs/2602.20300", "authors": ["William Watson", "Nicole Cho", "Sumitra Ganesh", "Manuela Veloso"], "title": "What Makes a Good Query? Measuring the Impact of Human-Confusing Linguistic Features on LLM Performance", "comment": "EACL 2026 Findings", "summary": "Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response. We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask: Are there certain types of queries that make hallucination more likely? A large-scale analysis reveals a consistent \"risk landscape\": certain features such as deep clause nesting and underspecification align with higher hallucination propensity. In contrast, clear intention grounding and answerability align with lower hallucination rates. Others, including domain specificity, show mixed, dataset- and model-dependent effects. Thus, these findings establish an empirically observable query-feature representation correlated with hallucination risk, paving the way for guided query rewriting and future intervention studies.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u67e5\u8be2\u8bed\u53e5\u7684\u7279\u5b9a\u8bed\u8a00\u7279\u5f81\uff08\u5982\u4ece\u53e5\u5d4c\u5957\u3001\u6307\u4ee3\u6a21\u7cca\uff09\u4e0eLLM\u5e7b\u89c9\u98ce\u9669\u76f8\u5173\uff0c\u800c\u610f\u56fe\u660e\u786e\u3001\u53ef\u56de\u7b54\u6027\u5f3a\u7684\u67e5\u8be2\u5219\u964d\u4f4e\u5e7b\u89c9\u98ce\u9669\u3002", "motivation": "\u4f20\u7edf\u4e0aLLM\u5e7b\u89c9\u88ab\u5f52\u56e0\u4e8e\u6a21\u578b\u6216\u89e3\u7801\u7b56\u7565\u7684\u7f3a\u9677\uff0c\u4f46\u7814\u7a76\u8ba4\u4e3a\u67e5\u8be2\u8bed\u53e5\u7684\u5f62\u5f0f\u4e5f\u4f1a\u5f71\u54cd\u6a21\u578b\u54cd\u5e94\u3002\u57fa\u4e8e\u8bed\u8a00\u5b66\u7406\u8bba\uff0c\u63a2\u7d22\u662f\u5426\u5b58\u5728\u67d0\u4e9b\u7c7b\u578b\u7684\u67e5\u8be2\u66f4\u5bb9\u6613\u5bfc\u81f4\u5e7b\u89c9\u3002", "method": "\u6784\u5efa22\u7ef4\u67e5\u8be2\u7279\u5f81\u5411\u91cf\uff0c\u6db5\u76d6\u4ece\u53e5\u590d\u6742\u6027\u3001\u8bcd\u6c47\u7a00\u6709\u6027\u3001\u6307\u4ee3\u3001\u5426\u5b9a\u3001\u53ef\u56de\u7b54\u6027\u3001\u610f\u56fe\u57fa\u7840\u7b49\u8bed\u8a00\u5b66\u7279\u5f81\u3002\u4f7f\u7528369,837\u4e2a\u771f\u5b9e\u4e16\u754c\u67e5\u8be2\u8fdb\u884c\u5927\u89c4\u6a21\u5206\u6790\uff0c\u8bc6\u522b\u4e0e\u5e7b\u89c9\u98ce\u9669\u76f8\u5173\u7684\u67e5\u8be2\u7279\u5f81\u6a21\u5f0f\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\"\u98ce\u9669\u666f\u89c2\"\uff1a\u67d0\u4e9b\u7279\u5f81\uff08\u5982\u6df1\u5c42\u4ece\u53e5\u5d4c\u5957\u3001\u6307\u4ee3\u6a21\u7cca\uff09\u4e0e\u66f4\u9ad8\u7684\u5e7b\u89c9\u503e\u5411\u76f8\u5173\uff1b\u800c\u6e05\u6670\u7684\u610f\u56fe\u57fa\u7840\u548c\u53ef\u56de\u7b54\u6027\u5219\u4e0e\u8f83\u4f4e\u7684\u5e7b\u89c9\u7387\u76f8\u5173\u3002\u9886\u57df\u7279\u5f02\u6027\u7b49\u7279\u5f81\u5219\u8868\u73b0\u51fa\u6df7\u5408\u3001\u6570\u636e\u96c6\u548c\u6a21\u578b\u4f9d\u8d56\u7684\u6548\u5e94\u3002", "conclusion": "\u7814\u7a76\u786e\u7acb\u4e86\u4e0e\u5e7b\u89c9\u98ce\u9669\u76f8\u5173\u7684\u53ef\u89c2\u5bdf\u67e5\u8be2\u7279\u5f81\u8868\u793a\uff0c\u4e3a\u5f15\u5bfc\u6027\u67e5\u8be2\u91cd\u5199\u548c\u672a\u6765\u5e72\u9884\u7814\u7a76\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u8868\u660e\u67e5\u8be2\u5f62\u5f0f\u662f\u5f71\u54cdLLM\u5e7b\u89c9\u7684\u91cd\u8981\u56e0\u7d20\u3002"}}
{"id": "2602.21009", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21009", "abs": "https://arxiv.org/abs/2602.21009", "authors": ["Kun Yuan", "Junyu Bi", "Daixuan Cheng", "Changfa Wu", "Shuwen Xiao", "Binbin Cao", "Jian Wu", "Yuning Jiang"], "title": "HiSAC: Hierarchical Sparse Activation Compression for Ultra-long Sequence Modeling in Recommenders", "comment": null, "summary": "Modern recommender systems leverage ultra-long user behavior sequences to capture dynamic preferences, but end-to-end modeling is infeasible in production due to latency and memory constraints. While summarizing history via interest centers offers a practical alternative, existing methods struggle to (1) identify user-specific centers at appropriate granularity and (2) accurately assign behaviors, leading to quantization errors and loss of long-tail preferences. To alleviate these issues, we propose Hierarchical Sparse Activation Compression (HiSAC), an efficient framework for personalized sequence modeling. HiSAC encodes interactions into multi-level semantic IDs and constructs a global hierarchical codebook. A hierarchical voting mechanism sparsely activates personalized interest-agents as fine-grained preference centers. Guided by these agents, Soft-Routing Attention aggregates historical signals in semantic space, weighting by similarity to minimize quantization error and retain long-tail behaviors. Deployed on Taobao's \"Guess What You Like\" homepage, HiSAC achieves significant compression and cost reduction, with online A/B tests showing a consistent 1.65% CTR uplift -- demonstrating its scalability and real-world effectiveness.", "AI": {"tldr": "HiSAC\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u7a00\u758f\u6fc0\u6d3b\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7ea7\u8bed\u4e49ID\u548c\u5168\u5c40\u5206\u5c42\u7801\u672c\uff0c\u7a00\u758f\u6fc0\u6d3b\u4e2a\u6027\u5316\u5174\u8da3\u4ee3\u7406\u4f5c\u4e3a\u7ec6\u7c92\u5ea6\u504f\u597d\u4e2d\u5fc3\uff0c\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u805a\u5408\u5386\u53f2\u4fe1\u53f7\u4ee5\u51cf\u5c11\u91cf\u5316\u8bef\u5dee\u5e76\u4fdd\u7559\u957f\u5c3e\u884c\u4e3a\u3002", "motivation": "\u73b0\u4ee3\u63a8\u8350\u7cfb\u7edf\u5229\u7528\u8d85\u957f\u7528\u6237\u884c\u4e3a\u5e8f\u5217\u6355\u6349\u52a8\u6001\u504f\u597d\uff0c\u4f46\u7aef\u5230\u7aef\u5efa\u6a21\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u5b58\u5728\u5ef6\u8fdf\u548c\u5185\u5b58\u9650\u5236\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u8bc6\u522b\u7528\u6237\u7279\u5b9a\u4e2d\u5fc3\u7c92\u5ea6\u3001\u51c6\u786e\u5206\u914d\u884c\u4e3a\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5bfc\u81f4\u91cf\u5316\u8bef\u5dee\u548c\u957f\u5c3e\u504f\u597d\u4e22\u5931\u3002", "method": "HiSAC\u5c06\u4ea4\u4e92\u7f16\u7801\u4e3a\u591a\u7ea7\u8bed\u4e49ID\uff0c\u6784\u5efa\u5168\u5c40\u5206\u5c42\u7801\u672c\uff0c\u901a\u8fc7\u5206\u5c42\u6295\u7968\u673a\u5236\u7a00\u758f\u6fc0\u6d3b\u4e2a\u6027\u5316\u5174\u8da3\u4ee3\u7406\u4f5c\u4e3a\u7ec6\u7c92\u5ea6\u504f\u597d\u4e2d\u5fc3\uff0c\u4f7f\u7528Soft-Routing Attention\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u805a\u5408\u5386\u53f2\u4fe1\u53f7\uff0c\u6839\u636e\u76f8\u4f3c\u5ea6\u52a0\u6743\u4ee5\u6700\u5c0f\u5316\u91cf\u5316\u8bef\u5dee\u3002", "result": "\u5728\u6dd8\u5b9d\"\u731c\u4f60\u559c\u6b22\"\u9996\u9875\u90e8\u7f72\u4e2d\uff0cHiSAC\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u538b\u7f29\u548c\u6210\u672c\u964d\u4f4e\uff0c\u5728\u7ebfA/B\u6d4b\u8bd5\u663e\u793aCTR\u6301\u7eed\u63d0\u53471.65%\uff0c\u8bc1\u660e\u4e86\u5176\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u9645\u6709\u6548\u6027\u3002", "conclusion": "HiSAC\u901a\u8fc7\u5206\u5c42\u7a00\u758f\u6fc0\u6d3b\u538b\u7f29\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8d85\u957f\u7528\u6237\u884c\u4e3a\u5e8f\u5217\u5efa\u6a21\u7684\u6548\u7387\u548c\u7cbe\u5ea6\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u63a8\u8350\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u548c\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2602.20332", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20332", "abs": "https://arxiv.org/abs/2602.20332", "authors": ["Nicole Cho", "William Watson", "Alec Koppel", "Sumitra Ganesh", "Manuela Veloso"], "title": "No One Size Fits All: QueryBandits for Hallucination Mitigation", "comment": null, "summary": "Advanced reasoning capabilities in Large Language Models (LLMs) have led to more frequent hallucinations; yet most mitigation work focuses on open-source models for post-hoc detection and parameter editing. The dearth of studies focusing on hallucinations in closed-source models is especially concerning, as they constitute the vast majority of models in institutional deployments. We introduce QueryBandits, a model-agnostic contextual bandit framework that adaptively learns online to select the optimal query-rewrite strategy by leveraging an empirically validated and calibrated reward function. Across 16 QA scenarios, our top QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a No-Rewrite baseline and outperforms zero-shot static policies (e.g., Paraphrase or Expand) by 42.6% and 60.3%, respectively. Moreover, all contextual bandits outperform vanilla bandits across all datasets, with higher feature variance coinciding with greater variance in arm selection. This substantiates our finding that there is no single rewrite policy optimal for all queries. We also discover that certain static policies incur higher cumulative regret than No-Rewrite, indicating that an inflexible query-rewriting policy can worsen hallucinations. Thus, learning an online policy over semantic features with QueryBandits can shift model behavior purely through forward-pass mechanisms, enabling its use with closed-source models and bypassing the need for retraining or gradient-based adaptation.", "AI": {"tldr": "QueryBandits\uff1a\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u4e0a\u4e0b\u6587\u8001\u864e\u673a\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u9009\u62e9\u6700\u4f18\u67e5\u8be2\u91cd\u5199\u7b56\u7565\u6765\u7f13\u89e3\u95ed\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u7f13\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u7684\u7814\u7a76\u90fd\u96c6\u4e2d\u5728\u5f00\u6e90\u6a21\u578b\u4e0a\uff0c\u91c7\u7528\u4e8b\u540e\u68c0\u6d4b\u548c\u53c2\u6570\u7f16\u8f91\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u673a\u6784\u90e8\u7f72\u4e2d\u7edd\u5927\u591a\u6570\u662f\u95ed\u6e90\u6a21\u578b\uff0c\u9488\u5bf9\u95ed\u6e90\u6a21\u578b\u5e7b\u89c9\u7f13\u89e3\u7684\u7814\u7a76\u4e25\u91cd\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u68af\u5ea6\u8c03\u6574\u7684\u7eaf\u524d\u5411\u673a\u5236\u3002", "method": "\u63d0\u51faQueryBandits\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e0a\u4e0b\u6587\u8001\u864e\u673a\u7684\u6a21\u578b\u65e0\u5173\u65b9\u6cd5\u3002\u5b83\u901a\u8fc7\u7ecf\u9a8c\u9a8c\u8bc1\u548c\u6821\u51c6\u7684\u5956\u52b1\u51fd\u6570\uff0c\u5728\u7ebf\u5b66\u4e60\u9009\u62e9\u6700\u4f18\u67e5\u8be2\u91cd\u5199\u7b56\u7565\uff08\u5982\u6539\u5199\u6216\u6269\u5c55\uff09\u3002\u6846\u67b6\u4f7f\u7528Thompson Sampling\u7b49\u7b97\u6cd5\uff0c\u6839\u636e\u8bed\u4e49\u7279\u5f81\u52a8\u6001\u8c03\u6574\u7b56\u7565\u9009\u62e9\u3002", "result": "\u572816\u4e2aQA\u573a\u666f\u4e2d\uff0c\u6700\u4f73QueryBandit\uff08Thompson Sampling\uff09\u76f8\u6bd4No-Rewrite\u57fa\u7ebf\u83b7\u5f9787.5%\u7684\u80dc\u7387\uff0c\u5206\u522b\u6bd4\u96f6\u6837\u672c\u9759\u6001\u7b56\u7565\uff08\u5982Paraphrase\u6216Expand\uff09\u9ad8\u51fa42.6%\u548c60.3%\u3002\u6240\u6709\u4e0a\u4e0b\u6587\u8001\u864e\u673a\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u90fd\u4f18\u4e8e\u666e\u901a\u8001\u864e\u673a\uff0c\u7279\u5f81\u65b9\u5dee\u8d8a\u5927\uff0c\u81c2\u9009\u62e9\u65b9\u5dee\u4e5f\u8d8a\u5927\uff0c\u8bc1\u5b9e\u4e86\u6ca1\u6709\u5355\u4e00\u91cd\u5199\u7b56\u7565\u5bf9\u6240\u6709\u67e5\u8be2\u90fd\u6700\u4f18\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u4e0d\u7075\u6d3b\u7684\u67e5\u8be2\u91cd\u5199\u7b56\u7565\u53cd\u800c\u53ef\u80fd\u52a0\u5267\u5e7b\u89c9\u3002\u901a\u8fc7QueryBandits\u5728\u7ebf\u5b66\u4e60\u8bed\u4e49\u7279\u5f81\u4e0a\u7684\u7b56\u7565\uff0c\u53ef\u4ee5\u7eaf\u901a\u8fc7\u524d\u5411\u673a\u5236\u6539\u53d8\u6a21\u578b\u884c\u4e3a\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u95ed\u6e90\u6a21\u578b\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u57fa\u4e8e\u68af\u5ea6\u7684\u8c03\u6574\u3002"}}
{"id": "2602.21052", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21052", "abs": "https://arxiv.org/abs/2602.21052", "authors": ["Timur Nabiev", "Evgeny Frolov"], "title": "Position-Aware Sequential Attention for Accurate Next Item Recommendations", "comment": null, "summary": "Sequential self-attention models usually rely on additive positional embeddings, which inject positional information into item representations at the input. In the absence of positional signals, the attention block is permutation-equivariant over sequence positions and thus has no intrinsic notion of temporal order beyond causal masking. We argue that additive positional embeddings make the attention mechanism only superficially sensitive to sequence order: positional information is entangled with item embedding semantics, propagates weakly in deep architectures, and limits the ability to capture rich sequential patterns. To address these limitations, we introduce a kernelized self-attention mechanism, where a learnable positional kernel operates purely in the position space, disentangled from semantic similarity, and directly modulates attention weights. When applied per attention block, this kernel enables adaptive multi-scale sequential modeling. Experiments on standard next-item prediction benchmarks show that our positional kernel attention consistently improves over strong competing baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f4d\u7f6e\u6838\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u89e3\u8026\u4f4d\u7f6e\u4fe1\u606f\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u76f4\u63a5\u5728\u4f4d\u7f6e\u7a7a\u95f4\u4e0a\u8c03\u8282\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u6539\u5584\u4e86\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u52a0\u6027\u4f4d\u7f6e\u5d4c\u5165\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u4f4d\u7f6e\u4fe1\u606f\u4e0e\u9879\u76ee\u8bed\u4e49\u4fe1\u606f\u7ea0\u7f20\u5728\u4e00\u8d77\uff0c\u5728\u6df1\u5c42\u67b6\u6784\u4e2d\u4f20\u64ad\u8f83\u5f31\uff0c\u9650\u5236\u4e86\u6355\u83b7\u4e30\u5bcc\u5e8f\u5217\u6a21\u5f0f\u7684\u80fd\u529b\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u6838\u5316\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u4f4d\u7f6e\u6838\u5728\u7eaf\u4f4d\u7f6e\u7a7a\u95f4\u64cd\u4f5c\uff0c\u4e0e\u8bed\u4e49\u76f8\u4f3c\u6027\u89e3\u8026\uff0c\u76f4\u63a5\u8c03\u8282\u6ce8\u610f\u529b\u6743\u91cd\u3002\u5728\u6bcf\u4e2a\u6ce8\u610f\u529b\u5757\u4e2d\u5e94\u7528\u8be5\u6838\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u591a\u5c3a\u5ea6\u5e8f\u5217\u5efa\u6a21\u3002", "result": "\u5728\u6807\u51c6\u7684\u4e0b\u4e00\u4e2a\u9879\u76ee\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f4d\u7f6e\u6838\u6ce8\u610f\u529b\u673a\u5236\u6301\u7eed\u4f18\u4e8e\u5f3a\u5927\u7684\u7ade\u4e89\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u4f4d\u7f6e\u6838\u6ce8\u610f\u529b\u673a\u5236\u901a\u8fc7\u89e3\u8026\u4f4d\u7f6e\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u76f4\u63a5\u8c03\u8282\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u6355\u83b7\u5e8f\u5217\u6a21\u5f0f\uff0c\u5728\u5e8f\u5217\u5efa\u6a21\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.20336", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20336", "abs": "https://arxiv.org/abs/2602.20336", "authors": ["Radoslaw Roszczyk", "Pawel Tecza", "Maciej Stodolski", "Krzysztof Siwek"], "title": "Natural Language Processing Models for Robust Document Categorization", "comment": "13 pages, 1 fiure, 5 tables", "summary": "This article presents an evaluation of several machine learning methods applied to automated text classification, alongside the design of a demonstrative system for unbalanced document categorization and distribution. The study focuses on balancing classification accuracy with computational efficiency, a key consideration when integrating AI into real world automation pipelines. Three models of varying complexity were examined: a Naive Bayes classifier, a bidirectional LSTM network, and a fine tuned transformer based BERT model.\n  The experiments reveal substantial differences in performance. BERT achieved the highest accuracy, consistently exceeding 99\\%, but required significantly longer training times and greater computational resources. The BiLSTM model provided a strong compromise, reaching approximately 98.56\\% accuracy while maintaining moderate training costs and offering robust contextual understanding. Naive Bayes proved to be the fastest to train, on the order of milliseconds, yet delivered the lowest accuracy, averaging around 94.5\\%. Class imbalance influenced all methods, particularly in the recognition of minority categories.\n  A fully functional demonstrative system was implemented to validate practical applicability, enabling automated routing of technical requests with throughput unattainable through manual processing. The study concludes that BiLSTM offers the most balanced solution for the examined scenario, while also outlining opportunities for future improvements and further exploration of transformer architectures.", "AI": {"tldr": "\u8bc4\u4f30\u4e86\u4e09\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u81ea\u52a8\u6587\u672c\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e0d\u5e73\u8861\u6587\u6863\u5206\u7c7b\u6f14\u793a\u7cfb\u7edf\uff0c\u53d1\u73b0BiLSTM\u5728\u51c6\u786e\u7387\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u63d0\u4f9b\u4e86\u6700\u4f73\u5e73\u8861\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u5c06AI\u96c6\u6210\u5230\u5b9e\u9645\u81ea\u52a8\u5316\u6d41\u7a0b\u4e2d\u65f6\uff0c\u5e73\u8861\u5206\u7c7b\u51c6\u786e\u7387\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u7279\u522b\u5173\u6ce8\u5904\u7406\u4e0d\u5e73\u8861\u6587\u6863\u5206\u7c7b\u7684\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u3002", "method": "\u8bc4\u4f30\u4e86\u4e09\u79cd\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u6a21\u578b\uff1a\u6734\u7d20\u8d1d\u53f6\u65af\u5206\u7c7b\u5668\u3001\u53cc\u5411LSTM\u7f51\u7edc\u548c\u5fae\u8c03\u7684\u57fa\u4e8eTransformer\u7684BERT\u6a21\u578b\u3002\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u6f14\u793a\u7cfb\u7edf\uff0c\u7528\u4e8e\u9a8c\u8bc1\u5b9e\u9645\u5e94\u7528\u6027\u3002", "result": "BERT\u51c6\u786e\u7387\u6700\u9ad8\uff08\u8d85\u8fc799%\uff09\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u6700\u9ad8\uff1b\u6734\u7d20\u8d1d\u53f6\u65af\u8bad\u7ec3\u6700\u5feb\uff08\u6beb\u79d2\u7ea7\uff09\uff0c\u4f46\u51c6\u786e\u7387\u6700\u4f4e\uff08\u7ea694.5%\uff09\uff1bBiLSTM\u63d0\u4f9b\u6700\u4f73\u5e73\u8861\uff08\u7ea698.56%\u51c6\u786e\u7387\uff0c\u4e2d\u7b49\u8ba1\u7b97\u6210\u672c\uff09\u3002\u7c7b\u522b\u4e0d\u5e73\u8861\u5bf9\u6240\u6709\u65b9\u6cd5\u90fd\u6709\u5f71\u54cd\u3002", "conclusion": "\u5bf9\u4e8e\u6240\u7814\u7a76\u7684\u573a\u666f\uff0cBiLSTM\u63d0\u4f9b\u4e86\u6700\u5e73\u8861\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u4e3a\u672a\u6765\u6539\u8fdb\u548c\u8fdb\u4e00\u6b65\u63a2\u7d22Transformer\u67b6\u6784\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002"}}
{"id": "2602.21099", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.21099", "abs": "https://arxiv.org/abs/2602.21099", "authors": ["Junjie Meng", "Ranxu zhang", "Wei Wu", "Rui Zhang", "Chuan Qin", "Qi Zhang", "Qi Liu", "Hui Xiong", "Chao Wang"], "title": "Turning Semantics into Topology: LLM-Driven Attribute Augmentation for Collaborative Filtering", "comment": null, "summary": "Large Language Models (LLMs) have shown great potential for enhancing recommender systems through their extensive world knowledge and reasoning capabilities. However, effectively translating these semantic signals into traditional collaborative embeddings remains an open challenge. Existing approaches typically fall into two extremes: direct inference methods are computationally prohibitive for large-scale retrieval, while embedding-based methods primarily focus on unilateral feature augmentation rather than holistic collaborative signal enhancement. To bridge this gap, we propose Topology-Augmented Graph Collaborative Filtering (TAGCF), a novel framework that transforms semantic knowledge into topological connectivity. Unlike existing approaches that depend on textual features or direct interaction synthesis, TAGCF employs LLMs to infer interaction intents and underlying causal relationships from user-item pairs, representing these insights as intermediate attribute nodes within an enriched User-Attribute-Item (U-A-I) graph. Furthermore, to effectively model the heterogeneous relations in this augmented structure, we propose Adaptive Relation-weighted Graph Convolution (ARGC), which employs relation-specific prediction networks to dynamically estimate the importance of each relation type. Extensive experiments across multiple benchmark datasets and CF backbones demonstrate consistent improvements, with comprehensive evaluations including cold-start scenarios validating the effectiveness and robustness of our framework. All code will be made publicly available. For anonymous review, our code is available at the following anonymous link: https://anonymous.4open.science/r/AGCF-2441353190/.", "AI": {"tldr": "TAGCF\u6846\u67b6\u5c06LLM\u7684\u8bed\u4e49\u77e5\u8bc6\u8f6c\u5316\u4e3a\u56fe\u62d3\u6251\u8fde\u63a5\uff0c\u901a\u8fc7\u7528\u6237-\u5c5e\u6027-\u7269\u54c1\u56fe\u589e\u5f3a\u534f\u540c\u8fc7\u6ee4\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff08\u76f4\u63a5\u63a8\u7406\uff09\uff0c\u8981\u4e48\u53ea\u5173\u6ce8\u5355\u8fb9\u7279\u5f81\u589e\u5f3a\u800c\u975e\u6574\u4f53\u534f\u540c\u4fe1\u53f7\u589e\u5f3a\u3002\u9700\u8981\u4e00\u79cd\u6709\u6548\u5c06LLM\u8bed\u4e49\u4fe1\u53f7\u8f6c\u5316\u4e3a\u4f20\u7edf\u534f\u540c\u5d4c\u5165\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faTAGCF\u6846\u67b6\uff1a1) \u4f7f\u7528LLM\u4ece\u7528\u6237-\u7269\u54c1\u5bf9\u63a8\u65ad\u4ea4\u4e92\u610f\u56fe\u548c\u56e0\u679c\u5173\u7cfb\uff0c\u8868\u793a\u4e3a\u7528\u6237-\u5c5e\u6027-\u7269\u54c1\u56fe\u4e2d\u7684\u4e2d\u95f4\u5c5e\u6027\u8282\u70b9\uff1b2) \u63d0\u51fa\u81ea\u9002\u5e94\u5173\u7cfb\u6743\u91cd\u56fe\u5377\u79ef\uff08ARGC\uff09\uff0c\u4f7f\u7528\u5173\u7cfb\u7279\u5b9a\u9884\u6d4b\u7f51\u7edc\u52a8\u6001\u4f30\u8ba1\u6bcf\u79cd\u5173\u7cfb\u7c7b\u578b\u7684\u91cd\u8981\u6027\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548cCF\u9aa8\u5e72\u7f51\u7edc\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u663e\u793a\u51fa\u4e00\u81f4\u7684\u6539\u8fdb\uff0c\u5305\u62ec\u51b7\u542f\u52a8\u573a\u666f\u7684\u5168\u9762\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "TAGCF\u901a\u8fc7\u5c06\u8bed\u4e49\u77e5\u8bc6\u8f6c\u5316\u4e3a\u62d3\u6251\u8fde\u63a5\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u534f\u540c\u8fc7\u6ee4\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u4e49\u4fe1\u53f7\u8f6c\u5316\u65b9\u9762\u7684\u6311\u6218\u3002"}}
{"id": "2602.20372", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20372", "abs": "https://arxiv.org/abs/2602.20372", "authors": ["Chundra Cathcart", "Arne Rubehn", "Katja Bocklage", "Luca Ciucci", "Kellen Parker van Dam", "Al\u017eb\u011bta Ku\u010derov\u00e1", "Jekaterina Ma\u017eara", "Carlo Y. Meloni", "David Snee", "Johann-Mattis List"], "title": "How communicatively optimal are exact numeral systems? Once more on lexicon size and morphosyntactic complexity", "comment": null, "summary": "Recent research argues that exact recursive numeral systems optimize communicative efficiency by balancing a tradeoff between the size of the numeral lexicon and the average morphosyntactic complexity (roughly length in morphemes) of numeral terms. We argue that previous studies have not characterized the data in a fashion that accounts for the degree of complexity languages display. Using data from 52 genetically diverse languages and an annotation scheme distinguishing between predictable and unpredictable allomorphy (formal variation), we show that many of the world's languages are decisively less efficient than one would expect. We discuss the implications of our findings for the study of numeral systems and linguistic evolution more generally.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53d1\u73b0\u8bb8\u591a\u8bed\u8a00\u7684\u6570\u5b57\u7cfb\u7edf\u5728\u6c9f\u901a\u6548\u7387\u4e0a\u4f4e\u4e8e\u9884\u671f\uff0c\u6311\u6218\u4e86\u5148\u524d\u5173\u4e8e\u9012\u5f52\u6570\u5b57\u7cfb\u7edf\u4f18\u5316\u6548\u7387\u7684\u89c2\u70b9\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u8ba4\u4e3a\u7cbe\u786e\u9012\u5f52\u6570\u5b57\u7cfb\u7edf\u901a\u8fc7\u5e73\u8861\u6570\u5b57\u8bcd\u6c47\u5e93\u5927\u5c0f\u548c\u6570\u5b57\u672f\u8bed\u7684\u5e73\u5747\u5f62\u6001\u53e5\u6cd5\u590d\u6742\u6027\u6765\u4f18\u5316\u6c9f\u901a\u6548\u7387\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u4e9b\u7814\u7a76\u672a\u80fd\u5145\u5206\u89e3\u91ca\u8bed\u8a00\u6240\u5c55\u73b0\u7684\u590d\u6742\u7a0b\u5ea6\u3002", "method": "\u4f7f\u7528\u6765\u81ea52\u79cd\u9057\u4f20\u591a\u6837\u8bed\u8a00\u7684\u6570\u636e\uff0c\u91c7\u7528\u533a\u5206\u53ef\u9884\u6d4b\u548c\u4e0d\u53ef\u9884\u6d4b\u8bcd\u5f62\u53d8\u5316\uff08\u5f62\u5f0f\u53d8\u5f02\uff09\u7684\u6ce8\u91ca\u65b9\u6848\u8fdb\u884c\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e16\u754c\u4e0a\u8bb8\u591a\u8bed\u8a00\u7684\u6570\u5b57\u7cfb\u7edf\u6548\u7387\u660e\u663e\u4f4e\u4e8e\u9884\u671f\u6c34\u5e73\uff0c\u8868\u660e\u8fd9\u4e9b\u7cfb\u7edf\u5e76\u975e\u5982\u5148\u524d\u7814\u7a76\u6240\u8ba4\u4e3a\u7684\u90a3\u6837\u9ad8\u6548\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5bf9\u6570\u5b57\u7cfb\u7edf\u7814\u7a76\u548c\u66f4\u5e7f\u6cdb\u7684\u8bed\u8a00\u6f14\u5316\u7814\u7a76\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u6311\u6218\u4e86\u5173\u4e8e\u8bed\u8a00\u7cfb\u7edf\u4f18\u5316\u6548\u7387\u7684\u73b0\u6709\u5047\u8bbe\u3002"}}
{"id": "2602.21202", "categories": ["cs.IR", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21202", "abs": "https://arxiv.org/abs/2602.21202", "authors": ["Hanxiang Qin", "Alexander Martin", "Rohan Jha", "Chunsheng Zuo", "Reno Kriz", "Benjamin Van Durme"], "title": "Multi-Vector Index Compression in Any Modality", "comment": "12 pages, 4 figures", "summary": "We study efficient multi-vector retrieval for late interaction in any modality. Late interaction has emerged as a dominant paradigm for information retrieval in text, images, visual documents, and videos, but its computation and storage costs grow linearly with document length, making it costly for image-, video-, and audio-rich corpora. To address this limitation, we explore query-agnostic methods for compressing multi-vector document representations under a constant vector budget. We introduce four approaches for index compression: sequence resizing, memory tokens, hierarchical pooling, and a novel attention-guided clustering (AGC). AGC uses an attention-guided mechanism to identify the most semantically salient regions of a document as cluster centroids and to weight token aggregation. Evaluating these methods on retrieval tasks spanning text (BEIR), visual-document (ViDoRe), and video (MSR-VTT, MultiVENT 2.0), we show that attention-guided clustering consistently outperforms other parameterized compression methods (sequence resizing and memory tokens), provides greater flexibility in index size than non-parametric hierarchical clustering, and achieves competitive or improved performance compared to a full, uncompressed index. The source code is available at: github.com/hanxiangqin/omni-col-press.", "AI": {"tldr": "\u63d0\u51fa\u56db\u79cd\u7528\u4e8e\u591a\u5411\u91cf\u68c0\u7d22\u7684\u7d22\u5f15\u538b\u7f29\u65b9\u6cd5\uff0c\u5176\u4e2d\u6ce8\u610f\u529b\u5f15\u5bfc\u805a\u7c7b(AGC)\u5728\u6587\u672c\u3001\u89c6\u89c9\u6587\u6863\u548c\u89c6\u9891\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73", "motivation": "\u665a\u671f\u4ea4\u4e92\u8303\u5f0f\u5728\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u89c9\u6587\u6863\u548c\u89c6\u9891\u68c0\u7d22\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4f46\u5176\u8ba1\u7b97\u548c\u5b58\u50a8\u6210\u672c\u968f\u6587\u6863\u957f\u5ea6\u7ebf\u6027\u589e\u957f\uff0c\u5bf9\u4e8e\u56fe\u50cf\u3001\u89c6\u9891\u3001\u97f3\u9891\u4e30\u5bcc\u7684\u8bed\u6599\u5e93\u6210\u672c\u8fc7\u9ad8", "method": "\u63d0\u51fa\u4e86\u56db\u79cd\u67e5\u8be2\u65e0\u5173\u7684\u591a\u5411\u91cf\u6587\u6863\u8868\u793a\u538b\u7f29\u65b9\u6cd5\uff1a\u5e8f\u5217\u8c03\u6574\u3001\u8bb0\u5fc6\u6807\u8bb0\u3001\u5c42\u6b21\u6c60\u5316\uff0c\u4ee5\u53ca\u65b0\u9896\u7684\u6ce8\u610f\u529b\u5f15\u5bfc\u805a\u7c7b(AGC)\u3002AGC\u4f7f\u7528\u6ce8\u610f\u529b\u5f15\u5bfc\u673a\u5236\u8bc6\u522b\u6587\u6863\u4e2d\u6700\u5177\u8bed\u4e49\u663e\u8457\u6027\u7684\u533a\u57df\u4f5c\u4e3a\u805a\u7c7b\u4e2d\u5fc3\uff0c\u5e76\u52a0\u6743\u6807\u8bb0\u805a\u5408", "result": "\u5728\u6587\u672c(BEIR)\u3001\u89c6\u89c9\u6587\u6863(ViDoRe)\u548c\u89c6\u9891(MSR-VTT, MultiVENT 2.0)\u68c0\u7d22\u4efb\u52a1\u4e2d\uff0c\u6ce8\u610f\u529b\u5f15\u5bfc\u805a\u7c7b\u4e00\u81f4\u4f18\u4e8e\u5176\u4ed6\u53c2\u6570\u5316\u538b\u7f29\u65b9\u6cd5\uff0c\u6bd4\u975e\u53c2\u6570\u5c42\u6b21\u805a\u7c7b\u63d0\u4f9b\u66f4\u5927\u7684\u7d22\u5f15\u5927\u5c0f\u7075\u6d3b\u6027\uff0c\u4e14\u4e0e\u5b8c\u6574\u672a\u538b\u7f29\u7d22\u5f15\u76f8\u6bd4\u8fbe\u5230\u7ade\u4e89\u6027\u6216\u6539\u8fdb\u7684\u6027\u80fd", "conclusion": "\u6ce8\u610f\u529b\u5f15\u5bfc\u805a\u7c7b(AGC)\u662f\u4e00\u79cd\u6709\u6548\u7684\u591a\u5411\u91cf\u68c0\u7d22\u538b\u7f29\u65b9\u6cd5\uff0c\u5728\u5404\u79cd\u6a21\u6001\u7684\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u5904\u7406\u591a\u6a21\u6001\u4e30\u5bcc\u8bed\u6599\u5e93\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.20379", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20379", "abs": "https://arxiv.org/abs/2602.20379", "authors": ["Mukul Chhabra", "Luigi Medrano", "Arush Verma"], "title": "Case-Aware LLM-as-a-Judge Evaluation for Enterprise-Scale RAG Systems", "comment": "12 pages including appendix, 6 figures", "summary": "Enterprise Retrieval-Augmented Generation (RAG) assistants operate in multi-turn, case-based workflows such as technical support and IT operations, where evaluation must reflect operational constraints, structured identifiers (e.g., error codes, versions), and resolution workflows. Existing RAG evaluation frameworks are primarily designed for benchmark-style or single-turn settings and often fail to capture enterprise-specific failure modes such as case misidentification, workflow misalignment, and partial resolution across turns.\n  We present a case-aware LLM-as-a-Judge evaluation framework for enterprise multi-turn RAG systems. The framework evaluates each turn using eight operationally grounded metrics that separate retrieval quality, grounding fidelity, answer utility, precision integrity, and case/workflow alignment. A severity-aware scoring protocol reduces score inflation and improves diagnostic clarity across heterogeneous enterprise cases. The system uses deterministic prompting with strict JSON outputs, enabling scalable batch evaluation, regression testing, and production monitoring.\n  Through a comparative study of two instruction-tuned models across short and long workflows, we show that generic proxy metrics provide ambiguous signals, while the proposed framework exposes enterprise-critical tradeoffs that are actionable for system improvement.", "AI": {"tldr": "\u63d0\u51fa\u9762\u5411\u4f01\u4e1a\u591a\u8f6eRAG\u7cfb\u7edf\u7684\u6848\u4f8b\u611f\u77e5\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u516b\u4e2a\u64cd\u4f5c\u6307\u6807\u548c\u4e25\u91cd\u6027\u611f\u77e5\u8bc4\u5206\u534f\u8bae\uff0c\u89e3\u51b3\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5728\u4f01\u4e1a\u573a\u666f\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f01\u4e1aRAG\u52a9\u624b\u5728\u591a\u8f6e\u6848\u4f8b\u5de5\u4f5c\u6d41\uff08\u5982\u6280\u672f\u652f\u6301\u3001IT\u8fd0\u7ef4\uff09\u4e2d\u8fd0\u884c\uff0c\u73b0\u6709RAG\u8bc4\u4f30\u6846\u67b6\u4e3b\u8981\u4e3a\u57fa\u51c6\u6d4b\u8bd5\u6216\u5355\u8f6e\u573a\u666f\u8bbe\u8ba1\uff0c\u65e0\u6cd5\u6355\u6349\u4f01\u4e1a\u7279\u6709\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5982\u6848\u4f8b\u8bc6\u522b\u9519\u8bef\u3001\u5de5\u4f5c\u6d41\u9519\u4f4d\u3001\u8de8\u8f6e\u6b21\u90e8\u5206\u89e3\u51b3\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u6848\u4f8b\u611f\u77e5\u7684LLM-as-a-Judge\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u7528\u516b\u4e2a\u57fa\u4e8e\u64cd\u4f5c\u7684\u6307\u6807\u5206\u522b\u8bc4\u4f30\u68c0\u7d22\u8d28\u91cf\u3001\u57fa\u7840\u4fdd\u771f\u5ea6\u3001\u7b54\u6848\u6548\u7528\u3001\u7cbe\u5ea6\u5b8c\u6574\u6027\u4ee5\u53ca\u6848\u4f8b/\u5de5\u4f5c\u6d41\u5bf9\u9f50\u3002\u91c7\u7528\u4e25\u91cd\u6027\u611f\u77e5\u8bc4\u5206\u534f\u8bae\u51cf\u5c11\u5206\u6570\u81a8\u80c0\uff0c\u901a\u8fc7\u786e\u5b9a\u6027\u63d0\u793a\u548c\u4e25\u683cJSON\u8f93\u51fa\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u6279\u91cf\u8bc4\u4f30\u3001\u56de\u5f52\u6d4b\u8bd5\u548c\u751f\u4ea7\u76d1\u63a7\u3002", "result": "\u901a\u8fc7\u5bf9\u6bd4\u4e24\u4e2a\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u5728\u77ed\u5de5\u4f5c\u6d41\u548c\u957f\u5de5\u4f5c\u6d41\u4e2d\u7684\u8868\u73b0\uff0c\u7814\u7a76\u53d1\u73b0\u901a\u7528\u4ee3\u7406\u6307\u6807\u63d0\u4f9b\u6a21\u7cca\u4fe1\u53f7\uff0c\u800c\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u63ed\u793a\u5bf9\u4f01\u4e1a\u5173\u952e\u7684\u53ef\u64cd\u4f5c\u6743\u8861\uff0c\u6709\u52a9\u4e8e\u7cfb\u7edf\u6539\u8fdb\u3002", "conclusion": "\u8be5\u6848\u4f8b\u611f\u77e5\u8bc4\u4f30\u6846\u67b6\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u4f01\u4e1a\u591a\u8f6eRAG\u7cfb\u7edf\uff0c\u89e3\u51b3\u4f01\u4e1a\u7279\u6709\u7684\u8bc4\u4f30\u9700\u6c42\uff0c\u4e3a\u7cfb\u7edf\u4f18\u5316\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u6d1e\u5bdf\uff0c\u4f18\u4e8e\u73b0\u6709\u901a\u7528\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2602.21103", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.21103", "abs": "https://arxiv.org/abs/2602.21103", "authors": ["Sanket Badhe", "Deep Shah"], "title": "Prompt-Level Distillation: A Non-Parametric Alternative to Model Fine-Tuning for Efficient Reasoning", "comment": null, "summary": "Advanced reasoning typically requires Chain-of-Thought prompting, which is accurate but incurs prohibitive latency and substantial test-time inference costs. The standard alternative, fine-tuning smaller models, often sacrifices interpretability while introducing significant resource and operational overhead. To address these limitations, we introduce Prompt-Level Distillation (PLD). We extract explicit reasoning patterns from a Teacher model and organize them into a structured list of expressive instructions for the Student model's System Prompt. Evaluated on the StereoSet and Contract-NLI datasets using Gemma-3 4B, PLD improved Macro F1 scores from 57\\% to 90.0\\% and 67\\% to 83\\% respectively, enabling this compact model to match frontier performance with negligible latency overhead. These expressive instructions render the decision-making process transparent, allowing for full human verification of logic, making this approach ideal for regulated industries such as law, finance, and content moderation, as well as high-volume use cases and edge devices.", "AI": {"tldr": "\u901a\u8fc7Prompt-Level Distillation\u6280\u672f\uff0c\u4ece\u5c0f\u578b\u6a21\u578b\u4e2d\u63d0\u53d6\u663e\u5f0f\u63a8\u7406\u6a21\u5f0f\u5e76\u7ec4\u7ec7\u6210\u7ed3\u6784\u5316\u6307\u4ee4\uff0c\u4f7f\u5c0f\u578b\u6a21\u578b\u5728\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u7684\u540c\u65f6\u8fbe\u5230\u524d\u6cbf\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u5b8c\u5168\u53ef\u9a8c\u8bc1\u7684\u900f\u660e\u63a8\u7406\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u660e\u663e\u7f3a\u9677\uff1aChain-of-Thought\u63d0\u793a\u51c6\u786e\u4f46\u5ef6\u8fdf\u9ad8\u3001\u63a8\u7406\u6210\u672c\u5927\uff1b\u5fae\u8c03\u5c0f\u6a21\u578b\u5219\u727a\u7272\u53ef\u89e3\u91ca\u6027\u4e14\u5f15\u5165\u8d44\u6e90\u5f00\u9500\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u6027\u80fd\u53c8\u5177\u6709\u4f4e\u5ef6\u8fdf\u3001\u53ef\u89e3\u91ca\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faPrompt-Level Distillation\uff08PLD\uff09\u65b9\u6cd5\uff1a\u4ece\u6559\u5e08\u6a21\u578b\u4e2d\u63d0\u53d6\u663e\u5f0f\u63a8\u7406\u6a21\u5f0f\uff0c\u5c06\u5176\u7ec4\u7ec7\u6210\u7ed3\u6784\u5316\u3001\u8868\u8fbe\u6027\u5f3a\u7684\u6307\u4ee4\uff0c\u5e76\u4f5c\u4e3a\u5b66\u751f\u6a21\u578b\u7684\u7cfb\u7edf\u63d0\u793a\u3002\u8be5\u65b9\u6cd5\u4fdd\u7559\u4e86\u63a8\u7406\u7684\u900f\u660e\u5ea6\uff0c\u540c\u65f6\u907f\u514d\u4e86\u4f20\u7edf\u5fae\u8c03\u7684\u8d44\u6e90\u5f00\u9500\u3002", "result": "\u5728StereoSet\u548cContract-NLI\u6570\u636e\u96c6\u4e0a\u4f7f\u7528Gemma-3 4B\u6a21\u578b\u6d4b\u8bd5\uff0cPLD\u5c06Macro F1\u5206\u6570\u5206\u522b\u4ece57%\u63d0\u5347\u523090.0%\u548c\u4ece67%\u63d0\u5347\u523083%\u3002\u8be5\u7d27\u51d1\u6a21\u578b\u5728\u4fdd\u6301\u53ef\u5ffd\u7565\u5ef6\u8fdf\u5f00\u9500\u7684\u540c\u65f6\uff0c\u8fbe\u5230\u4e86\u524d\u6cbf\u6a21\u578b\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "PLD\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd-\u5ef6\u8fdf-\u53ef\u89e3\u91ca\u6027\u6743\u8861\u95ee\u9898\uff0c\u4f7f\u5c0f\u578b\u6a21\u578b\u80fd\u591f\u4ee5\u900f\u660e\u3001\u53ef\u9a8c\u8bc1\u7684\u65b9\u5f0f\u8fbe\u5230\u524d\u6cbf\u6027\u80fd\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6cd5\u5f8b\u3001\u91d1\u878d\u3001\u5185\u5bb9\u5ba1\u6838\u7b49\u53d7\u76d1\u7ba1\u884c\u4e1a\uff0c\u4ee5\u53ca\u9ad8\u541e\u5410\u91cf\u7528\u4f8b\u548c\u8fb9\u7f18\u8bbe\u5907\u3002"}}
{"id": "2602.20433", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20433", "abs": "https://arxiv.org/abs/2602.20433", "authors": ["Atharva Kulkarni", "Jacob Mitchell Springer", "Arjun Subramonian", "Swabha Swayamdipta"], "title": "Disentangling Geometry, Performance, and Training in Language Models", "comment": null, "summary": "Geometric properties of Transformer weights, particularly the unembedding matrix, have been widely useful in language model interpretability research. Yet, their utility for estimating downstream performance remains unclear. In this work, we systematically investigate the relationship between model performance and the unembedding matrix geometry, particularly its effective rank. Our experiments, involving a suite of 108 OLMo-style language models trained under controlled variation, reveal several key findings. While the best-performing models often exhibit a high effective rank, this trend is not universal across tasks and training setups. Contrary to prior work, we find that low effective rank does not cause late-stage performance degradation in small models, but instead co-occurs with it; we find adversarial cases where low-rank models do not exhibit saturation. Moreover, we show that effective rank is strongly influenced by pre-training hyperparameters, such as batch size and weight decay, which in-turn affect the model's performance. Lastly, extending our analysis to other geometric metrics and final-layer representation, we find that these metrics are largely aligned, but none can reliably predict downstream performance. Overall, our findings suggest that the model's geometry, as captured by existing metrics, primarily reflects training choices rather than performance.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0Transformer\u6743\u91cd\u51e0\u4f55\u7279\u6027\uff08\u7279\u522b\u662funembedding\u77e9\u9635\u7684\u6709\u6548\u79e9\uff09\u4e0e\u6a21\u578b\u6027\u80fd\u7684\u5173\u7cfb\u590d\u6742\uff0c\u4e0d\u80fd\u53ef\u9760\u9884\u6d4b\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\uff0c\u4e3b\u8981\u53cd\u6620\u8bad\u7ec3\u9009\u62e9\u800c\u975e\u6027\u80fd", "motivation": "Transformer\u6743\u91cd\u7684\u51e0\u4f55\u7279\u6027\uff08\u7279\u522b\u662funembedding\u77e9\u9635\uff09\u5728\u8bed\u8a00\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u4e2d\u5f88\u6709\u7528\uff0c\u4f46\u5176\u5bf9\u4e0b\u6e38\u6027\u80fd\u9884\u6d4b\u7684\u6548\u7528\u5c1a\u4e0d\u6e05\u695a\u3002\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u63a2\u7a76\u6a21\u578b\u6027\u80fd\u4e0eunembedding\u77e9\u9635\u51e0\u4f55\u7279\u6027\uff08\u7279\u522b\u662f\u6709\u6548\u79e9\uff09\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u4f7f\u7528108\u4e2aOLMo\u98ce\u683c\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u53d7\u63a7\u53d8\u5316\u6761\u4ef6\u4e0b\u8bad\u7ec3\u3002\u5206\u6790unembedding\u77e9\u9635\u7684\u6709\u6548\u79e9\uff0c\u5e76\u6269\u5c55\u5230\u5176\u4ed6\u51e0\u4f55\u6307\u6807\u548c\u6700\u7ec8\u5c42\u8868\u793a\uff0c\u7814\u7a76\u5b83\u4eec\u4e0e\u6a21\u578b\u6027\u80fd\u7684\u5173\u7cfb\u3002", "result": "1. \u6700\u4f73\u6027\u80fd\u6a21\u578b\u901a\u5e38\u5177\u6709\u8f83\u9ad8\u7684\u6709\u6548\u79e9\uff0c\u4f46\u8fd9\u4e00\u8d8b\u52bf\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u8bad\u7ec3\u8bbe\u7f6e\u4e2d\u5e76\u4e0d\u666e\u904d\uff1b2. \u4f4e\u6709\u6548\u79e9\u4e0d\u4f1a\u5bfc\u81f4\u5c0f\u578b\u6a21\u578b\u540e\u671f\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u662f\u4e0e\u5176\u540c\u65f6\u53d1\u751f\uff1b3. \u5b58\u5728\u4f4e\u79e9\u6a21\u578b\u4e0d\u8868\u73b0\u51fa\u9971\u548c\u7684\u5bf9\u6297\u6027\u6848\u4f8b\uff1b4. \u6709\u6548\u79e9\u53d7\u9884\u8bad\u7ec3\u8d85\u53c2\u6570\uff08\u5982\u6279\u91cf\u5927\u5c0f\u548c\u6743\u91cd\u8870\u51cf\uff09\u5f3a\u70c8\u5f71\u54cd\uff0c\u8fdb\u800c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff1b5. \u5176\u4ed6\u51e0\u4f55\u6307\u6807\u4e0e\u6709\u6548\u79e9\u57fa\u672c\u4e00\u81f4\uff0c\u4f46\u90fd\u4e0d\u80fd\u53ef\u9760\u9884\u6d4b\u4e0b\u6e38\u6027\u80fd\u3002", "conclusion": "\u6a21\u578b\u51e0\u4f55\u7279\u6027\uff08\u901a\u8fc7\u73b0\u6709\u6307\u6807\u6355\u83b7\uff09\u4e3b\u8981\u53cd\u6620\u8bad\u7ec3\u9009\u62e9\u800c\u975e\u6027\u80fd\uff0c\u4e0d\u80fd\u53ef\u9760\u5730\u7528\u4e8e\u9884\u6d4b\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u3002"}}
{"id": "2602.20513", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20513", "abs": "https://arxiv.org/abs/2602.20513", "authors": ["Gavin Levinson", "Keith Feldman"], "title": "From Performance to Purpose: A Sociotechnical Taxonomy for Evaluating Large Language Model Utility", "comment": null, "summary": "As large language models (LLMs) continue to improve at completing discrete tasks, they are being integrated into increasingly complex and diverse real-world systems. However, task-level success alone does not establish a model's fit for use in practice. In applied, high-stakes settings, LLM effectiveness is driven by a wider array of sociotechnical determinants that extend beyond conventional performance measures. Although a growing set of metrics capture many of these considerations, they are rarely organized in a way that supports consistent evaluation, leaving no unified taxonomy for assessing and comparing LLM utility across use cases. To address this gap, we introduce the Language Model Utility Taxonomy (LUX), a comprehensive framework that structures utility evaluation across four domains: performance, interaction, operations, and governance. Within each domain, LUX is organized hierarchically into thematically aligned dimensions and components, each grounded in metrics that enable quantitative comparison and alignment of model selection with intended use. In addition, an external dynamic web tool is provided to support exploration of the framework by connecting each component to a repository of relevant metrics (factors) for applied evaluation.", "AI": {"tldr": "\u63d0\u51faLUX\u6846\u67b6\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5b9e\u7528\u6027\u7684\u7efc\u5408\u5206\u7c7b\u6cd5\uff0c\u6db5\u76d6\u6027\u80fd\u3001\u4ea4\u4e92\u3001\u8fd0\u8425\u548c\u6cbb\u7406\u56db\u4e2a\u9886\u57df\uff0c\u652f\u6301\u5b9a\u91cf\u6bd4\u8f83\u548c\u6a21\u578b\u9009\u62e9\u3002", "motivation": "\u5f53\u524dLLM\u5728\u73b0\u5b9e\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u8d8a\u6765\u8d8a\u5e7f\u6cdb\uff0c\u4f46\u4ec5\u9760\u4efb\u52a1\u7ea7\u6210\u529f\u4e0d\u8db3\u4ee5\u8bc4\u4f30\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9002\u7528\u6027\u3002\u9ad8\u98ce\u9669\u5e94\u7528\u73af\u5883\u4e2d\uff0cLLM\u7684\u6709\u6548\u6027\u53d7\u591a\u79cd\u793e\u4f1a\u6280\u672f\u56e0\u7d20\u5f71\u54cd\uff0c\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u5206\u6563\u4e14\u7f3a\u4e4f\u7edf\u4e00\u5206\u7c7b\uff0c\u65e0\u6cd5\u652f\u6301\u8de8\u7528\u4f8b\u7684\u4e00\u81f4\u6027\u8bc4\u4f30\u548c\u6bd4\u8f83\u3002", "method": "\u5f15\u5165\u8bed\u8a00\u6a21\u578b\u5b9e\u7528\u6027\u5206\u7c7b\u6cd5(LUX)\uff0c\u5c06\u5b9e\u7528\u6027\u8bc4\u4f30\u7ed3\u6784\u5316\u5230\u56db\u4e2a\u9886\u57df\uff1a\u6027\u80fd\u3001\u4ea4\u4e92\u3001\u8fd0\u8425\u548c\u6cbb\u7406\u3002\u6bcf\u4e2a\u9886\u57df\u6309\u5c42\u6b21\u7ec4\u7ec7\u4e3a\u4e3b\u9898\u5bf9\u9f50\u7684\u7ef4\u5ea6\u548c\u7ec4\u4ef6\uff0c\u6bcf\u4e2a\u7ec4\u4ef6\u90fd\u57fa\u4e8e\u53ef\u91cf\u5316\u6bd4\u8f83\u7684\u6307\u6807\u3002\u8fd8\u5f00\u53d1\u4e86\u5916\u90e8\u52a8\u6001\u7f51\u7edc\u5de5\u5177\uff0c\u8fde\u63a5\u7ec4\u4ef6\u5230\u76f8\u5173\u6307\u6807\u5e93\u3002", "result": "LUX\u6846\u67b6\u63d0\u4f9b\u4e86\u8bc4\u4f30LLM\u5b9e\u7528\u6027\u7684\u7edf\u4e00\u5206\u7c7b\u6cd5\uff0c\u652f\u6301\u8de8\u7528\u4f8b\u7684\u5b9a\u91cf\u6bd4\u8f83\u548c\u6a21\u578b\u9009\u62e9\u5bf9\u9f50\u3002\u7f51\u7edc\u5de5\u5177\u4fbf\u4e8e\u63a2\u7d22\u6846\u67b6\u5e76\u8bbf\u95ee\u76f8\u5173\u8bc4\u4f30\u6307\u6807\u3002", "conclusion": "LUX\u586b\u8865\u4e86LLM\u5b9e\u7528\u6027\u8bc4\u4f30\u7f3a\u4e4f\u7edf\u4e00\u5206\u7c7b\u6cd5\u7684\u7a7a\u767d\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6a21\u578b\u8bc4\u4f30\u548c\u9009\u62e9\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u5728\u590d\u6742\u73b0\u5b9e\u7cfb\u7edf\u4e2d\u505a\u51fa\u66f4\u660e\u667a\u7684\u51b3\u7b56\u3002"}}
{"id": "2602.20528", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20528", "abs": "https://arxiv.org/abs/2602.20528", "authors": ["Justin Lovelace", "Christian Belardi", "Sofian Zalouk", "Adhitya Polavaram", "Srivatsa Kundurthy", "Kilian Q. Weinberger"], "title": "Stop-Think-AutoRegress: Language Modeling with Latent Diffusion Planning", "comment": "COLM 2025", "summary": "The Stop-Think-AutoRegress Language Diffusion Model (STAR-LDM) integrates latent diffusion planning with autoregressive generation. Unlike conventional autoregressive language models limited to token-by-token decisions, STAR-LDM incorporates a \"thinking\" phase that pauses generation to refine a semantic plan through diffusion before continuing. This enables global planning in continuous space prior to committing to discrete tokens. Evaluations show STAR-LDM significantly outperforms similar-sized models on language understanding benchmarks and achieves $>70\\%$ win rates in LLM-as-judge comparisons for narrative coherence and commonsense reasoning. The architecture also allows straightforward control through lightweight classifiers, enabling fine-grained steering of attributes without model retraining while maintaining better fluency-control trade-offs than specialized approaches.", "AI": {"tldr": "STAR-LDM\u7ed3\u5408\u6f5c\u5728\u6269\u6563\u89c4\u5212\u4e0e\u81ea\u56de\u5f52\u751f\u6210\uff0c\u901a\u8fc7\"\u601d\u8003\"\u9636\u6bb5\u5728\u8fde\u7eed\u7a7a\u95f4\u8fdb\u884c\u5168\u5c40\u89c4\u5212\uff0c\u518d\u751f\u6210\u79bb\u6563token\uff0c\u5728\u8bed\u8a00\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u53d7\u9650\u4e8etoken-by-token\u51b3\u7b56\uff0c\u7f3a\u4e4f\u5168\u5c40\u89c4\u5212\u80fd\u529b\uff0c\u65e0\u6cd5\u5728\u8fde\u7eed\u8bed\u4e49\u7a7a\u95f4\u8fdb\u884c\u601d\u8003\uff0c\u5bfc\u81f4\u751f\u6210\u5185\u5bb9\u53ef\u80fd\u7f3a\u4e4f\u8fde\u8d2f\u6027\u548c\u903b\u8f91\u6027\u3002", "method": "\u5f15\u5165\"\u601d\u8003\"\u9636\u6bb5\u6682\u505c\u751f\u6210\uff0c\u901a\u8fc7\u6269\u6563\u8fc7\u7a0b\u5728\u6f5c\u5728\u7a7a\u95f4\u7cbe\u5316\u8bed\u4e49\u8ba1\u5212\uff0c\u7136\u540e\u5c06\u8be5\u8ba1\u5212\u7528\u4e8e\u6307\u5bfc\u540e\u7eed\u7684\u81ea\u56de\u5f52\u751f\u6210\uff0c\u5b9e\u73b0\u5168\u5c40\u89c4\u5212\u4e0e\u5c40\u90e8\u751f\u6210\u76f8\u7ed3\u5408\u3002", "result": "\u5728\u8bed\u8a00\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u76f8\u4f3c\u89c4\u6a21\u6a21\u578b\uff0c\u5728LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u6bd4\u8f83\u4e2d\uff0c\u53d9\u4e8b\u8fde\u8d2f\u6027\u548c\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u4e0a\u83b7\u5f97>70%\u80dc\u7387\uff0c\u4e14\u53ef\u901a\u8fc7\u8f7b\u91cf\u5206\u7c7b\u5668\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002", "conclusion": "STAR-LDM\u6210\u529f\u6574\u5408\u4e86\u6269\u6563\u89c4\u5212\u548c\u81ea\u56de\u5f52\u751f\u6210\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5168\u5c40\u89c4\u5212\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u8d28\u91cf\u548c\u53ef\u63a7\u6027\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u67b6\u6784\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.20580", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20580", "abs": "https://arxiv.org/abs/2602.20580", "authors": ["Nishant Subramani", "Kshitish Ghate", "Mona Diab"], "title": "Personal Information Parroting in Language Models", "comment": "EACL Findings 2026", "summary": "Modern language models (LM) are trained on large scrapes of the Web, containing millions of personal information (PI) instances, many of which LMs memorize, increasing privacy risks. In this work, we develop the regexes and rules (R&R) detector suite to detect email addresses, phone numbers, and IP addresses, which outperforms the best regex-based PI detectors. On a manually curated set of 483 instances of PI, we measure memorization: finding that 13.6% are parroted verbatim by the Pythia-6.9b model, i.e., when the model is prompted with the tokens that precede the PI in the original document, greedy decoding generates the entire PI span exactly. We expand this analysis to study models of varying sizes (160M-6.9B) and pretraining time steps (70k-143k iterations) in the Pythia model suite and find that both model size and amount of pretraining are positively correlated with memorization. Even the smallest model, Pythia-160m, parrots 2.7% of the instances exactly. Consequently, we strongly recommend that pretraining datasets be aggressively filtered and anonymized to minimize PI parroting.", "AI": {"tldr": "\u5f00\u53d1\u4e86R&R\u68c0\u6d4b\u5668\u5957\u4ef6\u6765\u8bc6\u522b\u4e2a\u4eba\u4fe1\u606f\uff0c\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u4f1a\u8bb0\u5fc6\u5e76\u590d\u73b0\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u4e2a\u4eba\u4fe1\u606f\uff0c\u4e14\u6a21\u578b\u89c4\u6a21\u548c\u9884\u8bad\u7ec3\u65f6\u95f4\u4e0e\u8bb0\u5fc6\u7a0b\u5ea6\u6b63\u76f8\u5173\u3002", "motivation": "\u73b0\u4ee3\u8bed\u8a00\u6a21\u578b\u5728\u5927\u89c4\u6a21\u7f51\u7edc\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5305\u542b\u5927\u91cf\u4e2a\u4eba\u4fe1\u606f\u5b9e\u4f8b\uff0c\u8fd9\u4e9b\u4fe1\u606f\u53ef\u80fd\u88ab\u6a21\u578b\u8bb0\u5fc6\uff0c\u589e\u52a0\u4e86\u9690\u79c1\u98ce\u9669\u3002\u9700\u8981\u91cf\u5316\u8fd9\u79cd\u8bb0\u5fc6\u73b0\u8c61\u5e76\u7814\u7a76\u5176\u5f71\u54cd\u56e0\u7d20\u3002", "method": "\u5f00\u53d1\u4e86R&R\u68c0\u6d4b\u5668\u5957\u4ef6\u6765\u68c0\u6d4b\u7535\u5b50\u90ae\u4ef6\u5730\u5740\u3001\u7535\u8bdd\u53f7\u7801\u548cIP\u5730\u5740\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u6b63\u5219\u8868\u8fbe\u5f0f\u7684PI\u68c0\u6d4b\u5668\u3002\u5728\u624b\u52a8\u6574\u7406\u7684483\u4e2aPI\u5b9e\u4f8b\u4e0a\uff0c\u6d4b\u91cf\u4e86Pythia\u6a21\u578b\u5957\u4ef6\uff08160M-6.9B\uff09\u5728\u4e0d\u540c\u9884\u8bad\u7ec3\u6b65\u6570\uff0870k-143k\u8fed\u4ee3\uff09\u4e0b\u7684\u8bb0\u5fc6\u60c5\u51b5\u3002", "result": "Pythia-6.9b\u6a21\u578b\u9010\u5b57\u590d\u73b0\u4e8613.6%\u7684PI\u5b9e\u4f8b\u3002\u6a21\u578b\u89c4\u6a21\u548c\u9884\u8bad\u7ec3\u65f6\u95f4\u4e0e\u8bb0\u5fc6\u7a0b\u5ea6\u5448\u6b63\u76f8\u5173\uff1a\u5373\u4f7f\u6700\u5c0f\u7684Pythia-160m\u6a21\u578b\u4e5f\u590d\u73b0\u4e862.7%\u7684\u5b9e\u4f8b\u3002", "conclusion": "\u5f3a\u70c8\u5efa\u8bae\u5bf9\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u8fdb\u884c\u79ef\u6781\u8fc7\u6ee4\u548c\u533f\u540d\u5316\u5904\u7406\uff0c\u4ee5\u6700\u5c0f\u5316\u4e2a\u4eba\u4fe1\u606f\u88ab\u6a21\u578b\u590d\u73b0\u7684\u98ce\u9669\u3002"}}
{"id": "2602.20634", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20634", "abs": "https://arxiv.org/abs/2602.20634", "authors": ["Saurabh Mishra", "Shivani Thakur", "Radhika Mamidi"], "title": "Enhancing Hate Speech Detection on Social Media: A Comparative Analysis of Machine Learning Models and Text Transformation Approaches", "comment": "32 pages, 24 figures", "summary": "The proliferation of hate speech on social media platforms has necessitated the development of effective detection and moderation tools. This study evaluates the efficacy of various machine learning models in identifying hate speech and offensive language and investigates the potential of text transformation techniques to neutralize such content. We compare traditional models like CNNs and LSTMs with advanced neural network models such as BERT and its derivatives, alongside exploring hybrid models that combine different architectural features. Our results indicate that while advanced models like BERT show superior accuracy due to their deep contextual understanding, hybrid models exhibit improved capabilities in certain scenarios. Furthermore, we introduce innovative text transformation approaches that convert negative expressions into neutral ones, thereby potentially mitigating the impact of harmful content. The implications of these findings are discussed, highlighting the strengths and limitations of current technologies and proposing future directions for more robust hate speech detection systems.", "AI": {"tldr": "\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u4e2d\u7684\u6548\u679c\uff0c\u5e76\u63a2\u7d22\u6587\u672c\u8f6c\u6362\u6280\u672f\u6765\u4e2d\u6027\u5316\u6709\u5bb3\u5185\u5bb9", "motivation": "\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u4e0a\u4ec7\u6068\u8a00\u8bba\u7684\u6cdb\u6ee5\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u68c0\u6d4b\u548c\u8c03\u8282\u5de5\u5177", "method": "\u6bd4\u8f83\u4f20\u7edf\u6a21\u578b\uff08CNN\u3001LSTM\uff09\u4e0e\u5148\u8fdb\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff08BERT\u53ca\u5176\u53d8\u4f53\uff09\uff0c\u63a2\u7d22\u7ed3\u5408\u4e0d\u540c\u67b6\u6784\u7279\u5f81\u7684\u6df7\u5408\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u521b\u65b0\u7684\u6587\u672c\u8f6c\u6362\u6280\u672f", "result": "BERT\u7b49\u5148\u8fdb\u6a21\u578b\u56e0\u6df1\u5ea6\u4e0a\u4e0b\u6587\u7406\u89e3\u800c\u8868\u73b0\u51fa\u66f4\u9ad8\u51c6\u786e\u6027\uff0c\u6df7\u5408\u6a21\u578b\u5728\u67d0\u4e9b\u573a\u666f\u4e2d\u5c55\u73b0\u6539\u8fdb\u80fd\u529b\uff0c\u6587\u672c\u8f6c\u6362\u6280\u672f\u80fd\u6709\u6548\u5c06\u8d1f\u9762\u8868\u8fbe\u8f6c\u6362\u4e3a\u4e2d\u6027\u8868\u8fbe", "conclusion": "\u8ba8\u8bba\u4e86\u5f53\u524d\u6280\u672f\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u6784\u5efa\u66f4\u5f3a\u5927\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u7cfb\u7edf\u7684\u65b9\u5411"}}
{"id": "2602.20647", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20647", "abs": "https://arxiv.org/abs/2602.20647", "authors": ["W. Frederick Zimmerman"], "title": "Semantic Novelty at Scale: Narrative Shape Taxonomy and Readership Prediction in 28,606 Books", "comment": "six figures. dataset available at Hugging Face", "summary": "I introduce semantic novelty--cosine distance between each paragraph's sentence embedding and the running centroid of all preceding paragraphs--as an information-theoretic measure of narrative structure at corpus scale. Applying it to 28,606 books in PG19 (pre-1920 English literature), I compute paragraph-level novelty curves using 768-dimensional SBERT embeddings, then reduce each to a 16-segment Piecewise Aggregate Approximation (PAA). Ward-linkage clustering on PAA vectors reveals eight canonical narrative shape archetypes, from Steep Descent (rapid convergence) to Steep Ascent (escalating unpredictability). Volume--variance of the novelty trajectory--is the strongest length-independent predictor of readership (partial rho = 0.32), followed by speed (rho = 0.19) and Terminal/Initial ratio (rho = 0.19). Circuitousness shows strong raw correlation (rho = 0.41) but is 93 percent correlated with length; after control, partial rho drops to 0.11--demonstrating that naive correlations in corpus studies can be dominated by length confounds. Genre strongly constrains narrative shape (chi squared = 2121.6, p < 10 to the power negative 242), with fiction maintaining plateau profiles while nonfiction front-loads information. Historical analysis shows books became progressively more predictable between 1840 and 1910 (T/I ratio trend r = negative 0.74, p = 0.037). SAX analysis reveals 85 percent signature uniqueness, suggesting each book traces a nearly unique path through semantic space. These findings demonstrate that information-density dynamics, distinct from sentiment or topic, constitute a fundamental dimension of narrative structure with measurable consequences for reader engagement. Dataset: https://huggingface.co/datasets/wfzimmerman/pg19-semantic-novelty", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u8bed\u4e49\u65b0\u9896\u6027\u4f5c\u4e3a\u53d9\u4e8b\u7ed3\u6784\u7684\u4fe1\u606f\u8bba\u5ea6\u91cf\uff0c\u5e94\u7528\u4e8e28,606\u672c\u524d1920\u5e74\u82f1\u6587\u4e66\u7c4d\uff0c\u53d1\u73b0\u516b\u79cd\u53d9\u4e8b\u5f62\u6001\u539f\u578b\uff0c\u5176\u4e2d\u65b0\u9896\u6027\u8f68\u8ff9\u7684\u6ce2\u52a8\u6027\u662f\u8bfb\u8005\u53c2\u4e0e\u5ea6\u7684\u6700\u5f3a\u9884\u6d4b\u56e0\u5b50\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u5f00\u53d1\u4e00\u79cd\u5728\u8bed\u6599\u5e93\u5c3a\u5ea6\u4e0a\u91cf\u5316\u53d9\u4e8b\u7ed3\u6784\u7684\u4fe1\u606f\u8bba\u65b9\u6cd5\u3002\u4f20\u7edf\u7814\u7a76\u591a\u5173\u6ce8\u60c5\u611f\u6216\u4e3b\u9898\uff0c\u800c\u4fe1\u606f\u5bc6\u5ea6\u52a8\u6001\u53d8\u5316\u8fd9\u4e00\u57fa\u672c\u7ef4\u5ea6\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u8005\u5e0c\u671b\u4e86\u89e3\u53d9\u4e8b\u4e2d\u7684\u8bed\u4e49\u65b0\u9896\u6027\u6a21\u5f0f\u5982\u4f55\u5f71\u54cd\u8bfb\u8005\u53c2\u4e0e\u5ea6\uff0c\u4ee5\u53ca\u4e0d\u540c\u4f53\u88c1\u548c\u5386\u53f2\u65f6\u671f\u662f\u5426\u5b58\u5728\u7cfb\u7edf\u6027\u5dee\u5f02\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u5b9a\u4e49\u8bed\u4e49\u65b0\u9896\u6027\u4e3a\u6bcf\u4e2a\u6bb5\u843d\u7684\u53e5\u5b50\u5d4c\u5165\u4e0e\u524d\u9762\u6240\u6709\u6bb5\u843d\u8fd0\u884c\u8d28\u5fc3\u7684\u4f59\u5f26\u8ddd\u79bb\uff1b2\uff09\u4f7f\u7528768\u7ef4SBERT\u5d4c\u5165\u8ba1\u7b97PG19\u6570\u636e\u96c6\u4e2d28,606\u672c\u4e66\u7684\u6bb5\u843d\u7ea7\u65b0\u9896\u6027\u66f2\u7ebf\uff1b3\uff09\u901a\u8fc716\u6bb5\u5206\u6bb5\u805a\u5408\u8fd1\u4f3c\uff08PAA\uff09\u964d\u7ef4\uff1b4\uff09\u4f7f\u7528Ward-linkage\u805a\u7c7b\u8bc6\u522b\u53d9\u4e8b\u5f62\u6001\u539f\u578b\uff1b5\uff09\u5206\u6790\u4f53\u79ef\uff08\u65b9\u5dee\uff09\u3001\u901f\u5ea6\u3001\u7ec8\u59cb\u6bd4\u7b49\u6307\u6807\u4e0e\u8bfb\u8005\u53c2\u4e0e\u5ea6\u7684\u5173\u7cfb\uff1b6\uff09\u63a7\u5236\u957f\u5ea6\u6df7\u6dc6\u56e0\u7d20\uff1b7\uff09\u8fdb\u884c\u4f53\u88c1\u548c\u5386\u65f6\u5206\u6790\u3002", "result": "\u4e3b\u8981\u7ed3\u679c\uff1a1\uff09\u8bc6\u522b\u51fa\u516b\u79cd\u53d9\u4e8b\u5f62\u6001\u539f\u578b\uff0c\u4ece\u9661\u964d\uff08\u5feb\u901f\u6536\u655b\uff09\u5230\u9661\u5347\uff08\u4e0d\u53ef\u9884\u6d4b\u6027\u9012\u589e\uff09\uff1b2\uff09\u4f53\u79ef\uff08\u65b0\u9896\u6027\u8f68\u8ff9\u65b9\u5dee\uff09\u662f\u8bfb\u8005\u53c2\u4e0e\u5ea6\u7684\u6700\u5f3a\u957f\u5ea6\u65e0\u5173\u9884\u6d4b\u56e0\u5b50\uff08\u90e8\u5206rho=0.32\uff09\uff1b3\uff09\u4f53\u88c1\u5f3a\u70c8\u7ea6\u675f\u53d9\u4e8b\u5f62\u6001\uff08\u5361\u65b9=2121.6\uff0cp<10^{-242}\uff09\uff0c\u5c0f\u8bf4\u4fdd\u6301\u5e73\u53f0\u578b\uff0c\u975e\u5c0f\u8bf4\u524d\u8f7d\u4fe1\u606f\uff1b4\uff09\u5386\u53f2\u5206\u6790\u663e\u793a1840-1910\u5e74\u95f4\u4e66\u7c4d\u8d8a\u6765\u8d8a\u53ef\u9884\u6d4b\uff08\u7ec8\u59cb\u6bd4\u8d8b\u52bfr=-0.74\uff0cp=0.037\uff09\uff1b5\uff09SAX\u5206\u6790\u663e\u793a85%\u7b7e\u540d\u72ec\u7279\u6027\uff0c\u8868\u660e\u6bcf\u672c\u4e66\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u51e0\u4e4e\u6709\u552f\u4e00\u8def\u5f84\u3002", "conclusion": "\u7ed3\u8bba\u662f\u4fe1\u606f\u5bc6\u5ea6\u52a8\u6001\u6784\u6210\u4e86\u53d9\u4e8b\u7ed3\u6784\u7684\u57fa\u672c\u7ef4\u5ea6\uff0c\u5bf9\u8bfb\u8005\u53c2\u4e0e\u5ea6\u6709\u53ef\u6d4b\u91cf\u7684\u5f71\u54cd\u3002\u7814\u7a76\u5c55\u793a\u4e86\u8bed\u6599\u5e93\u7814\u7a76\u4e2d\u6734\u7d20\u76f8\u5173\u6027\u53ef\u80fd\u88ab\u957f\u5ea6\u6df7\u6dc6\u56e0\u7d20\u4e3b\u5bfc\uff0c\u5f3a\u8c03\u4e86\u63a7\u5236\u6df7\u6dc6\u7684\u91cd\u8981\u6027\u3002\u8bed\u4e49\u65b0\u9896\u6027\u4e3a\u7406\u89e3\u53d9\u4e8b\u7ed3\u6784\u548c\u8bfb\u8005\u53c2\u4e0e\u63d0\u4f9b\u4e86\u65b0\u7684\u91cf\u5316\u6846\u67b6\u3002"}}
{"id": "2602.20648", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20648", "abs": "https://arxiv.org/abs/2602.20648", "authors": ["Anqi Li", "Chenxiao Wang", "Yu Lu", "Renjun Xu", "Lizhi Ma", "Zhenzhong Lan"], "title": "CARE: An Explainable Computational Framework for Assessing Client-Perceived Therapeutic Alliance Using Large Language Models", "comment": "14 pages, 4 figures", "summary": "Client perceptions of the therapeutic alliance are critical for counseling effectiveness. Accurately capturing these perceptions remains challenging, as traditional post-session questionnaires are burdensome and often delayed, while existing computational approaches produce coarse scores, lack interpretable rationales, and fail to model holistic session context. We present CARE, an LLM-based framework to automatically predict multi-dimensional alliance scores and generate interpretable rationales from counseling transcripts. Built on the CounselingWAI dataset and enriched with 9,516 expert-curated rationales, CARE is fine-tuned using rationale-augmented supervision with the LLaMA-3.1-8B-Instruct backbone. Experiments show that CARE outperforms leading LLMs and substantially reduces the gap between counselor evaluations and client-perceived alliance, achieving over 70% higher Pearson correlation with client ratings. Rationale-augmented supervision further improves predictive accuracy. CARE also produces high-quality, contextually grounded rationales, validated by both automatic and human evaluations. Applied to real-world Chinese online counseling sessions, CARE uncovers common alliance-building challenges, illustrates how interaction patterns shape alliance development, and provides actionable insights, demonstrating its potential as an AI-assisted tool for supporting mental health care.", "AI": {"tldr": "CARE\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u5fc3\u7406\u54a8\u8be2\u5bf9\u8bdd\u4e2d\u81ea\u52a8\u9884\u6d4b\u591a\u7ef4\u8054\u76df\u8bc4\u5206\u5e76\u751f\u6210\u53ef\u89e3\u91ca\u7684\u7406\u7531\uff0c\u663e\u8457\u63d0\u5347\u4e0e\u5ba2\u6237\u611f\u77e5\u8054\u76df\u7684\u76f8\u5173\u6027\u3002", "motivation": "\u4f20\u7edf\u5fc3\u7406\u54a8\u8be2\u8054\u76df\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u540e\u95ee\u5377\u8d1f\u62c5\u91cd\u4e14\u5ef6\u8fdf\uff0c\u73b0\u6709\u8ba1\u7b97\u65b9\u6cd5\u8bc4\u5206\u7c97\u7cd9\u3001\u7f3a\u4e4f\u53ef\u89e3\u91ca\u7406\u7531\uff0c\u4e14\u65e0\u6cd5\u5efa\u6a21\u6574\u4f53\u4f1a\u8bdd\u4e0a\u4e0b\u6587\u3002", "method": "\u57fa\u4e8eCounselingWAI\u6570\u636e\u96c6\uff0c\u4f7f\u75289,516\u4e2a\u4e13\u5bb6\u6807\u6ce8\u7406\u7531\u589e\u5f3a\uff0c\u91c7\u7528LLaMA-3.1-8B-Instruct\u9aa8\u5e72\u6a21\u578b\uff0c\u901a\u8fc7\u7406\u7531\u589e\u5f3a\u76d1\u7763\u8fdb\u884c\u5fae\u8c03\u3002", "result": "CARE\u4f18\u4e8e\u4e3b\u6d41LLM\uff0c\u5c06\u54a8\u8be2\u5e08\u8bc4\u4f30\u4e0e\u5ba2\u6237\u611f\u77e5\u8054\u76df\u7684\u5dee\u8ddd\u5927\u5e45\u7f29\u5c0f\uff0c\u4e0e\u5ba2\u6237\u8bc4\u5206\u7684Pearson\u76f8\u5173\u6027\u63d0\u534770%\u4ee5\u4e0a\uff1b\u7406\u7531\u589e\u5f3a\u76d1\u7763\u8fdb\u4e00\u6b65\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\uff1b\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u60c5\u5883\u5316\u7684\u7406\u7531\u3002", "conclusion": "CARE\u5728\u771f\u5b9e\u4e2d\u6587\u5728\u7ebf\u5fc3\u7406\u54a8\u8be2\u4e2d\u8bc6\u522b\u8054\u76df\u5efa\u8bbe\u6311\u6218\uff0c\u5c55\u793a\u4e92\u52a8\u6a21\u5f0f\u5982\u4f55\u5f71\u54cd\u8054\u76df\u53d1\u5c55\uff0c\u63d0\u4f9b\u53ef\u884c\u89c1\u89e3\uff0c\u8bc1\u660e\u5176\u4f5c\u4e3aAI\u8f85\u52a9\u5de5\u5177\u652f\u6301\u5fc3\u7406\u5065\u5eb7\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.20670", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20670", "abs": "https://arxiv.org/abs/2602.20670", "authors": ["Zirui Zhu", "Hailun Xu", "Yang Luo", "Yong Liu", "Kanchan Sarkar", "Kun Xu", "Yang You"], "title": "CAMEL: Confidence-Gated Reflection for Reward Modeling", "comment": "Preprint. 13 pages", "summary": "Reward models play a fundamental role in aligning large language models with human preferences. Existing methods predominantly follow two paradigms: scalar discriminative preference models, which are efficient but lack interpretability, and generative judging models, which offer richer reasoning at the cost of higher computational overhead. We observe that the log-probability margin between verdict tokens strongly correlates with prediction correctness, providing a reliable proxy for instance difficulty without additional inference cost. Building on this insight, we propose CAMEL, a confidence-gated reflection framework that performs a lightweight single-token preference decision first and selectively invokes reflection only for low-confidence instances. To induce effective self-correction, we train the model via reinforcement learning with counterfactual prefix augmentation, which exposes the model to diverse initial verdicts and encourages genuine revision. Empirically, CAMEL achieves state-of-the-art performance on three widely used reward-model benchmarks with 82.9% average accuracy, surpassing the best prior model by 3.2% and outperforming 70B-parameter models using only 14B parameters, while establishing a strictly better accuracy-efficiency Pareto frontier.", "AI": {"tldr": "CAMEL\uff1a\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u95e8\u63a7\u7684\u53cd\u601d\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u4ee4\u724c\u504f\u597d\u51b3\u7b56\u548c\u9009\u62e9\u6027\u53cd\u601d\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u7684\u540c\u65f6\u63d0\u5347\u5956\u52b1\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u6709\u5956\u52b1\u6a21\u578b\u5b58\u5728\u6548\u7387\u4e0e\u53ef\u89e3\u91ca\u6027\u7684\u6743\u8861\uff1a\u6807\u91cf\u5224\u522b\u6a21\u578b\u6548\u7387\u9ad8\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u751f\u6210\u5f0f\u5224\u65ad\u6a21\u578b\u63d0\u4f9b\u4e30\u5bcc\u63a8\u7406\u4f46\u8ba1\u7b97\u5f00\u9500\u5927\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u6548\u53c8\u80fd\u63d0\u4f9b\u9ad8\u8d28\u91cf\u5224\u65ad\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCAMEL\u6846\u67b6\uff1a1\uff09\u5229\u7528log-probability margin\u4f5c\u4e3a\u7f6e\u4fe1\u5ea6\u4ee3\u7406\uff0c2\uff09\u5148\u8fdb\u884c\u8f7b\u91cf\u7ea7\u5355\u4ee4\u724c\u504f\u597d\u51b3\u7b56\uff0c3\uff09\u4ec5\u5bf9\u4f4e\u7f6e\u4fe1\u5ea6\u5b9e\u4f8b\u9009\u62e9\u6027\u8c03\u7528\u53cd\u601d\u673a\u5236\uff0c4\uff09\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4e0e\u53cd\u4e8b\u5b9e\u524d\u7f00\u589e\u5f3a\u8bad\u7ec3\u6a21\u578b\uff0c\u4fc3\u8fdb\u771f\u6b63\u7684\u81ea\u6211\u4fee\u6b63\u3002", "result": "\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u5956\u52b1\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523082.9%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u6bd4\u4e4b\u524d\u6700\u4f73\u6a21\u578b\u63d0\u53473.2%\uff0c\u4ec5\u752814B\u53c2\u6570\u5c31\u8d85\u8d8a\u4e8670B\u53c2\u6570\u6a21\u578b\uff0c\u5efa\u7acb\u4e86\u66f4\u597d\u7684\u51c6\u786e\u7387-\u6548\u7387Pareto\u524d\u6cbf\u3002", "conclusion": "CAMEL\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u95e8\u63a7\u673a\u5236\u5b9e\u73b0\u4e86\u6548\u7387\u4e0e\u6027\u80fd\u7684\u5e73\u8861\uff0c\u4e3a\u5956\u52b1\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u6548\u8ba1\u7b97\u53c8\u80fd\u8fdb\u884c\u6df1\u5165\u53cd\u601d\u7684\u5b9e\u7528\u6846\u67b6\uff0c\u5728\u53c2\u6570\u6548\u7387\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2602.20727", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20727", "abs": "https://arxiv.org/abs/2602.20727", "authors": ["Xindian Ma", "Rundong Kong", "Peng Zhang", "Ruoxiang Huang", "Yongyu Jiang"], "title": "ID-LoRA: Efficient Low-Rank Adaptation Inspired by Matrix Interpolative Decomposition", "comment": null, "summary": "LoRA has become a universal Parameter-Efficient Fine-Tuning (PEFT) technique that equips Large Language Models (LLMs) to adapt quickly to new tasks. However, when these models are scaled up, even the latest LoRA variants still introduce considerable overhead in trainable parameters. Conversely, aggressively lowering the rank to curb this overhead markedly degrades performance in complex multi-task settings. We propose ID-LoRA, a novel PEFT framework that breaks the trade-off. Its core innovation lies in extracting and reusing clustered parameter groups from the pretrained weight matrix. These groups are then used to form multiple low-rank components, all of which share only a single initialized trainable low-rank matrix. This approach cuts the number of trainable parameters while keeping the model's capacity intact. We evaluate ID-LoRA on five diverse benchmarks: Mathematical Reasoning, Code Generation, MMLU, CommonsenseQA, and Safety Alignment. ID-LoRA outperforms both full fine-tuning and existing PEFT baselines (e.g., LoRA, DoRA, HydraLoRA) while using up to 46% fewer trainable parameters than the standard LoRA. In multi-task scenarios, it surpasses LoRA and its recent variants (e.g., DoRA and HydraLoRA) on both Code and MMLU tasks, yet requires only 54% of the trainable parameters demanded by the conventional LoRA.", "AI": {"tldr": "ID-LoRA\uff1a\u4e00\u79cd\u65b0\u578b\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u9884\u8bad\u7ec3\u6743\u91cd\u77e9\u9635\u4e2d\u63d0\u53d6\u548c\u91cd\u7528\u805a\u7c7b\u53c2\u6570\u7ec4\uff0c\u5728\u663e\u8457\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u7684\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u80fd\u529b\uff0c\u6253\u7834\u4f20\u7edfLoRA\u7684\u6027\u80fd\u4e0e\u53c2\u6570\u6548\u7387\u6743\u8861\u3002", "motivation": "\u5f53\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u6269\u5927\u65f6\uff0c\u5373\u4f7f\u6700\u65b0\u7684LoRA\u53d8\u4f53\u4ecd\u4f1a\u5f15\u5165\u5927\u91cf\u53ef\u8bad\u7ec3\u53c2\u6570\u5f00\u9500\u3002\u800c\u4e3a\u4e86\u964d\u4f4e\u5f00\u9500\u6fc0\u8fdb\u5730\u51cf\u5c0f\u79e9\u4f1a\u663e\u8457\u964d\u4f4e\u590d\u6742\u591a\u4efb\u52a1\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u5b58\u5728\u6027\u80fd\u4e0e\u53c2\u6570\u6548\u7387\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "ID-LoRA\u4ece\u9884\u8bad\u7ec3\u6743\u91cd\u77e9\u9635\u4e2d\u63d0\u53d6\u548c\u91cd\u7528\u805a\u7c7b\u53c2\u6570\u7ec4\uff0c\u7528\u8fd9\u4e9b\u7ec4\u5f62\u6210\u591a\u4e2a\u4f4e\u79e9\u7ec4\u4ef6\uff0c\u6240\u6709\u7ec4\u4ef6\u5171\u4eab\u5355\u4e2a\u521d\u59cb\u5316\u7684\u53ef\u8bad\u7ec3\u4f4e\u79e9\u77e9\u9635\uff0c\u4ece\u800c\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u5bb9\u91cf\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u751f\u6210\u3001MMLU\u3001\u5e38\u8bc6\u95ee\u7b54\u548c\u5b89\u5168\u5bf9\u9f50\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cID-LoRA\u4f18\u4e8e\u5168\u91cf\u5fae\u8c03\u548c\u73b0\u6709PEFT\u57fa\u7ebf\uff0c\u540c\u65f6\u4f7f\u7528\u6bd4\u6807\u51c6LoRA\u5c1146%\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u3002\u5728\u591a\u4efb\u52a1\u573a\u666f\u4e2d\uff0c\u5728\u4ee3\u7801\u548cMMLU\u4efb\u52a1\u4e0a\u8d85\u8d8aLoRA\u53ca\u5176\u53d8\u4f53\uff0c\u4ec5\u9700\u4f20\u7edfLoRA 54%\u7684\u53c2\u6570\u3002", "conclusion": "ID-LoRA\u6253\u7834\u4e86PEFT\u4e2d\u6027\u80fd\u4e0e\u53c2\u6570\u6548\u7387\u7684\u6743\u8861\uff0c\u901a\u8fc7\u521b\u65b0\u5730\u91cd\u7528\u9884\u8bad\u7ec3\u6743\u91cd\u77e9\u9635\u4e2d\u7684\u805a\u7c7b\u53c2\u6570\u7ec4\uff0c\u5728\u663e\u8457\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u7684\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2602.20743", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20743", "abs": "https://arxiv.org/abs/2602.20743", "authors": ["Gabriel Loiseau", "Damien Sileo", "Damien Riquet", "Maxime Meyer", "Marc Tommasi"], "title": "Adaptive Text Anonymization: Learning Privacy-Utility Trade-offs via Prompt Optimization", "comment": null, "summary": "Anonymizing textual documents is a highly context-sensitive problem: the appropriate balance between privacy protection and utility preservation varies with the data domain, privacy objectives, and downstream application. However, existing anonymization methods rely on static, manually designed strategies that lack the flexibility to adjust to diverse requirements and often fail to generalize across domains. We introduce adaptive text anonymization, a new task formulation in which anonymization strategies are automatically adapted to specific privacy-utility requirements. We propose a framework for task-specific prompt optimization that automatically constructs anonymization instructions for language models, enabling adaptation to different privacy goals, domains, and downstream usage patterns. To evaluate our approach, we present a benchmark spanning five datasets with diverse domains, privacy constraints, and utility objectives. Across all evaluated settings, our framework consistently achieves a better privacy-utility trade-off than existing baselines, while remaining computationally efficient and effective on open-source language models, with performance comparable to larger closed-source models. Additionally, we show that our method can discover novel anonymization strategies that explore different points along the privacy-utility trade-off frontier.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u6587\u672c\u533f\u540d\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u4f18\u5316\u81ea\u52a8\u6784\u5efa\u533f\u540d\u5316\u6307\u4ee4\uff0c\u9002\u5e94\u4e0d\u540c\u9690\u79c1-\u6548\u7528\u9700\u6c42", "motivation": "\u73b0\u6709\u533f\u540d\u5316\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u624b\u5de5\u8bbe\u8ba1\u7b56\u7565\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u6570\u636e\u57df\u3001\u9690\u79c1\u76ee\u6807\u548c\u4e0b\u6e38\u5e94\u7528\u7684\u9700\u6c42", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u6587\u672c\u533f\u540d\u5316\u4efb\u52a1\uff0c\u5f00\u53d1\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff0c\u81ea\u52a8\u4e3a\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u533f\u540d\u5316\u6307\u4ee4\uff0c\u9002\u5e94\u4e0d\u540c\u9690\u79c1\u76ee\u6807\u3001\u9886\u57df\u548c\u4f7f\u7528\u6a21\u5f0f", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u6846\u67b6\u5728\u6240\u6709\u8bbe\u7f6e\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5728\u9690\u79c1-\u6548\u7528\u6743\u8861\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u5f00\u6e90\u6a21\u578b\u6027\u80fd\u4e0e\u5927\u578b\u95ed\u6e90\u6a21\u578b\u76f8\u5f53\uff0c\u8fd8\u80fd\u53d1\u73b0\u65b0\u7684\u533f\u540d\u5316\u7b56\u7565", "conclusion": "\u81ea\u9002\u5e94\u6587\u672c\u533f\u540d\u5316\u6846\u67b6\u80fd\u6709\u6548\u9002\u5e94\u591a\u6837\u5316\u7684\u9690\u79c1-\u6548\u7528\u9700\u6c42\uff0c\u5728\u591a\u4e2a\u9886\u57df\u548c\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2602.20749", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20749", "abs": "https://arxiv.org/abs/2602.20749", "authors": ["Azrin Sultana", "Firoz Ahmed"], "title": "Explicit Grammar Semantic Feature Fusion for Robust Text Classification", "comment": "30 pages, 9 figures", "summary": "Natural Language Processing enables computers to understand human language by analysing and classifying text efficiently with deep-level grammatical and semantic features. Existing models capture features by learning from large corpora with transformer models, which are computationally intensive and unsuitable for resource-constrained environments. Therefore, our proposed study incorporates comprehensive grammatical rules alongside semantic information to build a robust, lightweight classification model without resorting to full parameterised transformer models or heavy deep learning architectures. The novelty of our approach lies in its explicit encoding of sentence-level grammatical structure, including syntactic composition, phrase patterns, and complexity indicators, into a compact grammar vector, which is then fused with frozen contextual embeddings. These heterogeneous elements unified a single representation that captures both the structural and semantic characteristics of the text. Deep learning models such as Deep Belief Networks (DBNs), Long Short-Term Memory (LSTMs), BiLSTMs, and transformer-based BERT and XLNET were used to train and evaluate the model, with the number of epochs varied. Based on experimental results, the unified feature representation model captures both the semantic and structural properties of text, outperforming baseline models by 2%-15%, enabling more effective learning across heterogeneous domains. Unlike prior syntax-aware transformer models that inject grammatical structure through additional attention layers, tree encoders, or full fine-tuning, the proposed framework treats grammar as an explicit inductive bias rather than a learnable module, resulting in a very lightweight model that delivers better performance on edge devices", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6587\u672c\u5206\u7c7b\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u53e5\u6cd5\u7ed3\u6784\u4e0e\u8bed\u4e49\u4fe1\u606f\u878d\u5408\uff0c\u907f\u514d\u4f7f\u7528\u8ba1\u7b97\u5bc6\u96c6\u7684\u5b8c\u6574Transformer\u6a21\u578b\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u5206\u7c7b", "motivation": "\u73b0\u6709Transformer\u6a21\u578b\u867d\u7136\u80fd\u6355\u6349\u6587\u672c\u7684\u6df1\u5c42\u8bed\u6cd5\u548c\u8bed\u4e49\u7279\u5f81\uff0c\u4f46\u8ba1\u7b97\u91cf\u5927\uff0c\u4e0d\u9002\u5408\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3002\u9700\u8981\u4e00\u79cd\u65e2\u8f7b\u91cf\u53c8\u80fd\u6709\u6548\u7ed3\u5408\u8bed\u6cd5\u548c\u8bed\u4e49\u4fe1\u606f\u7684\u5206\u7c7b\u65b9\u6cd5", "method": "\u5c06\u53e5\u5b50\u7ea7\u8bed\u6cd5\u7ed3\u6784\uff08\u53e5\u6cd5\u7ec4\u6210\u3001\u77ed\u8bed\u6a21\u5f0f\u3001\u590d\u6742\u5ea6\u6307\u6807\uff09\u7f16\u7801\u4e3a\u7d27\u51d1\u7684\u8bed\u6cd5\u5411\u91cf\uff0c\u4e0e\u51bb\u7ed3\u7684\u4e0a\u4e0b\u6587\u5d4c\u5165\u878d\u5408\uff0c\u5f62\u6210\u7edf\u4e00\u8868\u793a\u3002\u4f7f\u7528DBNs\u3001LSTMs\u3001BiLSTMs\u3001BERT\u548cXLNET\u7b49\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u8bc4\u4f30", "result": "\u7edf\u4e00\u7279\u5f81\u8868\u793a\u6a21\u578b\u80fd\u540c\u65f6\u6355\u6349\u6587\u672c\u7684\u8bed\u4e49\u548c\u7ed3\u6784\u7279\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b2%-15%\uff0c\u5728\u5f02\u6784\u9886\u57df\u5b66\u4e60\u66f4\u6709\u6548\u3002\u6a21\u578b\u975e\u5e38\u8f7b\u91cf\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8868\u73b0\u66f4\u597d", "conclusion": "\u5c06\u8bed\u6cd5\u4f5c\u4e3a\u663e\u5f0f\u5f52\u7eb3\u504f\u7f6e\u800c\u975e\u53ef\u5b66\u4e60\u6a21\u5757\uff0c\u6784\u5efa\u4e86\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u73af\u5883"}}
{"id": "2602.20751", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20751", "abs": "https://arxiv.org/abs/2602.20751", "authors": ["Yifei Xu", "Guilherme Potje", "Shivam Shandilya", "Tiancheng Yuan", "Leonardo de Oliveira Nunes", "Rakshanda Agarwal", "Saeid Asgari", "Adam Atkinson", "Emre K\u0131c\u0131man", "Songwu Lu", "Ranveer Chandra", "Tusher Chakraborty"], "title": "SibylSense: Adaptive Rubric Learning via Memory Tuning and Adversarial Probing", "comment": null, "summary": "Designing aligned and robust rewards for open-ended generation remains a key barrier to RL post-training. Rubrics provide structured, interpretable supervision, but scaling rubric construction is difficult: expert rubrics are costly, prompted rubrics are often superficial or inconsistent, and fixed-pool discriminative rubrics can saturate and drift, enabling reward hacking. We present SibylSense, an inference-time learning approach that adapts a frozen rubric generator through a tunable memory bank of validated rubric items. Memory is updated via verifier-based item rewards measured by reference-candidate answer discriminative gaps from a handful of examples. SibylSense alternates memory tuning with a rubric-adversarial policy update that produces rubric-satisfying candidate answers, shrinking discriminative gaps and driving the rubric generator to capture new quality dimensions. Experiments on two open-ended tasks show that SibylSense yields more discriminative rubrics and improves downstream RL performance over static and non-adaptive baselines.", "AI": {"tldr": "SibylSense\uff1a\u4e00\u79cd\u901a\u8fc7\u53ef\u8c03\u8bb0\u5fc6\u5e93\u81ea\u9002\u5e94\u751f\u6210\u8bc4\u4f30\u6807\u51c6\u7684\u63a8\u7406\u65f6\u5b66\u4e60\u65b9\u6cd5\uff0c\u63d0\u5347\u5f00\u653e\u751f\u6210\u4efb\u52a1\u4e2dRL\u540e\u8bad\u7ec3\u7684\u6548\u679c", "motivation": "\u4e3a\u5f00\u653e\u751f\u6210\u4efb\u52a1\u8bbe\u8ba1\u5bf9\u9f50\u4e14\u9c81\u68d2\u7684\u5956\u52b1\u51fd\u6570\u662fRL\u540e\u8bad\u7ec3\u7684\u5173\u952e\u969c\u788d\u3002\u73b0\u6709\u8bc4\u4f30\u6807\u51c6\u6784\u5efa\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\uff1a\u4e13\u5bb6\u8bc4\u4f30\u6807\u51c6\u6210\u672c\u9ad8\uff0c\u63d0\u793a\u751f\u6210\u7684\u8bc4\u4f30\u6807\u51c6\u80a4\u6d45\u6216\u4e0d\u4e00\u81f4\uff0c\u56fa\u5b9a\u6c60\u5224\u522b\u6027\u8bc4\u4f30\u6807\u51c6\u6613\u9971\u548c\u548c\u6f02\u79fb\u5bfc\u81f4\u5956\u52b1\u653b\u51fb\u3002", "method": "\u63d0\u51faSibylSense\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u8c03\u8bb0\u5fc6\u5e93\u81ea\u9002\u5e94\u751f\u6210\u8bc4\u4f30\u6807\u51c6\u3002\u4f7f\u7528\u9a8c\u8bc1\u5668\u57fa\u4e8e\u5c11\u91cf\u793a\u4f8b\u7684\u53c2\u8003-\u5019\u9009\u7b54\u6848\u5224\u522b\u6027\u5dee\u8ddd\u8ba1\u7b97\u9879\u76ee\u5956\u52b1\u6765\u66f4\u65b0\u8bb0\u5fc6\u3002\u4ea4\u66ff\u8fdb\u884c\u8bb0\u5fc6\u8c03\u4f18\u548c\u8bc4\u4f30\u6807\u51c6\u5bf9\u6297\u7b56\u7565\u66f4\u65b0\uff0c\u751f\u6210\u6ee1\u8db3\u8bc4\u4f30\u6807\u51c6\u7684\u5019\u9009\u7b54\u6848\uff0c\u7f29\u5c0f\u5224\u522b\u6027\u5dee\u8ddd\uff0c\u9a71\u52a8\u8bc4\u4f30\u6807\u51c6\u751f\u6210\u5668\u6355\u6349\u65b0\u8d28\u91cf\u7ef4\u5ea6\u3002", "result": "\u5728\u4e24\u4e2a\u5f00\u653e\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSibylSense\u80fd\u4ea7\u751f\u66f4\u5177\u5224\u522b\u6027\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u76f8\u6bd4\u9759\u6001\u548c\u975e\u81ea\u9002\u5e94\u57fa\u7ebf\uff0c\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\u3002", "conclusion": "SibylSense\u901a\u8fc7\u63a8\u7406\u65f6\u81ea\u9002\u5e94\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8bc4\u4f30\u6807\u51c6\u6784\u5efa\u7684\u6269\u5c55\u6027\u95ee\u9898\uff0c\u4e3a\u5f00\u653e\u751f\u6210\u4efb\u52a1\u7684RL\u540e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5956\u52b1\u673a\u5236\u3002"}}
{"id": "2602.20759", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20759", "abs": "https://arxiv.org/abs/2602.20759", "authors": ["Yu Fu", "Seongho Son", "Ilija Bogunovic"], "title": "Overton Pluralistic Reinforcement Learning for Large Language Models", "comment": "28 pages, 8 figures", "summary": "Existing alignment paradigms remain limited in capturing the pluralistic nature of human values. Overton Pluralism addresses this gap by generating responses with diverse perspectives from a single query. This paper introduces OP-GRPO (Overton Pluralistic Group Relative Policy Optimization), a reinforcement learning framework for implicit Overton Pluralism that enables a single large language model to produce pluralistic responses without explicit prompting or modular orchestration. Our workflow consists of two main steps. First, similarity estimator training fine-tunes a Sentence Transformer for Overton Pluralism tasks to provide more accurate coverage evaluation of generated responses. Second, OP-GRPO training incorporates this similarity estimator into a dual-reward system designed to ensure both broad coverage of genuine human perspectives and the uniqueness of each perspective, thereby promoting diversity. Empirical results demonstrate a \"small models, big perspective coverage\" effect. The trained Qwen2.5-3B-Instruct model surpasses a 20B GPT-OSS baseline with a 37.4 percent relative accuracy gain on a Natural Language Inference benchmark, and also outperforms a modular architecture baseline with a 19.1 percent relative improvement. Additional evaluations using GPT-4.1 as a large language model judge further confirm the robustness of the approach.", "AI": {"tldr": "\u63d0\u51faOP-GRPO\u6846\u67b6\uff0c\u8ba9\u5355\u4e00LLM\u65e0\u9700\u663e\u5f0f\u63d0\u793a\u5c31\u80fd\u751f\u6210\u591a\u5143\u89c6\u89d2\u7684\u56de\u590d\uff0c\u5b9e\u73b0\"\u5c0f\u6a21\u578b\uff0c\u5927\u89c6\u89d2\u8986\u76d6\"\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u5bf9\u9f50\u8303\u5f0f\u96be\u4ee5\u6355\u6349\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u591a\u5143\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u8ba9\u5355\u4e00\u6a21\u578b\u751f\u6210\u591a\u6837\u5316\u89c6\u89d2\u7684\u65b9\u6cd5\u3002", "method": "1. \u8bad\u7ec3\u76f8\u4f3c\u5ea6\u4f30\u8ba1\u5668\uff08Sentence Transformer\uff09\u7528\u4e8e\u8bc4\u4f30\u56de\u590d\u7684\u8986\u76d6\u8303\u56f4\uff1b2. \u4f7f\u7528OP-GRPO\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u53cc\u5956\u52b1\u7cfb\u7edf\u786e\u4fdd\u89c6\u89d2\u7684\u5e7f\u6cdb\u8986\u76d6\u548c\u72ec\u7279\u6027\u3002", "result": "Qwen2.5-3B-Instruct\u6a21\u578b\u76f8\u6bd420B GPT-OSS\u57fa\u7ebf\u5728NLI\u57fa\u51c6\u4e0a\u83b7\u5f9737.4%\u76f8\u5bf9\u51c6\u786e\u7387\u63d0\u5347\uff0c\u76f8\u6bd4\u6a21\u5757\u5316\u67b6\u6784\u57fa\u7ebf\u670919.1%\u76f8\u5bf9\u6539\u8fdb\uff0cGPT-4.1\u8bc4\u4f30\u4e5f\u8bc1\u5b9e\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "OP-GRPO\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u9690\u5f0f\u591a\u5143\u4e3b\u4e49\uff0c\u8ba9\u5c0f\u6a21\u578b\u4e5f\u80fd\u83b7\u5f97\u5e7f\u6cdb\u7684\u89c6\u89d2\u8986\u76d6\uff0c\u4e3a\u4ef7\u503c\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.20816", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20816", "abs": "https://arxiv.org/abs/2602.20816", "authors": ["Sayantan Dasgupta", "Trevor Cohn", "Timothy Baldwin"], "title": "Don't Ignore the Tail: Decoupling top-K Probabilities for Efficient Language Model Distillation", "comment": null, "summary": "The core learning signal used in language model distillation is the standard Kullback-Leibler (KL) divergence between the student and teacher distributions. Traditional KL divergence tends to be dominated by the next tokens with the highest probabilities, i.e., the teacher's modes, thereby diminishing the influence of less probable yet potentially informative components of the output distribution. We propose a new tail-aware divergence that decouples the contribution of the teacher model's top-K predicted probabilities from that of lower-probability predictions, while maintaining the same computational profile as the KL Divergence. Our decoupled approach reduces the impact of the teacher modes and, consequently, increases the contribution of the tail of the distribution. Experimental results demonstrate that our modified distillation method yields competitive performance in both pre-training and supervised distillation of decoder models across various datasets. Furthermore, the distillation process is efficient and can be performed with a modest academic budget for large datasets, eliminating the need for industry-scale computing.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5c3e\u611f\u77e5\u6563\u5ea6\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bed\u8a00\u6a21\u578b\u84b8\u998f\uff0c\u901a\u8fc7\u89e3\u8026\u6559\u5e08\u6a21\u578b\u9884\u6d4b\u6982\u7387\u4e2d\u524dK\u4e2a\u6700\u9ad8\u6982\u7387\u4e0e\u8f83\u4f4e\u6982\u7387\u7684\u8d21\u732e\uff0c\u63d0\u5347\u5206\u5e03\u5c3e\u90e8\u7684\u5f71\u54cd", "motivation": "\u4f20\u7edfKL\u6563\u5ea6\u5728\u8bed\u8a00\u6a21\u578b\u84b8\u998f\u4e2d\u4e3b\u8981\u53d7\u6559\u5e08\u6a21\u578b\u6700\u9ad8\u6982\u7387\u7684token\u4e3b\u5bfc\uff0c\u8fd9\u524a\u5f31\u4e86\u8f83\u4f4e\u6982\u7387\u4f46\u53ef\u80fd\u5305\u542b\u6709\u7528\u4fe1\u606f\u7684\u5206\u5e03\u5c3e\u90e8\u5bf9\u5b66\u4e60\u4fe1\u53f7\u7684\u8d21\u732e", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5c3e\u611f\u77e5\u6563\u5ea6\uff0c\u5c06\u6559\u5e08\u6a21\u578b\u9884\u6d4b\u6982\u7387\u4e2d\u524dK\u4e2a\u6700\u9ad8\u6982\u7387\u4e0e\u8f83\u4f4e\u6982\u7387\u7684\u8d21\u732e\u89e3\u8026\uff0c\u540c\u65f6\u4fdd\u6301\u4e0eKL\u6563\u5ea6\u76f8\u540c\u7684\u8ba1\u7b97\u590d\u6742\u5ea6", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6539\u8fdb\u7684\u84b8\u998f\u65b9\u6cd5\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\uff0c\u5728\u9884\u8bad\u7ec3\u548c\u76d1\u7763\u84b8\u998f\u89e3\u7801\u5668\u6a21\u578b\u4e2d\u90fd\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u4e14\u84b8\u998f\u8fc7\u7a0b\u9ad8\u6548\uff0c\u53ef\u5728\u5b66\u672f\u9884\u7b97\u4e0b\u5904\u7406\u5927\u578b\u6570\u636e\u96c6", "conclusion": "\u63d0\u51fa\u7684\u5c3e\u611f\u77e5\u6563\u5ea6\u65b9\u6cd5\u901a\u8fc7\u51cf\u5c11\u6559\u5e08\u6a21\u578b\u4e3b\u5bfc\u6a21\u5f0f\u7684\u5f71\u54cd\u5e76\u589e\u52a0\u5206\u5e03\u5c3e\u90e8\u7684\u8d21\u732e\uff0c\u5728\u8bed\u8a00\u6a21\u578b\u84b8\u998f\u4e2d\u53d6\u5f97\u4e86\u826f\u597d\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387"}}
{"id": "2602.20859", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20859", "abs": "https://arxiv.org/abs/2602.20859", "authors": ["Zirui He", "Huopu Zhang", "Yanguang Liu", "Sirui Wu", "Mengnan Du"], "title": "FinAnchor: Aligned Multi-Model Representations for Financial Prediction", "comment": "11 pages, 4 figures, 5 tables", "summary": "Financial prediction from long documents involves significant challenges, as actionable signals are often sparse and obscured by noise, and the optimal LLM for generating embeddings varies across tasks and time periods. In this paper, we propose FinAnchor(Financial Anchored Representations), a lightweight framework that integrates embeddings from multiple LLMs without fine-tuning the underlying models. FinAnchor addresses the incompatibility of feature spaces by selecting an anchor embedding space and learning linear mappings to align representations from other models into this anchor. These aligned features are then aggregated to form a unified representation for downstream prediction. Across multiple financial NLP tasks, FinAnchor consistently outperforms strong single-model baselines and standard ensemble methods, demonstrating the effectiveness of anchoring heterogeneous representations for robust financial prediction.", "AI": {"tldr": "FinAnchor\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u951a\u5b9a\u5d4c\u5165\u7a7a\u95f4\u548c\u7ebf\u6027\u6620\u5c04\u5bf9\u9f50\u591aLLM\u8868\u793a\uff0c\u63d0\u5347\u91d1\u878d\u957f\u6587\u6863\u9884\u6d4b\u6027\u80fd", "motivation": "\u91d1\u878d\u957f\u6587\u6863\u9884\u6d4b\u9762\u4e34\u4fe1\u53f7\u7a00\u758f\u3001\u566a\u58f0\u5e72\u6270\u7684\u6311\u6218\uff0c\u4e14\u4e0d\u540c\u4efb\u52a1\u548c\u65f6\u95f4\u6bb5\u7684\u6700\u4f18LLM\u5d4c\u5165\u6a21\u578b\u5b58\u5728\u5dee\u5f02", "method": "\u9009\u62e9\u951a\u5b9a\u5d4c\u5165\u7a7a\u95f4\uff0c\u5b66\u4e60\u7ebf\u6027\u6620\u5c04\u5c06\u5176\u4ed6\u6a21\u578b\u7684\u8868\u793a\u5bf9\u9f50\u5230\u8be5\u7a7a\u95f4\uff0c\u7136\u540e\u805a\u5408\u5bf9\u9f50\u540e\u7684\u7279\u5f81\u5f62\u6210\u7edf\u4e00\u8868\u793a", "result": "\u5728\u591a\u4e2a\u91d1\u878dNLP\u4efb\u52a1\u4e2d\uff0cFinAnchor\u6301\u7eed\u4f18\u4e8e\u5f3a\u5355\u6a21\u578b\u57fa\u7ebf\u548c\u6807\u51c6\u96c6\u6210\u65b9\u6cd5", "conclusion": "\u951a\u5b9a\u5f02\u6784\u8868\u793a\u5bf9\u4e8e\u7a33\u5065\u7684\u91d1\u878d\u9884\u6d4b\u662f\u6709\u6548\u7684\uff0cFinAnchor\u6846\u67b6\u65e0\u9700\u5fae\u8c03\u5e95\u5c42\u6a21\u578b\u5373\u53ef\u96c6\u6210\u591aLLM\u5d4c\u5165"}}
{"id": "2602.20892", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20892", "abs": "https://arxiv.org/abs/2602.20892", "authors": ["Seyed Himan Ghaderi", "Saeed Sarbazi Azad", "Mohammad Mehdi Jaziriyan", "Ahmad Akbari"], "title": "Exa-PSD: a new Persian sentiment analysis dataset on Twitter", "comment": "This is the original submitted (preprint) version of a paper published in Language Resources and Evaluation. The final published version is available at Springer via DOI: https://doi.org/10.1007/s10579-025-09886-5", "summary": "Today, Social networks such as Twitter are the most widely used platforms for communication of people. Analyzing this data has useful information to recognize the opinion of people in tweets. Sentiment analysis plays a vital role in NLP, which identifies the opinion of the individuals about a specific topic. Natural language processing in Persian has many challenges despite the adventure of strong language models. The datasets available in Persian are generally in special topics such as products, foods, hotels, etc while users may use ironies, colloquial phrases in social media To overcome these challenges, there is a necessity for having a dataset of Persian sentiment analysis on Twitter. In this paper, we introduce the Exa sentiment analysis Persian dataset, which is collected from Persian tweets. This dataset contains 12,000 tweets, annotated by 5 native Persian taggers. The aforementioned data is labeled in 3 classes: positive, neutral and negative. We present the characteristics and statistics of this dataset and use the pre-trained Pars Bert and Roberta as the base model to evaluate this dataset. Our evaluation reached a 79.87 Macro F-score, which shows the model and data can be adequately valuable for a sentiment analysis system.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6ce2\u65af\u8bed\u63a8\u7279\u60c5\u611f\u5206\u6790\u6570\u636e\u96c6Exa\uff0c\u5305\u542b12,000\u6761\u6807\u6ce8\u6570\u636e\uff0c\u5e76\u8bc4\u4f30\u4e86\u9884\u8bad\u7ec3\u6a21\u578bPars Bert\u548cRoberta\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u6ce2\u65af\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u6570\u636e\u96c6\u591a\u4e3a\u7279\u5b9a\u9886\u57df\uff08\u5982\u4ea7\u54c1\u3001\u98df\u54c1\u3001\u9152\u5e97\u7b49\uff09\uff0c\u800c\u793e\u4ea4\u5a92\u4f53\u4e2d\u7528\u6237\u5e38\u4f7f\u7528\u8bbd\u523a\u3001\u53e3\u8bed\u5316\u8868\u8fbe\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u63a8\u7279\u5e73\u53f0\u7684\u6ce2\u65af\u8bed\u60c5\u611f\u5206\u6790\u6570\u636e\u96c6\u3002", "method": "\u6536\u96c6\u6ce2\u65af\u8bed\u63a8\u7279\u6570\u636e\uff0c\u75315\u540d\u6ce2\u65af\u8bed\u6bcd\u8bed\u6807\u6ce8\u8005\u6807\u6ce8\u4e3a\u79ef\u6781\u3001\u4e2d\u6027\u3001\u6d88\u6781\u4e09\u7c7b\uff0c\u517112,000\u6761\u63a8\u6587\u3002\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578bPars Bert\u548cRoberta\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u8fbe\u5230\u4e8679.87\u7684\u5b8f\u89c2F1\u5206\u6570\uff0c\u8868\u660e\u6a21\u578b\u548c\u6570\u636e\u96c6\u5bf9\u4e8e\u60c5\u611f\u5206\u6790\u7cfb\u7edf\u5177\u6709\u8db3\u591f\u4ef7\u503c\u3002", "conclusion": "Exa\u6570\u636e\u96c6\u586b\u8865\u4e86\u6ce2\u65af\u8bed\u63a8\u7279\u60c5\u611f\u5206\u6790\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u4e3a\u6ce2\u65af\u8bedNLP\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\u3002"}}
{"id": "2602.20945", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20945", "abs": "https://arxiv.org/abs/2602.20945", "authors": ["Taiqiang Wu", "Zenan Zu", "Bo Zhou", "Ngai Wong"], "title": "The Art of Efficient Reasoning: Data, Reward, and Optimization", "comment": "Tech Report, Insights on Efficient Reasoning via Reward Shaping", "summary": "Large Language Models (LLMs) consistently benefit from scaled Chain-of-Thought (CoT) reasoning, but also suffer from heavy computational overhead. To address this issue, efficient reasoning aims to incentivize short yet accurate thinking trajectories, typically through reward shaping with Reinforcement Learning (RL). In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs. For comprehensive evaluation, we advocate for more fine-grained metrics, including length distribution conditioned on correctness and performance across a wide spectrum of token budgets ranging from 2k to 32k. First, we reveal that the training process follows a two-stage paradigm: length adaptation and reasoning refinement. After that, we conduct extensive experiments (about 0.2 million GPU hours) in a unified protocol, deconstructing training prompts and rollouts, reward shaping, and optimization strategies. In particular, a key finding is to train on relatively easier prompts, ensuring the density of positive reward signals and thus avoiding the length collapse. Meanwhile, the learned length bias can be generalized across domains. We distill all findings into valuable insights and practical guidelines, and further validate them across the Qwen3 series, ranging from 0.6B to 30B, demonstrating the robustness and generalization.", "AI": {"tldr": "\u7cfb\u7edf\u7814\u7a76LLM\u9ad8\u6548\u63a8\u7406\u673a\u5236\uff0c\u63d0\u51fa\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff0c\u53d1\u73b0\u8bad\u7ec3\u76f8\u5bf9\u7b80\u5355\u63d0\u793a\u53ef\u907f\u514d\u957f\u5ea6\u5d29\u6e83\uff0c\u5b66\u4e60\u5230\u7684\u957f\u5ea6\u504f\u7f6e\u53ef\u8de8\u57df\u6cdb\u5316", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u6269\u5c55\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u4e2d\u83b7\u76ca\uff0c\u4f46\u8ba1\u7b97\u5f00\u9500\u5de8\u5927\u3002\u9ad8\u6548\u63a8\u7406\u65e8\u5728\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6fc0\u52b1\u77ed\u800c\u51c6\u786e\u7684\u601d\u7ef4\u8f68\u8ff9\uff0c\u4f46\u76f8\u5173\u673a\u5236\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u91c7\u7528\u66f4\u7ec6\u7c92\u5ea6\u7684\u8bc4\u4f30\u6307\u6807\uff08\u5305\u62ec\u57fa\u4e8e\u6b63\u786e\u6027\u7684\u957f\u5ea6\u5206\u5e03\u548c\u4e0d\u540ctoken\u9884\u7b97\u4e0b\u7684\u6027\u80fd\uff09\uff0c\u901a\u8fc7\u7edf\u4e00\u534f\u8bae\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff08\u7ea620\u4e07GPU\u5c0f\u65f6\uff09\uff0c\u5206\u89e3\u8bad\u7ec3\u63d0\u793a\u3001rollout\u3001\u5956\u52b1\u5851\u9020\u548c\u4f18\u5316\u7b56\u7565\u3002", "result": "\u53d1\u73b0\u8bad\u7ec3\u8fc7\u7a0b\u9075\u5faa\u4e24\u9636\u6bb5\u8303\u5f0f\uff1a\u957f\u5ea6\u9002\u5e94\u548c\u63a8\u7406\u7cbe\u70bc\u3002\u5173\u952e\u53d1\u73b0\u662f\u8bad\u7ec3\u76f8\u5bf9\u7b80\u5355\u7684\u63d0\u793a\u53ef\u786e\u4fdd\u6b63\u5956\u52b1\u4fe1\u53f7\u7684\u5bc6\u5ea6\uff0c\u907f\u514d\u957f\u5ea6\u5d29\u6e83\u3002\u5b66\u4e60\u5230\u7684\u957f\u5ea6\u504f\u7f6e\u53ef\u8de8\u57df\u6cdb\u5316\u3002\u5728Qwen3\u7cfb\u5217\uff080.6B-30B\uff09\u4e0a\u9a8c\u8bc1\u4e86\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u3002", "conclusion": "\u7cfb\u7edf\u7814\u7a76\u63ed\u793a\u4e86\u9ad8\u6548\u63a8\u7406\u7684\u673a\u5236\uff0c\u63d0\u51fa\u4e86\u5b9e\u7528\u7684\u8bad\u7ec3\u6307\u5357\uff0c\u8bc1\u660e\u5b66\u4e60\u5230\u7684\u957f\u5ea6\u504f\u7f6e\u5177\u6709\u8de8\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2602.20966", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20966", "abs": "https://arxiv.org/abs/2602.20966", "authors": ["Paola Merlo", "Chunyang Jiang", "Giuseppe Samo", "Vivi Nastase"], "title": "Blackbird Language Matrices: A Framework to Investigate the Linguistic Competence of Language Models", "comment": "Under review, 46 pages, 5 tables, 28 figures", "summary": "This article describes a novel language task, the Blackbird Language Matrices (BLM) task, inspired by intelligence tests, and illustrates the BLM datasets, their construction and benchmarking, and targeted experiments on chunking and systematicity. BLMs are multiple-choice problems, structured at multiple levels: within each sentence, across the input sequence, within each candidate answer. Because of their rich structure, these curated, but naturalistic datasets are key to answer some core questions about current large language models abilities: do LLMs detect linguistic objects and their properties? Do they detect and use systematic patterns across sentences? Are they more prone to linguistic or reasoning errors, and how do these interact?\n  We show that BLMs, while challenging, can be solved at good levels of performance, in more than one language, with simple baseline models or, at better performance levels, with more tailored models. We show that their representations contain the grammatical objects and attributes relevant to solve a linguistic task. We also show that these solutions are reached by detecting systematic patterns across sentences.\n  The paper supports the point of view that curated, structured datasets support multi-faceted investigations of properties of language and large language models. Because they present a curated, articulated structure, because they comprise both learning contexts and expected answers, and because they are partly built by hand, BLMs fall in the category of datasets that can support explainability investigations, and be useful to ask why large language models behave the way they do.", "AI": {"tldr": "BLM\u4efb\u52a1\u662f\u4e00\u4e2a\u53d7\u667a\u529b\u6d4b\u8bd5\u542f\u53d1\u7684\u591a\u9009\u8bed\u8a00\u77e9\u9635\u4efb\u52a1\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6570\u636e\u96c6\u8bc4\u4f30LLM\u7684\u8bed\u8a00\u5bf9\u8c61\u68c0\u6d4b\u3001\u7cfb\u7edf\u6a21\u5f0f\u8bc6\u522b\u548c\u63a8\u7406\u80fd\u529b", "motivation": "\u4e3a\u4e86\u63a2\u7a76\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6838\u5fc3\u80fd\u529b\uff1a\u662f\u5426\u80fd\u68c0\u6d4b\u8bed\u8a00\u5bf9\u8c61\u53ca\u5176\u5c5e\u6027\uff1f\u80fd\u5426\u53d1\u73b0\u548c\u4f7f\u7528\u8de8\u53e5\u5b50\u7684\u7cfb\u7edf\u6a21\u5f0f\uff1f\u66f4\u5bb9\u6613\u72af\u8bed\u8a00\u9519\u8bef\u8fd8\u662f\u63a8\u7406\u9519\u8bef\uff1f\u8fd9\u4e9b\u9519\u8bef\u5982\u4f55\u76f8\u4e92\u4f5c\u7528\uff1f", "method": "\u521b\u5efaBlackbird Language Matrices(BLM)\u4efb\u52a1\uff0c\u6784\u5efa\u7ed3\u6784\u5316\u591a\u7ea7\u6570\u636e\u96c6\uff08\u53e5\u5b50\u5185\u3001\u8f93\u5165\u5e8f\u5217\u95f4\u3001\u5019\u9009\u7b54\u6848\u5185\uff09\uff0c\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u548c\u9488\u5bf9\u6027\u5b9e\u9a8c\uff08\u5206\u5757\u548c\u7cfb\u7edf\u6027\uff09\u8bc4\u4f30\u4e0d\u540c\u6a21\u578b", "result": "BLM\u4efb\u52a1\u867d\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u4f46\u53ef\u4ee5\u901a\u8fc7\u7b80\u5355\u57fa\u7ebf\u6a21\u578b\u5728\u591a\u8bed\u8a00\u4e2d\u83b7\u5f97\u826f\u597d\u6027\u80fd\uff0c\u5b9a\u5236\u6a21\u578b\u6027\u80fd\u66f4\u4f18\uff1b\u6a21\u578b\u8868\u793a\u5305\u542b\u89e3\u51b3\u8bed\u8a00\u4efb\u52a1\u6240\u9700\u7684\u8bed\u6cd5\u5bf9\u8c61\u548c\u5c5e\u6027\uff1b\u89e3\u51b3\u65b9\u6848\u901a\u8fc7\u68c0\u6d4b\u8de8\u53e5\u5b50\u7684\u7cfb\u7edf\u6a21\u5f0f\u5b9e\u73b0", "conclusion": "\u7cbe\u5fc3\u7b56\u5212\u7684\u7ed3\u6784\u5316\u6570\u636e\u96c6\u652f\u6301\u591a\u65b9\u9762\u7684\u8bed\u8a00\u548c\u5927\u8bed\u8a00\u6a21\u578b\u5c5e\u6027\u7814\u7a76\uff0cBLM\u6570\u636e\u96c6\u56e0\u5176\u7ed3\u6784\u5316\u7279\u6027\u3001\u5305\u542b\u5b66\u4e60\u4e0a\u4e0b\u6587\u548c\u9884\u671f\u7b54\u6848\u3001\u90e8\u5206\u624b\u5de5\u6784\u5efa\uff0c\u53ef\u7528\u4e8e\u53ef\u89e3\u91ca\u6027\u7814\u7a76\uff0c\u5e2e\u52a9\u7406\u89e3LLM\u7684\u884c\u4e3a\u539f\u56e0"}}
{"id": "2602.20973", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20973", "abs": "https://arxiv.org/abs/2602.20973", "authors": ["Yuliang Ji", "Fuchen Shen", "Jian Wu", "Qiujie Xie", "Yue Zhang"], "title": "Linear Reasoning vs. Proof by Cases: Obstacles for Large Language Models in FOL Problem Solving", "comment": null, "summary": "To comprehensively evaluate the mathematical reasoning capabilities of Large Language Models (LLMs), researchers have introduced abundant mathematical reasoning datasets. However, most existing datasets primarily focus on linear reasoning, neglecting other parts such as proof by contradiction and proof by cases, which are crucial for investigating LLMs' reasoning abilities. To address this limitation, we first introduce a novel first-order logic (FOL) dataset named PC-FOL, annotated by professional mathematicians, focusing on case-based reasoning problems. All instances in this dataset are equipped with a manually written natural language proof, clearly distinguishing it from conventional linear reasoning datasets. Our experimental results over leading LLMs demonstrate a substantial performance gap between linear reasoning and case-based reasoning problems. To further investigate this phenomenon, we provide a theoretical analysis grounded in graphical model, which provides an explanation for the observed disparity between the two types of reasoning problems. We hope this work can reveal the core challenges in the field of automated natural language mathematical proof generation, paving the way for future research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86PC-FOL\u6570\u636e\u96c6\uff0c\u4e13\u95e8\u9488\u5bf9\u57fa\u4e8e\u6848\u4f8b\u7684\u6570\u5b66\u63a8\u7406\uff0c\u4ee5\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u975e\u7ebf\u6027\u7684\u6848\u4f8b\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u63ed\u793a\u4e86\u5176\u4e0e\u7ebf\u6027\u63a8\u7406\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u7ebf\u6027\u63a8\u7406\uff0c\u800c\u5ffd\u7565\u4e86\u53cd\u8bc1\u6cd5\u3001\u5206\u60c5\u51b5\u8bc1\u660e\u7b49\u91cd\u8981\u7684\u63a8\u7406\u5f62\u5f0f\uff0c\u8fd9\u9650\u5236\u4e86\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u5168\u9762\u8bc4\u4f30\u3002", "method": "\u9996\u5148\u5f15\u5165\u7531\u4e13\u4e1a\u6570\u5b66\u5bb6\u6807\u6ce8\u7684PC-FOL\u6570\u636e\u96c6\uff0c\u4e13\u6ce8\u4e8e\u57fa\u4e8e\u6848\u4f8b\u7684\u63a8\u7406\u95ee\u9898\uff0c\u5e76\u5305\u542b\u624b\u52a8\u7f16\u5199\u7684\u81ea\u7136\u8bed\u8a00\u8bc1\u660e\u3002\u7136\u540e\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u9886\u5148\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u57fa\u4e8e\u56fe\u6a21\u578b\u8fdb\u884c\u7406\u8bba\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u57fa\u4e8e\u6848\u4f8b\u7684\u63a8\u7406\u95ee\u9898\u4e0a\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\u3002\u7406\u8bba\u5206\u6790\u4ece\u56fe\u6a21\u578b\u89d2\u5ea6\u89e3\u91ca\u4e86\u8fd9\u79cd\u5dee\u5f02\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63ed\u793a\u4e86\u81ea\u52a8\u81ea\u7136\u8bed\u8a00\u6570\u5b66\u8bc1\u660e\u751f\u6210\u9886\u57df\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2602.20976", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.20976", "abs": "https://arxiv.org/abs/2602.20976", "authors": ["Xuan Luo", "Yubin Chen", "Zhiyu Hou", "Linpu Yu", "Geng Tu", "Jing Li", "Ruifeng Xu"], "title": "Evaluating Proactive Risk Awareness of Large Language Models", "comment": null, "summary": "As large language models (LLMs) are increasingly embedded in everyday decision-making, their safety responsibilities extend beyond reacting to explicit harmful intent toward anticipating unintended but consequential risks. In this work, we introduce a proactive risk awareness evaluation framework that measures whether LLMs can anticipate potential harms and provide warnings before damage occurs. We construct the Butterfly dataset to instantiate this framework in the environmental and ecological domain. It contains 1,094 queries that simulate ordinary solution-seeking activities whose responses may induce latent ecological impact. Through experiments across five widely used LLMs, we analyze the effects of response length, languages, and modality. Experimental results reveal consistent, significant declines in proactive awareness under length-restricted responses, cross-lingual similarities, and persistent blind spots in (multimodal) species protection. These findings highlight a critical gap between current safety alignment and the requirements of real-world ecological responsibility, underscoring the need for proactive safeguards in LLM deployment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3b\u52a8\u98ce\u9669\u610f\u8bc6\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8861\u91cfLLMs\u80fd\u5426\u5728\u635f\u5bb3\u53d1\u751f\u524d\u9884\u6d4b\u6f5c\u5728\u5371\u5bb3\u5e76\u63d0\u4f9b\u8b66\u544a\uff0c\u5e76\u6784\u5efa\u4e86Butterfly\u6570\u636e\u96c6\u6765\u5b9e\u4f8b\u5316\u8be5\u6846\u67b6\u5728\u73af\u5883\u751f\u6001\u9886\u57df\u7684\u5e94\u7528\u3002", "motivation": "\u968f\u7740LLMs\u8d8a\u6765\u8d8a\u591a\u5730\u5d4c\u5165\u65e5\u5e38\u51b3\u7b56\uff0c\u5176\u5b89\u5168\u8d23\u4efb\u5df2\u4ece\u5e94\u5bf9\u663e\u6027\u6709\u5bb3\u610f\u56fe\u6269\u5c55\u5230\u9884\u6d4b\u975e\u9884\u671f\u4f46\u5177\u6709\u540e\u679c\u7684\u98ce\u9669\u3002\u5f53\u524d\u7684\u5b89\u5168\u5bf9\u9f50\u4e0e\u771f\u5b9e\u4e16\u754c\u751f\u6001\u8d23\u4efb\u8981\u6c42\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e3b\u52a8\u98ce\u9669\u610f\u8bc6\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u6784\u5efa\u4e86Butterfly\u6570\u636e\u96c6\uff08\u5305\u542b1,094\u4e2a\u67e5\u8be2\uff09\uff0c\u6a21\u62df\u666e\u901a\u89e3\u51b3\u65b9\u6848\u5bfb\u6c42\u6d3b\u52a8\uff0c\u8fd9\u4e9b\u6d3b\u52a8\u7684\u54cd\u5e94\u53ef\u80fd\u5f15\u53d1\u6f5c\u5728\u751f\u6001\u5f71\u54cd\u3002\u5728\u4e94\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684LLMs\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5206\u6790\u4e86\u54cd\u5e94\u957f\u5ea6\u3001\u8bed\u8a00\u548c\u591a\u6a21\u6001\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u957f\u5ea6\u9650\u5236\u7684\u54cd\u5e94\u4e0b\u4e3b\u52a8\u610f\u8bc6\u663e\u8457\u4e0b\u964d\uff0c\u8de8\u8bed\u8a00\u8868\u73b0\u76f8\u4f3c\uff0c\u4e14\u5728\u591a\u6a21\u6001\u7269\u79cd\u4fdd\u62a4\u65b9\u9762\u5b58\u5728\u6301\u7eed\u7684\u76f2\u70b9\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u5f53\u524dLLMs\u7684\u5b89\u5168\u5bf9\u9f50\u4e0e\u771f\u5b9e\u4e16\u754c\u751f\u6001\u8d23\u4efb\u8981\u6c42\u4e4b\u95f4\u5b58\u5728\u5173\u952e\u5dee\u8ddd\uff0c\u5f3a\u8c03\u4e86\u5728LLM\u90e8\u7f72\u4e2d\u9700\u8981\u4e3b\u52a8\u5b89\u5168\u9632\u62a4\u63aa\u65bd\u3002"}}
{"id": "2602.21082", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21082", "abs": "https://arxiv.org/abs/2602.21082", "authors": ["Vishal Patil", "Shree Vaishnavi Bacha", "Revanth Yamani", "Yidan Sun", "Mayank Kejriwal"], "title": "Beyond the Star Rating: A Scalable Framework for Aspect-Based Sentiment Analysis Using LLMs and Text Classification", "comment": null, "summary": "Customer-provided reviews have become an important source of information for business owners and other customers alike. However, effectively analyzing millions of unstructured reviews remains challenging. While large language models (LLMs) show promise for natural language understanding, their application to large-scale review analysis has been limited by computational costs and scalability concerns. This study proposes a hybrid approach that uses LLMs for aspect identification while employing classic machine-learning methods for sentiment classification at scale. Using ChatGPT to analyze sampled restaurant reviews, we identified key aspects of dining experiences and developed sentiment classifiers using human-labeled reviews, which we subsequently applied to 4.7 million reviews collected over 17 years from a major online platform. Regression analysis reveals that our machine-labeled aspects significantly explain variance in overall restaurant ratings across different aspects of dining experiences, cuisines, and geographical regions. Our findings demonstrate that combining LLMs with traditional machine learning approaches can effectively automate aspect-based sentiment analysis of large-scale customer feedback, suggesting a practical framework for both researchers and practitioners in the hospitality industry and potentially, other service sectors.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u6df7\u5408\u65b9\u6cd5\uff1a\u7528LLM\u8bc6\u522b\u8bc4\u8bba\u4e2d\u7684\u65b9\u9762\uff0c\u7528\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u8fdb\u884c\u60c5\u611f\u5206\u7c7b\uff0c\u6210\u529f\u5206\u6790\u4e86470\u4e07\u6761\u9910\u5385\u8bc4\u8bba\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8fdb\u884c\u5927\u89c4\u6a21\u57fa\u4e8e\u65b9\u9762\u7684\u60c5\u611f\u5206\u6790\u3002", "motivation": "\u5ba2\u6237\u8bc4\u8bba\u662f\u91cd\u8981\u4fe1\u606f\u6765\u6e90\uff0c\u4f46\u5206\u6790\u6570\u767e\u4e07\u6761\u975e\u7ed3\u6784\u5316\u8bc4\u8bba\u5177\u6709\u6311\u6218\u6027\u3002LLMs\u867d\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u5927\u89c4\u6a21\u5e94\u7528\u53d7\u9650\u4e8e\u8ba1\u7b97\u6210\u672c\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff1a\u4f7f\u7528ChatGPT\u5206\u6790\u9910\u5385\u8bc4\u8bba\u6837\u672c\u8bc6\u522b\u5173\u952e\u65b9\u9762\uff0c\u57fa\u4e8e\u4eba\u5de5\u6807\u6ce8\u8bc4\u8bba\u5f00\u53d1\u60c5\u611f\u5206\u7c7b\u5668\uff0c\u7136\u540e\u5c06\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8e\u4ece\u4e3b\u8981\u5728\u7ebf\u5e73\u53f0\u6536\u96c6\u7684470\u4e07\u676117\u5e74\u95f4\u7684\u8bc4\u8bba\u3002", "result": "\u56de\u5f52\u5206\u6790\u663e\u793a\uff0c\u673a\u5668\u6807\u6ce8\u7684\u65b9\u9762\u80fd\u663e\u8457\u89e3\u91ca\u9910\u5385\u603b\u4f53\u8bc4\u5206\u5728\u4e0d\u540c\u7528\u9910\u4f53\u9a8c\u65b9\u9762\u3001\u83dc\u7cfb\u548c\u5730\u7406\u533a\u57df\u4e2d\u7684\u65b9\u5dee\u3002\u8be5\u65b9\u6cd5\u6210\u529f\u8bc6\u522b\u4e86\u5173\u952e\u65b9\u9762\u5e76\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u60c5\u611f\u5206\u6790\u3002", "conclusion": "LLMs\u4e0e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7ed3\u5408\u53ef\u4ee5\u6709\u6548\u81ea\u52a8\u5316\u5927\u89c4\u6a21\u5ba2\u6237\u53cd\u9988\u7684\u57fa\u4e8e\u65b9\u9762\u7684\u60c5\u611f\u5206\u6790\uff0c\u4e3a\u9152\u5e97\u4e1a\u548c\u5176\u4ed6\u670d\u52a1\u884c\u4e1a\u7684\u7814\u7a76\u8005\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\u3002"}}
{"id": "2602.21165", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21165", "abs": "https://arxiv.org/abs/2602.21165", "authors": ["Samah Fodeh", "Linhai Ma", "Yan Wang", "Srivani Talakokkul", "Ganesh Puthiaraju", "Afshan Khan", "Ashley Hagaman", "Sarah Lowe", "Aimee Roundtree"], "title": "PVminer: A Domain-Specific Tool to Detect the Patient Voice in Patient Generated Data", "comment": null, "summary": "Patient-generated text such as secure messages, surveys, and interviews contains rich expressions of the patient voice (PV), reflecting communicative behaviors and social determinants of health (SDoH). Traditional qualitative coding frameworks are labor intensive and do not scale to large volumes of patient-authored messages across health systems. Existing machine learning (ML) and natural language processing (NLP) approaches provide partial solutions but often treat patient-centered communication (PCC) and SDoH as separate tasks or rely on models not well suited to patient-facing language. We introduce PVminer, a domain-adapted NLP framework for structuring patient voice in secure patient-provider communication. PVminer formulates PV detection as a multi-label, multi-class prediction task integrating patient-specific BERT encoders (PV-BERT-base and PV-BERT-large), unsupervised topic modeling for thematic augmentation (PV-Topic-BERT), and fine-tuned classifiers for Code, Subcode, and Combo-level labels. Topic representations are incorporated during fine-tuning and inference to enrich semantic inputs. PVminer achieves strong performance across hierarchical tasks and outperforms biomedical and clinical pre-trained baselines, achieving F1 scores of 82.25% (Code), 80.14% (Subcode), and up to 77.87% (Combo). An ablation study further shows that author identity and topic-based augmentation each contribute meaningful gains. Pre-trained models, source code, and documentation will be publicly released, with annotated datasets available upon request for research use.", "AI": {"tldr": "PVminer\uff1a\u4e00\u4e2a\u7528\u4e8e\u7ed3\u6784\u5316\u60a3\u8005\u8bed\u97f3\u7684\u9886\u57df\u81ea\u9002\u5e94NLP\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6807\u7b7e\u591a\u5206\u7c7b\u9884\u6d4b\u4efb\u52a1\u6574\u5408BERT\u7f16\u7801\u5668\u548c\u4e3b\u9898\u5efa\u6a21\uff0c\u5728\u60a3\u8005-\u63d0\u4f9b\u8005\u901a\u4fe1\u4e2d\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684\u60a3\u8005\u8bed\u97f3\u68c0\u6d4b\u3002", "motivation": "\u60a3\u8005\u751f\u6210\u7684\u6587\u672c\uff08\u5982\u5b89\u5168\u6d88\u606f\u3001\u8c03\u67e5\u3001\u8bbf\u8c08\uff09\u5305\u542b\u4e30\u5bcc\u7684\u60a3\u8005\u8bed\u97f3\u8868\u8fbe\uff0c\u53cd\u6620\u4e86\u6c9f\u901a\u884c\u4e3a\u548c\u793e\u4f1a\u5065\u5eb7\u51b3\u5b9a\u56e0\u7d20\u3002\u4f20\u7edf\u7684\u5b9a\u6027\u7f16\u7801\u65b9\u6cd5\u52b3\u52a8\u5bc6\u96c6\u4e14\u65e0\u6cd5\u6269\u5c55\u5230\u5927\u89c4\u6a21\u60a3\u8005\u6d88\u606f\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5c06\u60a3\u8005\u4e2d\u5fc3\u6c9f\u901a\u548c\u793e\u4f1a\u5065\u5eb7\u51b3\u5b9a\u56e0\u7d20\u89c6\u4e3a\u72ec\u7acb\u4efb\u52a1\uff0c\u6216\u4f9d\u8d56\u4e0d\u9002\u5408\u60a3\u8005\u8bed\u8a00\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51faPVminer\u6846\u67b6\uff0c\u5c06\u60a3\u8005\u8bed\u97f3\u68c0\u6d4b\u5236\u5b9a\u4e3a\u591a\u6807\u7b7e\u591a\u5206\u7c7b\u9884\u6d4b\u4efb\u52a1\uff0c\u6574\u5408\u60a3\u8005\u7279\u5b9aBERT\u7f16\u7801\u5668\uff08PV-BERT-base\u548cPV-BERT-large\uff09\u3001\u7528\u4e8e\u4e3b\u9898\u589e\u5f3a\u7684\u65e0\u76d1\u7763\u4e3b\u9898\u5efa\u6a21\uff08PV-Topic-BERT\uff09\uff0c\u4ee5\u53ca\u7528\u4e8e\u4ee3\u7801\u3001\u5b50\u4ee3\u7801\u548c\u7ec4\u5408\u7ea7\u522b\u6807\u7b7e\u7684\u5fae\u8c03\u5206\u7c7b\u5668\u3002\u5728\u5fae\u8c03\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u878d\u5165\u4e3b\u9898\u8868\u793a\u4ee5\u4e30\u5bcc\u8bed\u4e49\u8f93\u5165\u3002", "result": "PVminer\u5728\u5206\u5c42\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u751f\u7269\u533b\u5b66\u548c\u4e34\u5e8a\u9884\u8bad\u7ec3\u57fa\u7ebf\uff0cF1\u5206\u6570\u8fbe\u523082.25%\uff08\u4ee3\u7801\uff09\u300180.14%\uff08\u5b50\u4ee3\u7801\uff09\u548c\u6700\u9ad877.87%\uff08\u7ec4\u5408\uff09\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\u4f5c\u8005\u8eab\u4efd\u548c\u4e3b\u9898\u589e\u5f3a\u90fd\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "PVminer\u662f\u4e00\u4e2a\u6709\u6548\u7684\u9886\u57df\u81ea\u9002\u5e94NLP\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u60a3\u8005\u751f\u6210\u6587\u672c\u4e2d\u7ed3\u6784\u5316\u63d0\u53d6\u60a3\u8005\u8bed\u97f3\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5e76\u4e3a\u60a3\u8005\u4e2d\u5fc3\u6c9f\u901a\u548c\u793e\u4f1a\u5065\u5eb7\u51b3\u5b9a\u56e0\u7d20\u5206\u6790\u63d0\u4f9b\u4e86\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002\u9884\u8bad\u7ec3\u6a21\u578b\u3001\u6e90\u4ee3\u7801\u548c\u6587\u6863\u5c06\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2602.21193", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21193", "abs": "https://arxiv.org/abs/2602.21193", "authors": ["Renjie Pi", "Grace Lam", "Mohammad Shoeybi", "Pooya Jannaty", "Bryan Catanzaro", "Wei Ping"], "title": "On Data Engineering for Scaling LLM Terminal Capabilities", "comment": null, "summary": "Despite rapid recent progress in the terminal capabilities of large language models, the training data strategies behind state-of-the-art terminal agents remain largely undisclosed. We address this gap through a systematic study of data engineering practices for terminal agents, making two key contributions: (1) Terminal-Task-Gen, a lightweight synthetic task generation pipeline that supports seed-based and skill-based task construction, and (2) a comprehensive analysis of data and training strategies, including filtering, curriculum learning, long context training, and scaling behavior. Our pipeline yields Terminal-Corpus, a large-scale open-source dataset for terminal tasks. Using this dataset, we train Nemotron-Terminal, a family of models initialized from Qwen3(8B, 14B, 32B) that achieve substantial gains on Terminal-Bench 2.0: Nemotron-Terminal-8B improves from 2.5% to 13.0% Nemotron-Terminal-14B improves from 4.0% to 20.2%, and Nemotron-Terminal-32B improves from 3.4% to 27.4%, matching the performance of significantly larger models. To accelerate research in this domain, we open-source our model checkpoints and most of our synthetic datasets at https://huggingface.co/collections/nvidia/nemotron-terminal.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7ec8\u7aef\u4ee3\u7406\u7684\u6570\u636e\u5de5\u7a0b\u65b9\u6cd5\uff0c\u5305\u62ecTerminal-Task-Gen\u4efb\u52a1\u751f\u6210\u7ba1\u9053\u548cTerminal-Corpus\u6570\u636e\u96c6\uff0c\u5e76\u8bad\u7ec3\u4e86Nemotron-Terminal\u6a21\u578b\u7cfb\u5217\uff0c\u5728Terminal-Bench 2.0\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ec8\u7aef\u80fd\u529b\u65b9\u9762\u53d6\u5f97\u4e86\u5feb\u901f\u8fdb\u5c55\uff0c\u4f46\u6700\u5148\u8fdb\u7ec8\u7aef\u4ee3\u7406\u7684\u8bad\u7ec3\u6570\u636e\u7b56\u7565\u4ecd\u7136\u5f88\u5927\u7a0b\u5ea6\u4e0a\u672a\u516c\u5f00\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7cfb\u7edf\u7814\u7a76\u7ec8\u7aef\u4ee3\u7406\u7684\u6570\u636e\u5de5\u7a0b\u5b9e\u8df5\u3002", "method": "\u63d0\u51fa\u4e86Terminal-Task-Gen\u8f7b\u91cf\u7ea7\u5408\u6210\u4efb\u52a1\u751f\u6210\u7ba1\u9053\uff0c\u652f\u6301\u57fa\u4e8e\u79cd\u5b50\u548c\u57fa\u4e8e\u6280\u80fd\u7684\u4efb\u52a1\u6784\u5efa\uff1b\u5206\u6790\u4e86\u6570\u636e\u8fc7\u6ee4\u3001\u8bfe\u7a0b\u5b66\u4e60\u3001\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u548c\u7f29\u653e\u884c\u4e3a\u7b49\u6570\u636e\u4e0e\u8bad\u7ec3\u7b56\u7565\uff1b\u521b\u5efa\u4e86Terminal-Corpus\u5927\u89c4\u6a21\u5f00\u6e90\u7ec8\u7aef\u4efb\u52a1\u6570\u636e\u96c6\u3002", "result": "\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u8bad\u7ec3\u4e86\u57fa\u4e8eQwen3(8B, 14B, 32B)\u521d\u59cb\u5316\u7684Nemotron-Terminal\u6a21\u578b\u7cfb\u5217\uff0c\u5728Terminal-Bench 2.0\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\uff1a8B\u6a21\u578b\u4ece2.5%\u63d0\u5347\u81f313.0%\uff0c14B\u6a21\u578b\u4ece4.0%\u63d0\u5347\u81f320.2%\uff0c32B\u6a21\u578b\u4ece3.4%\u63d0\u5347\u81f327.4%\uff0c\u6027\u80fd\u5339\u914d\u4e86\u66f4\u5927\u7684\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7ec8\u7aef\u4ee3\u7406\u9886\u57df\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u6570\u636e\u5de5\u7a0b\u65b9\u6cd5\uff0c\u5f00\u6e90\u4e86\u6a21\u578b\u68c0\u67e5\u70b9\u548c\u5927\u90e8\u5206\u5408\u6210\u6570\u636e\u96c6\uff0c\u5c06\u52a0\u901f\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002"}}
