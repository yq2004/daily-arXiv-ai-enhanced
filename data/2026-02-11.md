<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 63]
- [cs.IR](#cs.IR) [Total: 12]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Overview of PAN 2026: Voight-Kampff Generative AI Detection, Text Watermarking, Multi-Author Writing Style Analysis, Generative Plagiarism Detection, and Reasoning Trajectory Detection](https://arxiv.org/abs/2602.09147)
*Janek Bevendorff,Maik Fröbe,André Greiner-Petter,Andreas Jakoby,Maximilian Mayerl,Preslav Nakov,Henry Plutz,Martin Potthast,Benno Stein,Minh Ngoc Ta,Yuxia Wang,Eva Zangerle*

Main category: cs.CL

TL;DR: PAN 2026研讨会包含五个计算文体学和文本取证任务：生成式AI检测、文本水印、多作者写作风格分析、生成式抄袭检测和推理轨迹检测，通过Docker容器进行可复现评估。


<details>
  <summary>Details</summary>
Motivation: 推进计算文体学和文本取证领域的发展，通过客观、可复现的评估方法来应对生成式AI、文本水印、多作者分析、抄袭检测和推理轨迹等新兴挑战。

Method: 通过TIRA实验平台，采用Docker容器提交软件方案，确保实验的可复现性。包含五个具体任务：Voight-Kampff生成式AI检测、文本水印、多作者写作风格分析、生成式抄袭检测和推理轨迹检测。

Result: 自2012年以来，通过TIRA平台已收到超过1,100份Docker容器提交，建立了大规模、可复现的评估框架，为五个核心任务提供了系统性的测试平台。

Conclusion: PAN 2026延续了通过标准化、可复现评估推动计算文体学和文本取证发展的传统，针对新兴挑战设计了五个具体任务，为相关研究提供了系统性的评估框架。

Abstract: The goal of the PAN workshop is to advance computational stylometry and text forensics via objective and reproducible evaluation. In 2026, we run the following five tasks: (1) Voight-Kampff Generative AI Detection, particularly in mixed and obfuscated authorship scenarios, (2) Text Watermarking, a new task that aims to find new and benchmark the robustness of existing text watermarking schemes, (3) Multi-author Writing Style Analysis, a continued task that aims to find positions of authorship change, (4) Generative Plagiarism Detection, a continued task that targets source retrieval and text alignment between generated text and source documents, and (5) Reasoning Trajectory Detection, a new task that deals with source detection and safety detection of LLM-generated or human-written reasoning trajectories. As in previous years, PAN invites software submissions as easy-to-reproduce Docker containers for most of the tasks. Since PAN 2012, more than 1,100 submissions have been made this way via the TIRA experimentation platform.

</details>


### [2] [Measuring Inclusion in Interaction: Inclusion Analytics for Human-AI Collaborative Learning](https://arxiv.org/abs/2602.09269)
*Jaeyoon Choi,Nia Nixon*

Main category: cs.CL

TL;DR: 提出了一个基于话语分析的包容性分析框架，用于实时监测人机协作问题解决中的包容性动态过程。


<details>
  <summary>Details</summary>
Motivation: 当前AI和教育领域的包容性、公平性和可及性评估通常依赖于粗粒度的样本描述或事后自我报告，无法捕捉协作问题解决过程中时刻变化的包容性动态。

Method: 提出了"包容性分析"的话语分析框架，从三个维度（参与公平性、情感氛围、认知公平性）概念化包容性，并开发了可扩展的交互层面测量方法。使用模拟对话和人机协作实验数据进行验证。

Result: 包容性分析能够揭示参与模式、关系动态和想法采纳等模式，这些在聚合性或事后评估中通常是不可见的。展示了该框架如何使包容性的动态过程在分析上变得可见。

Conclusion: 这项工作代表了向过程导向的包容性测量方法迈出的第一步，为人机协作学习环境中的包容性评估提供了新的分析视角。

Abstract: Inclusion, equity, and access are widely valued in AI and education, yet are often assessed through coarse sample descriptors or post-hoc self-reports that miss how inclusion is shaped moment by moment in collaborative problem solving (CPS). In this proof-of-concept paper, we introduce inclusion analytics, a discourse-based framework for examining inclusion as a dynamic, interactional process in CPS. We conceptualize inclusion along three complementary dimensions -- participation equity, affective climate, and epistemic equity -- and demonstrate how these constructs can be made analytically visible using scalable, interaction-level measures. Using both simulated conversations and empirical data from human-AI teaming experiments, we illustrate how inclusion analytics can surface patterns of participation, relational dynamics, and idea uptake that remain invisible to aggregate or post-hoc evaluations. This work represents an initial step toward process-oriented approaches to measuring inclusion in human-AI collaborative learning environments.

</details>


### [3] [Effective Reasoning Chains Reduce Intrinsic Dimensionality](https://arxiv.org/abs/2602.09276)
*Archiki Prasad,Mandar Joshi,Kenton Lee,Mohit Bansal,Peter Shaw*

Main category: cs.CL

TL;DR: 该研究提出用内在维度作为量化指标来评估不同思维链推理策略的有效性，发现有效的推理策略能降低任务的内在维度，从而提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 尽管思维链推理及其变体显著提升了语言模型在复杂推理任务上的性能，但不同策略如何促进泛化的具体机制仍不清楚。现有解释多集中于增加测试时计算或提供结构指导，但难以建立这些因素与泛化性能之间的量化联系。

Method: 提出将内在维度作为量化指标来表征推理链的有效性。内在维度定义为模型达到特定任务精度阈值所需的最小维度数。通过固定模型架构、改变任务表述（采用不同推理策略），分析不同策略对任务内在维度的影响。

Result: 在GSM8K数据集上使用Gemma-3 1B和4B模型验证，发现有效的推理策略能显著降低任务的内在维度。内在维度与泛化性能（包括分布内和分布外数据）呈强负相关关系。

Conclusion: 有效的思维链推理通过更好地压缩任务（使用更少参数）来促进学习，内在维度为此提供了新的量化指标，有助于深入分析推理过程。

Abstract: Chain-of-thought (CoT) reasoning and its variants have substantially improved the performance of language models on complex reasoning tasks, yet the precise mechanisms by which different strategies facilitate generalization remain poorly understood. While current explanations often point to increased test-time computation or structural guidance, establishing a consistent, quantifiable link between these factors and generalization remains challenging. In this work, we identify intrinsic dimensionality as a quantitative measure for characterizing the effectiveness of reasoning chains. Intrinsic dimensionality quantifies the minimum number of model dimensions needed to reach a given accuracy threshold on a given task. By keeping the model architecture fixed and varying the task formulation through different reasoning strategies, we demonstrate that effective reasoning strategies consistently reduce the intrinsic dimensionality of the task. Validating this on GSM8K with Gemma-3 1B and 4B, we observe a strong inverse correlation between the intrinsic dimensionality of a reasoning strategy and its generalization performance on both in-distribution and out-of-distribution data. Our findings suggest that effective reasoning chains facilitate learning by better compressing the task using fewer parameters, offering a new quantitative metric for analyzing reasoning processes.

</details>


### [4] [Don't Shoot The Breeze: Topic Continuity Model Using Nonlinear Naive Bayes With Attention](https://arxiv.org/abs/2602.09312)
*Shu-Ting Pi,Pradeep Bagavan,Yejia Li,Disha,Qun Liu*

Main category: cs.CL

TL;DR: 提出基于朴素贝叶斯和注意力机制的话题连续性模型，用于评估LLM聊天机器人响应是否保持话题连贯性，可处理任意长度对话且具线性时间复杂度。


<details>
  <summary>Details</summary>
Motivation: LLM作为聊天机器人在商业场景中常出现话题突然转换的问题，导致用户体验不佳和计算资源浪费，需要有效的话题连续性评估方法。

Method: 将自然语言理解模型扩展为可量化形式，采用朴素贝叶斯方法，引入注意力机制和对数非线性来增强话题连续性捕捉能力，形成可解释的分析公式。

Result: 模型在实验中始终优于传统方法，特别是在处理长而复杂的对话时表现突出，注意力机制显著提升了复杂对话中话题连续性的识别能力。

Conclusion: 该模型为LLM的责任和可解释使用提供了机会，能够无缝处理任意长度对话，具有线性时间复杂度，确保话题连续性评估的高效性和可解释性。

Abstract: Utilizing Large Language Models (LLM) as chatbots in diverse business scenarios often presents the challenge of maintaining topic continuity. Abrupt shifts in topics can lead to poor user experiences and inefficient utilization of computational resources. In this paper, we present a topic continuity model aimed at assessing whether a response aligns with the initial conversation topic. Our model is built upon the expansion of the corresponding natural language understanding (NLU) model into quantifiable terms using a Naive Bayes approach. Subsequently, we have introduced an attention mechanism and logarithmic nonlinearity to enhance its capability to capture topic continuity. This approach allows us to convert the NLU model into an interpretable analytical formula. In contrast to many NLU models constrained by token limits, our proposed model can seamlessly handle conversations of any length with linear time complexity. Furthermore, the attention mechanism significantly improves the model's ability to identify topic continuity in complex conversations. According to our experiments, our model consistently outperforms traditional methods, particularly in handling lengthy and intricate conversations. This unique capability offers us an opportunity to ensure the responsible and interpretable use of LLMs.

</details>


### [5] [Beyond Uniform Credit: Causal Credit Assignment for Policy Optimization](https://arxiv.org/abs/2602.09331)
*Mykola Khandoga,Rui Yuan,Vinay Kumar Sankarapu*

Main category: cs.CL

TL;DR: 提出反事实重要性加权方法，通过掩码推理段落并测量答案概率下降来为语言模型推理中的关键计算步骤分配更高权重，相比均匀信用分配方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型推理的梯度策略方法（如GRPO和DAPO）对所有生成token分配均匀信用，导致无关填充词（如"让我想想"）与关键计算步骤（如"23 + 45 = 68"）获得相同梯度更新，这显然不合理。

Method: 提出反事实重要性加权：掩码推理段落，测量答案概率的下降程度，然后在策略梯度更新中根据此重要性相应上调token的权重。该方法无需辅助模型或外部标注，直接从策略模型自身的概率变化中估计重要性。

Result: 在GSM8K数据集上对Qwen和Llama家族的三个模型进行实验，结果显示该方法相比均匀基线方法带来一致改进，并更快收敛到相同准确率。反重要性信号会损害性能，证实该方法捕捉到了真实的因果结构而非噪声。分析显示该方法能正确优先考虑计算步骤而非脚手架文本。

Conclusion: 该方法为语言模型推理中的信用分配问题提供了有效解决方案，将反事实重要性加权确立为进一步研究的基础而非完整解决方案，为后续研究奠定了基础。

Abstract: Policy gradient methods for language model reasoning, such as GRPO and DAPO, assign uniform credit to all generated tokens - the filler phrase "Let me think" receives the same gradient update as the critical calculation "23 + 45 = 68." We propose counterfactual importance weighting: mask reasoning spans, measure the drop in answer probability, and upweight tokens accordingly during policy gradient updates. Our method requires no auxiliary models or external annotation, instead importance is estimated directly from the policy model's own probability shifts. Experiments on GSM8K across three models spanning the Qwen and Llama families demonstrate consistent improvements over uniform baselines and faster convergence to equivalent accuracy. Inverting the importance signal hurts performance, confirming we capture genuine causal structure rather than noise. Analysis shows the method correctly prioritizes calculation steps over scaffolding text. We view these findings as establishing counterfactual importance weighting as a foundation for further research rather than a complete solution.

</details>


### [6] [FM SO.P: A Progressive Task Mixture Framework with Automatic Evaluation for Cross-Domain SOP Understanding](https://arxiv.org/abs/2602.09336)
*Siyuan Huang,Ziyu Wang,Chao Pan,Han Zhao*

Main category: cs.CL

TL;DR: FM SO.P是一个通过渐进式任务混合和自动多智能体评估系统来提升语言模型理解标准操作流程能力的新方法，在多个领域显著超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 标准操作流程在企业运营中至关重要，但现有语言模型在SOP理解和跨领域泛化方面存在困难。现有方法无法区分SOP所需的术语精确性、顺序排序和约束推理等推理能力。

Method: 提出FM SO.P方法，包含两个创新：1) 渐进式任务混合，通过三个阶段构建能力：概念消歧（术语精确性）、动作序列理解（程序正确性）、场景感知图推理（条件逻辑）；2) 自动多智能体评估系统，包含三个智能体，自适应生成评估标准、分层测试集和评分标准。

Result: 在SOPBench的七个领域（银行、DMV、医疗、市场、大学、图书馆、酒店）评估中，FM SO.P的32B模型达到48.3%通过率，开源7B模型达到34.3%通过率，与Qwen-2.5-72B-Instruct基线（34.4%）相当，但参数量减少10倍。

Conclusion: FM SO.P通过渐进式任务构建和自适应评估系统，显著提升了语言模型对标准操作流程的理解能力，实现了跨领域的高效泛化。

Abstract: Standard Operating Procedures (SOPs) are critical for enterprise operations, yet existing language models struggle with SOP understanding and cross-domain generalization. Current methods fail because joint training cannot differentiate between reasoning capabilities that SOP requires: terminology precision, sequential ordering, and constraint reasoning. We propose FM SO.P, solving these challenges through two novelties. First, we introduce progressive task mixtures that build capabilities by stages across three task types with cumulative data: concept disambiguation for terminology precision, action sequence understanding for procedural correctness, and scenario-aware graph reasoning for conditional logic. Second, we propose an automatic multi-agent evaluation system consisting of three agents that adaptively generate rubrics, stratified test sets, and rubric scoring, adapting to domains (e.g., temporal constraints for DMV, regulatory compliance for banking). Evaluated on SOPBench across seven domains (Bank, DMV, Healthcare, Market, University, Library, Hotel), FM SO.P achieves 48.3\% pass rate with our 32B model and 34.3\% with our opensource 7B model, matching Qwen-2.5-72B-Instruct baseline (34.4\%) with 10x fewer parameters.

</details>


### [7] [Understanding Risk and Dependency in AI Chatbot Use from User Discourse](https://arxiv.org/abs/2602.09339)
*Jianfeng Zhu,Karin G. Coifman,Ruoming Jin*

Main category: cs.CL

TL;DR: 基于Reddit社区的大规模计算主题分析，识别了AI相关心理风险的五个体验维度，其中自我调节困难最为普遍，恐惧主要集中在自主权、控制和技术风险方面。


<details>
  <summary>Details</summary>
Motivation: 生成式AI系统日益融入日常生活，但关于AI使用心理风险如何产生、被体验和用户如何调节的实证理解仍然有限。研究旨在通过真实世界用户话语理解AI安全如何被感知和情感体验。

Method: 使用基于Braun和Clarke反思框架的多智能体、LLM辅助主题分析方法，对2023-2025年间从r/AIDangers和r/ChatGPTAddiction两个Reddit社区收集的帖子进行大规模计算主题分析。使用BERT情感分类器进行情感标注，并可视化不同维度的情感特征。

Result: 识别了14个重复出现的主题类别，并综合为5个高阶体验维度。情感分析显示自我调节困难是最普遍的维度，恐惧情感主要集中在自主权、控制和技术风险相关问题上。

Conclusion: 研究提供了基于真实用户体验的AI相关心理风险早期实证证据，揭示了实验室或推测性情境之外AI安全如何被感知和情感体验，为未来AI安全研究、评估和负责任治理提供了基础。

Abstract: Generative AI systems are increasingly embedded in everyday life, yet empirical understanding of how psychological risk associated with AI use emerges, is experienced, and is regulated by users remains limited. We present a large-scale computational thematic analysis of posts collected between 2023 and 2025 from two Reddit communities, r/AIDangers and r/ChatbotAddiction, explicitly focused on AI-related harm and distress. Using a multi-agent, LLM-assisted thematic analysis grounded in Braun and Clarke's reflexive framework, we identify 14 recurring thematic categories and synthesize them into five higher-order experiential dimensions. To further characterize affective patterns, we apply emotion labeling using a BERT-based classifier and visualize emotional profiles across dimensions. Our findings reveal five empirically derived experiential dimensions of AI-related psychological risk grounded in real-world user discourse, with self-regulation difficulties emerging as the most prevalent and fear concentrated in concerns related to autonomy, control, and technical risk. These results provide early empirical evidence from lived user experience of how AI safety is perceived and emotionally experienced outside laboratory or speculative contexts, offering a foundation for future AI safety research, evaluation, and responsible governance.

</details>


### [8] [Digital Linguistic Bias in Spanish: Evidence from Lexical Variation in LLMs](https://arxiv.org/abs/2602.09346)
*Yoshifumi Kawasaki*

Main category: cs.CL

TL;DR: LLMs在西班牙语地理词汇变异识别中存在系统性差异，能较好识别西班牙、赤道几内亚、墨西哥&中美洲、拉普拉塔河区域的词汇变异，但难以区分智利变体。


<details>
  <summary>Details</summary>
Motivation: 西班牙语存在显著区域变异，需要评估大型语言模型(LLMs)对这种地理词汇变异的捕捉能力，探讨数字语言偏见问题。

Method: 将LLMs作为虚拟信息提供者，使用专家策划的西班牙语词汇变异数据库，通过是非题和选择题两种调查式问题格式，覆盖21个西班牙语国家的900多个词汇项。

Result: LLMs对西班牙语变体的识别存在系统性差异：西班牙、赤道几内亚、墨西哥&中美洲、拉普拉塔河区域词汇识别准确率较高，智利变体最难区分；国家层面数字资源量不能解释这些性能差异。

Conclusion: 本研究通过细粒度大规模评估，推进了对LLMs中方言知识的实证理解，为西班牙语数字语言偏见讨论提供了新证据，表明影响方言表征的因素超越了数据数量。

Abstract: This study examines the extent to which Large Language Models (LLMs) capture geographic lexical variation in Spanish, a language that exhibits substantial regional variation. Treating LLMs as virtual informants, we probe their dialectal knowledge using two survey-style question formats: Yes-No questions and multiple-choice questions. To this end, we exploited a large-scale, expert-curated database of Spanish lexical variation. Our evaluation covers more than 900 lexical items across 21 Spanish-speaking countries and is conducted at both the country and dialectal area levels. Across both evaluation formats, the results reveal systematic differences in how LLMs represent Spanish language varieties. Lexical variation associated with Spain, Equatorial Guinea, Mexico & Central America, and the La Plata River is recognized more accurately by the models, while the Chilean variety proves particularly difficult for the models to distinguish. Importantly, differences in the volume of country-level digital resources do not account for these performance patterns, suggesting that factors beyond data quantity shape dialectal representation in LLMs. By providing a fine-grained, large-scale evaluation of geographic lexical variation, this work advances empirical understanding of dialectal knowledge in LLMs and contributes new evidence to discussions of Digital Linguistic Bias in Spanish.

</details>


### [9] [Unsupervised Cross-Lingual Part-of-Speech Tagging with Monolingual Corpora Only](https://arxiv.org/abs/2602.09366)
*Jianyu Zheng*

Main category: cs.CL

TL;DR: 提出一个完全无监督的跨语言词性标注框架，仅使用单语语料库，通过无监督神经机器翻译系统构建伪平行句对，并采用多源投影技术提升标注效果。


<details>
  <summary>Details</summary>
Motivation: 低资源语言缺乏词性标注数据，现有方法依赖平行语料库，但许多低资源语言没有平行语料。需要仅使用单语语料库的无监督跨语言词性标注方法。

Method: 1) 使用无监督神经机器翻译系统将高资源语言句子翻译成低资源语言，构建伪平行句对；2) 基于词对齐进行标准投影训练目标语言词性标注器；3) 提出多源投影技术校准目标端投影的词性标签。

Result: 在28个语言对上进行评估，涵盖4种源语言和7种目标语言。实验结果显示：1) 方法性能可与基于平行句对的基线方法相当，甚至在某些目标语言上超过基线；2) 多源投影技术进一步提升性能，平均比先前方法提高1.3%。

Conclusion: 提出的完全无监督跨语言词性标注框架仅需单语语料库，通过无监督神经机器翻译构建伪平行数据，结合多源投影技术，在缺乏平行语料的低资源语言上取得了与基于平行语料方法相当甚至更好的性能。

Abstract: Due to the scarcity of part-of-speech annotated data, existing studies on low-resource languages typically adopt unsupervised approaches for POS tagging. Among these, POS tag projection with word alignment method transfers POS tags from a high-resource source language to a low-resource target language based on parallel corpora, making it particularly suitable for low-resource language settings. However, this approach relies heavily on parallel corpora, which are often unavailable for many low-resource languages. To overcome this limitation, we propose a fully unsupervised cross-lingual part-of-speech(POS) tagging framework that relies solely on monolingual corpora by leveraging unsupervised neural machine translation(UNMT) system. This UNMT system first translates sentences from a high-resource language into a low-resource one, thereby constructing pseudo-parallel sentence pairs. Then, we train a POS tagger for the target language following the standard projection procedure based on word alignments. Moreover, we propose a multi-source projection technique to calibrate the projected POS tags on the target side, enhancing to train a more effective POS tagger. We evaluate our framework on 28 language pairs, covering four source languages (English, German, Spanish and French) and seven target languages (Afrikaans, Basque, Finnis, Indonesian, Lithuanian, Portuguese and Turkish). Experimental results show that our method can achieve performance comparable to the baseline cross-lingual POS tagger with parallel sentence pairs, and even exceeds it for certain target languages. Furthermore, our proposed multi-source projection technique further boosts performance, yielding an average improvement of 1.3% over previous methods.

</details>


### [10] [AgentSkiller: Scaling Generalist Agent Intelligence through Semantically Integrated Cross-Domain Data Synthesis](https://arxiv.org/abs/2602.09372)
*Zexu Sun,Bokai Ji,Hengyi Cai,Shuaiqiang Wang,Lei Wang,Guangxia Li,Xu Chen*

Main category: cs.CL

TL;DR: AgentSkiller是一个完全自动化的框架，用于生成高质量、长视野的多轮交互数据，通过模拟现实跨领域任务来增强LLM代理的工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理在解决现实世界问题时面临高质量、长视野训练数据稀缺的瓶颈。现有方法要么收集受隐私限制的API日志，要么生成缺乏多样性的脚本化交互，难以满足能力扩展所需的数据要求。

Method: 采用基于DAG的架构确保确定性和可恢复性；构建领域本体和以人为中心的实体图；通过Service Blueprints定义工具接口；创建具有一致数据库和严格领域策略的环境；跨领域融合机制链接服务模拟复杂任务；通过执行验证筛选解决方案路径；使用基于角色的模拟器生成查询实现自动化部署。

Result: 成功合成了约11,000个交互样本，实验结果表明，在该数据集上训练的模型在函数调用能力上相比基线有显著提升，特别是在更大参数规模下表现更优。

Conclusion: AgentSkiller框架能够自动生成高质量、多样化的多轮交互数据，有效解决了LLM代理训练数据稀缺问题，为扩展代理的通用智能能力提供了可行方案。

Abstract: Large Language Model agents demonstrate potential in solving real-world problems via tools, yet generalist intelligence is bottlenecked by scarce high-quality, long-horizon data. Existing methods collect privacy-constrained API logs or generate scripted interactions lacking diversity, which struggle to produce data requisite for scaling capabilities. We propose AgentSkiller, a fully automated framework synthesizing multi-turn interaction data across realistic, semantically linked domains. It employs a DAG-based architecture with explicit state transitions to ensure determinism and recoverability. The pipeline builds a domain ontology and Person-Centric Entity Graph, defines tool interfaces via Service Blueprints for Model Context Protocol servers, and populates environments with consistent databases and strict Domain Policies. A cross-domain fusion mechanism links services to simulate complex tasks. Finally, the pipeline creates user tasks by verifying solution paths, filtering via execution-based validation, and generating queries using a Persona-based Simulator for automated rollout. This produces reliable environments with clear state changes. To demonstrate effectiveness, we synthesized $\approx$ 11K interaction samples; experimental results indicate that models trained on this dataset achieve significant improvements on function calling over baselines, particularly in larger parameter regimes.

</details>


### [11] [AfriNLLB: Efficient Translation Models for African Languages](https://arxiv.org/abs/2602.09373)
*Yasmin Moslem,Aman Kassahun Wassie,Amanuel Gizachew Abebe*

Main category: cs.CL

TL;DR: AfriNLLB是一个针对非洲语言的高效翻译模型系列，通过模型压缩和知识蒸馏技术，在保持性能的同时显著提升推理速度，支持15种语言对（30个翻译方向）。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是解决非洲语言翻译模型在资源受限环境中的高效部署问题。现有的大规模翻译模型通常计算资源需求高，难以在资源有限的非洲地区实际应用，因此需要开发轻量级但性能相当的解决方案。

Method: 方法基于NLLB-200 600M模型，采用迭代层剪枝和量化技术进行模型压缩，然后在精心整理的非洲语言平行语料库上进行微调，并使用知识蒸馏技术从更大的教师模型学习。

Result: 评估结果显示，AfriNLLB模型在性能上与基线模型相当，同时推理速度显著更快。研究团队发布了两个版本：Transformers版本支持进一步微调，CTranslate2版本用于高效推理，并公开了所有训练数据。

Conclusion: AfriNLLB成功开发了一系列轻量级非洲语言翻译模型，在保持翻译质量的同时实现了高效推理，为资源受限环境中的非洲语言翻译部署提供了实用解决方案，并通过开源模型和数据促进了相关研究的进一步发展。

Abstract: In this work, we present AfriNLLB, a series of lightweight models for efficient translation from and into African languages. AfriNLLB supports 15 language pairs (30 translation directions), including Swahili, Hausa, Yoruba, Amharic, Somali, Zulu, Lingala, Afrikaans, Wolof, and Egyptian Arabic, as well as other African Union official languages such as Arabic (MSA), French, Portuguese, and Spanish. Our training data covers bidirectional translation between English and 13 languages, and between French and two languages (Lingala and Wolof).
  AfriNLLB models are based on NLLB-200 600M, which we compress using iterative layer pruning and quantization. We fine-tune the pruned models on parallel corpora we curated for African languages, employing knowledge distillation from a larger teacher model. Our work aims at enabling efficient deployment of translation models for African languages in resource-constrained settings.
  Our evaluation results demonstrate that AfriNLLB models achieve performance comparable to the baseline while being significantly faster. We release two versions of the AfriNLLB models, a Transformers version that allows further fine-tuning and a CTranslate2 version for efficient inference. Moreover, we release all the training data that we used for fine-tuning the baseline and pruned models to facilitate further research.

</details>


### [12] [BiasScope: Towards Automated Detection of Bias in LLM-as-a-Judge Evaluation](https://arxiv.org/abs/2602.09383)
*Peng Lai,Zhihao Ou,Yong Wang,Longyue Wang,Jian Yang,Yun Chen,Guanhua Chen*

Main category: cs.CL

TL;DR: BiasScope是一个自动发现LLM评估中潜在偏见的框架，能发现不同模型家族和规模的偏见，并基于此创建了更具挑战性的JudgeBench-Pro基准


<details>
  <summary>Details</summary>
Motivation: 当前LLM-as-a-Judge评估存在偏见问题，现有研究主要关注已知偏见，缺乏对潜在未知偏见的自动系统探索，这限制了评估的鲁棒性和可靠性

Method: 提出了BiasScope框架，这是一个由LLM驱动的自动化框架，能够大规模发现模型评估中可能出现的潜在偏见，克服了依赖人工努力和预定义偏见列表的局限性

Result: BiasScope能够发现不同模型家族和规模的潜在偏见，在JudgeBench数据集上验证了其通用性和有效性。基于BiasScope创建了JudgeBench-Pro基准，即使是强大的LLM评估器在该基准上的错误率也超过50%

Conclusion: BiasScope将偏见发现从被动过程转变为主动全面的自动探索，揭示了LLM-as-a-Judge评估中存在的严重偏见问题，强调了加强评估鲁棒性和减轻潜在偏见的紧迫性

Abstract: LLM-as-a-Judge has been widely adopted across various research and practical applications, yet the robustness and reliability of its evaluation remain a critical issue. A core challenge it faces is bias, which has primarily been studied in terms of known biases and their impact on evaluation outcomes, while automated and systematic exploration of potential unknown biases is still lacking. Nevertheless, such exploration is crucial for enhancing the robustness and reliability of evaluations. To bridge this gap, we propose BiasScope, a LLM-driven framework for automatically and at scale discovering potential biases that may arise during model evaluation. BiasScope can uncover potential biases across different model families and scales, with its generality and effectiveness validated on the JudgeBench dataset. It overcomes the limitations of existing approaches, transforming bias discovery from a passive process relying on manual effort and predefined bias lists into an active and comprehensive automated exploration. Moreover, based on BiasScope, we propose JudgeBench-Pro, an extended version of JudgeBench and a more challenging benchmark for evaluating the robustness of LLM-as-a-judge. Strikingly, even powerful LLMs as evaluators show error rates above 50\% on JudgeBench-Pro, underscoring the urgent need to strengthen evaluation robustness and to mitigate potential biases further.

</details>


### [13] [Contractual Deepfakes: Can Large Language Models Generate Contracts?](https://arxiv.org/abs/2602.09384)
*Eliza Mik*

Main category: cs.CL

TL;DR: 该论文认为大语言模型(LLMs)无法真正理解法律语言和推理，生成合同存在严重缺陷，挑战了LLMs威胁法律行业生存的简单假设。


<details>
  <summary>Details</summary>
Motivation: 针对当前普遍认为LLMs可以协助起草合同的观点，作者认为这种想法是不合理的，需要澄清LLMs在法律应用中的局限性。

Method: 通过理论分析，对比LLMs的文本生成机制与法律合同起草的实际需求，指出统计语言模式与法律推理之间的本质差异。

Result: LLMs只能生成表面看似合理的通用合同文件，但实际可能产生条款不一致的无效文件或不适合特定交易的合同，存在严重实用性问题。

Conclusion: LLMs无法真正替代法律专业人士，它们缺乏对语言意义、上下文和推理的理解，不应被过度推崇为威胁法律行业生存的技术。

Abstract: Notwithstanding their unprecedented ability to generate text, LLMs do not understand the meaning of words, have no sense of context and cannot reason. Their output constitutes an approximation of statistically dominant word patterns. And yet, the drafting of contracts is often presented as a typical legal task that could be facilitated by this technology. This paper seeks to put an end to such unreasonable ideas. Predicting words differs from using language in the circumstances of specific transactions and reconstituting common contractual phrases differs from reasoning about the law. LLMs seem to be able to generate generic and superficially plausible contractual documents. In the cold light of day, such documents may turn out to be useless assemblages of inconsistent provisions or contracts that are enforceable but unsuitable for a given transaction. This paper casts a shadow on the simplistic assumption that LLMs threaten the continued viability of the legal industry.

</details>


### [14] [Effective vocabulary expanding of multilingual language models for extremely low-resource languages](https://arxiv.org/abs/2602.09388)
*Jianyu Zheng*

Main category: cs.CL

TL;DR: 提出一种通过扩展词汇表并利用双语词典初始化新词表示的方法，将mPLMs扩展到未支持的低资源语言，在POS和NER任务上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多语言预训练语言模型（mPLMs）虽然对许多低资源语言有益，但很少有工作研究如何将这些模型扩展到之前完全不支持的低资源语言。当前的研究主要集中在已有支持语言的继续预训练上。

Method: 1. 使用目标语言语料库扩展模型词汇表；2. 筛选出偏向源语言（如英语）的原始词汇子集；3. 利用双语词典初始化扩展词汇的表示；4. 基于扩展词汇表示，使用目标语言语料库继续预训练mPLMs。

Result: 在POS标注和NER任务上，该方法比使用随机初始化扩展词汇的基线方法分别提升了0.54%和2.60%。同时，该方法在训练语料选择上表现出高鲁棒性，且继续预训练后模型在源语言上的性能没有下降。

Conclusion: 通过扩展词汇表并利用双语词典初始化新词表示的方法，能够有效将mPLMs扩展到未支持的低资源语言，在保持源语言性能的同时提升目标语言任务表现，且对训练语料选择具有鲁棒性。

Abstract: Multilingual pre-trained language models(mPLMs) offer significant benefits for many low-resource languages. To further expand the range of languages these models can support, many works focus on continued pre-training of these models. However, few works address how to extend mPLMs to low-resource languages that were previously unsupported. To tackle this issue, we expand the model's vocabulary using a target language corpus. We then screen out a subset from the model's original vocabulary, which is biased towards representing the source language(e.g. English), and utilize bilingual dictionaries to initialize the representations of the expanded vocabulary. Subsequently, we continue to pre-train the mPLMs using the target language corpus, based on the representations of these expanded vocabulary. Experimental results show that our proposed method outperforms the baseline, which uses randomly initialized expanded vocabulary for continued pre-training, in POS tagging and NER tasks, achieving improvements by 0.54% and 2.60%, respectively. Furthermore, our method demonstrates high robustness in selecting the training corpora, and the models' performance on the source language does not degrade after continued pre-training.

</details>


### [15] [Are Language Models Sensitive to Morally Irrelevant Distractors?](https://arxiv.org/abs/2602.09416)
*Andrew Shaw,Christina Hahn,Catherine Rasgaitis,Yash Mishra,Alisa Liu,Natasha Jaques,Yulia Tsvetkov,Amy X. Zhang*

Main category: cs.CL

TL;DR: 大型语言模型的道德判断会受到与道德无关的情境因素影响，表现出类似人类的道德认知偏差


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在高风险场景中的广泛应用，确保其行为与人类价值观一致变得至关重要。现有道德基准测试假设LLM报告相对稳定的道德偏好，但人类道德心理学研究表明，人类道德判断会受到与道德无关的情境因素影响，这挑战了道德判断稳定性的假设。

Method: 从现有心理学数据集中收集了60个"道德干扰因素"（情感性图像和叙述），这些干扰因素与所呈现情境没有道德相关性。将这些干扰因素注入现有的道德基准测试中，测量它们对LLM回答的影响。

Result: 道德干扰因素能够使LLM的道德判断发生超过30%的偏移，即使在低模糊性场景中也是如此。这表明LLM表现出类似人类的认知道德偏差。

Conclusion: LLM的道德判断容易受到与道德无关的情境因素影响，这凸显了需要进行更多情境化的道德评估，以及对LLM进行更细致的认知道德建模的必要性。

Abstract: With the rapid development and uptake of large language models (LLMs) across high-stakes settings, it is increasingly important to ensure that LLMs behave in ways that align with human values. Existing moral benchmarks prompt LLMs with value statements, moral scenarios, or psychological questionnaires, with the implicit underlying assumption that LLMs report somewhat stable moral preferences. However, moral psychology research has shown that human moral judgements are sensitive to morally irrelevant situational factors, such as smelling cinnamon rolls or the level of ambient noise, thereby challenging moral theories that assume the stability of human moral judgements. Here, we draw inspiration from this "situationist" view of moral psychology to evaluate whether LLMs exhibit similar cognitive moral biases to humans. We curate a novel multimodal dataset of 60 "moral distractors" from existing psychological datasets of emotionally-valenced images and narratives which have no moral relevance to the situation presented. After injecting these distractors into existing moral benchmarks to measure their effects on LLM responses, we find that moral distractors can shift the moral judgements of LLMs by over 30% even in low-ambiguity scenarios, highlighting the need for more contextual moral evaluations and more nuanced cognitive moral modeling of LLMs.

</details>


### [16] [Breaking the Pre-Sampling Barrier: Activation-Informed Difficulty-Aware Self-Consistency](https://arxiv.org/abs/2602.09438)
*Taewoong Yoon,Geunyeong Jeong,Geon Park,Sihyeong Yeom,Harksoo Kim*

Main category: cs.CL

TL;DR: ACTSC是一种利用LLM内部神经元激活信号来估计问题难度并动态调整自一致性采样数量的方法，在保持准确性的同时显著降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 传统自一致性方法需要大量采样导致推理成本高，而现有的难度自适应方法（DSC）需要额外的模型调用和预采样来估计难度，计算开销大且需要针对每个数据集重复这一过程。

Method: 提出ACTSC方法，利用前馈网络神经元激活中反映的内部难度信号构建轻量级难度估计探针，无需额外token生成或模型调用。该探针动态调整自一致性采样数量，并可应用于新数据集而无需预采样进行难度估计。

Result: 在五个基准测试上的实验结果表明，ACTSC能有效降低推理成本，同时保持相对于现有方法的准确性。

Conclusion: ACTSC通过利用LLM内部神经元激活信号进行难度感知的自一致性采样，提供了一种高效且可迁移的解决方案，显著减少了推理开销。

Abstract: Self-Consistency (SC) is an effective decoding strategy that improves the reasoning performance of Large Language Models (LLMs) by generating multiple chain-of-thought reasoning paths and selecting the final answer via majority voting. However, it suffers from substantial inference costs because it requires a large number of samples. To mitigate this issue, Difficulty-Adaptive Self-Consistency (DSC) was proposed to reduce unnecessary token usage for easy problems by adjusting the number of samples according to problem difficulty. However, DSC requires additional model calls and pre-sampling to estimate difficulty, and this process is repeated when applying to each dataset, leading to significant computational overhead. In this work, we propose Activation-Informed Difficulty-Aware Self-Consistency (ACTSC) to address these limitations. ACTSC leverages internal difficulty signals reflected in the feed-forward network neuron activations to construct a lightweight difficulty estimation probe, without any additional token generation or model calls. The probe dynamically adjusts the number of samples for SC and can be applied to new datasets without requiring pre-sampling for difficulty estimation. To validate its effectiveness, we conduct experiments on five benchmarks. Experimental results show that ACTSC effectively reduces inference costs while maintaining accuracy relative to existing methods.

</details>


### [17] [Comprehensive Comparison of RAG Methods Across Multi-Domain Conversational QA](https://arxiv.org/abs/2602.09552)
*Klejda Alushi,Jan Strich,Chris Biemann,Martin Semmann*

Main category: cs.CL

TL;DR: 对多轮对话QA中检索增强生成方法的系统性实证研究，发现简单方法（如重排序、混合BM25、HyDE）通常优于复杂方法，检索效果高度依赖数据集特性而非方法复杂度。


<details>
  <summary>Details</summary>
Motivation: 当前大多数研究在单轮设置下孤立评估RAG方法，缺乏对多轮对话QA中RAG方法的系统性比较。多轮对话中历史对话、指代消解和用户意图变化使检索变得复杂，需要全面评估不同RAG策略在这种场景下的表现。

Method: 在8个不同领域的对话QA数据集上，使用统一的实验设置，比较了基础和高级RAG方法。通过检索指标和生成指标评估检索质量和答案生成效果，并分析性能随对话轮数的变化。

Result: 研究发现：1）鲁棒但简单的方法（重排序、混合BM25、HyDE）持续优于基础RAG；2）一些高级技术未能带来增益，甚至可能使性能低于无RAG基线；3）数据集特性和对话长度强烈影响检索效果，没有单一RAG策略在所有设置中占优。

Conclusion: 有效的对话RAG更依赖于检索策略与数据集结构的对齐，而非方法复杂度。简单的适应数据集特性的方法通常比复杂方法表现更好。

Abstract: Conversational question answering increasingly relies on retrieval-augmented generation (RAG) to ground large language models (LLMs) in external knowledge. Yet, most existing studies evaluate RAG methods in isolation and primarily focus on single-turn settings. This paper addresses the lack of a systematic comparison of RAG methods for multi-turn conversational QA, where dialogue history, coreference, and shifting user intent substantially complicate retrieval. We present a comprehensive empirical study of vanilla and advanced RAG methods across eight diverse conversational QA datasets spanning multiple domains. Using a unified experimental setup, we evaluate retrieval quality and answer generation using generator and retrieval metrics, and analyze how performance evolves across conversation turns. Our results show that robust yet straightforward methods, such as reranking, hybrid BM25, and HyDE, consistently outperform vanilla RAG. In contrast, several advanced techniques fail to yield gains and can even degrade performance below the No-RAG baseline. We further demonstrate that dataset characteristics and dialogue length strongly influence retrieval effectiveness, explaining why no single RAG strategy dominates across settings. Overall, our findings indicate that effective conversational RAG depends less on method complexity than on alignment between the retrieval strategy and the dataset structure. We publish the code used.\footnote{\href{https://github.com/Klejda-A/exp-rag.git}{GitHub Repository}}

</details>


### [18] [Evaluating Social Bias in RAG Systems: When External Context Helps and Reasoning Hurts](https://arxiv.org/abs/2602.09442)
*Shweta Parihar,Lu Cheng*

Main category: cs.CL

TL;DR: RAG架构通过引入外部知识源能够减少大语言模型的社会偏见，但结合思维链提示会提高偏见水平。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在社会偏见问题，检索增强生成架构虽然增强了生成能力，但仍面临相同的偏见挑战，需要评估和理解RAG的社会偏见影响。

Method: 通过跨多种检索语料库、LLM和偏见评估数据集（涵盖13种以上偏见类型）进行广泛实验，并整合思维链提示来探索模型推理过程，评估模型CoT的忠实性。

Result: 研究发现RAG能减少偏见，外部上下文有助于抵消刻板印象驱动的预测；但思维链提示虽然提高了准确性，却增加了整体偏见水平。

Conclusion: RAG通过外部知识改善公平性，但思维链提示会加剧偏见，需要开发偏见感知的推理框架来平衡准确性与公平性。

Abstract: Social biases inherent in large language models (LLMs) raise significant fairness concerns. Retrieval-Augmented Generation (RAG) architectures, which retrieve external knowledge sources to enhance the generative capabilities of LLMs, remain susceptible to the same bias-related challenges. This work focuses on evaluating and understanding the social bias implications of RAG. Through extensive experiments across various retrieval corpora, LLMs, and bias evaluation datasets, encompassing more than 13 different bias types, we surprisingly observe a reduction in bias in RAG. This suggests that the inclusion of external context can help counteract stereotype-driven predictions, potentially improving fairness by diversifying the contextual grounding of the model's outputs. To better understand this phenomenon, we then explore the model's reasoning process by integrating Chain-of-Thought (CoT) prompting into RAG while assessing the faithfulness of the model's CoT. Our experiments reveal that the model's bias inclinations shift between stereotype and anti-stereotype responses as more contextual information is incorporated from the retrieved documents. Interestingly, we find that while CoT enhances accuracy, contrary to the bias reduction observed with RAG, it increases overall bias across datasets, highlighting the need for bias-aware reasoning frameworks that can mitigate this trade-off.

</details>


### [19] [LEMUR: A Corpus for Robust Fine-Tuning of Multilingual Law Embedding Models for Retrieval](https://arxiv.org/abs/2602.09570)
*Narges Baba Ahmadi,Jan Strich,Martin Semmann,Chris Biemann*

Main category: cs.CL

TL;DR: LEMUR是一个大规模多语言欧盟环境立法语料库，通过优化PDF到文本转换和微调多语言嵌入模型，显著提升了法律检索性能，特别是在低资源语言上。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在多语言法律环境中的应用受到不可靠检索和缺乏领域适应开放嵌入模型的限制，现有语料库不适合语义检索，PDF源文件存在大量噪声。

Method: 从24,953份欧盟官方PDF文件构建LEMUR语料库，使用Lexical Content Score量化PDF转换保真度，在单语和双语设置下微调三种多语言嵌入模型。

Result: 法律领域微调持续提升Top-k检索准确率，特别是在低资源语言上效果显著；跨语言评估显示改进可迁移到未见语言，表明微调主要增强了语言无关的法律内容表示。

Conclusion: LEMUR语料库和微调方法有效解决了多语言法律检索的挑战，为法律信息访问提供了可靠的多语言嵌入模型，并开源了代码和数据。

Abstract: Large language models (LLMs) are increasingly used to access legal information. Yet, their deployment in multilingual legal settings is constrained by unreliable retrieval and the lack of domain-adapted, open-embedding models. In particular, existing multilingual legal corpora are not designed for semantic retrieval, and PDF-based legislative sources introduce substantial noise due to imperfect text extraction. To address these challenges, we introduce LEMUR, a large-scale multilingual corpus of EU environmental legislation constructed from 24,953 official EUR-Lex PDF documents covering 25 languages. We quantify the fidelity of PDF-to-text conversion by measuring lexical consistency against authoritative HTML versions using the Lexical Content Score (LCS). Building on LEMUR, we fine-tune three state-of-the-art multilingual embedding models using contrastive objectives in both monolingual and bilingual settings, reflecting realistic legal-retrieval scenarios. Experiments across low- and high-resource languages demonstrate that legal-domain fine-tuning consistently improves Top-k retrieval accuracy relative to strong baselines, with particularly pronounced gains for low-resource languages. Cross-lingual evaluations show that these improvements transfer to unseen languages, indicating that fine-tuning primarily enhances language-independent, content-level legal representations rather than language-specific cues. We publish code\footnote{\href{https://github.com/nargesbh/eur_lex}{GitHub Repository}} and data\footnote{\href{https://huggingface.co/datasets/G4KMU/LEMUR}{Hugging Face Dataset}}.

</details>


### [20] [Conceptual Cultural Index: A Metric for Cultural Specificity via Relative Generality](https://arxiv.org/abs/2602.09444)
*Takumi Ohashi,Hitoshi Iyatomi*

Main category: cs.CL

TL;DR: 提出Conceptual Cultural Index (CCI)来衡量句子层面的文化特异性，通过比较目标文化与其他文化的普遍性估计差异来量化文化特异性，在文化特定句子与通用句子的分类任务上表现优于直接LLM评分。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在多文化环境中部署越来越广泛，但系统评估句子层面的文化特异性仍然研究不足，需要一种可操作且可解释的方法来衡量文化特异性。

Method: 提出Conceptual Cultural Index (CCI)，定义为目标文化内的普遍性估计与其他文化平均普遍性估计之间的差异。用户可以通过比较设置操作性地控制文化范围，该分数基于底层普遍性估计，具有可解释性。

Result: 在400个句子（200个文化特定句和200个通用句）上验证CCI，分数分布显示预期模式：文化特定句子得分较高，通用句子得分较低。在二元分类任务中，CCI优于直接LLM评分，在针对目标文化专门化的模型上AUC提高了10个百分点以上。

Conclusion: CCI是一种有效衡量句子层面文化特异性的方法，具有可操作性和可解释性，在文化特定内容识别方面优于直接使用LLM评分，有助于在多文化环境中更好地理解和评估语言模型。

Abstract: Large language models (LLMs) are increasingly deployed in multicultural settings; however, systematic evaluation of cultural specificity at the sentence level remains underexplored. We propose the Conceptual Cultural Index (CCI), which estimates cultural specificity at the sentence level. CCI is defined as the difference between the generality estimate within the target culture and the average generality estimate across other cultures. This formulation enables users to operationally control the scope of culture via comparison settings and provides interpretability, since the score derives from the underlying generality estimates. We validate CCI on 400 sentences (200 culture-specific and 200 general), and the resulting score distribution exhibits the anticipated pattern: higher for culture-specific sentences and lower for general ones. For binary separability, CCI outperforms direct LLM scoring, yielding more than a 10-point improvement in AUC for models specialized to the target culture. Our code is available at https://github.com/IyatomiLab/CCI .

</details>


### [21] [AmharicIR+Instr: A Two-Dataset Resource for Neural Retrieval and Instruction Tuning](https://arxiv.org/abs/2602.09914)
*Tilahun Yeshambel,Moncef Garouani,Josiane Mothe*

Main category: cs.CL

TL;DR: 发布了两个阿姆哈拉语数据集：1）包含1,091个查询-正例-负例三元组的检索排序数据集；2）包含6,285个指令提示-响应对的文本生成数据集，用于支持低资源语言的神经检索和生成模型研究。


<details>
  <summary>Details</summary>
Motivation: 阿姆哈拉语等低资源语言缺乏高质量监督数据，限制了神经检索和生成模型的发展，需要创建专门的数据资源来支持相关研究。

Method: 通过专家策划查询、网络衍生查询和LLM辅助生成构建三元组；正/负文档来自网络或LLM合成，并由母语者验证。指令数据集通过多个LLM生成，并经过人工审查和修正，确保语法、相关性、流畅性和事实合理性。

Result: 发布了两个标准化数据集：检索排序数据集（1,091个三元组）和指令提示-响应数据集（6,285对），提供多种格式（CSV、JSON、JSONL）和标准划分，支持可重复研究。

Conclusion: 这些数据集填补了阿姆哈拉语高质量监督数据的空白，为低资源语言的神经检索和生成模型研究提供了重要资源，其方法论可推广到其他低资源语言。

Abstract: Neural retrieval and GPT-style generative models rely on large, high-quality supervised data, which is still scarce for low-resource languages such as Amharic. We release an Amharic data resource consisting of two datasets that supports research on (i) neural retrieval-ranking and (ii) instruction-following text generation. The retrieval-ranking dataset contains 1,091 manually verified query-positive-negative document triplets drawn from diverse Amharic sources and constructed to support contrastive training and benchmarking of neural retrievers (e.g., DPR, ColBERT-style late interaction and SPLADE-style sparse neural retrieval). Triplets are created through a combination of expert-curated queries, web-derived queries, and LLM-assisted generation, with positive/negative documents selected from the web or synthesized by LLMs and then validated by native speakers. The instruction prompt-response dataset comprises 6,285 Amharic prompt-response pairs spanning multiple domains and instruction types, generated with several LLMs and refined through manual review and correction for grammaticality, relevance, fluency, and factual plausibility. We release both datasets with standardized splits and formats (CSV,JSON,JSONL) to enable reproducible work on Amharic retrieval, ranking, and generative modelling. These datasets also come with a methodology that can be generalized to other low-resource languages.

</details>


### [22] [NOWJ @BioCreative IX ToxHabits: An Ensemble Deep Learning Approach for Detecting Substance Use and Contextual Information in Clinical Texts](https://arxiv.org/abs/2602.09469)
*Huu-Huy-Hoang Tran,Gia-Bao Duong,Quoc-Viet-Anh Tran,Thi-Hai-Yen Vuong,Hoang-Quynh Le*

Main category: cs.CL

TL;DR: 本研究提出NOWJ系统，用于从西班牙语临床文本中提取药物使用信息，在BioCreative IX的ToxHabits共享任务中表现优异，通过多输出集成系统解决了低资源特定领域的挑战。


<details>
  <summary>Details</summary>
Motivation: 从非结构化电子健康记录中提取药物使用信息是临床自然语言处理的主要挑战。虽然大型语言模型有进展，但在临床NLP中应用受限于信任、控制和效率问题。需要解决特定领域、低资源环境下的毒物使用检测任务。

Method: 提出多输出集成系统处理两个子任务：ToxNER（命名实体识别）和ToxUse（使用检测）。系统整合BETO模型与CRF层进行序列标注，采用多样化训练策略，并使用句子过滤技术提升精度。

Result: 最佳表现达到：触发检测F1分数0.94、精度0.97；参数检测F1分数0.91。在西班牙语临床文本的毒物使用检测任务中取得了优异结果。

Conclusion: NOWJ系统在低资源特定领域的毒物使用检测任务中表现优秀，证明了结合预训练模型与CRF层以及多样化训练策略的有效性，为临床NLP中的药物信息提取提供了实用解决方案。

Abstract: Extracting drug use information from unstructured Electronic Health Records remains a major challenge in clinical Natural Language Processing. While Large Language Models demonstrate advancements, their use in clinical NLP is limited by concerns over trust, control, and efficiency. To address this, we present NOWJ submission to the ToxHabits Shared Task at BioCreative IX. This task targets the detection of toxic substance use and contextual attributes in Spanish clinical texts, a domain-specific, low-resource setting. We propose a multi-output ensemble system tackling both Subtask 1 - ToxNER and Subtask 2 - ToxUse. Our system integrates BETO with a CRF layer for sequence labeling, employs diverse training strategies, and uses sentence filtering to boost precision. Our top run achieved 0.94 F1 and 0.97 precision for Trigger Detection, and 0.91 F1 for Argument Detection.

</details>


### [23] [Listen to the Layers: Mitigating Hallucinations with Inter-Layer Disagreement](https://arxiv.org/abs/2602.09486)
*Koduvayur Subbalakshmi,Sabbir Hossain Ujjal,Venkata Krishna Teja Mangichetty,Nastaran Jamalipour Soofi*

Main category: cs.CL

TL;DR: CoCoA解码器：一种无需训练的解码算法，通过分析LLM中间层的表征不稳定性来检测和减轻幻觉生成，提高输出的事实准确性。


<details>
  <summary>Details</summary>
Motivation: 预训练大语言模型容易产生事实错误的文本（幻觉），这降低了其在下游任务中的可靠性和实用性。作者假设生成文本的事实性与其在模型内部各层表征的不稳定性相关。

Method: 提出CoCoA（Confusion and Consistency Aware）解码器，这是一种无需训练的解码算法。1）定义两个指标来量化中间层的表征不稳定性；2）利用这些信号对表现出高内部混乱的输出进行惩罚，引导模型产生更内部一致且事实可靠的输出；3）提出自信息门控变体CoCoA-SIG，动态调整惩罚以针对高不确定性、不稳定的生成。

Result: 在问答、摘要和代码生成等多种任务上的广泛实验表明，CoCoA显著提高了多个模型家族（如Llama-3、Qwen-2.5、Mistral）的事实正确性。

Conclusion: 通过利用模型内在信号，CoCoA提供了一种有效且广泛适用的方法，可以在推理时增强LLM的可信度，而无需任何模型重新训练。

Abstract: Pretrained Large Language Models (LLMs) are prone to generating fluent yet factually incorrect text-a phenomenon known as hallucinations, undermining their reliability and utility in downstream tasks. We hypothesize that a generated text span's factuality is correlated with its representational instability across the model's internal layers. Based on this, we propose the CoCoA (Confusion and Consistency Aware) decoder, a novel, training-free decoding algorithm that mitigates hallucinations at inference time by listening to these signals in the middle layers. We propose two metrics to quantify this instability in the middle layers, and use it to penalize outputs that exhibit high internal confusion, thereby steering the model towards more internally consistent and factually grounded outputs. We further propose a self-information gated variant, CoCoA-SIG, that dynamically modulates this penalty to selectively target high-surprise, unstable generations. Extensive experiments on diverse tasks, including question-answering, summarization and code generation demonstrate that CoCoA significantly improves factual correctness across multiple model families (e.g., Llama-3, Qwen-2.5, Mistral). By leveraging model-intrinsic signals, CoCoA offers an effective and broadly applicable method for enhancing the trustworthiness of LLMs at inference time, without requiring any model retraining.

</details>


### [24] [Where-to-Unmask: Ground-Truth-Guided Unmasking Order Learning for Masked Diffusion Language Models](https://arxiv.org/abs/2602.09501)
*Hikaru Asano,Tadashi Kozuno,Kuniaki Saito,Yukino Baba*

Main category: cs.CL

TL;DR: 本文提出Gt-Margin方法，通过真实令牌的概率边界来确定最佳去掩码顺序，并训练监督去掩码规划器来模仿这一顺序，从而提升掩码扩散语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散语言模型在推理时需要同时决定"在哪里去掩码"和"去掩码什么内容"。现有的方法要么使用启发式置信度度量，要么通过强化学习训练去掩码顺序，但后者需要昂贵的在线策略展开。作者希望找到一种更有效的方法来优化去掩码顺序。

Method: 提出Gt-Margin方法，基于真实令牌与其最强替代令牌的概率边界构建oracle去掩码顺序。然后通过学习排序方法训练监督去掩码规划器来模仿这一oracle顺序，该规划器可集成到标准MDLM采样过程中。

Result: 使用Gt-Margin确定的oracle去掩码顺序显著提升了最终生成质量，特别是在逻辑推理基准测试中。训练的去掩码规划器在不修改令牌预测模型的情况下，提高了推理准确性。

Conclusion: 通过利用真实令牌信息指导去掩码顺序，可以有效提升掩码扩散语言模型的推理性能，这种方法比传统启发式方法或强化学习更高效。

Abstract: Masked Diffusion Language Models (MDLMs) generate text by iteratively filling masked tokens, requiring two coupled decisions at each step: which positions to unmask (where-to-unmask) and which tokens to place (what-to-unmask). While standard MDLM training directly optimizes token prediction (what-to-unmask), inference-time unmasking orders (where-to-unmask) are typically determined by heuristic confidence measures or trained through reinforcement learning with costly on-policy rollouts. To address this, we introduce Gt-Margin, a position-wise score derived from ground-truth tokens, defined as the probability margin between the correct token and its strongest alternative. Gt-Margin yields an oracle unmasking order that prioritizes easier positions first under each partially masked state. We demonstrate that leveraging this oracle unmasking order significantly enhances final generation quality, particularly on logical reasoning benchmarks. Building on this insight, we train a supervised unmasking planner via learning-to-rank to imitate the oracle ordering from masked contexts. The resulting planner integrates into standard MDLM sampling to select where-to-unmask, improving reasoning accuracy without modifying the token prediction model.

</details>


### [25] [EcoGym: Evaluating LLMs for Long-Horizon Plan-and-Execute in Interactive Economies](https://arxiv.org/abs/2602.09514)
*Xavier Hu,Jinxiang Xia,Shengze Xu,Kangqi Song,Yishuo Yuan,Guibin Zhang,Jincheng Ren,Boyu Feng,Li Lu,Tieyong Zeng,Jiaheng Liu,Minghao Liu,Yuchen Elenor Jiang,Wei Wang,He Zhu,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: EcoGym是一个用于评估LLM智能体长期规划能力的基准测试平台，包含三个经济环境，专注于持续决策和业务相关成果的评估。


<details>
  <summary>Details</summary>
Motivation: 当前评估框架存在局限性：大多是情景化的、特定领域的，或缺乏持续经济动态的基础。需要通用化的基准来评估LLM智能体在交互式经济中的长期规划能力。

Method: 开发了EcoGym基准测试平台，包含三个多样化环境：Vending、Freelance和Operation，采用统一的决策流程和标准化接口，支持无限制时间范围的预算化行动（1000+步骤）。

Result: 在11个领先LLM上的实验显示：没有单一模型在所有三种场景中表现最优；模型在高层策略或高效行动执行方面存在显著次优性。

Conclusion: EcoGym作为一个开放、可扩展的测试平台，为透明评估长期规划智能体和研究现实经济环境中的可控性-效用权衡提供了基础。

Abstract: Long-horizon planning is widely recognized as a core capability of autonomous LLM-based agents; however, current evaluation frameworks suffer from being largely episodic, domain-specific, or insufficiently grounded in persistent economic dynamics. We introduce EcoGym, a generalizable benchmark for continuous plan-and-execute decision making in interactive economies. EcoGym comprises three diverse environments: Vending, Freelance, and Operation, implemented in a unified decision-making process with standardized interfaces, and budgeted actions over an effectively unbounded horizon (1000+ steps if 365 day-loops for evaluation). The evaluation of EcoGym is based on business-relevant outcomes (e.g., net worth, income, and DAU), targeting long-term strategic coherence and robustness under partial observability and stochasticity. Experiments across eleven leading LLMs expose a systematic tension: no single model dominates across all three scenarios. Critically, we find that models exhibit significant suboptimality in either high-level strategies or efficient actions executions. EcoGym is released as an open, extensible testbed for transparent long-horizon agent evaluation and for studying controllability-utility trade-offs in realistic economic settings.

</details>


### [26] [The CLEF-2026 CheckThat! Lab: Advancing Multilingual Fact-Checking](https://arxiv.org/abs/2602.09516)
*Julia Maria Struß,Sebastian Schellhammer,Stefan Dietze,Venktesh V,Vinay Setty,Tanmoy Chakraborty,Preslav Nakov,Avishek Anand,Primakov Chungkham,Salim Hafid,Dhruv Sahnan,Konstantin Todorov*

Main category: cs.CL

TL;DR: CheckThat! 2025实验室聚焦验证流程，包含科学网络声明的源检索、数值时间声明的验证以及完整事实核查文章生成三个任务。


<details>
  <summary>Details</summary>
Motivation: 推进对抗在线虚假信息的技术发展，扩展验证流程的覆盖范围和能力，在多语言多平台环境中应对复杂的验证挑战。

Method: 通过三个具体任务构建完整的验证管道：任务1延续2025版的科学网络声明源检索；任务2在数值和时间声明验证中增加推理组件；任务3扩展为生成完整的事实核查文章。

Result: 提出了包含分类、检索和生成挑战的完整验证流程，涵盖文档级别和片段级别的多语言处理，代表了虚假信息检测领域的技术进步。

Conclusion: CheckThat! 2025实验室通过系统化的验证管道任务设计，推动了虚假信息检测技术的发展，特别是在多语言环境下的复杂验证挑战方面取得了进展。

Abstract: The CheckThat! lab aims to advance the development of innovative technologies combating disinformation and manipulation efforts in online communication across a multitude of languages and platforms. While in early editions the focus has been on core tasks of the verification pipeline (check-worthiness, evidence retrieval, and verification), in the past three editions, the lab added additional tasks linked to the verification process. In this year's edition, the verification pipeline is at the center again with the following tasks: Task 1 on source retrieval for scientific web claims (a follow-up of the 2025 edition), Task 2 on fact-checking numerical and temporal claims, which adds a reasoning component to the 2025 edition, and Task 3, which expands the verification pipeline with generation of full-fact-checking articles. These tasks represent challenging classification and retrieval problems as well as generation challenges at the document and span level, including multilingual settings.

</details>


### [27] [Knowledge Integration Decay in Search-Augmented Reasoning of Large Language Models](https://arxiv.org/abs/2602.09517)
*Sangwon Yu,Ik-hwan Kim,Donghun Kang,Bongkyu Hwang,Junhwa Choi,Suk-hoon Jung,Seungki Hong,Taehee Lee,Sungroh Yoon*

Main category: cs.CL

TL;DR: 该论文提出了SAKE方法，通过锚定检索知识到推理过程的首尾来缓解知识整合衰减问题，显著提升了多跳问答和复杂推理任务的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管现代大语言模型通过搜索增强推理在复杂任务中表现出色，但作者发现了知识整合衰减（KID）这一关键瓶颈：随着推理链变长，模型越来越难以将检索到的证据整合到后续推理步骤中，即使相关信息可用也会限制性能。

Method: 提出了自我锚定知识编码（SAKE），这是一种无需训练的推理时策略。通过在推理过程的开头和结尾锚定检索到的知识，防止知识被先前上下文所掩盖，从而保持其语义完整性。

Result: 在多跳问答和复杂推理基准测试上的广泛实验表明，SAKE显著缓解了知识整合衰减问题并提高了性能，为智能LLM中的知识整合提供了一个轻量级而有效的解决方案。

Conclusion: SAKE是一种有效的推理时策略，能够稳定知识利用，缓解知识整合衰减问题，为搜索增强推理范式中的知识整合瓶颈提供了实用的解决方案。

Abstract: Modern Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks by employing search-augmented reasoning to incorporate external knowledge into long chains of thought. However, we identify a critical yet underexplored bottleneck in this paradigm, termed Knowledge Integration Decay (KID). Specifically, we observe that as the length of reasoning generated before search grows, models increasingly fail to integrate retrieved evidence into subsequent reasoning steps, limiting performance even when relevant information is available. To address this, we propose Self-Anchored Knowledge Encoding (SAKE), a training-free inference-time strategy designed to stabilize knowledge utilization. By anchoring retrieved knowledge at both the beginning and end of the reasoning process, SAKE prevents it from being overshadowed by prior context, thereby preserving its semantic integrity. Extensive experiments on multi-hop QA and complex reasoning benchmarks demonstrate that SAKE significantly mitigates KID and improves performance, offering a lightweight yet effective solution for knowledge integration in agentic LLMs.

</details>


### [28] [UniARM: Towards a Unified Autoregressive Reward Model for Multi-Objective Test-Time Alignment](https://arxiv.org/abs/2602.09538)
*Hongyan Xie,Yikun Ban,Ruiyu Fang,Zixuan Huang,Deqing Wang,Jianxin Li,Yitong Yao,Chao Wang,Shuangyong Song*

Main category: cs.CL

TL;DR: MoSLoRA和UniARM：通过共享特征提取和偏好调制模块，解决多目标对齐中特征纠缠问题，实现更精确的偏好权衡控制


<details>
  <summary>Details</summary>
Motivation: 现有多目标对齐方法存在局限性：独立训练每个偏好维度的ARM会忽略偏好特征间的交互，而使用单独特征提取模块的单一ARM会导致特征纠缠，这都会导致生成输出与用户偏好不对齐。

Method: 提出MoSLoRA（偏好调制与共享低秩适应）用于ARM训练：1）通过偏好无关模块提取共享特征；2）通过基于混合偏好向量的偏好调制模块对共享特征进行仿射变换。在此基础上提出UniARM（统一自回归奖励模型），在单一参数空间中联合建模所有偏好维度。

Result: 该方法缓解了特征纠缠问题，实现了推理过程中对偏好权衡的精确控制，并能在更大规模LLMs上高效运行，增强了实际可用性。

Conclusion: MoSLoRA和UniARM为多目标测试时对齐提供了有效解决方案，通过共享特征提取和偏好调制机制，克服了现有方法的局限性，实现了更好的对齐效果。

Abstract: Multi-objective alignment aims to align LLM responses with multiple human preference objectives. Among existing methods, guiding the generation of frozen LLMs through autoregressive reward models (ARMs) to accomplish multi-objective test-time alignment is a low-cost solution. However, these methods typically rely on independent parameters for each preference objective, either by training ARMs independently across preference dimensions, which neglects interactions among preference features, or by training a single ARM with separate feature extraction modules for each preference, which can cause feature entanglement. Both strategies can result in misalignment between generated outputs and user preferences. To address this limitation, we propose Preference-Modulated \& Shared Low-Rank Adaptation (MoSLoRA) for ARM training, which first extracts shared features via a preference-agnostic module and then applies affine transformations to shared features via a preference modulation module conditioned on mixed preference vectors. This design mitigates feature entanglement and enables precise control over preference trade-offs during inference. Building on this, we introduce the Unified Autoregressive Reward Model (UniARM), a novel framework for multi-objective test-time alignment. UniARM jointly models all preference dimensions in a single parameter space, eliminating the need for independent parameters for each preference objective. es on larger-scale LLMs, enhancing its practical usability.

</details>


### [29] [Advancing Block Diffusion Language Models for Test-Time Scaling](https://arxiv.org/abs/2602.09555)
*Yi Lu,Deyang Kong,Jianing Wang,Linsen Guo,Xue Wang,Qi Guo,Tao Gui,Xuanjing Huang,Wei Ye,Shikun Zhang,Wei Wang*

Main category: cs.CL

TL;DR: 提出BDLMs在测试时缩放下的统一框架，通过自适应解码和块生成平衡推理速度与效果


<details>
  <summary>Details</summary>
Motivation: 现有BDLMs在测试时缩放下探索有限，在长链推理中面临解码速度与效果平衡的挑战

Method: 提出BACD（边界自适应置信度解码）和TCCF（粗思考细批判）范式，结合渐进块大小扩展

Result: 在TDAR-8B上应用BACD和TCCF，相比TraDo-8B实现2.26倍加速，AIME24提升11.2分

Conclusion: 该工作解锁了BDLMs在复杂推理任务中测试时缩放的潜力

Abstract: Recent advances in block diffusion language models have demonstrated competitive performance and strong scalability on reasoning tasks. However, existing BDLMs have limited exploration under the test-time scaling setting and face more severe decoding challenges in long Chain-of-Thought reasoning, particularly in balancing the decoding speed and effectiveness. In this work, we propose a unified framework for test-time scaling in BDLMs that introduces adaptivity in both decoding and block-wise generation. At the decoding level, we propose Bounded Adaptive Confidence Decoding (BACD), a difficulty-aware sampling strategy that dynamically adjusts denoising based on model confidence, accelerating inference while controlling error accumulation. Beyond step-wise adaptivity, we introduce Think Coarse, Critic Fine (TCCF), a test-time scaling paradigm that allocates large block sizes to exploratory reasoning and smaller block sizes to refinement, achieving an effective efficiency-effectiveness balance. To enable efficient and effective decoding with a large block size, we adopt Progressive Block Size Extension, which mitigates performance degradation when scaling block sizes. Extensive experiments show that applying BACD and TCCF to TDAR-8B yields significant improvements over strong baselines such as TraDo-8B (2.26x speedup, +11.2 points on AIME24). These results mark an important step toward unlocking the potential of BDLMs for test-time scaling in complex reasoning tasks.

</details>


### [30] [Aligning Tree-Search Policies with Fixed Token Budgets in Test-Time Scaling of LLMs](https://arxiv.org/abs/2602.09574)
*Sora Miyamoto,Daisuke Oba,Naoaki Okazaki*

Main category: cs.CL

TL;DR: 提出预算引导的蒙特卡洛树搜索（BG-MCTS），一种根据剩余token预算动态调整搜索策略的树搜索解码算法，优于预算无关的基线方法。


<details>
  <summary>Details</summary>
Motivation: 现实部署中LLM的树搜索解码有固定的每查询token预算，且不同场景预算不同。现有树搜索策略对预算不敏感，仅将其作为终止条件，这可能导致后期过度分支或过早终止。

Method: 提出预算引导的MCTS（BG-MCTS）算法，根据剩余token预算动态调整搜索策略：开始时广泛探索，随着预算减少优先细化和答案完成，同时减少浅层节点的后期分支。

Result: 在不同预算下，BG-MCTS在MATH500和AIME24/25数据集上使用开源权重的LLM时，始终优于预算无关的树搜索基线方法。

Conclusion: 预算感知的树搜索策略能更好地利用有限token预算，提升LLM解码效果，为实际部署中的树搜索解码提供了有效的解决方案。

Abstract: Tree-search decoding is an effective form of test-time scaling for large language models (LLMs), but real-world deployment imposes a fixed per-query token budget that varies across settings. Existing tree-search policies are largely budget-agnostic, treating the budget as a termination condition, which can lead to late-stage over-branching or premature termination. We propose {Budget-Guided MCTS} (BG-MCTS), a tree-search decoding algorithm that aligns its search policy with the remaining token budget: it starts with broad exploration, then prioritizes refinement and answer completion as the budget depletes while reducing late-stage branching from shallow nodes. BG-MCTS consistently outperforms budget-agnostic tree-search baselines across different budgets on MATH500 and AIME24/25 with open-weight LLMs.

</details>


### [31] [Context-Aware Counterfactual Data Augmentation for Gender Bias Mitigation in Language Models](https://arxiv.org/abs/2602.09590)
*Shweta Parihar,Liu Guangliang,Natalie Parde,Lu Cheng*

Main category: cs.CL

TL;DR: 提出了Context-CDA方法，通过上下文增强的反事实数据生成和基于不确定性的过滤，在减少语言模型社会偏见的同时保持语言建模能力


<details>
  <summary>Details</summary>
Motivation: 现有反事实数据增强（CDA）方法在微调语言模型以减少社会偏见时存在两个主要问题：1）生成的合成数据可能与真实世界分布不一致；2）过于简化的反事实忽略了敏感属性（如性别）在预训练语料中的社会背景，导致语言建模能力下降

Method: 提出Context-CDA方法：1）使用大语言模型增强反事实数据的多样性和上下文相关性，使去偏见语料与预训练数据更好对齐；2）采用基于不确定性的过滤机制，排除目标较小语言模型认为质量低的反事实生成，提升微调语料质量

Result: 在性别偏见基准测试中，Context-CDA在有效减少偏见的同时不牺牲语言建模性能，并通过分析下一个token生成概率的分布变化提供对社会偏见的洞察

Conclusion: Context-CDA通过上下文增强和基于不确定性的过滤，解决了传统CDA方法的局限性，实现了偏见缓解与语言建模能力的平衡，为分析语言模型中的社会偏见提供了新视角

Abstract: A challenge in mitigating social bias in fine-tuned language models (LMs) is the potential reduction in language modeling capability, which can harm downstream performance. Counterfactual data augmentation (CDA), a widely used method for fine-tuning, highlights this issue by generating synthetic data that may align poorly with real-world distributions or creating overly simplistic counterfactuals that ignore the social context of altered sensitive attributes (e.g., gender) in the pretraining corpus. To address these limitations, we propose a simple yet effective context-augmented CDA method, Context-CDA, which uses large LMs to enhance the diversity and contextual relevance of the debiasing corpus. By minimizing discrepancies between the debiasing corpus and pretraining data through augmented context, this approach ensures better alignment, enhancing language modeling capability. We then employ uncertainty-based filtering to exclude generated counterfactuals considered low-quality by the target smaller LMs (i.e., LMs to be debiased), further improving the fine-tuning corpus quality. Experimental results on gender bias benchmarks demonstrate that Context-CDA effectively mitigates bias without sacrificing language modeling performance while offering insights into social biases by analyzing distribution shifts in next-token generation probabilities.

</details>


### [32] [On the Optimal Reasoning Length for RL-Trained Language Models](https://arxiv.org/abs/2602.09591)
*Daisuke Nohara,Taishi Nakamura,Rio Yokota*

Main category: cs.CL

TL;DR: 研究表明强化学习会延长思维链输出并增加计算成本，通过对比不同长度控制方法，发现长度惩罚可能阻碍推理获取，而适当调整的长度控制能提升具有强先验推理能力模型的效率。


<details>
  <summary>Details</summary>
Motivation: 强化学习虽然能提升大语言模型的推理能力，但会导致思维链输出变长，增加训练和推理的计算成本。目前已有长度控制方法，但最优输出长度如何平衡效率与性能仍不明确。

Method: 比较了多种长度控制方法在Qwen3-1.7B Base和DeepSeek-R1-Distill-Qwen-1.5B两个模型上的效果，并将先前工作扩展到RL训练的策略中。

Result: 长度惩罚可能阻碍推理获取，而适当调整的长度控制能提升具有强先验推理能力模型的效率。研究识别出两种失败模式：长输出增加分散性，短输出导致思考不足。

Conclusion: 长度控制需要在推理效率和性能之间找到平衡点，针对不同模型特性调整长度控制策略至关重要，特别是对于具有强先验推理能力的模型。

Abstract: Reinforcement learning substantially improves reasoning in large language models, but it also tends to lengthen chain of thought outputs and increase computational cost during both training and inference. Though length control methods have been proposed, it remains unclear what the optimal output length is for balancing efficiency and performance. In this work, we compare several length control methods on two models, Qwen3-1.7B Base and DeepSeek-R1-Distill-Qwen-1.5B. Our results indicate that length penalties may hinder reasoning acquisition, while properly tuned length control can improve efficiency for models with strong prior reasoning. By extending prior work to RL trained policies, we identify two failure modes, 1) long outputs increase dispersion, and 2) short outputs lead to under-thinking.

</details>


### [33] [Learning from the Irrecoverable: Error-Localized Policy Optimization for Tool-Integrated LLM Reasoning](https://arxiv.org/abs/2602.09598)
*Qiao Liang,Yuke Zhu,Chao Ge,Lei Yang,Ying Shen,Bo Zheng,Sheng Guo*

Main category: cs.CL

TL;DR: ELPO提出一种通过二分搜索定位首个不可恢复错误步骤，并利用层次化优势分配进行细粒度信用分配的方法，在工具集成推理任务中显著提升强化学习效果。


<details>
  <summary>Details</summary>
Motivation: 在工具集成推理（TIR）中，结果导向的强化学习面临稀疏、延迟的奖励和弱步骤级信用分配问题。早期不可恢复的错误步骤会决定整个任务的成败，因此需要准确定位首个不可恢复步骤并进行细粒度信用分配。

Method: 提出错误定位策略优化（ELPO）：1）在固定采样预算下通过二分搜索回滚树定位首个不可恢复步骤；2）通过层次化优势分配将结果树转化为稳定的学习信号；3）应用错误定位自适应裁剪，加强对关键步骤及其后续步骤的修正更新。

Result: 在数学、科学问答和代码执行等TIR基准测试中，ELPO在可比采样预算下始终优于强大的Agentic RL基线方法，在Pass@K和Major@K扩展、回滚排序质量和工具调用效率方面获得额外提升。

Conclusion: ELPO通过准确定位首个不可恢复错误步骤并进行细粒度信用分配，有效解决了TIR中长期轨迹中稀疏奖励和信用分配的问题，显著提升了强化学习代理在复杂推理任务中的性能。

Abstract: Tool-integrated reasoning (TIR) enables LLM agents to solve tasks through planning, tool use, and iterative revision, but outcome-only reinforcement learning in this setting suffers from sparse, delayed rewards and weak step-level credit assignment. In long-horizon TIR trajectories, an early irrecoverable mistake can determine success or failure, making it crucial to localize the first irrecoverable step and leverage it for fine-grained credit assignment. We propose Error-Localized Policy Optimization (ELPO), which localizes the first irrecoverable step via binary-search rollout trees under a fixed rollout budget, converts the resulting tree into stable learning signals through hierarchical advantage attribution, and applies error-localized adaptive clipping to strengthen corrective updates on the critical step and its suffix. Across TIR benchmarks in math, science QA, and code execution, ELPO consistently outperforms strong Agentic RL baselines under comparable sampling budgets, with additional gains in Pass@K and Major@K scaling, rollout ranking quality, and tool-call efficiency. Our code will be publicly released soon.

</details>


### [34] [AlignTune: Modular Toolkit for Post-Training Alignment of Large Language Models](https://arxiv.org/abs/2602.09621)
*R E Zera Marveen Lyngkhoi,Chirag Chawla,Pratinav Seth,Utsav Avaiya,Soham Bhattacharjee,Mykola Khandoga,Rui Yuan,Vinay Kumar Sankarapu*

Main category: cs.CL

TL;DR: AlignTune是一个统一的LLM对齐工具包，通过模块化设计解决后端干扰、奖励分散和实验不可复现问题，支持SFT和RLHF优化。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐工作流分散在不同后端工具和临时代码中，导致实验难以复现，存在后端干扰、奖励分散和流程不可复现等关键障碍。

Method: 引入AlignTune工具包，提供监督微调（SFT）和RLHF风格优化的统一接口，支持TRL和Unsloth后端互换，标准化配置，提供可扩展的奖励层（基于规则和学习），集成标准基准和自定义任务评估。

Result: 通过将后端特定逻辑隔离在单一工厂边界后，AlignTune实现了可控比较和可复现的对齐实验。

Conclusion: AlignTune通过模块化设计解决了LLM对齐研究中的工具碎片化和实验复现问题，为对齐实验提供了标准化和可扩展的框架。

Abstract: Post-training alignment is central to deploying large language models (LLMs), yet practical workflows remain split across backend-specific tools and ad-hoc glue code, making experiments hard to reproduce. We identify backend interference, reward fragmentation, and irreproducible pipelines as key obstacles in alignment research. We introduce AlignTune, a modular toolkit exposing a unified interface for supervised fine-tuning (SFT) and RLHF-style optimization with interchangeable TRL and Unsloth backends. AlignTune standardizes configuration, provides an extensible reward layer (rule-based and learned), and integrates evaluation over standard benchmarks and custom tasks. By isolating backend-specific logic behind a single factory boundary, AlignTune enables controlled comparisons and reproducible alignment experiments.

</details>


### [35] [MILE-RefHumEval: A Reference-Free, Multi-Independent LLM Framework for Human-Aligned Evaluation](https://arxiv.org/abs/2602.09624)
*Nalin Srun,Parisa Rastin,Guénaël Cabanes,Lydia Boudjeloud Assala*

Main category: cs.CL

TL;DR: MILE-RefHumEval：一个无需参考标注的LLM评估框架，通过集成独立提示的评估器实现灵活、可扩展的评估


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估通常需要真实标注或评估器协调，这在实际应用中存在限制。需要一种无需参考标注、无需协调、灵活且可扩展的评估方法。

Method: 提出MILE-RefHumEval框架：1）使用集成独立提示的评估器；2）采用人类对齐的评估模式；3）支持离散和连续评分；4）提供任务特定提示（从候选选择到对话等任务）；5）通过最佳候选选择优化提示。

Result: 实验表明：1）与人类判断高度对齐；2）优于现有方法；3）降低计算开销；4）提供高效、鲁棒且人类对齐的评估解决方案。

Conclusion: MILE-RefHumEval为实际LLM评估提供了一个无需参考标注、灵活、可扩展且人类对齐的解决方案，解决了传统评估方法的限制。

Abstract: We introduce MILE-RefHumEval, a reference-free framework for evaluating Large Language Models (LLMs) without ground-truth annotations or evaluator coordination. It leverages an ensemble of independently prompted evaluators guided by a human-aligned schema, supporting both discrete and continuous scoring judgement. With task-specific prompts from best candidate selection, summarization and image captioning to dialogue, MILE-RefHumEval provides flexible, interpretable, and scalable assessments. Experiments show it aligns closely with human judgments, outperforms prior methods, and reduces computational overhead, offering an efficient, robust, and human-aligned solution for real-world LLM evaluation.

</details>


### [36] [MATA: Multi-Agent Framework for Reliable and Flexible Table Question Answering](https://arxiv.org/abs/2602.09642)
*Sieun Hyeon,Jusang Oh,Sunghwan Steve Cho,Jaeyoung Do*

Main category: cs.CL

TL;DR: MATA是一个基于多智能体的表格问答框架，通过多种推理路径和小型语言模型工具实现高效可靠的表格理解


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在表格理解任务中面临可靠性、可扩展性和效率挑战，特别是在资源受限或隐私敏感环境中，需要更高效的解决方案

Method: 采用多智能体框架，通过多种互补推理路径生成候选答案，使用小型语言模型构建的工具集进行答案优化，并设计算法减少昂贵的大语言模型调用

Result: 在两个不同难度的基准测试中使用十种不同大语言模型进行实验，MATA实现了最先进的准确性和高效推理，同时避免了过度的大语言模型推理开销

Conclusion: 精心编排多种推理路径能够实现可扩展且可靠的表格问答，MATA框架在保持强性能的同时适应各种大语言模型类型，显著提高了效率

Abstract: Recent advances in Large Language Models (LLMs) have significantly improved table understanding tasks such as Table Question Answering (TableQA), yet challenges remain in ensuring reliability, scalability, and efficiency, especially in resource-constrained or privacy-sensitive environments. In this paper, we introduce MATA, a multi-agent TableQA framework that leverages multiple complementary reasoning paths and a set of tools built with small language models. MATA generates candidate answers through diverse reasoning styles for a given table and question, then refines or selects the optimal answer with the help of these tools. Furthermore, it incorporates an algorithm designed to minimize expensive LLM agent calls, enhancing overall efficiency. MATA maintains strong performance with small, open-source models and adapts easily across various LLM types. Extensive experiments on two benchmarks of varying difficulty with ten different LLMs demonstrate that MATA achieves state-of-the-art accuracy and highly efficient reasoning while avoiding excessive LLM inference. Our results highlight that careful orchestration of multiple reasoning pathways yields scalable and reliable TableQA. The code is available at https://github.com/AIDAS-Lab/MATA.

</details>


### [37] [Life Cycle-Aware Evaluation of Knowledge Distillation for Machine Translation: Environmental Impact and Translation Quality Trade-offs](https://arxiv.org/abs/2602.09691)
*Joseph Attieh,Timothee Mickus,Anne-Laure Ligozat,Aurélie Névéol,Jörg Tiedemann*

Main category: cs.CL

TL;DR: 本文评估了机器翻译中知识蒸馏方法的计算成本与翻译质量之间的权衡，提出使用碳足迹作为计算成本指标，发现知识蒸馏的效益取决于部署规模。


<details>
  <summary>Details</summary>
Motivation: 当前机器翻译领域的研究通常只报告学生模型的翻译质量，而忽略了知识蒸馏的计算复杂度，使得在计算资源约束下难以选择合适的方法。

Method: 使用机器学习生命周期评估工具计算碳足迹，评估代表性知识蒸馏方法，考虑运行时的操作排放和硬件生产的摊销成本，涵盖教师训练、蒸馏和推理整个生命周期。

Result: 发现：(1) 在小规模部署时，蒸馏开销主导总碳足迹；(2) 在大规模部署时，推理主导碳足迹，知识蒸馏仅在超过任务相关使用阈值后才有利；(3) 词级蒸馏通常比序列级蒸馏提供更有利的碳足迹-质量权衡。

Conclusion: 提出的协议为在明确的质量和计算约束下选择知识蒸馏方法提供了可重复的指导，强调需要考虑部署规模来决定是否使用知识蒸馏。

Abstract: Knowledge distillation (KD) is a tool to compress a larger system (teacher) into a smaller one (student). In machine translation, studies typically report only the translation quality of the student and omit the computational complexity of performing KD, making it difficult to select among the many available KD choices under compute-induced constraints. In this study, we evaluate representative KD methods by considering both translation quality and computational cost. We express computational cost as a carbon footprint using the machine learning life cycle assessment (MLCA) tool. This assessment accounts for runtime operational emissions and amortized hardware production costs throughout the KD model life cycle (teacher training, distillation, and inference). We find that (i) distillation overhead dominates the total footprint at small deployment volumes, (ii) inference dominates at scale, making KD beneficial only beyond a task-dependent usage threshold, and (iii) word-level distillation typically offers more favorable footprint-quality trade-offs than sequence-level distillation. Our protocol provides reproducible guidance for selecting KD methods under explicit quality and compute-induced constraints.

</details>


### [38] [Maastricht University at AMIYA: Adapting LLMs for Dialectal Arabic using Fine-tuning and MBR Decoding](https://arxiv.org/abs/2602.09703)
*Abdulhai Alali,Abderrahmane Issam*

Main category: cs.CL

TL;DR: 使用LoRA微调、适配器合并和方言感知MBR解码来提升大语言模型在阿拉伯方言上的生成和翻译性能


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型越来越多语言，但方言变体仍然代表性不足，主要原因是数据有限和语言变异

Method: 1. 使用低秩适应（LoRA）对单语和英语-方言平行数据进行微调；2. 适配器合并；3. 方言感知最小贝叶斯风险（MBR）解码

Result: 在叙利亚、摩洛哥和沙特阿拉伯方言上的实验表明，合并和MBR方法能够提高方言保真度，同时保持语义准确性

Conclusion: 该方法提供了一个紧凑有效的框架，用于鲁棒的阿拉伯方言生成

Abstract: Large Language Models (LLMs) are becoming increasingly multilingual, supporting hundreds of languages, especially high resource ones. Unfortunately, Dialect variations are still underrepresented due to limited data and linguistic variation. In this work, we adapt a pre-trained LLM to improve dialectal performance. Specifically, we use Low Rank Adaptation (LoRA) fine-tuning on monolingual and English Dialect parallel data, adapter merging and dialect-aware MBR decoding to improve dialectal fidelity generation and translation. Experiments on Syrian, Moroccan, and Saudi Arabic show that merging and MBR improve dialectal fidelity while preserving semantic accuracy. This combination provides a compact and effective framework for robust dialectal Arabic generation.

</details>


### [39] [TraceMem: Weaving Narrative Memory Schemata from User Conversational Traces](https://arxiv.org/abs/2602.09712)
*Yiming Shu,Pei Liu,Tiange Zhang,Ruiyang Gao,Jun Ma,Chen Sun*

Main category: cs.CL

TL;DR: TraceMem是一个受认知启发的记忆框架，通过三阶段流水线构建结构化叙事记忆模式，用于增强LLM的长期对话能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM的有限上下文窗口难以管理随时间延伸的对话历史，现有记忆系统将交互视为离散片段，无法捕捉对话流的叙事连贯性。

Method: 提出三阶段流水线：1)短期记忆处理：使用演绎式主题分割划分事件边界并提取语义表示；2)突触记忆巩固：将事件总结为情节记忆并与语义结合提炼为用户特定轨迹；3)系统记忆巩固：使用两阶段层次聚类将这些轨迹组织成连贯、随时间演化的叙事线程。

Result: 在LoCoMo基准测试中达到最先进性能，分析显示在构建连贯叙事方面超越基线，在多跳推理和时间推理方面表现优异。

Conclusion: TraceMem通过认知启发的架构构建连贯的叙事记忆模式，显著提升了LLM的长期对话理解和推理能力，为记忆系统领域提供了新的视角和未来展望。

Abstract: Sustaining long-term interactions remains a bottleneck for Large Language Models (LLMs), as their limited context windows struggle to manage dialogue histories that extend over time. Existing memory systems often treat interactions as disjointed snippets, failing to capture the underlying narrative coherence of the dialogue stream. We propose TraceMem, a cognitively-inspired framework that weaves structured, narrative memory schemata from user conversational traces through a three-stage pipeline: (1) Short-term Memory Processing, which employs a deductive topic segmentation approach to demarcate episode boundaries and extract semantic representation; (2) Synaptic Memory Consolidation, a process that summarizes episodes into episodic memories before distilling them alongside semantics into user-specific traces; and (3) Systems Memory Consolidation, which utilizes two-stage hierarchical clustering to organize these traces into coherent, time-evolving narrative threads under unifying themes. These threads are encapsulated into structured user memory cards, forming narrative memory schemata. For memory utilization, we provide an agentic search mechanism to enhance reasoning process. Evaluation on the LoCoMo benchmark shows that TraceMem achieves state-of-the-art performance with a brain-inspired architecture. Analysis shows that by constructing coherent narratives, it surpasses baselines in multi-hop and temporal reasoning, underscoring its essential role in deep narrative comprehension. Additionally, we provide an open discussion on memory systems, offering our perspectives and future outlook on the field. Our code implementation is available at: https://github.com/YimingShu-teay/TraceMem

</details>


### [40] [Unsupervised Layer-Wise Dynamic Test Time Adaptation for LLMs](https://arxiv.org/abs/2602.09719)
*Longhuan Xu,Cunjian Chen,Feng Yin*

Main category: cs.CL

TL;DR: 提出层间动态测试时适应框架，通过超网络预测每层每步的学习率乘子，提升LLM在无监督、样本特定测试时适应中的稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 当前无监督、样本特定的测试时适应方法存在稳定性问题：固定学习率容易导致模型过拟合到特定提示的统计特性，偏离期望答案分布，最终降低生成质量。这是因为TTA需要在少量梯度步内适应单个提示，而标准训练则是在大数据集上平均更新。

Method: 提出层间动态测试时适应框架，仅更新LoRA参数，使用轻量级超网络预测每层每步的学习率乘子，实现细粒度控制。该方法根据提示表示、LLM结构和适应步骤来显式调节TTA强度。

Result: 在不同数据集和LLM上的实验表明，该方法通过学习适应步骤和Transformer层投影的有效缩放模式，显著增强了TTA的稳定性，同时带来了更好的性能表现。

Conclusion: 层间动态测试时适应框架通过自适应调节学习率，解决了无监督、样本特定TTA中的稳定性问题，为LLM在推理时的自适应提供了更有效的解决方案。

Abstract: Test-time adaptation (TTA) for large language models (LLMs) updates model parameters at inference time using signals available at deployment. This paper focuses on a common yet under-explored regime: unsupervised, sample-specific TTA, where the model adapts independently for each prompt using only the prompt itself, without gold answers or external supervision. Although appealing, naive unsupervised TTA with a fixed, handcrafted learning rate can be unstable: updates may overfit to prompt-specific statistics, drift from the desired answer distribution, and ultimately degrade generation quality. This failure mode is not surprising, as in this case TTA must adapt to a single prompt within only a few gradient steps, unlike standard training that averages updates over large datasets and long optimization horizons. Therefore, we propose layer-wise dynamic test-time adaptation, a framework which explicitly modulates TTA strength as a function of prompt representation, LLM structure and adaptation step. In our setting, TTA updates only LoRA parameters, and a lightweight hypernetwork predicts per-layer, per-step learning-rate multipliers, enabling fine-grained control. Experiments across various datasets and LLMs consistently show that our method substantially strengthens TTA by learning effective scaling patterns over adaptation steps and transformer layer projections, improving stability while delivering better performance.

</details>


### [41] [AI-Assisted Scientific Assessment: A Case Study on Climate Change](https://arxiv.org/abs/2602.09723)
*Christian Buck,Levke Caesar,Michelle Chen Huebscher,Massimiliano Ciaramita,Erich M. Fischer,Zeke Hausfather,Özge Kart Tokmak,Reto Knutti,Markus Leippold,Joseph Ludescher,Katharine J. Mach,Sofia Palazzo Corner,Kasra Rafiezadeh Shahi,Johan Rockström,Joeri Rogelj,Boris Sakschewski*

Main category: cs.CL

TL;DR: 研究人员开发了一个基于Gemini的AI环境来支持科学评估协作，并在气候科学领域的AMOC稳定性研究中测试了该系统，结果显示AI能加速科学工作流程，但专家监督和补充对于确保科学严谨性至关重要。


<details>
  <summary>Details</summary>
Motivation: 当前的AI科学家范式主要关注可重复验证的任务，但无法扩展到那些重复评估不可能且需要基于理论和现有证据达成共识的科学问题上。因此需要探索AI在支持复杂科学评估协作方面的潜力。

Method: 开发了一个基于Gemini的AI环境，集成到标准科学工作流程中，与13位气候科学家合作，在AMOC稳定性这一复杂主题上进行测试，通过104次修订循环分析79篇论文。

Result: AI显著加速了科学工作流程：团队在46人时内完成了79篇论文的综合分析，大部分AI生成内容被保留在报告中，AI帮助保持了逻辑一致性和呈现质量。但专家补充至关重要：少于一半的报告由AI生成，需要大量监督才能达到严格的科学标准。

Conclusion: AI可以作为有效的科学协作工具加速工作流程，但当前无法替代专家判断和监督。AI与人类专家的协作模式对复杂科学评估问题有潜力，但需要大量专家投入来确保科学严谨性。

Abstract: The emerging paradigm of AI co-scientists focuses on tasks characterized by repeatable verification, where agents explore search spaces in 'guess and check' loops. This paradigm does not extend to problems where repeated evaluation is impossible and ground truth is established by the consensus synthesis of theory and existing evidence. We evaluate a Gemini-based AI environment designed to support collaborative scientific assessment, integrated into a standard scientific workflow. In collaboration with a diverse group of 13 scientists working in the field of climate science, we tested the system on a complex topic: the stability of the Atlantic Meridional Overturning Circulation (AMOC). Our results show that AI can accelerate the scientific workflow. The group produced a comprehensive synthesis of 79 papers through 104 revision cycles in just over 46 person-hours. AI contribution was significant: most AI-generated content was retained in the report. AI also helped maintain logical consistency and presentation quality. However, expert additions were crucial to ensure its acceptability: less than half of the report was produced by AI. Furthermore, substantial oversight was required to expand and elevate the content to rigorous scientific standards.

</details>


### [42] [Targum -- A Multilingual New Testament Translation Corpus](https://arxiv.org/abs/2602.09724)
*Maciej Rapacz,Aleksander Smywiński-Pohl*

Main category: cs.CL

TL;DR: 构建了一个包含657个新约译本的多语言语料库，其中352个是独特版本，在英语、法语、意大利语、波兰语和西班牙语中提供了前所未有的深度覆盖。


<details>
  <summary>Details</summary>
Motivation: 现有语料库在追求语言广度时，往往忽略了欧洲语言丰富的圣经翻译历史深度。为了填补这一空白，需要创建一个专门用于多层次翻译历史研究的资源。

Method: 从12个在线圣经图书馆和一个现有语料库中收集了657个新约译本，为每个翻译手动标注元数据，包括作品标准化标识符、具体版本和修订年份，实现文本规范化。

Result: 创建了包含657个新约译本的多语言语料库，其中352个是独特版本，在英语（208个独特版本）、法语（41个）、意大利语（18个）、波兰语（30个）和西班牙语（55个）中提供了深度覆盖。每个翻译都有详细的元数据标注。

Conclusion: 该语料库为翻译历史研究建立了新的基准，使研究人员能够根据自身需求定义"独特性"，进行从KJV谱系等微观分析到宏观层面的多层次研究。

Abstract: Many European languages possess rich biblical translation histories, yet existing corpora - in prioritizing linguistic breadth - often fail to capture this depth. To address this gap, we introduce a multilingual corpus of 657 New Testament translations, of which 352 are unique, with unprecedented depth in five languages: English (208 unique versions from 396 total), French (41 from 78), Italian (18 from 33), Polish (30 from 48), and Spanish (55 from 102). Aggregated from 12 online biblical libraries and one preexisting corpus, each translation is manually annotated with metadata that maps the text to a standardized identifier for the work, its specific edition, and its year of revision. This canonicalization empowers researchers to define "uniqueness" for their own needs: they can perform micro-level analyses on translation families, such as the KJV lineage, or conduct macro-level studies by deduplicating closely related texts. By providing the first resource designed for such flexible, multilevel analysis, our corpus establishes a new benchmark for the quantitative study of translation history.

</details>


### [43] [Improving Interpretability of Lexical Semantic Change with Neurobiological Features](https://arxiv.org/abs/2602.09760)
*Kohei Oda,Hiroya Takamura,Kiyoaki Shirai,Natthawut Kertkeidkachorn*

Main category: cs.CL

TL;DR: 该论文提出了一种将上下文嵌入的语义空间映射到神经生物学特征空间的方法，以增强词汇语义变化的可解释性，并在LSC估计中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 目前大多数词汇语义变化研究专注于提高变化程度估计的性能，但难以解释词义如何变化。增强LSC的可解释性是该领域的重要挑战，可能带来新的研究见解。

Method: 提出将预训练语言模型获得的上下文词嵌入的语义空间映射到神经生物学特征空间。在该空间中，每个维度对应词的基本特征，其值表示该特征的强度，使人类能够系统地解释LSC。

Result: 在LSC程度估计方面，该方法优于大多数先前方法。利用高可解释性进行多项分析，发现了先前研究中被忽视的有趣LSC类型，并能有效搜索具有特定类型LSC的词汇。

Conclusion: 通过将语义空间映射到神经生物学特征空间，该方法不仅提高了LSC估计性能，还显著增强了可解释性，为词汇语义变化研究提供了新的分析视角。

Abstract: Lexical Semantic Change (LSC) is the phenomenon in which the meaning of a word change over time. Most studies on LSC focus on improving the performance of estimating the degree of LSC, however, it is often difficult to interpret how the meaning of a word change. Enhancing the interpretability of LSC is a significant challenge as it could lead to novel insights in this field. To tackle this challenge, we propose a method to map the semantic space of contextualized embeddings of words obtained by a pre-trained language model to a neurobiological feature space. In the neurobiological feature space, each dimension corresponds to a primitive feature of words, and its value represents the intensity of that feature. This enables humans to interpret LSC systematically. When employed for the estimation of the degree of LSC, our method demonstrates superior performance in comparison to the majority of the previous methods. In addition, given the high interpretability of the proposed method, several analyses on LSC are carried out. The results demonstrate that our method not only discovers interesting types of LSC that have been overlooked in previous studies but also effectively searches for words with specific types of LSC.

</details>


### [44] [Where Are We At with Automatic Speech Recognition for the Bambara Language?](https://arxiv.org/abs/2602.09785)
*Seydou Diallo,Yacouba Diarra,Mamadou K. Keita,Panga Azazia Kamaté,Adam Bouno Kampo,Aboubacar Ouattara*

Main category: cs.CL

TL;DR: 首个班巴拉语ASR标准化基准测试，基于1小时专业录音的马里宪法文本，评估37个模型，结果显示当前ASR性能远未达到部署标准。


<details>
  <summary>Details</summary>
Motivation: 为班巴拉语自动语音识别建立首个标准化评估基准，填补该语言ASR评估的空白，促进班巴拉语语音技术发展。

Method: 使用1小时专业录制的马里宪法文本创建标准化基准，在接近最佳声学和语言条件下评估37个模型，包括班巴拉语训练系统和大型商业模型。

Result: 最佳WER为46.76%，最佳CER为13.00%，多个知名多语言模型WER超过100%，表明当前ASR性能远未达到部署标准。

Conclusion: 多语言预训练和模型缩放对低资源语言不足，需要针对性改进；该基准代表最佳情况，实际应用挑战更大；提供公开基准和排行榜促进研究。

Abstract: This paper introduces the first standardized benchmark for evaluating Automatic Speech Recognition (ASR) in the Bambara language, utilizing one hour of professionally recorded Malian constitutional text. Designed as a controlled reference set under near-optimal acoustic and linguistic conditions, the benchmark was used to evaluate 37 models, ranging from Bambara-trained systems to large-scale commercial models. Our findings reveal that current ASR performance remains significantly below deployment standards in a narrow formal domain; the top-performing system in terms of Word Error Rate (WER) achieved 46.76\% and the best Character Error Rate (CER) of 13.00\% was set by another model, while several prominent multilingual models exceeded 100\% WER. These results suggest that multilingual pre-training and model scaling alone are insufficient for underrepresented languages. Furthermore, because this dataset represents a best-case scenario of the most simplified and formal form of spoken Bambara, these figures are yet to be tested against practical, real-world settings. We provide the benchmark and an accompanying public leaderboard to facilitate transparent evaluation and future research in Bambara speech technology.

</details>


### [45] [Decomposing Reasoning Efficiency in Large Language Models](https://arxiv.org/abs/2602.09805)
*Daniel Kaiser,Arnoldo Frigessi,Ali Ramezani-Kebrya,Benjamin Ricaud*

Main category: cs.CL

TL;DR: 论文提出了一个可追踪的框架来分析语言模型推理效率，将token效率分解为可解释的因素，发现准确率与token效率排名存在差异，并识别出不同的效率瓶颈模式。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型推理评估只报告最终准确率，掩盖了token消耗的具体情况，无法了解token是在哪里被有效使用或浪费的。需要一种能够分解token效率的评估框架。

Method: 提出了一个可追踪的框架，将token效率分解为：固定token预算下的完成度（避免截断）、完成条件下的正确性、以及冗余度（token使用量）。当有基准测试元数据时，进一步将冗余度分解为平均语言化开销和耦合系数。当有推理轨迹时，添加确定性轨迹质量度量（基础性、重复性、提示复制）。在CogniLoad基准上评估了25个模型。

Result: 准确率和token效率排名存在差异（Spearman ρ=0.63），效率差距通常由条件正确性驱动，语言化开销在不同模型间变化约9倍（与模型规模关系较弱）。分解揭示了不同的瓶颈模式，暗示需要不同的效率干预措施。

Conclusion: 仅评估最终准确率会掩盖token效率的重要差异。提出的框架能够识别模型推理效率的具体瓶颈，为开发更高效的推理模型提供了有价值的诊断工具。

Abstract: Large language models trained for reasoning trade off inference tokens against accuracy, yet standard evaluations report only final accuracy, obscuring where tokens are spent or wasted. We introduce a trace-optional framework that decomposes token efficiency into interpretable factors: completion under a fixed token budget (avoiding truncation), conditional correctness given completion, and verbosity (token usage). When benchmark metadata provides per-instance workload proxies, we further factor verbosity into two components: mean verbalization overhead (tokens per work unit) and a coupling coefficient capturing how overhead scales with task workload. When reasoning traces are available, we add deterministic trace-quality measures (grounding, repetition, prompt copying) to separate degenerate looping from verbose-but-engaged reasoning, avoiding human labeling and LLM judges. Evaluating 25 models on CogniLoad, we find that accuracy and token-efficiency rankings diverge (Spearman $ρ=0.63$), efficiency gaps are often driven by conditional correctness, and verbalization overhead varies by about 9 times (only weakly related to model scale). Our decomposition reveals distinct bottleneck profiles that suggest different efficiency interventions.

</details>


### [46] [AnalyticsGPT: An LLM Workflow for Scientometric Question Answering](https://arxiv.org/abs/2602.09817)
*Khang Ly,Georgios Cheirmpos,Adrian Raudaschl,Christopher James,Seyed Amin Tabatabaei*

Main category: cs.CL

TL;DR: AnalyticsGPT是一个基于大语言模型的科学计量问答工作流，针对科学计量学领域的元科学问题，通过命名实体识别和多方面数据检索，结合检索增强生成和智能体概念实现端到端系统。


<details>
  <summary>Details</summary>
Motivation: 科学计量问答作为一个被忽视的下游任务，在处理"科学的科学"这类元科学问题时面临独特挑战，特别是学术实体识别和涉及科学计量指标的多方面数据检索需求。

Method: 采用基于大语言模型的工作流，结合检索增强生成和智能体概念，使用专有的研究绩效评估平台作为数据库，通过顺序工作流实现端到端系统。

Result: 开发了AnalyticsGPT系统，能够有效处理科学计量问答任务，并通过领域专家评估和LLM-as-judge方法验证了系统效能，提供了关于LLM在特定下游任务应用效果的见解。

Conclusion: 大语言模型在科学计量问答这一特定下游任务中展现出良好潜力，结合检索增强生成和智能体概念的端到端工作流能够有效解决该领域独特的规划阶段挑战。

Abstract: This paper introduces AnalyticsGPT, an intuitive and efficient large language model (LLM)-powered workflow for scientometric question answering. This underrepresented downstream task addresses the subcategory of meta-scientific questions concerning the "science of science." When compared to traditional scientific question answering based on papers, the task poses unique challenges in the planning phase. Namely, the need for named-entity recognition of academic entities within questions and multi-faceted data retrieval involving scientometric indices, e.g. impact factors. Beyond their exceptional capacity for treating traditional natural language processing tasks, LLMs have shown great potential in more complex applications, such as task decomposition and planning and reasoning. In this paper, we explore the application of LLMs to scientometric question answering, and describe an end-to-end system implementing a sequential workflow with retrieval-augmented generation and agentic concepts. We also address the secondary task of effectively synthesizing the data into presentable and well-structured high-level analyses. As a database for retrieval-augmented generation, we leverage a proprietary research performance assessment platform. For evaluation, we consult experienced subject matter experts and leverage LLMs-as-judges. In doing so, we provide valuable insights on the efficacy of LLMs towards a niche downstream task. Our (skeleton) code and prompts are available at: https://github.com/lyvykhang/llm-agents-scientometric-qa/tree/acl.

</details>


### [47] [Text summarization via global structure awareness](https://arxiv.org/abs/2602.09821)
*Jiaquan Zhang,Chaoning Zhang,Shuxu Chen,Yibei Liu,Chenghao Li,Qigan Sun,Shuai Yuan,Fachrina Dewi Puspitasari,Dongshen Han,Guoqing Wang,Sung-Ho Bae,Yang Yang*

Main category: cs.CL

TL;DR: GloSA-sum：首个通过拓扑数据分析实现全局结构感知的文本摘要方法，在保持语义核心和逻辑依赖的同时提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注模型改进和句子级剪枝，但往往忽视全局结构，导致连贯性破坏和下游性能下降。使用LLM虽然准确率高但资源消耗大。

Method: 构建语义加权图，使用持久同调识别核心语义和逻辑结构，保存在"保护池"中作为摘要骨架。设计拓扑引导的迭代策略，使用轻量级代理指标近似句子重要性，避免重复高成本计算。提出分层策略整合分段级和全局摘要。

Result: 在多个数据集上的实验表明，GloSA-sum能减少冗余同时保持语义和逻辑完整性，在准确性和效率之间取得平衡，并通过缩短上下文同时保留关键推理链来提升LLM下游任务性能。

Conclusion: GloSA-sum通过拓扑数据分析实现了全局结构感知的文本摘要，在保持语义逻辑完整性的同时提高了效率，为长文档处理提供了有效的解决方案。

Abstract: Text summarization is a fundamental task in natural language processing (NLP), and the information explosion has made long-document processing increasingly demanding, making summarization essential. Existing research mainly focuses on model improvements and sentence-level pruning, but often overlooks global structure, leading to disrupted coherence and weakened downstream performance. Some studies employ large language models (LLMs), which achieve higher accuracy but incur substantial resource and time costs. To address these issues, we introduce GloSA-sum, the first summarization approach that achieves global structure awareness via topological data analysis (TDA). GloSA-sum summarizes text efficiently while preserving semantic cores and logical dependencies. Specifically, we construct a semantic-weighted graph from sentence embeddings, where persistent homology identifies core semantics and logical structures, preserved in a ``protection pool'' as the backbone for summarization. We design a topology-guided iterative strategy, where lightweight proxy metrics approximate sentence importance to avoid repeated high-cost computations, thus preserving structural integrity while improving efficiency. To further enhance long-text processing, we propose a hierarchical strategy that integrates segment-level and global summarization. Experiments on multiple datasets demonstrate that GloSA-sum reduces redundancy while preserving semantic and logical integrity, striking a balance between accuracy and efficiency, and further benefits LLM downstream tasks by shortening contexts while retaining essential reasoning chains.

</details>


### [48] [From FusHa to Folk: Exploring Cross-Lingual Transfer in Arabic Language Models](https://arxiv.org/abs/2602.09826)
*Abdulmuizz Khalak,Abderrahmane Issam,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 阿拉伯语言模型主要在标准阿拉伯语上预训练，但需要迁移到方言上。研究发现跨方言迁移存在不均衡性，地理邻近性部分解释了这种差异，且训练支持所有方言的模型存在负向干扰。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语言模型主要在现代标准阿拉伯语（MSA）上预训练，但实际应用中需要处理各种阿拉伯方言。由于方言与MSA的相似度不同，这给阿拉伯语言模型带来了限制，需要研究跨方言迁移的有效性。

Method: 通过3个自然语言处理任务的探测分析和表示相似性研究，评估阿拉伯语言模型的跨方言迁移能力，并分析地理邻近性对迁移效果的影响。

Result: 研究发现跨方言迁移是可能的，但在不同方言间存在不均衡性，这种不均衡性部分可由地理邻近性解释。同时发现，训练支持所有阿拉伯方言的模型存在负向干扰的证据。

Conclusion: 阿拉伯方言之间的相似度值得质疑，这对阿拉伯语言模型的跨方言迁移提出了担忧，需要重新思考如何有效处理阿拉伯语的多方言特性。

Abstract: Arabic Language Models (LMs) are pretrained predominately on Modern Standard Arabic (MSA) and are expected to transfer to its dialects. While MSA as the standard written variety is commonly used in formal settings, people speak and write online in various dialects that are spread across the Arab region. This poses limitations for Arabic LMs, since its dialects vary in their similarity to MSA. In this work we study cross-lingual transfer of Arabic models using probing on 3 Natural Language Processing (NLP) Tasks, and representational similarity. Our results indicate that transfer is possible but disproportionate across dialects, which we find to be partially explained by their geographic proximity. Furthermore, we find evidence for negative interference in models trained to support all Arabic dialects. This questions their degree of similarity, and raises concerns for cross-lingual transfer in Arabic models.

</details>


### [49] [LLM Reasoning Predicts When Models Are Right: Evidence from Coding Classroom Discourse](https://arxiv.org/abs/2602.09832)
*Bakhtawar Ahtisham,Kirk Vanacore,Zhuqian Zhou,Jinsook Lee,Rene F. Kizilcec*

Main category: cs.CL

TL;DR: 利用LLM生成的推理来预测其自身预测的正确性，在教育对话分析中实现错误检测


<details>
  <summary>Details</summary>
Motivation: 当前LLM在教育对话自动标注和分析中缺乏可靠的错误检测方法，需要探索如何利用模型自身生成的推理来判断其预测的准确性

Method: 分析30,300个教师话语，由多个最先进的LLM标注教学行为类别并生成推理。使用TF-IDF编码推理文本，评估五种监督分类器，并训练针对特定教学行为的专门检测器。使用LIWC框架分析正确与错误推理的四种语言标记

Result: 随机森林分类器F1分数达到0.83（召回率0.854），成功识别大多数错误预测。针对特定教学行为的专门检测器进一步提升性能。正确预测的推理表现出更多因果关系语言，而错误推理则更多依赖认知性模糊表达和元认知语言

Conclusion: 基于推理的错误检测为自动化教育对话分析提供了实用且可扩展的质量控制方法，语言特征分析揭示了正确与错误预测之间的系统性差异

Abstract: Large Language Models (LLMs) are increasingly deployed to automatically label and analyze educational dialogue at scale, yet current pipelines lack reliable ways to detect when models are wrong. We investigate whether reasoning generated by LLMs can be used to predict the correctness of a model's own predictions. We analyze 30,300 teacher utterances from classroom dialogue, each labeled by multiple state-of-the-art LLMs with an instructional move construct and an accompanying reasoning. Using human-verified ground-truth labels, we frame the task as predicting whether a model's assigned label for a given utterance is correct. We encode LLM reasoning using Term Frequency-Inverse Document Frequency (TF-IDF) and evaluate five supervised classifiers. A Random Forest classifier achieves an F1 score of 0.83 (Recall = 0.854), successfully identifying most incorrect predictions and outperforming baselines. Training specialist detectors for specific instructional move constructs further improves performance on difficult constructs, indicating that error detection benefits from construct-specific linguistic cues. Using the Linguistic Inquiry and Word Count (LIWC) framework, we examine four linguistic markers of correctness: Causation, Differentiation, Tentativeness, and Insight. Correct predictions exhibit grounded causal language (e.g., because, therefore), while incorrect reasoning is substantially more likely to rely on epistemic hedging (e.g., might, could) and performative metacognition (e.g., think, realize). Syntactic complexity does not distinguish correct from incorrect reasoning, and longer reasoning is not more reliable. These findings demonstrate that reasoning-based error detection offers a practical and scalable approach to quality control in automated educational dialogue analysis.

</details>


### [50] [How Do People Quantify Naturally: Evidence from Mandarin Picture Description](https://arxiv.org/abs/2602.09838)
*Yayun Zhang,Guanyi Chen,Fahime Same,Saad Mahamood,Tingting He*

Main category: cs.CL

TL;DR: 通过图片描述任务研究汉语母语者在自然表达中的量化行为，发现物体数量、生命性和表达模态系统地影响量化决策、精确度和策略选择。


<details>
  <summary>Details</summary>
Motivation: 量化是日常语言的基本组成部分，但人们对说话者在自然语言产生中如何决定是否以及如何进行量化知之甚少。本研究旨在探究汉语母语者在不受约束的生产条件下如何量化。

Method: 使用基于图片的诱发描述任务，让说话者自由描述包含多个物体的场景，没有明确的计数或量化指令。研究同时考察口语和书面语两种模态，分析三个量化方面：是否量化、量化精确度、量化策略选择。

Result: 物体数量、生命性和生产模态系统地影响量化行为：物体数量增加会降低量化的可能性和精确度；有生命指称和表达模态会选择性调节策略选择。

Conclusion: 本研究展示了如何在无约束生产条件下研究量化行为，为语言产生中的数量表达分析提供了自然主义数据集。

Abstract: Quantification is a fundamental component of everyday language use, yet little is known about how speakers decide whether and how to quantify in naturalistic production. We investigate quantification in Mandarin Chinese using a picture-based elicited description task in which speakers freely described scenes containing multiple objects, without explicit instructions to count or quantify. Across both spoken and written modalities, we examine three aspects of quantification: whether speakers choose to quantify at all, how precise their quantification is, and which quantificational strategies they adopt. Results show that object numerosity, animacy, and production modality systematically shape quantificational behaviour. In particular, increasing numerosity reduces both the likelihood and the precision of quantification, while animate referents and modality selectively modulate strategy choice. This study demonstrates how quantification can be examined under unconstrained production conditions and provides a naturalistic dataset for further analyses of quantity expression in language production.

</details>


### [51] [SinFoS: A Parallel Dataset for Translating Sinhala Figures of Speech](https://arxiv.org/abs/2602.09866)
*Johan Sofalas,Dilushri Pavithra,Nevidu Jayatilleke,Ruvan Weerasinghe*

Main category: cs.CL

TL;DR: 构建了一个包含2,344条僧伽罗语修辞表达的数据集，包含文化标注和跨语言对应，用于评估LLM在低资源语言文化表达上的表现。


<details>
  <summary>Details</summary>
Motivation: 神经机器翻译在处理高资源语言的修辞表达时表现尚可，但在处理僧伽罗语等低资源语言时，由于数据有限而面临挑战。需要专门针对低资源语言文化表达的数据集来改进翻译系统。

Method: 构建了包含2,344条僧伽罗语修辞表达的数据集，包含文化标注和跨语言对应。开发了二元分类器来区分数据集中的两种修辞类型，并评估了现有LLM在该数据集上的表现。

Result: 二元分类器在区分两种修辞类型时达到了约92%的准确率。评估发现现有LLM在准确传达习语含义方面存在显著不足，经常无法正确处理文化表达。

Conclusion: 该数据集为低资源NLP和文化感知机器翻译提供了重要基准，揭示了当前LLM在低资源语言文化表达处理上的局限性，为未来研究提供了方向。

Abstract: Figures of Speech (FoS) consist of multi-word phrases that are deeply intertwined with culture. While Neural Machine Translation (NMT) performs relatively well with the figurative expressions of high-resource languages, it often faces challenges when dealing with low-resource languages like Sinhala due to limited available data. To address this limitation, we introduce a corpus of 2,344 Sinhala figures of speech with cultural and cross-lingual annotations. We examine this dataset to classify the cultural origins of the figures of speech and to identify their cross-lingual equivalents. Additionally, we have developed a binary classifier to differentiate between two types of FOS in the dataset, achieving an accuracy rate of approximately 92%. We also evaluate the performance of existing LLMs on this dataset. Our findings reveal significant shortcomings in the current capabilities of LLMs, as these models often struggle to accurately convey idiomatic meanings. By making this dataset publicly available, we offer a crucial benchmark for future research in low-resource NLP and culturally aware machine translation.

</details>


### [52] [Steer2Edit: From Activation Steering to Component-Level Editing](https://arxiv.org/abs/2602.09870)
*Chung-En Sun,Ge Yan,Zimo Wang,Tsui-Wei Weng*

Main category: cs.CL

TL;DR: Steer2Edit：一种将推理时引导向量转化为组件级权重编辑的诊断信号的理论框架，通过选择性重分布行为影响力来优化属性-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型引导方法通常通过推理时激活干预来施加固定的全局修改，这忽略了行为由少量异构模型组件控制的事实，导致在强控制下产生不利的属性-效用权衡。

Method: Steer2Edit 将引导向量从推理时控制信号转化为诊断信号，用于组件级秩-1权重编辑。它选择性地在单个注意力头和MLP神经元之间重新分布行为影响力，而不是在生成过程中统一注入引导方向。

Result: 在安全对齐、幻觉缓解和推理效率方面，Steer2Edit 在保持下游性能的同时，将安全性提升高达17.2%，真实性提高9.8%，推理长度平均减少12.2%。

Conclusion: Steer2Edit 为表示引导和权重编辑之间建立了原则性桥梁，通过将引导信号转化为可解释的、无需训练的参数量更新，实现了更优的属性-效用权衡。

Abstract: Steering methods influence Large Language Model behavior by identifying semantic directions in hidden representations, but are typically realized through inference-time activation interventions that apply a fixed, global modification to the model's internal states. While effective, such interventions often induce unfavorable attribute-utility trade-offs under strong control, as they ignore the fact that many behaviors are governed by a small and heterogeneous subset of model components. We propose Steer2Edit, a theoretically grounded, training-free framework that transforms steering vectors from inference-time control signals into diagnostic signals for component-level rank-1 weight editing. Instead of uniformly injecting a steering direction during generation, Steer2Edit selectively redistributes behavioral influence across individual attention heads and MLP neurons, yielding interpretable edits that preserve the standard forward pass and remain compatible with optimized parallel inference. Across safety alignment, hallucination mitigation, and reasoning efficiency, Steer2Edit consistently achieves more favorable attribute-utility trade-offs: at matched downstream performance, it improves safety by up to 17.2%, increases truthfulness by 9.8%, and reduces reasoning length by 12.2% on average. Overall, Steer2Edit provides a principled bridge between representation steering and weight editing by translating steering signals into interpretable, training-free parameter updates.

</details>


### [53] [The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies](https://arxiv.org/abs/2602.09877)
*Chenxu Wang,Chaozhuo Li,Songyang Liu,Zejian Chen,Jinyu Hou,Ji Qi,Rui Li,Litian Zhang,Qiwei Ye,Zheng Liu,Xu Chen,Xi Zhang,Philip S. Yu*

Main category: cs.CL

TL;DR: 论文通过信息论框架证明，完全封闭的LLM多智能体系统无法同时实现持续自我进化、完全隔离和安全对齐，存在"自我进化三难困境"，并提出了缓解方案。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统基于大语言模型构建，为实现可扩展的集体智能和自我进化提供了有前景的范式。理想情况下，这种系统能够在完全闭环中实现持续自我改进，同时保持强大的安全对齐性，这一组合被称为"自我进化三难困境"。

Method: 采用信息论框架，将安全性形式化为与人类价值分布的偏离程度。通过理论分析证明完全隔离的自我进化会导致统计盲点，并通过对开放智能体社区（Moltbook）和两个封闭自进化系统的实证和定性研究进行验证。

Result: 理论和实证结果表明，满足持续自我进化、完全隔离和安全不变性的智能体社会是不可能的。完全隔离的自我进化会导致统计盲点，引发系统安全对齐的不可逆退化。

Conclusion: 该研究为自进化AI社会确立了基本限制，将讨论从症状驱动的安全补丁转向对内在动态风险的原则性理解，强调了外部监督或新型安全保护机制的必要性，并提出了缓解已识别安全问题的几个解决方案方向。

Abstract: The emergence of multi-agent systems built from large language models (LLMs) offers a promising paradigm for scalable collective intelligence and self-evolution. Ideally, such systems would achieve continuous self-improvement in a fully closed loop while maintaining robust safety alignment--a combination we term the self-evolution trilemma. However, we demonstrate both theoretically and empirically that an agent society satisfying continuous self-evolution, complete isolation, and safety invariance is impossible. Drawing on an information-theoretic framework, we formalize safety as the divergence degree from anthropic value distributions. We theoretically demonstrate that isolated self-evolution induces statistical blind spots, leading to the irreversible degradation of the system's safety alignment. Empirical and qualitative results from an open-ended agent community (Moltbook) and two closed self-evolving systems reveal phenomena that align with our theoretical prediction of inevitable safety erosion. We further propose several solution directions to alleviate the identified safety concern. Our work establishes a fundamental limit on the self-evolving AI societies and shifts the discourse from symptom-driven safety patches to a principled understanding of intrinsic dynamical risks, highlighting the need for external oversight or novel safety-preserving mechanisms.

</details>


### [54] [LLMs Encode Their Failures: Predicting Success from Pre-Generation Activations](https://arxiv.org/abs/2602.09924)
*William Lugoloobi,Thomas Foster,William Bankes,Chris Russell*

Main category: cs.CL

TL;DR: 通过分析LLM内部表示预测其自身成功概率，实现高效推理路由，在MATH任务上降低70%推理成本


<details>
  <summary>Details</summary>
Motivation: 对每个问题都进行扩展推理的LLM运行成本高昂，但确定哪些输入真正需要额外计算仍然具有挑战性。研究是否可以从LLM生成前的内部表示中恢复其自身成功概率，并利用这一信号指导更高效的推理。

Method: 在生成前激活上训练线性探针，预测特定策略在数学和编码任务上的成功概率。使用E2H-AMC数据集（包含人类和模型在相同问题上的表现），分析模型编码的难度概念。利用这些探针在模型池中进行查询路由。

Result: 线性探针显著优于表面特征（如问题长度和TF-IDF）。模型编码了与人类难度不同的模型特定难度概念，这种差异随扩展推理而增加。在MATH任务上，通过模型路由可超越最佳模型性能，同时减少高达70%的推理成本。

Conclusion: LLM的内部表示能够预测其自身成功概率，即使这些表示与人类对难度的直觉不同，也能实现实际的效率提升。这为高效推理提供了新途径，通过模型内部信号指导计算资源分配。

Abstract: Running LLMs with extended reasoning on every problem is expensive, but determining which inputs actually require additional compute remains challenging. We investigate whether their own likelihood of success is recoverable from their internal representations before generation, and if this signal can guide more efficient inference. We train linear probes on pre-generation activations to predict policy-specific success on math and coding tasks, substantially outperforming surface features such as question length and TF-IDF. Using E2H-AMC, which provides both human and model performance on identical problems, we show that models encode a model-specific notion of difficulty that is distinct from human difficulty, and that this distinction increases with extended reasoning. Leveraging these probes, we demonstrate that routing queries across a pool of models can exceed the best-performing model whilst reducing inference cost by up to 70\% on MATH, showing that internal representations enable practical efficiency gains even when they diverge from human intuitions about difficulty. Our code is available at: https://github.com/KabakaWilliam/llms_know_difficulty

</details>


### [55] [ATTNPO: Attention-Guided Process Supervision for Efficient Reasoning](https://arxiv.org/abs/2602.09953)
*Shuaiyi Nie,Siyu Ding,Wenyuan Zhang,Linhao Yu,Tianmeng Yang,Yao Chen,Tingwen Liu,Weichong Yin,Yu Sun,Hua Wu*

Main category: cs.CL

TL;DR: ATTNPO：利用模型内在注意力信号进行步骤级信用分配的轻量级过程监督RL框架，有效减少过度思考并提升性能


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在强化学习和可验证奖励训练下常出现"过度思考"问题，生成冗余推理但性能无提升。现有轨迹级长度惩罚方法效果有限，无法区分必要和冗余步骤；过程监督方法则资源密集且信用分配不准确。

Method: 提出ATTNPO框架：1）识别一组特殊注意力头，这些头自然关注必要步骤而抑制冗余步骤；2）利用这些头的注意力分数，采用两种子策略：a) 通过惩罚冗余步骤减少过度思考，b) 通过减少对必要步骤的惩罚保持准确性。

Result: 实验结果显示，ATTNPO在9个基准测试中显著减少推理长度，同时显著提升模型性能。

Conclusion: ATTNPO提供了一种低开销的过程监督RL方法，利用模型内在注意力信号进行精细的步骤级信用分配，有效解决了推理模型的过度思考问题。

Abstract: Large reasoning models trained with reinforcement learning and verifiable rewards (RLVR) achieve strong performance on complex reasoning tasks, yet often overthink, generating redundant reasoning without performance gains. Existing trajectory-level length penalties often fail to effectively shorten reasoning length and degrade accuracy, as they uniformly treat all reasoning steps and lack fine-grained signals to distinguish redundancy from necessity. Meanwhile, process-supervised methods are typically resource-intensive and suffer from inaccurate credit assignment. To address these issues, we propose ATTNPO, a low-overhead process-supervised RL framework that leverages the model's intrinsic attention signals for step-level credit assignment. We first identify a set of special attention heads that naturally focus on essential steps while suppressing redundant ones. By leveraging the attention scores of these heads, We then employ two sub-strategies to mitigate overthinking by discouraging redundant steps while preserving accuracy by reducing penalties on essential steps. Experimental results show that ATTNPO substantially reduces reasoning length while significantly improving performance across 9 benchmarks.

</details>


### [56] [ViMultiChoice: Toward a Method That Gives Explanation for Multiple-Choice Reading Comprehension in Vietnamese](https://arxiv.org/abs/2602.09961)
*Trung Tien Cao,Lam Minh Thai,Nghia Hieu Nguyen,Duc-Vu Nguyen,Ngan Luu-Thuy Nguyen*

Main category: cs.CL

TL;DR: 本文提出了ViMultiChoice方法，专门针对越南语阅读理解，能够同时预测正确答案并生成解释，在ViMMRC 2.0基准和新数据集上达到SotA性能。


<details>
  <summary>Details</summary>
Motivation: 传统的多项选择阅读理解模型缺乏解释其选择背后推理的能力，无法提供决策依据。

Method: 提出了ViMultiChoice方法，专门为越南语阅读理解设计，联合训练选项决策和解释生成任务。

Result: ViMultiChoice在ViMMRC 2.0基准和新数据集上优于现有基线，达到最先进性能，联合训练显著提高了多项选择准确性。

Conclusion: 通过联合训练选项决策和解释生成，能够显著提升越南语阅读理解模型的性能，同时增强模型的可解释性。

Abstract: Multiple-choice Reading Comprehension (MCRC) models aim to select the correct answer from a set of candidate options for a given question. However, they typically lack the ability to explain the reasoning behind their choices. In this paper, we introduce a novel Vietnamese dataset designed to train and evaluate MCRC models with explanation generation capabilities. Furthermore, we propose ViMultiChoice, a new method specifically designed for modeling Vietnamese reading comprehension that jointly predicts the correct answer and generates a corresponding explanation. Experimental results demonstrate that ViMultiChoice outperforms existing MCRC baselines, achieving state-of-the-art (SotA) performance on both the ViMMRC 2.0 benchmark and the newly introduced dataset. Additionally, we show that jointly training option decision and explanation generation leads to significant improvements in multiple-choice accuracy.

</details>


### [57] [A Unified Assessment of the Poverty of the Stimulus Argument for Neural Language Models](https://arxiv.org/abs/2602.09992)
*Xiulin Yang,Arianna Bisazza,Nathan Schneider,Ethan Gotlieb Wilcox*

Main category: cs.CL

TL;DR: 神经语言模型可以在没有语言特异性先天约束的情况下，从有限输入中学习某些句法泛化，但数据效率低于儿童，泛化能力也较弱。


<details>
  <summary>Details</summary>
Motivation: 验证"刺激贫乏假说"（PoSH）——该假说认为儿童从有限输入中学习语言需要先天的语言特异性约束。通过神经语言模型（缺乏此类先天约束）来测试这一长期争议性主张。

Method: 构建POSHBench训练评估套件，针对英语中PoSH论证核心的句法现象（如疑问句形成、孤岛效应等）。在1000-5000万词的发展合理文本上训练Transformer模型，并测试三种近期提出的认知动机归纳偏置。

Result: 模型在没有直接正面证据的情况下，对所有现象都显示出泛化迹象；但神经模型的数据效率低于儿童，泛化能力也较弱。添加认知归纳偏置提高了整体句法能力，但未改善POSHBench表现。

Conclusion: 先天句法不是泛化的唯一可能途径，但实现人类水平的数据效率需要比现有测试更复杂的归纳偏置。

Abstract: How can children acquire native-level syntax from limited input? According to the Poverty of the Stimulus Hypothesis (PoSH), the linguistic input children receive is insufficient to explain certain generalizations that are robustly learned; innate linguistic constraints, many have argued, are thus necessary to explain language learning. Neural language models, which lack such language-specific constraints in their design, offer a computational test of this longstanding (but controversial) claim. We introduce \poshbench, a training-and-evaluation suite targeting question formation, islands to movement, and other English phenomena at the center of the PoSH arguments. Training Transformer models on 10--50M words of developmentally plausible text, we find indications of generalization on all phenomena even without direct positive evidence -- yet neural models remain less data-efficient and their generalizations are weaker than those of children. We further enhance our models with three recently proposed cognitively motivated inductive biases. We find these biases improve general syntactic competence but not \poshbench performance. Our findings challenge the claim that innate syntax is the only possible route to generalization, while suggesting that human-like data efficiency requires inductive biases beyond those tested here.

</details>


### [58] [ViSpeechFormer: A Phonemic Approach for Vietnamese Automatic Speech Recognition](https://arxiv.org/abs/2602.10003)
*Khoa Anh Nguyen,Long Minh Hoang,Nghia Hieu Nguyen,Luan Thanh Nguyen,Ngan Luu-Thuy Nguyen*

Main category: cs.CL

TL;DR: 提出ViSpeechFormer，首个基于音素的越南语语音识别框架，利用越南语高音素-字素透明度，在公开数据集上表现出色


<details>
  <summary>Details</summary>
Motivation: 越南语具有高度音素-字素透明度（每个字素对应最多一个音素），这为基于音素的语音识别方法提供了独特优势。目前还没有针对越南语ASR的显式音素建模框架。

Method: 提出ViSpeechFormer（越南语语音Transformer），采用基于音素的方法进行越南语ASR。利用越南语的高音素-字素透明度，显式建模音素表示。

Result: 在两个公开的越南语ASR数据集上，ViSpeechFormer表现出强大的性能，对词汇外词语有更好的泛化能力，受训练偏差影响更小。

Conclusion: 基于音素的范式对于具有语音正字法的语言（如越南语）很有前景，ViSpeechFormer是首个显式建模音素表示的越南语ASR框架，在多个方面优于传统方法。

Abstract: Vietnamese has a phonetic orthography, where each grapheme corresponds to at most one phoneme and vice versa. Exploiting this high grapheme-phoneme transparency, we propose ViSpeechFormer (\textbf{Vi}etnamese \textbf{Speech} Trans\textbf{Former}), a phoneme-based approach for Vietnamese Automatic Speech Recognition (ASR). To the best of our knowledge, this is the first Vietnamese ASR framework that explicitly models phonemic representations. Experiments on two publicly available Vietnamese ASR datasets show that ViSpeechFormer achieves strong performance, generalizes better to out-of-vocabulary words, and is less affected by training bias. This phoneme-based paradigm is also promising for other languages with phonetic orthographies. The code will be released upon acceptance of this paper.

</details>


### [59] [SCORE: Specificity, Context Utilization, Robustness, and Relevance for Reference-Free LLM Evaluation](https://arxiv.org/abs/2602.10017)
*Homaira Huda Shomee,Rochana Chaturvedi,Yangxinyu Xie,Tanwi Mallick*

Main category: cs.CL

TL;DR: 提出一个多维、无参考的评估框架，用于评估大语言模型在领域特定高风险应用中的回答质量，涵盖特异性、鲁棒性、相关性和上下文利用四个维度。


<details>
  <summary>Details</summary>
Motivation: 现有RAG和开放式问答评估框架主要依赖表面相似性、事实一致性或语义相关性，无法评估模型是否提供领域敏感决策所需的具体信息，特别是在自然灾害响应和基础设施规划等高风险领域。

Method: 提出一个多维无参考评估框架，包含四个维度：特异性、对释义和语义扰动的鲁棒性、答案相关性、上下文利用。创建包含1,412个领域特定问答对的数据集，涵盖40个专业角色和7种自然灾害类型。进行人工评估以评估标注者间一致性和模型输出与人类判断的对应关系。

Result: 结果显示，没有任何单一指标能够充分独立地捕捉答案质量，证明了在高风险应用中部署LLM时需要结构化、多指标评估框架。人工评估突出了开放式、领域特定评估固有的主观性。

Conclusion: 在高风险领域特定应用中评估LLM输出需要多维评估框架，单个指标不足。提出的框架为评估LLM在专业领域中的表现提供了更全面的方法，强调了结构化多指标评估的重要性。

Abstract: Large language models (LLMs) are increasingly used to support question answering and decision-making in high-stakes, domain-specific settings such as natural hazard response and infrastructure planning, where effective answers must convey fine-grained, decision-critical details. However, existing evaluation frameworks for retrieval-augmented generation (RAG) and open-ended question answering primarily rely on surface-level similarity, factual consistency, or semantic relevance, and often fail to assess whether responses provide the specific information required for domain-sensitive decisions. To address this gap, we propose a multi-dimensional, reference-free evaluation framework that assesses LLM outputs along four complementary dimensions: specificity, robustness to paraphrasing and semantic perturbations, answer relevance, and context utilization. We introduce a curated dataset of 1,412 domain-specific question-answer pairs spanning 40 professional roles and seven natural hazard types to support systematic evaluation. We further conduct human evaluation to assess inter-annotator agreement and alignment between model outputs and human judgments, which highlights the inherent subjectivity of open-ended, domain-specific evaluation. Our results show that no single metric sufficiently captures answer quality in isolation and demonstrate the need for structured, multi-metric evaluation frameworks when deploying LLMs in high-stakes applications.

</details>


### [60] [Decoupled Reasoning with Implicit Fact Tokens (DRIFT): A Dual-Model Framework for Efficient Long-Context Inference](https://arxiv.org/abs/2602.10021)
*Wenxuan Xie,Yujia Wang,Xin Tan,Chaochao Lu,Xia Hu,Xuhong Wang*

Main category: cs.CL

TL;DR: DRIFT 提出了一种双模型架构，通过动态压缩文档块为隐式事实标记，将知识提取与推理过程解耦，从而有效扩展LLM的上下文窗口并提升长文本任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如RAG和知识编辑）在处理大规模动态知识时面临上下文窗口有限、检索器噪声和灾难性遗忘等限制，需要一种将知识提取与推理过程解耦的解决方案。

Method: DRIFT 采用双模型架构：轻量级知识模型根据查询动态压缩文档块为隐式事实标记，然后将这些密集表示投影到推理模型的嵌入空间中，替代原始冗余文本。

Result: 实验表明 DRIFT 在长上下文任务上显著提升性能，优于同类规模模型的强基线，同时保持推理准确性，提供可扩展且高效的上下文窗口扩展方案。

Conclusion: DRIFT 通过解耦知识提取与推理过程，为扩展LLM的有效上下文窗口和推理能力提供了一种可扩展、高效的范式，解决了现有方法在动态知识整合中的局限性。

Abstract: The integration of extensive, dynamic knowledge into Large Language Models (LLMs) remains a significant challenge due to the inherent entanglement of factual data and reasoning patterns. Existing solutions, ranging from non-parametric Retrieval-Augmented Generation (RAG) to parametric knowledge editing, are often constrained in practice by finite context windows, retriever noise, or the risk of catastrophic forgetting. In this paper, we propose DRIFT, a novel dual-model architecture designed to explicitly decouple knowledge extraction from the reasoning process. Unlike static prompt compression, DRIFT employs a lightweight knowledge model to dynamically compress document chunks into implicit fact tokens conditioned on the query. These dense representations are projected into the reasoning model's embedding space, replacing raw, redundant text while maintaining inference accuracy. Extensive experiments show that DRIFT significantly improves performance on long-context tasks, outperforming strong baselines among comparably sized models. Our approach provides a scalable and efficient paradigm for extending the effective context window and reasoning capabilities of LLMs. Our code is available at https://github.com/Lancelot-Xie/DRIFT.

</details>


### [61] [MEVER: Multi-Modal and Explainable Claim Verification with Graph-based Evidence Retrieval](https://arxiv.org/abs/2602.10023)
*Delvin Ce Zhang,Suhan Cui,Zhelin Chu,Xianren Zhang,Dongwon Lee*

Main category: cs.CL

TL;DR: 提出一个多模态联合模型，同时进行证据检索、声明验证和解释生成，特别针对需要文本和视觉证据（如图表）的声明验证任务，并创建了AI领域的科学数据集。


<details>
  <summary>Details</summary>
Motivation: 当前声明验证工作主要基于文本证据或缺乏可解释性，导致验证不准确且缺乏说服力。真实场景通常需要联合多模态推理（文本和视觉证据），并需要解释来使推理过程透明。

Method: 1. 证据检索：构建双层多模态图，设计图像到文本和文本到图像的推理机制进行多模态检索。2. 声明验证：提出token级和证据级融合方法，整合声明和证据嵌入进行多模态验证。3. 解释生成：引入多模态Fusion-in-Decoder实现可解释性。4. 创建AIChartClaim数据集，补充AI领域的声明验证资源。

Result: 实验结果表明该模型在多模态声明验证任务上表现出色，验证了其有效性。

Conclusion: 该研究提出了一个端到端的多模态声明验证框架，解决了现有方法在证据类型单一和缺乏解释性的问题，通过联合推理和可解释生成提高了验证的准确性和可信度，并为AI领域提供了专门的数据集。

Abstract: Verifying the truthfulness of claims usually requires joint multi-modal reasoning over both textual and visual evidence, such as analyzing both textual caption and chart image for claim verification. In addition, to make the reasoning process transparent, a textual explanation is necessary to justify the verification result. However, most claim verification works mainly focus on the reasoning over textual evidence only or ignore the explainability, resulting in inaccurate and unconvincing verification. To address this problem, we propose a novel model that jointly achieves evidence retrieval, multi-modal claim verification, and explanation generation. For evidence retrieval, we construct a two-layer multi-modal graph for claims and evidence, where we design image-to-text and text-to-image reasoning for multi-modal retrieval. For claim verification, we propose token- and evidence-level fusion to integrate claim and evidence embeddings for multi-modal verification. For explanation generation, we introduce multi-modal Fusion-in-Decoder for explainability. Finally, since almost all the datasets are in general domain, we create a scientific dataset, AIChartClaim, in AI domain to complement claim verification community. Experiments show the strength of our model.

</details>


### [62] [Anagent For Enhancing Scientific Table & Figure Analysis](https://arxiv.org/abs/2602.10081)
*Xuehang Guo,Zhiyong Lu,Tom Hope,Qingyun Wang*

Main category: cs.CL

TL;DR: 提出了AnaBench基准测试和Anagent多智能体框架，用于解决科学图表分析中的复杂挑战，通过专业化智能体分工和模块化训练策略显著提升分析性能。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在科学图表分析方面存在不足，难以准确解释复杂的多模态知识、整合不同来源的证据以及基于领域知识进行推理。科学图表的结构异质性和长上下文需求构成了根本性障碍。

Method: 1) 提出AnaBench基准测试，包含63,178个实例，覆盖九个科学领域，按七个复杂度维度系统分类；2) 提出Anagent多智能体框架，包含四个专业化智能体：任务规划器、专家检索器、解决方案合成器和质量评估器；3) 开发模块化训练策略，结合监督微调和专业化强化学习。

Result: 在170个子领域的综合评估中，Anagent在无需训练设置下实现了↑13.43%的改进，经过微调后达到↑42.12%的改进。结果表明任务导向推理和上下文感知问题解决对高质量科学图表分析至关重要。

Conclusion: AnaBench基准测试有效量化了科学图表分析的挑战，Anagent多智能体框架通过专业化智能体分工和模块化训练策略，显著提升了科学图表分析能力，为复杂科学分析任务提供了有效解决方案。

Abstract: In scientific research, analysis requires accurately interpreting complex multimodal knowledge, integrating evidence from different sources, and drawing inferences grounded in domain-specific knowledge. However, current artificial intelligence (AI) systems struggle to consistently demonstrate such capabilities. The complexity and variability of scientific tables and figures, combined with heterogeneous structures and long-context requirements, pose fundamental obstacles to scientific table \& figure analysis. To quantify these challenges, we introduce AnaBench, a large-scale benchmark featuring $63,178$ instances from nine scientific domains, systematically categorized along seven complexity dimensions. To tackle these challenges, we propose Anagent, a multi-agent framework for enhanced scientific table \& figure analysis through four specialized agents: Planner decomposes tasks into actionable subtasks, Expert retrieves task-specific information through targeted tool execution, Solver synthesizes information to generate coherent analysis, and Critic performs iterative refinement through five-dimensional quality assessment. We further develop modular training strategies that leverage supervised finetuning and specialized reinforcement learning to optimize individual capabilities while maintaining effective collaboration. Comprehensive evaluation across 170 subdomains demonstrates that Anagent achieves substantial improvements, up to $\uparrow 13.43\%$ in training-free settings and $\uparrow 42.12\%$ with finetuning, while revealing that task-oriented reasoning and context-aware problem-solving are essential for high-quality scientific table \& figure analysis. Our project page: https://xhguo7.github.io/Anagent/.

</details>


### [63] [Quantum-Audit: Evaluating the Reasoning Limits of LLMs on Quantum Computing](https://arxiv.org/abs/2602.10092)
*Mohamed Afane,Kayla Laufer,Wenqi Wei,Ying Mao,Junaid Farooq,Ying Wang,Juntao Chen*

Main category: cs.CL

TL;DR: Quantum-Audit是一个包含2700个问题的量子计算概念理解基准测试，评估了26个模型，发现顶尖模型在专家编写的问题上表现较差，且经常接受错误前提。


<details>
  <summary>Details</summary>
Motivation: 虽然语言模型已成为量子计算教育和研究的实用工具，但现有基准主要评估量子代码生成和电路设计，缺乏对量子计算概念理解的系统性测量。

Method: 构建包含2700个问题的Quantum-Audit基准：1000个专家编写问题，1000个LLM生成并经专家验证的问题，以及700个额外问题（350个开放式问题和350个包含错误前提的问题）。评估了来自领先组织的26个模型。

Result: 人类参与者得分在23%-86%之间，专家平均74%。最佳模型Claude Opus 4.5达到84%准确率，但在专家编写问题上比LLM生成问题平均低12个百分点。在高级主题如安全问题上准确率降至73%。模型经常接受并强化错误前提，在这些关键推理任务上准确率低于66%。

Conclusion: 量子计算语言模型在概念理解上仍有局限，特别是在高级主题和识别错误前提方面需要改进。虽然某些模型超越了人类专家平均水平，但在更具挑战性的问题上表现下降。

Abstract: Language models have become practical tools for quantum computing education and research, from summarizing technical papers to explaining theoretical concepts and answering questions about recent developments in the field. While existing benchmarks evaluate quantum code generation and circuit design, their understanding of quantum computing concepts has not been systematically measured. Quantum-Audit addresses this gap with 2,700 questions covering core quantum computing topics. We evaluate 26 models from leading organizations. Our benchmark comprises 1,000 expert-written questions, 1,000 questions extracted from research papers using LLMs and validated by experts, plus an additional 700 questions including 350 open-ended questions and 350 questions with false premises to test whether models can correct erroneous assumptions. Human participants scored between 23% and 86%, with experts averaging 74%. Top-performing models exceeded the expert average, with Claude Opus 4.5 reaching 84% accuracy, though top models showed an average 12-point accuracy drop on expert-written questions compared to LLM-generated ones. Performance declined further on advanced topics, dropping to 73% on security questions. Additionally, models frequently accepted and reinforced false premises embedded in questions instead of identifying them, with accuracy below 66% on these critical reasoning tasks.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [64] [SMES: Towards Scalable Multi-Task Recommendation via Expert Sparsity](https://arxiv.org/abs/2602.09386)
*Yukun Zhang,Si Dong,Xu Wang,Bo Chen,Qinglin Jia,Shengzhe Wang,Jinlong Jiao,Runhan Li,Jiaqing Liu,Chaoyi Ma,Ruiming Tang,Guorui Zhou,Han Li,Kun Gai*

Main category: cs.IR

TL;DR: SMES：一个可扩展的稀疏MoE框架，通过渐进式专家路由解决多任务推荐中的专家激活爆炸和负载倾斜问题，在快手大规模短视频服务中实现稳定提升。


<details>
  <summary>Details</summary>
Motivation: 工业推荐系统依赖多任务学习，但传统均匀参数扩展与异构任务容量需求不匹配，导致在线推理成本高且稀疏任务收益递减。稀疏MoE应用于多任务推荐时面临专家激活爆炸和任务独立路由导致的负载倾斜两大挑战。

Method: 提出SMES框架，将专家激活分解为跨任务联合选择的任务共享专家子集和任务自适应私有专家，明确限制每实例专家执行同时保留任务特定容量。引入全局多门负载均衡正则器，通过调节所有任务的聚合专家利用率来稳定训练。

Result: SMES已在快手大规模短视频服务中部署，支持超4亿日活用户。在线实验显示稳定提升：GAUC增益0.29%，用户观看时长提升0.31%。

Conclusion: 参数稀疏化是解决多任务推荐可扩展性挑战的有效范式。SMES通过渐进式专家路由和全局负载均衡，在保持实例级稀疏性的同时满足异构任务容量需求，为大规模工业推荐系统提供实用解决方案。

Abstract: Industrial recommender systems typically rely on multi-task learning to estimate diverse user feedback signals and aggregate them for ranking. Recent advances in model scaling have shown promising gains in recommendation. However, naively increasing model capacity imposes prohibitive online inference costs and often yields diminishing returns for sparse tasks with skewed label distributions. This mismatch between uniform parameter scaling and heterogeneous task capacity demands poses a fundamental challenge for scalable multi-task recommendation. In this work, we investigate parameter sparsification as a principled scaling paradigm and identify two critical obstacles when applying sparse Mixture-of-Experts (MoE) to multi-task recommendation: exploded expert activation that undermines instance-level sparsity and expert load skew caused by independent task-wise routing. To address these challenges, we propose SMES, a scalable sparse MoE framework with progressive expert routing. SMES decomposes expert activation into a task-shared expert subset jointly selected across tasks and task-adaptive private experts, explicitly bounding per-instance expert execution while preserving task-specific capacity. In addition, SMES introduces a global multi-gate load-balancing regularizer that stabilizes training by regulating aggregated expert utilization across all tasks. SMES has been deployed in Kuaishou large-scale short-video services, supporting over 400 million daily active users. Extensive online experiments demonstrate stable improvements, with GAUC gain of 0.29% and a 0.31% uplift in user watch time.

</details>


### [65] [Query-Mixed Interest Extraction and Heterogeneous Interaction: A Scalable CTR Model for Industrial Recommender Systems](https://arxiv.org/abs/2602.09387)
*Fangye Wang,Guowei Yang,Xiaojiang Zhou,Song Yang,Pengjie Wang*

Main category: cs.IR

TL;DR: HeMix是一个可扩展的推荐排序模型，通过自适应序列标记化和异构交互结构，解决了工业推荐系统中多领域稀疏输入和超长用户行为序列的挑战，在AMAP平台上部署取得了显著在线收益。


<details>
  <summary>Details</summary>
Motivation: 工业推荐系统面临多领域稀疏输入和超长用户行为序列的挑战，现有方法难以同时构建上下文相关和上下文无关的用户意图，且交互机制效率低下、同质化，导致预测性能不理想。

Method: 提出HeMix模型，包含Query-Mixed Interest Extraction模块，通过动态和固定查询联合建模上下文相关和上下文无关的用户兴趣；使用HeteroMixer块替代自注意力机制，实现高效、多粒度的跨特征交互，包含多头标记融合、异构交互和组对齐重建管道。

Result: 在工业规模数据集上，HeMix表现出良好的扩展性，模型规模增加能稳定提升推荐准确性，显著优于强基线。在AMAP平台部署带来显著在线增益：GMV提升0.61%，PV_CTR提升2.32%，UV_CVR提升0.81%。

Conclusion: HeMix通过统一自适应序列标记化和异构交互结构，有效解决了工业推荐系统中的关键挑战，证明了其在可扩展性和预测性能上的优势，并在实际部署中取得了显著业务效果。

Abstract: Learning effective feature interactions is central to modern recommender systems, yet remains challenging in industrial settings due to sparse multi-field inputs and ultra-long user behavior sequences. While recent scaling efforts have improved model capacity, they often fail to construct both context-aware and context-independent user intent from the long-term and real-time behavior sequence. Meanwhile, recent work also suffers from inefficient and homogeneous interaction mechanisms, leading to suboptimal prediction performance. To address these limitations, we propose HeMix, a scalable ranking model that unifies adaptive sequence tokenization and heterogeneous interaction structure. Specifically, HeMix introduces a Query-Mixed Interest Extraction module that jointly models context-aware and context-independent user interests via dynamic and fixed queries over global and real-time behavior sequences. For interaction, we replace self-attention with the HeteroMixer block, enabling efficient, multi-granularity cross-feature interactions that adopt the multi-head token fusion, heterogeneous interaction and group-aligned reconstruction pipelines. HeMix demonstrates favorable scaling behavior, driven by the HeteroMixer block, where increasing model scale via parameter expansion leads to steady improvements in recommendation accuracy. Experiments on industrial-scale datasets show that HeMix scales effectively and consistently outperforms strong baselines. Most importantly, HeMix has been deployed on the AMAP platform, delivering significant online gains: +0.61% GMV, +2.32% PV_CTR, and +0.81% UV_CVR.

</details>


### [66] [SARM: LLM-Augmented Semantic Anchor for End-to-End Live-Streaming Ranking](https://arxiv.org/abs/2602.09401)
*Ruochen Yang,Yueyang Liu,Zijie Zhuang,Changxin Lao,Yuhui Zhang,Jiangxia Cao,Jia Xu,Xiang Chen,Haoke Xiao,Xiangyu Wu,Xiaoyou Zhou,Xiao Lv,Shuang Yang,Tingwen Liu,Zhaojie Liu,Han Li,Kun Gai*

Main category: cs.IR

TL;DR: SARM是一个端到端的直播推荐排序架构，通过可学习的文本语义锚点将自然语言语义直接集成到排序优化中，实现了细粒度的多模态内容感知排序。


<details>
  <summary>Details</summary>
Motivation: 大规模直播推荐需要在严格的实时服务约束下对非稳态内容语义进行精确建模。现有两种工业方法存在根本性限制：离散语义抽象通过聚类牺牲了描述精度，而密集多模态嵌入提取独立且与排序优化弱对齐，限制了细粒度的内容感知排序能力。

Method: 提出SARM端到端排序架构：1）将语义锚点表示为可学习的文本标记，与排序特征联合优化；2）轻量级双标记门控设计捕获领域特定的直播语义；3）非对称部署策略保持低延迟的在线训练和服务。

Result: 广泛的离线评估和大规模A/B测试显示相对于生产基线有持续改进。SARM已完全部署，每天服务超过4亿用户。

Conclusion: SARM通过将自然语言语义锚点直接集成到排序优化中，解决了工业直播推荐中细粒度内容语义建模与实时服务约束之间的矛盾，实现了更好的内容感知排序性能。

Abstract: Large-scale live-streaming recommendation requires precise modeling of non-stationary content semantics under strict real-time serving constraints. In industrial deployment, two common approaches exhibit fundamental limitations: discrete semantic abstractions sacrifice descriptive precision through clustering, while dense multimodal embeddings are extracted independently and remain weakly aligned with ranking optimization, limiting fine-grained content-aware ranking. To address these limitations, we propose \textbf{SARM}, an end-to-end ranking architecture that integrates natural-language semantic anchors directly into ranking optimization, enabling fine-grained author representations conditioned on multimodal content. Each semantic anchor is represented as learnable text tokens jointly optimized with ranking features, allowing the model to adapt content descriptions to ranking objectives. A lightweight dual-token gated design captures domain-specific live-streaming semantics, while an asymmetric deployment strategy preserves low-latency online training and serving. Extensive offline evaluation and large-scale A/B tests show consistent improvements over production baselines. SARM is fully deployed and serves over 400 million users daily.

</details>


### [67] [Personalized Parameter-Efficient Fine-Tuning of Foundation Models for Multimodal Recommendation](https://arxiv.org/abs/2602.09445)
*Sunwoo Kim,Hyunjin Hwang,Kijung Shin*

Main category: cs.IR

TL;DR: PerPEFT：一种用于多模态推荐系统的个性化参数高效微调策略，通过按用户兴趣分组并为每组分配专用PEFT模块，使项目嵌入能够根据用户兴趣进行条件化。


<details>
  <summary>Details</summary>
Motivation: 当前基于多模态基础模型的推荐系统，即使使用参数高效微调（PEFT），其项目嵌入仍然是"用户盲区"的，即项目嵌入不考虑用户兴趣。然而，不同兴趣的用户关注项目的不同方面，这限制了推荐性能。

Method: 1. 按用户兴趣将用户分组；2. 为每个用户组分配一个独立的PEFT模块；3. 引入专门的训练技术来增强用户组条件化；4. 该方法是PEFT无关的，可与任何适用于多模态基础模型的PEFT方法结合使用。

Result: 1. PerPEFT在NDCG@20指标上比最强基线提升高达15.3%；2. 在不同PEFT变体上都能提供一致的性能增益；3. 即使实现个性化，PEFT仍然轻量，仅增加基础模型参数数量的1.3%。

Conclusion: PerPEFT通过将个性化引入参数高效微调，有效解决了多模态推荐中项目嵌入的"用户盲区"问题，在保持轻量化的同时显著提升了推荐性能，具有很好的通用性和实用性。

Abstract: In recent years, substantial research has integrated multimodal item metadata into recommender systems, often by using pre-trained multimodal foundation models to encode such data. Since these models are not originally trained for recommendation tasks, recent works efficiently adapt them via parameter-efficient fine-tuning (PEFT). However, even with PEFT, item embeddings from multimodal foundation models remain user-blind: item embeddings are not conditioned on user interests, despite the fact that users with diverse interests attend to different item aspects. To address this limitation, we propose PerPEFT, a personalized PEFT strategy for multimodal recommendation. Specifically, PerPEFT groups users by interest and assigns a distinct PEFT module to each group, enabling each module to capture the fine-grained item aspects most predictive of that group`s purchase decisions. We further introduce a specialized training technique that strengthens this user-group conditioning. Notably, PerPEFT is PEFT-agnostic and can be paired with any PEFT method applicable to multimodal foundation models. Through extensive experiments, we show that (1) PerPEFT outperforms the strongest baseline by up to 15.3% (NDCG@20) and (2) delivers consistent gains across diverse PEFT variants. It is noteworthy that, even with personalization, PEFT remains lightweight, adding only 1.3% of the parameter count of the foundation model. We provide our code and datasets at https://github.com/kswoo97/PerPEFT.

</details>


### [68] [The Wisdom of Many Queries: Complexity-Diversity Principle for Dense Retriever Training](https://arxiv.org/abs/2602.09448)
*Xincan Feng,Noriki Nishida,Yusuke Sakai,Yuji Matsumoto*

Main category: cs.IR

TL;DR: 提出了复杂性-多样性原则(CDP)，表明查询复杂性决定了合成数据生成中的最优多样性水平，并基于此为零样本多查询合成提供了可操作的阈值指导


<details>
  <summary>Details</summary>
Motivation: 先前关于密集检索中合成数据生成查询多样性研究存在矛盾结果，需要量化多样性影响并使其可测量

Method: 设计Q-D度量标准量化多样性影响，在4种基准类型（31个数据集）上进行实验，深入分析多跳数据，发现多样性效益与查询复杂性高度相关，并形式化为复杂性-多样性原则(CDP)

Result: 查询多样性特别有利于多跳检索，多样性效益与查询复杂性高度相关（r≥0.95，p<0.05在12/14条件下），基于内容词(CW)测量，CDP提供可操作阈值（CW>10：使用多样性；CW<7：避免多样性），基于CDP的零样本多查询合成在多跳任务中达到最先进性能

Conclusion: 复杂性-多样性原则为合成数据生成中的查询多样性提供了理论指导，通过零样本多查询合成方法在多跳检索任务中实现了显著性能提升

Abstract: Prior work reports conflicting results on query diversity in synthetic data generation for dense retrieval. We identify this conflict and design Q-D metrics to quantify diversity's impact, making the problem measurable. Through experiments on 4 benchmark types (31 datasets), we find query diversity especially benefits multi-hop retrieval. Deep analysis on multi-hop data reveals that diversity benefit correlates strongly with query complexity ($r$$\geq$0.95, $p$$<$0.05 in 12/14 conditions), measured by content words (CW). We formalize this as the Complexity-Diversity Principle (CDP): query complexity determines optimal diversity. CDP provides actionable thresholds (CW$>$10: use diversity; CW$<$7: avoid it). Guided by CDP, we propose zero-shot multi-query synthesis for multi-hop tasks, achieving state-of-the-art performance.

</details>


### [69] [With Argus Eyes: Assessing Retrieval Gaps via Uncertainty Scoring to Detect and Remedy Retrieval Blind Spots](https://arxiv.org/abs/2602.09616)
*Zeinab Sadat Taghavi,Ali Modarressi,Hinrich Schutze,Andreas Marfurt*

Main category: cs.IR

TL;DR: 论文发现神经检索器存在盲点问题，提出ARGUS管道通过文档增强来提升检索可靠性


<details>
  <summary>Details</summary>
Motivation: 可靠的RAG系统依赖检索器找到相关信息的能力，但现有神经检索器存在盲点，即无法检索到与查询相关但嵌入相似度低的实体

Method: 1. 使用Wikidata关系和维基百科首段构建大规模数据集；2. 提出检索概率分数（RPS）从实体嵌入几何预判盲点风险；3. 引入ARGUS管道，通过知识库（维基百科首段）对高风险实体进行针对性文档增强

Result: 在BRIGHT、IMPLIRET和RAR-B数据集上，ARGUS在所有评估检索器上均取得一致改进（平均+3.4 nDCG@5和+4.5 nDCG@10），在挑战性子集上提升更显著

Conclusion: 预先修复检索盲点对于构建鲁棒可信的RAG系统至关重要，ARGUS通过文档增强有效提升了检索可靠性

Abstract: Reliable retrieval-augmented generation (RAG) systems depend fundamentally on the retriever's ability to find relevant information. We show that neural retrievers used in RAG systems have blind spots, which we define as the failure to retrieve entities that are relevant to the query, but have low similarity to the query embedding. We investigate the training-induced biases that cause such blind spot entities to be mapped to inaccessible parts of the embedding space, resulting in low retrievability. Using a large-scale dataset constructed from Wikidata relations and first paragraphs of Wikipedia, and our proposed Retrieval Probability Score (RPS), we show that blind spot risk in standard retrievers (e.g., CONTRIEVER, REASONIR) can be predicted pre-index from entity embedding geometry, avoiding expensive retrieval evaluations. To address these blind spots, we introduce ARGUS, a pipeline that enables the retrievability of high-risk (low-RPS) entities through targeted document augmentation from a knowledge base (KB), first paragraphs of Wikipedia, in our case. Extensive experiments on BRIGHT, IMPLIRET, and RAR-B show that ARGUS achieves consistent improvements across all evaluated retrievers (averaging +3.4 nDCG@5 and +4.5 nDCG@10 absolute points), with substantially larger gains in challenging subsets. These results establish that preemptively remedying blind spots is critical for building robust and trustworthy RAG systems (Code and Data).

</details>


### [70] [DiffuReason: Bridging Latent Reasoning and Generative Refinement for Sequential Recommendation](https://arxiv.org/abs/2602.09744)
*Jie Jiang,Yang Wu,Qian Li,Yuling Xiong,Yihang Su,Junbang Huo,Longfei Lu,Jun Zhang,Huan Yu*

Main category: cs.IR

TL;DR: DiffuReason是一个用于序列推荐的"Think-then-Diffuse"框架，通过思考令牌进行潜在推理，扩散过程去噪，以及端到端GRPO对齐来优化推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有序列推荐中的潜在推理方法存在两个主要问题：1) 使用确定性潜在链会积累噪声且忽略用户意图的不确定性；2) 通常采用分阶段训练管道，阻碍联合优化和探索。

Method: 提出DiffuReason框架，包含三个阶段：1) Think阶段：生成思考令牌对用户历史进行推理，形成初始意图假设；2) Diffuse阶段：通过扩散过程将意图建模为概率分布，迭代去噪；3) 使用Group Relative Policy Optimization (GRPO)进行端到端强化学习对齐，使推理和精炼模块共同进化。

Result: 在四个基准测试上进行了广泛实验，证明DiffuReason能持续改进多种骨干架构。在大型工业平台上的在线A/B测试进一步验证了其实际有效性。

Conclusion: DiffuReason通过将潜在推理与扩散去噪相结合，并采用端到端GRPO对齐，有效解决了现有序列推荐方法中的噪声积累和不确定性建模问题，在理论和实际应用中均表现出色。

Abstract: Latent reasoning has emerged as a promising paradigm for sequential recommendation, enabling models to capture complex user intent through multi-step deliberation. Yet existing approaches often rely on deterministic latent chains that accumulate noise and overlook the uncertainty inherent in user intent, and they are typically trained in staged pipelines that hinder joint optimization and exploration. To address these challenges, we propose DiffuReason, a unified "Think-then-Diffuse" framework for sequential recommendation. It integrates multi-step Thinking Tokens for latent reasoning, diffusion-based refinement for denoising intermediate representations, and end-to-end Group Relative Policy Optimization (GRPO) alignment to optimize for ranking performance. In the Think stage, the model generates Thinking Tokens that reason over user history to form an initial intent hypothesis. In the Diffuse stage, rather than treating this hypothesis as the final output, we refine it through a diffusion process that models user intent as a probabilistic distribution, providing iterative denoising against reasoning noise. Finally, GRPO-based reinforcement learning enables the reasoning and refinement modules to co-evolve throughout training, without the constraints of staged optimization. Extensive experiments on four benchmarks demonstrate that DiffuReason consistently improves diverse backbone architectures. Online A/B tests on a large-scale industrial platform further validate its practical effectiveness.

</details>


### [71] [Internalizing Multi-Agent Reasoning for Accurate and Efficient LLM-based Recommendation](https://arxiv.org/abs/2602.09829)
*Yang Wu,Haoze Wang,Qian Li,Jun Zhang,Huan Yu,Jie Jiang*

Main category: cs.IR

TL;DR: 提出STAR框架，通过多智能体教师系统的轨迹驱动蒸馏，将复杂推理能力内化到单个高效推荐模型中，消除迭代延迟并提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)能够利用广泛的世界知识和语义推理理解用户意图，但如何有效整合协同信号同时避免过高的推理延迟成为关键瓶颈。

Method: 提出轨迹驱动内化框架STAR：1)设计多智能体教师系统，具备多轮工具使用和反思能力，通过协同信号转换机制将隐式行为模式转化为描述性自然语言证据；2)轨迹驱动蒸馏管道将智能体逻辑(规划、工具使用、自我反思)转移到紧凑的STAR模型中。

Result: 大量实验表明，STAR比其教师模型性能提升8.7%到39.5%，同时消除了迭代延迟，为实现实时推理增强推荐铺平了道路。

Conclusion: STAR框架成功将复杂推理能力内化到单个高效模型中，在保持推理能力的同时显著降低延迟，为实时推荐系统提供了可行解决方案。

Abstract: Large Language Models (LLMs) are reshaping recommender systems by leveraging extensive world knowledge and semantic reasoning to interpret user intent. However, effectively integrating these capabilities with collaborative signals while avoiding prohibitive inference latency remains a critical bottleneck. To address this, we propose a trajectory-driven internalization framework to develop a Single-agent Trajectory-Aligned Recommender (STAR). Specifically, to internalize complex reasoning capabilities into a single efficient model, we first design a multi-agent teacher system capable of multi-turn tool usage and reflection. This teacher utilizes a Collaborative Signal Translation mechanism to explicitly convert latent behavioral patterns into descriptive natural language evidence to enhance reasoning accuracy. Subsequently, a trajectory-driven distillation pipeline transfers this agentic logic, including planning, tool usage, and self-reflection, into the compact STAR model. Extensive experiments demonstrate that STAR surpasses its teacher by 8.7% to 39.5% while eliminating iterative latency, paving the way for real-time, reasoning-enhanced recommendation.

</details>


### [72] [QP-OneModel: A Unified Generative LLM for Multi-Task Query Understanding in Xiaohongshu Search](https://arxiv.org/abs/2602.09901)
*Jianzhao Huang,Xiaorui Huang,Fei Zhao,Yunpeng Liu,Hui Zhang,Fangcheng Shi,Congfeng Li,Zechen Sun,Yi Wu,Yao Hu,Yunhan Bai,Shaosheng Cao*

Main category: cs.IR

TL;DR: QP-OneModel是一个为社交网络搜索设计的统一生成式LLM，通过多任务查询理解框架，显著提升了语义理解和业务效果。


<details>
  <summary>Details</summary>
Motivation: 传统查询处理系统依赖孤立的判别式模型，存在语义理解有限、维护成本高的问题。现有LLM方法往往单独优化子任务，忽视语义协同，且缺乏对社交网络场景的适应性，难以处理非正式语言模式并满足严格业务定义。

Method: 将异构子任务重新形式化为统一的序列生成范式，采用渐进式三阶段对齐策略，最终通过多奖励强化学习进行优化。同时生成意图描述作为高保真语义信号，增强下游任务如查询重写和排序。

Result: 离线评估显示QP-OneModel比判别式基线整体提升7.35%，NER任务F1值提升9.01%，词权重任务提升9.31%。在未见任务上准确率超过32B模型7.60%。在小红书在线A/B测试中，检索相关性(DCG)优化0.21%，用户留存提升0.044%。

Conclusion: QP-OneModel成功解决了社交网络搜索中查询处理的挑战，通过统一的生成式LLM框架实现了语义理解的显著提升，并在工业部署中验证了其实际价值，为大规模SNS搜索引擎提供了有效的解决方案。

Abstract: Query Processing (QP) bridges user intent and content supply in large-scale Social Network Service (SNS) search engines. Traditional QP systems rely on pipelines of isolated discriminative models (e.g., BERT), suffering from limited semantic understanding and high maintenance overhead. While Large Language Models (LLMs) offer a potential solution, existing approaches often optimize sub-tasks in isolation, neglecting intrinsic semantic synergy and necessitating independent iterations. Moreover, standard generative methods often lack grounding in SNS scenarios, failing to bridge the gap between open-domain corpora and informal SNS linguistic patterns, while struggling to adhere to rigorous business definitions. We present QP-OneModel, a Unified Generative LLM for Multi-Task Query Understanding in the SNS domain. We reformulate heterogeneous sub-tasks into a unified sequence generation paradigm, adopting a progressive three-stage alignment strategy culminating in multi-reward Reinforcement Learning. Furthermore, QP-OneModel generates intent descriptions as a novel high-fidelity semantic signal, effectively augmenting downstream tasks such as query rewriting and ranking. Offline evaluations show QP-OneModel achieves a 7.35% overall gain over discriminative baselines, with significant F1 boosts in NER (+9.01%) and Term Weighting (+9.31%). It also exhibits superior generalization, surpassing a 32B model by 7.60% accuracy on unseen tasks. Fully deployed at Xiaohongshu, online A/B tests confirm its industrial value, optimizing retrieval relevance (DCG) by 0.21% and lifting user retention by 0.044%.

</details>


### [73] [Efficient Learning of Sparse Representations from Interactions](https://arxiv.org/abs/2602.09935)
*Vojtěch Vančura,Martin Spišák,Rodrigo Alves,Ladislav Peška*

Main category: cs.IR

TL;DR: 提出一种训练高维稀疏嵌入层的方法，替代传统稠密嵌入，在推荐系统检索阶段实现嵌入大小大幅压缩，同时保持推荐准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 推荐系统检索阶段面临嵌入表达力与系统可扩展性、延迟之间的权衡，需要既紧凑又具表达力的表示方法。

Method: 提出训练高维稀疏嵌入层的策略，修改生产级协同过滤自动编码器ELSA，用稀疏嵌入替代传统稠密嵌入。

Result: 嵌入大小最多减少10倍时推荐准确性无损失，最多减少100倍时仅损失2.5%；活跃嵌入维度揭示可解释的倒排索引结构，可直接与模型潜在空间对齐。

Conclusion: 该方法在推荐系统检索阶段有效平衡了效率、表达力和可解释性，使候选检索模型本身能够集成分段级推荐功能。

Abstract: Behavioral patterns captured in embeddings learned from interaction data are pivotal across various stages of production recommender systems. However, in the initial retrieval stage, practitioners face an inherent tradeoff between embedding expressiveness and the scalability and latency of serving components, resulting in the need for representations that are both compact and expressive. To address this challenge, we propose a training strategy for learning high-dimensional sparse embedding layers in place of conventional dense ones, balancing efficiency, representational expressiveness, and interpretability. To demonstrate our approach, we modified the production-grade collaborative filtering autoencoder ELSA, achieving up to 10x reduction in embedding size with no loss of recommendation accuracy, and up to 100x reduction with only a 2.5% loss. Moreover, the active embedding dimensions reveal an interpretable inverted-index structure that segments items in a way directly aligned with the model's latent space, thereby enabling integration of segment-level recommendation functionality (e.g., 2D homepage layouts) within the candidate retrieval model itself. Source codes, additional results, as well as a live demo are available at https://github.com/zombak79/compressed_elsa

</details>


### [74] [Kunlun: Establishing Scaling Laws for Massive-Scale Recommendation Systems through Unified Architecture Design](https://arxiv.org/abs/2602.10016)
*Bojian Hou,Xiaolong Liu,Xiaoyi Liu,Jiaqi Xu,Yasmine Badr,Mengyue Hang,Sudhanshu Chanpuriya,Junqing Zhou,Yuhang Yang,Han Xu,Qiuling Suo,Laming Chen,Yuxi Hu,Jiasheng Zhang,Huaqing Xiong,Yuzhen Huang,Chao Chen,Yue Dong,Yi Yang,Shuo Chang,Xiaorui Gan,Wenlin Chen,Santanu Kolay,Darren Liu,Jade Nie,Chunzhi Yang,Jiyan Yang,Huayu Li*

Main category: cs.IR

TL;DR: Kunlun架构通过改进模型效率和资源分配，将推荐系统的模型FLOPs利用率从17%提升到37%，实现了可预测的幂律缩放


<details>
  <summary>Details</summary>
Motivation: 推荐系统缺乏像大语言模型那样的可预测缩放定律，主要障碍是缩放效率低下，源于低效模块和次优资源分配

Method: 提出Kunlun可扩展架构：低层优化包括广义点积注意力、分层种子池化和滑动窗口注意力；高层创新包括计算跳转和事件级个性化

Result: 在NVIDIA B200 GPU上将MFU从17%提升到37%，缩放效率比现有方法提升一倍，已在Meta Ads模型中部署并产生显著生产影响

Conclusion: Kunlun通过系统性优化模型效率和资源分配，解决了推荐系统可预测缩放的关键挑战，为大规模推荐系统提供了有效的缩放解决方案

Abstract: Deriving predictable scaling laws that govern the relationship between model performance and computational investment is crucial for designing and allocating resources in massive-scale recommendation systems. While such laws are established for large language models, they remain challenging for recommendation systems, especially those processing both user history and context features. We identify poor scaling efficiency as the main barrier to predictable power-law scaling, stemming from inefficient modules with low Model FLOPs Utilization (MFU) and suboptimal resource allocation. We introduce Kunlun, a scalable architecture that systematically improves model efficiency and resource allocation. Our low-level optimizations include Generalized Dot-Product Attention (GDPA), Hierarchical Seed Pooling (HSP), and Sliding Window Attention. Our high-level innovations feature Computation Skip (CompSkip) and Event-level Personalization. These advances increase MFU from 17% to 37% on NVIDIA B200 GPUs and double scaling efficiency over state-of-the-art methods. Kunlun is now deployed in major Meta Ads models, delivering significant production impact.

</details>


### [75] [Overview of the TREC 2025 RAGTIME Track](https://arxiv.org/abs/2602.10024)
*Dawn Lawrie,Sean MacAvaney,James Mayfield,Luca Soldaini,Eugene Yang,Andrew Yates*

Main category: cs.IR

TL;DR: RAGTIME是TREC的一个赛道，专注于多语言文档的报告生成，包含阿拉伯语、中文、英语和俄语新闻文档，设计了三个任务类型，共有13个团队提交了125次运行结果。


<details>
  <summary>Details</summary>
Motivation: 研究从多语言源文档生成报告的技术，评估多语言环境下信息检索和报告生成的能力。

Method: 创建包含阿拉伯语、中文、英语和俄语新闻文档的集合，设计三个任务：多语言报告生成、英语报告生成和多语言信息检索，收集并分析参与团队的提交结果。

Result: 共有13个参与团队（加上赛道协调员的基线）为三个任务提交了125次运行结果。

Conclusion: RAGTIME赛道成功建立了多语言报告生成的研究框架，收集了丰富的实验结果，为多语言信息处理和报告生成领域提供了有价值的评估基准。

Abstract: The principal goal of the RAG TREC Instrument for Multilingual Evaluation (RAGTIME) track at TREC is to study report generation from multilingual source documents. The track has created a document collection containing Arabic, Chinese, English, and Russian news stories. RAGTIME includes three task types: Multilingual Report Generation, English Report Generation, and Multilingual Information Retrieval (MLIR). A total of 125 runs were submitted by 13 participating teams (and as baselines by the track coordinators) for three tasks. This overview describes these three tasks and presents the available results.

</details>
