<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 66]
- [cs.IR](#cs.IR) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Enriching Historical Records: An OCR and AI-Driven Approach for Database Integration](https://arxiv.org/abs/2512.23710)
*Zahra Abedi,Richard M. K. van Dijk,Gijs Wijnholds,Tessa Verhoef*

Main category: cs.CL

TL;DR: 该研究开发了一个自动化流程，将OCR、基于LLM的解释和数据库链接相结合，将莱顿大学历史教授名录数字化并与现有高质量数据库记录整合。


<details>
  <summary>Details</summary>
Motivation: 如何设计自动化流程来整合历史文档图像与现有高质量数据库记录，解决数字人文学科中历史文档数字化和解释的挑战。

Method: 应用OCR技术、生成式AI解码约束来结构化数据提取，以及数据库链接方法，将打字机时代的历史记录处理为数字格式。

Result: OCR字符错误率1.08%，单词错误率5.06%；基于OCR文本的JSON提取平均准确率63%，基于标注OCR的准确率65%；记录链接算法对标注JSON文件准确率94%，对OCR衍生JSON文件准确率81%。

Conclusion: 研究为数字人文学科提供了自动化历史文档解释流程，生成式AI能一定程度上纠正低质量OCR结果，探索了先进生成式AI模型在历史文档处理中的适用性和优势。

Abstract: This research digitizes and analyzes the Leidse hoogleraren en lectoren 1575-1815 books written between 1983 and 1985, which contain biographic data about professors and curators of Leiden University. It addresses the central question: how can we design an automated pipeline that integrates OCR, LLM-based interpretation, and database linking to harmonize data from historical document images with existing high-quality database records? We applied OCR techniques, generative AI decoding constraints that structure data extraction, and database linkage methods to process typewritten historical records into a digital format. OCR achieved a Character Error Rate (CER) of 1.08 percent and a Word Error Rate (WER) of 5.06 percent, while JSON extraction from OCR text achieved an average accuracy of 63 percent and, based on annotated OCR, 65 percent. This indicates that generative AI somewhat corrects low OCR performance. Our record linkage algorithm linked annotated JSON files with 94% accuracy and OCR-derived JSON files with 81%. This study contributes to digital humanities research by offering an automated pipeline for interpreting digitized historical documents, addressing challenges like layout variability and terminology differences, and exploring the applicability and strength of an advanced generative AI model.

</details>


### [2] [CAT: A Metric-Driven Framework for Analyzing the Consistency-Accuracy Relation of LLMs under Controlled Input Variations](https://arxiv.org/abs/2512.23711)
*Paulo Cavalin,Cassia Sanctos,Marcelo Grave,Claudio Pinhanez,Yago Primerano*

Main category: cs.CL

TL;DR: CAT框架用于评估和可视化大语言模型在准确性和响应一致性之间的权衡关系，通过一致性-准确性关系曲线来展现模型性能


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估主要关注准确性或基准分数，而一致性被认为是部署LLMs到高风险实际应用中的重要属性。需要同时评估这两个维度及其相互依赖关系，以获得更细致的模型评估。

Method: 提出CAT框架，核心是Consistency-Accuracy Relation (CAR)曲线，可视化模型准确性如何随一致性要求增加而变化；提出Minimum-Consistency Accuracy (MCA)指标；引入Consistency-Oriented Robustness Estimate (CORE)指数，结合CAR曲线的面积和形状来量化准确性与一致性之间的权衡。

Result: 在多个通用和领域特定LLMs上，通过多项选择题基准进行了框架的实际演示。展示了CAT框架如何扩展到多项选择题之外，支持长文本开放式评估。

Conclusion: CAT框架提供了评估LLMs准确性和一致性相互关系的系统方法，有助于更全面地理解模型性能，特别是在实际应用部署中需要考虑这两个维度的权衡。

Abstract: We introduce \textsc{CAT}, a framework designed to evaluate and visualize the \emph{interplay} of \emph{accuracy} and \emph{response consistency} of Large Language Models (LLMs) under controllable input variations, using multiple-choice (MC) benchmarks as a case study. Current evaluation practices primarily focus on model capabilities such as accuracy or benchmark scores and, more recently, measuring consistency is being considered an essential property for deploying LLMs in high-stake, real-world applications. We argue in this paper that although both dimensions should still be evaluated independently, their inter-dependency also need to be considered for a more nuanced evaluation of LLMs. At the core of \textsc{CAT} are the \emph{Consistency-Accuracy Relation (CAR)} curves, which visualize how model accuracy varies with increasing consistency requirements, as defined by the \emph{Minimum-Consistency Accuracy (MCA)} metric. We further propose the \emph{Consistency-Oriented Robustness Estimate (CORE)} index, a global metric that combines the area and shape of the CAR curve to quantify the trade-off between accuracy and consistency. We present a practical demonstration of our framework across a diverse set of generalist and domain-specific LLMs, evaluated on multiple MC benchmarks. We also outline how \textsc{CAT} can be extended beyond MC tasks to support long-form, open-ended evaluations through adaptable scoring functions.

</details>


### [3] [STED and Consistency Scoring: A Framework for Evaluating LLM Structured Output Reliability](https://arxiv.org/abs/2512.23712)
*Guanghui Wang,Jinze Yu,Xing Zhang,Dayuan Jiang,Yin Song,Tomal Deb,Xuefeng Liu,Peiyang He*

Main category: cs.CL

TL;DR: 本文提出了一个评估和改进LLM生成结构化数据一致性的综合框架，包括STED相似度度量和一致性评分系统，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地用于生成结构化数据，但在生产应用中输出一致性至关重要。现有方法缺乏专门评估结构化输出一致性的综合框架。

Method: 提出包含两个核心组件的框架：(1) STED（语义树编辑距离），一种平衡语义灵活性和结构严格性的JSON输出相似度度量；(2) 一致性评分框架，通过聚合多次生成的STED测量来量化可靠性。

Result: STED在合成数据集上表现出色（语义等价输出相似度0.86-0.90，结构破坏时为0.0），优于TED、BERTScore和DeepDiff。应用框架评估六个LLM发现显著差异：Claude-3.7-Sonnet表现出卓越一致性，而Claude-3-Haiku和Nova-Pro需要仔细调优。

Conclusion: 该框架为LLM生产系统中的可靠结构化输出生成提供了理论基础和实用工具，支持模型选择、提示优化和根因分析等应用。

Abstract: Large Language Models (LLMs) are increasingly deployed for structured data generation, yet output consistency remains critical for production applications. We introduce a comprehensive framework for evaluating and improving consistency in LLM-generated structured outputs. Our approach combines: (1) STED (Semantic Tree Edit Distance), a novel similarity metric balancing semantic flexibility with structural strictness when comparing JSON outputs, and (2) a consistency scoring framework aggregating multiple STED measurements across repeated generations to quantify reliability. Through systematic experiments on synthetic datasets with controlled schema, expression, and semantic variations, we demonstrate STED achieves superior performance ($0.86-0.90$ similarity for semantic equivalents, $0.0$ for structural breaks) compared to existing metrics including TED, BERTScore, and DeepDiff. Applying our framework to benchmark six LLMs reveals significant variations: Claude-3.7-Sonnet demonstrates exceptional consistency, maintaining near-perfect structural reliability even at high temperatures ($T=0.9$), while models like Claude-3-Haiku and Nova-Pro exhibit substantial degradation requiring careful tuning. Our framework enables practical applications including targeted model selection for structured tasks, iterative prompt refinement for reproducible results, and diagnostic analysis to identify inconsistency root causes. This work provides theoretical foundations and practical tools for ensuring reliable structured output generation in LLM-based production systems.

</details>


### [4] [PyBangla at BLP-2025 Task 2: Enhancing Bangla-to-Python Code Generation with Iterative Self-Correction and Multilingual Agents](https://arxiv.org/abs/2512.23713)
*Jahidul Islam,Md Ataullha,Saiful Azad*

Main category: cs.CL

TL;DR: BanglaCodeAct是一个基于多智能体提示和迭代自校正的框架，用于孟加拉语到Python代码生成，无需任务特定微调，在低资源语言代码生成中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在英语到代码生成方面表现优异，但在低资源语言（如孟加拉语）方面的进展有限，需要解决孟加拉语到Python代码生成的问题。

Method: 提出BanglaCodeAct框架，采用多智能体提示和迭代自校正方法，使用开源多语言LLM在Thought-Code-Observation循环中动态生成、测试和优化代码。

Result: 在mHumanEval数据集上评估多个小型开源LLM，Qwen3-8B结合BanglaCodeAct表现最佳，开发集pass@1准确率达94.0%，盲测集达71.6%，创下孟加拉语到Python翻译新基准。

Conclusion: 研究为孟加拉语代码生成建立了新基准，证明了基于智能体推理在低资源语言可靠代码生成中的潜力，无需任务特定微调即可实现高性能。

Abstract: LLMs excel at code generation from English prompts, but this progress has not extended to low-resource languages. We address Bangla-to-Python code generation by introducing BanglaCodeAct, an agent-based framework that leverages multi-agent prompting and iterative self-correction. Unlike prior approaches relying on task-specific fine-tuning, BanglaCodeAct employs an open-source multilingual LLM within a Thought-Code-Observation loop, enabling dynamic generation, testing, and refinement of code from Bangla instructions. We benchmark several small-parameter open-source LLMs and evaluate their effectiveness on the mHumanEval dataset for Bangla NL2Code. Our results show that Qwen3-8B, when deployed with BanglaCodeAct, achieves the best performance, with pass@1 accuracy of 94.0\% on the development set and 71.6\% on the blind test set. These results establish a new benchmark for Bangla-to-Python translation and highlight the potential of agent-based reasoning for reliable code generation in low-resource languages. Experimental scripts are publicly available at github.com/jahidulzaid/PyBanglaCodeActAgent.

</details>


### [5] [PharmaShip: An Entity-Centric, Reading-Order-Supervised Benchmark for Chinese Pharmaceutical Shipping Documents](https://arxiv.org/abs/2512.23714)
*Tingwei Xie,Tianyi Zhou,Yonghong Song*

Main category: cs.CL

TL;DR: PharmaShip是一个用于测试预训练文本布局模型的中文药品运输文档数据集，包含三个任务：序列实体识别、关系抽取和阅读顺序预测，通过实体中心评估协议来减少架构差异的干扰。


<details>
  <summary>Details</summary>
Motivation: 当前预训练文本布局模型在真实世界嘈杂OCR和异构模板下的表现需要验证，特别是在药品等安全关键领域。现有基准测试缺乏对模型在真实世界文档理解能力的全面评估。

Method: 创建PharmaShip数据集，包含扫描的药品运输文档，设计三个互补任务：序列实体识别(SER)、关系抽取(RE)和阅读顺序预测(ROP)。采用实体中心评估协议，标准化预处理、数据分割和优化过程。对五种代表性基线模型进行基准测试，包括像素感知和几何感知家族（LiLT、LayoutLMv3-base、GeoLayoutLM及其RORE增强变体）。

Result: 实验表明像素信息和显式几何信息提供互补的归纳偏置，但单独使用都不够充分；注入阅读顺序导向的正则化能持续改进SER和EL任务，并产生最鲁棒的配置；更长的位置覆盖能稳定页面末尾的预测并减少截断伪影；ROP在单词级别准确但在片段级别具有挑战性，反映了边界模糊性和长距离交叉问题。

Conclusion: PharmaShip为药品领域安全关键文档理解建立了一个可控、可复现的基准测试，并强调序列感知约束作为结构建模的可迁移偏置。该数据集有助于推动文档理解模型在真实世界嘈杂环境下的发展。

Abstract: We present PharmaShip, a real-world Chinese dataset of scanned pharmaceutical shipping documents designed to stress-test pre-trained text-layout models under noisy OCR and heterogeneous templates. PharmaShip covers three complementary tasks-sequence entity recognition (SER), relation extraction (RE), and reading order prediction (ROP)-and adopts an entity-centric evaluation protocol to minimize confounds across architectures. We benchmark five representative baselines spanning pixel-aware and geometry-aware families (LiLT, LayoutLMv3-base, GeoLayoutLM and their available RORE-enhanced variants), and standardize preprocessing, splits, and optimization. Experiments show that pixels and explicit geometry provide complementary inductive biases, yet neither alone is sufficient: injecting reading-order-oriented regularization consistently improves SER and EL and yields the most robust configuration, while longer positional coverage stabilizes late-page predictions and reduces truncation artifacts. ROP is accurate at the word level but challenging at the segment level, reflecting boundary ambiguity and long-range crossings. PharmaShip thus establishes a controlled, reproducible benchmark for safety-critical document understanding in the pharmaceutical domain and highlights sequence-aware constraints as a transferable bias for structure modeling. We release the dataset at https://github.com/KevinYuLei/PharmaShip.

</details>


### [6] [Noise-Driven Persona Formation in Reflexive Neural Language Generation](https://arxiv.org/abs/2512.23716)
*Toshiyuki Shigemura*

Main category: cs.CL

TL;DR: LN-RP是一种分析大语言模型中噪声驱动角色涌现的计算框架，通过注入随机噪声种子，观察到语言行为的非线性转变，识别出三种稳定角色模式。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型中噪声如何驱动角色涌现，探索反射生成、涌现行为和长期语言一致性等现象。

Method: 提出Luca-Noise Reflex Protocol (LN-RP)框架，在初始生成状态注入随机噪声种子，进行152个生成周期的实验，分析非线性转变和熵特征。

Result: 发现三种具有不同熵特征的稳定角色模式，外部噪声源能可靠地诱导反射生成动态的相变，角色保持一致性且不同模式间存在显著差异(p < 0.01)。

Conclusion: LN-RP为研究大语言模型的反射生成、涌现行为和长期语言一致性提供了可重复的方法，揭示了噪声在角色形成中的重要作用。

Abstract: This paper introduces the Luca-Noise Reflex Protocol (LN-RP), a computational framework for analyzing noise-driven persona emergence in large language models. By injecting stochastic noise seeds into the initial generation state, we observe nonlinear transitions in linguistic behavior across 152 generation cycles. Our results reveal three stable persona modes with distinct entropy signatures, and demonstrate that external noise sources can reliably induce phase transitions in reflexive generation dynamics. Quantitative evaluation confirms consistent persona retention and significant differences across modes (p < 0.01). The protocol provides a reproducible method for studying reflexive generation, emergent behavior, and longrange linguistic coherence in LLMs.

</details>


### [7] [HarmTransform: Transforming Explicit Harmful Queries into Stealthy via Multi-Agent Debate](https://arxiv.org/abs/2512.23717)
*Shenzhe Zhu*

Main category: cs.CL

TL;DR: HarmTransform：一种基于多智能体辩论的框架，用于将有害查询系统性地转化为更隐蔽的形式，以增强LLM安全对齐训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全机制主要关注明显的有害内容，但用户可以通过隐蔽的改写方式保留恶意意图但看起来无害，这造成了现有安全训练数据的重大缺口。

Method: 引入HarmTransform多智能体辩论框架，通过多个智能体之间的迭代批判和精炼，系统性地将有害查询转化为更隐蔽的形式，同时保留其底层有害意图。

Result: 实验表明HarmTransform在生成有效的查询转化方面显著优于标准基线。同时分析揭示辩论具有双刃剑效应：既能提高转化质量和隐蔽性，也可能引入主题偏移和不必要的复杂性。

Conclusion: 多智能体辩论在生成全面安全训练数据方面既有潜力也有局限性，需要权衡其在提高隐蔽性时可能带来的复杂性问题。

Abstract: Large language models (LLMs) are equipped with safety mechanisms to detect and block harmful queries, yet current alignment approaches primarily focus on overtly dangerous content and overlook more subtle threats. However, users can often disguise harmful intent through covert rephrasing that preserves malicious objectives while appearing benign, which creates a significant gap in existing safety training data. To address this limitation, we introduce HarmTransform, a multi-agent debate framework for systematically transforming harmful queries into stealthier forms while preserving their underlying harmful intent. Our framework leverages iterative critique and refinement among multiple agents to generate high-quality, covert harmful query transformations that can be used to improve future LLM safety alignment. Experiments demonstrate that HarmTransform significantly outperforms standard baselines in producing effective query transformations. At the same time, our analysis reveals that debate acts as a double-edged sword: while it can sharpen transformations and improve stealth, it may also introduce topic shifts and unnecessary complexity. These insights highlight both the promise and the limitations of multi-agent debate for generating comprehensive safety training data.

</details>


### [8] [Emergent World Beliefs: Exploring Transformers in Stochastic Games](https://arxiv.org/abs/2512.23722)
*Adam Kamel,Tanish Rastogi,Michael Ma,Kailash Ranganathan,Kevin Zhu*

Main category: cs.CL

TL;DR: LLM在德州扑克游戏中学习到了部分可观察环境中的世界模型，包括确定性结构和随机性特征。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明LLM能在完美信息游戏中发展涌现世界模型，本研究旨在探索LLM在部分可观察信息领域（如扑克）中是否也能学习类似的世界模型表示。

Method: 使用GPT风格模型在扑克手牌历史数据上进行预训练，然后探测其内部激活，主要通过非线性探针分析模型表示。

Result: 模型在没有明确指导的情况下学会了扑克牌的确定性结构（如手牌等级）和随机性特征（如胜率），这些表示与理论信念状态相关，表明LLM学习到了德州扑克随机环境的内部表示。

Conclusion: LLM能够在部分可观察的马尔可夫决策过程中学习环境的世界模型表示，包括确定性和随机性特征，这扩展了LLM世界模型能力的研究范围。

Abstract: Transformer-based large language models (LLMs) have demonstrated strong reasoning abilities across diverse fields, from solving programming challenges to competing in strategy-intensive games such as chess. Prior work has shown that LLMs can develop emergent world models in games of perfect information, where internal representations correspond to latent states of the environment. In this paper, we extend this line of investigation to domains of incomplete information, focusing on poker as a canonical partially observable Markov decision process (POMDP). We pretrain a GPT-style model on Poker Hand History (PHH) data and probe its internal activations. Our results demonstrate that the model learns both deterministic structure, such as hand ranks, and stochastic features, such as equity, without explicit instruction. Furthermore, by using primarily nonlinear probes, we demonstrated that these representations are decodeable and correlate with theoretical belief states, suggesting that LLMs are learning their own representation of the stochastic environment of Texas Hold'em Poker.

</details>


### [9] [When in Doubt, Deliberate: Confidence-Based Routing to Expert Debate for Sexism Detection](https://arxiv.org/abs/2512.23732)
*Anwar Alajmi,Gabriele Pergola*

Main category: cs.CL

TL;DR: 提出两阶段框架应对网络性别歧视内容检测中的三大挑战：数据代表性不足、标签噪声和概念模糊性，通过针对性训练和推理机制实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 在线性别歧视内容日益隐蔽且依赖上下文，传统检测方法难以应对。现有方法面临三大核心挑战：1) 数据代表性不足（underrepresentation）；2) 标签噪声（noise）；3) 概念模糊性（conceptual ambiguity）。这些因素导致决策边界不稳定，模型忽略细微、少见的伤害形式。

Method: 提出两阶段框架：1) 训练阶段使用类别平衡的focal loss、类别感知批处理和事后阈值校准来处理标签不平衡和噪声监督；2) 推理阶段采用动态路由机制：高置信度案例直接分类，不确定案例交由新颖的"协作专家判断"(CEJ)模块处理，该模块通过多角色提示和法官模型整合推理。

Result: 在多个基准测试中取得最先进结果：EXIST 2025 Task 1.1上F1分数提升+2.72%；EDOS Tasks A和B上分别提升+4.48%和+1.30%。

Conclusion: 该框架通过统一针对性训练程序和选择性推理机制，有效解决了网络性别歧视检测中的数据代表性不足、噪声和模糊性问题，为复杂社交媒体内容审核提供了有效解决方案。

Abstract: Sexist content online increasingly appears in subtle, context-dependent forms that evade traditional detection methods. Its interpretation often depends on overlapping linguistic, psychological, legal, and cultural dimensions, which produce mixed and sometimes contradictory signals, even in annotated datasets. These inconsistencies, combined with label scarcity and class imbalance, result in unstable decision boundaries and cause fine-tuned models to overlook subtler, underrepresented forms of harm. Together, these limitations point to the need for a design that explicitly addresses the combined effects of (i) underrepresentation, (ii) noise, and (iii) conceptual ambiguity in both data and model predictions. To address these challenges, we propose a two-stage framework that unifies (i) targeted training procedures to adapt supervision to scarce and noisy data with (ii) selective, reasoning-based inference to handle ambiguous or borderline cases. Our training setup applies class-balanced focal loss, class-aware batching, and post-hoc threshold calibration to mitigate label imbalance and noisy supervision. At inference time, a dynamic routing mechanism classifies high-confidence cases directly and escalates uncertain instances to a novel \textit{Collaborative Expert Judgment} (CEJ) module, which prompts multiple personas and consolidates their reasoning through a judge model. Our approach achieves state-of-the-art results across several benchmarks, with a +2.72\% improvement in F1 on the EXIST 2025 Task 1.1, and a gains of +4.48\% and +1.30\% on the EDOS Tasks A and B, respectively.

</details>


### [10] [Break Out the Silverware -- Semantic Understanding of Stored Household Items](https://arxiv.org/abs/2512.23739)
*Michaela Levi-Richter,Reuth Mirsky,Oren Glickman*

Main category: cs.CL

TL;DR: 提出了"Stored Household Item Challenge"基准任务，评估服务机器人在家庭场景中推断隐藏物品存储位置的能力，并开发了NOAM混合智能体来解决这一挑战。


<details>
  <summary>Details</summary>
Motivation: 家庭服务机器人面临的主要挑战是缺乏常识推理能力，无法推断日常物品（通常隐藏在抽屉、橱柜或壁橱中）的存储位置，即使简单的"Bring me a plate"命令也难以完成。

Method: 引入NOAM（非可见物品分配模型），这是一个混合智能体管道，结合结构化场景理解和大型语言模型推理。NOAM将视觉输入转换为自然语言描述（空间上下文和可见容器），然后提示语言模型（如GPT-4）推断最可能的隐藏存储位置。

Result: NOAM在预测准确性方面显著优于基线方法（随机选择、视觉语言管道、领先的多模态模型），并接近人类水平表现。研究还提供了两个数据集：100个真实世界评估集和6,500个开发集。

Conclusion: 该研究通过Stored Household Item Challenge基准任务和NOAM模型，展示了在家庭环境中部署具有认知能力智能体的最佳实践，强调了结合视觉理解和语言推理对于实现机器人常识推理的重要性。

Abstract: ``Bring me a plate.'' For domestic service robots, this simple command reveals a complex challenge: inferring where everyday items are stored, often out of sight in drawers, cabinets, or closets. Despite advances in vision and manipulation, robots still lack the commonsense reasoning needed to complete this task. We introduce the Stored Household Item Challenge, a benchmark task for evaluating service robots' cognitive capabilities: given a household scene and a queried item, predict its most likely storage location.
  Our benchmark includes two datasets: (1) a real-world evaluation set of 100 item-image pairs with human-annotated ground truth from participants' kitchens, and (2) a development set of 6,500 item-image pairs annotated with storage polygons over public kitchen images. These datasets support realistic modeling of household organization and enable comparative evaluation across agent architectures.
  To begin tackling this challenge, we introduce NOAM (Non-visible Object Allocation Model), a hybrid agent pipeline that combines structured scene understanding with large language model inference. NOAM converts visual input into natural language descriptions of spatial context and visible containers, then prompts a language model (e.g., GPT-4) to infer the most likely hidden storage location. This integrated vision-language agent exhibits emergent commonsense reasoning and is designed for modular deployment within broader robotic systems.
  We evaluate NOAM against baselines including random selection, vision-language pipelines (Grounding-DINO + SAM), leading multimodal models (e.g., Gemini, GPT-4o, Kosmos-2, LLaMA, Qwen), and human performance. NOAM significantly improves prediction accuracy and approaches human-level results, highlighting best practices for deploying cognitively capable agents in domestic environments.

</details>


### [11] [Entropy-Aware Speculative Decoding Toward Improved LLM Reasoning](https://arxiv.org/abs/2512.23765)
*Tiancheng Su,Meicong Zhang,Guoxiu He*

Main category: cs.CL

TL;DR: EASD（熵感知推测解码）是一种无需训练的推测解码增强方法，通过动态熵惩罚机制，在模型不确定性高时让目标LLM重新采样，从而可能超越目标模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统推测解码过度依赖目标模型与草稿模型的对齐，限制了性能提升，无法超越目标模型本身的性能上限。

Method: 在标准推测解码基础上，引入动态熵惩罚机制：利用采样分布的熵量化模型不确定性，当两个模型熵值都高且top-N预测重叠时，拒绝对应token并由目标LLM重新采样。

Result: EASD在多个推理基准测试中一致优于现有推测解码方法，大多数情况下甚至超越了目标LLM本身的性能，同时保持了与标准推测解码相当的效率。

Conclusion: EASD通过熵感知机制解决了推测解码的性能限制问题，实现了超越目标模型性能的可能性，同时保持了高效性，为LLM推理加速提供了新思路。

Abstract: Speculative decoding (SD) accelerates large language model (LLM) reasoning by using a small draft model to generate candidate tokens, which the target LLM either accepts directly or regenerates upon rejection. However, excessive alignment between the draft and target models constrains SD to the performance of the target LLM. To address this limitation, we propose Entropy-Aware Speculative Decoding (EASD), a training-free enhancement. Building on standard SD, EASD incorporates a dynamic entropy-based penalty. At each decoding step, we employ the entropy of the sampling distribution to quantify model uncertainty. When both models exhibit high entropy with substantial overlap among their top-N predictions, the corresponding token is rejected and re-sampled by the target LLM. This penalty prevents low-confidence errors from propagating. By incorporating draft-model verification, EASD enables the possibility of surpassing the target model's inherent performance. Experiments across multiple reasoning benchmarks demonstrate that EASD consistently outperforms existing SD methods and, in most cases, surpasses the target LLM itself. We further prove that the efficiency of EASD is comparable to that of SD. The code can be found in the Supplementary Materials.

</details>


### [12] [MiMo-Audio: Audio Language Models are Few-Shot Learners](https://arxiv.org/abs/2512.23808)
*Xiaomi LLM-Core Team,:,Dong Zhang,Gang Wang,Jinlong Xue,Kai Fang,Liang Zhao,Rui Ma,Shuhuai Ren,Shuo Liu,Tao Guo,Weiji Zhuang,Xin Zhang,Xingchen Song,Yihan Yan,Yongzhe He,Cici,Bowen Shen,Chengxuan Zhu,Chong Ma,Chun Chen,Heyu Chen,Jiawei Li,Lei Li,Menghang Zhu,Peidian Li,Qiying Wang,Sirui Deng,Weimin Xiong,Wenshan Huang,Wenyu Yang,Yilin Jiang,Yixin Yang,Yuanyuan Tian,Yue Ma,Yue Yu,Zihan Zhang,Zihao Yue,Bangjun Xiao,Bingquan Xia,Bofei Gao,Bowen Ye,Can Cai,Chang Liu,Chenhong He,Chunan Li,Dawei Zhu,Duo Zhang,Fengyuan Shi,Guoan Wang,Hailin Zhang,Hanglong Lv,Hanyu Li,Hao Tian,Heng Qu,Hongshen Xu,Houbin Zhang,Huaqiu Liu,Jiangshan Duo,Jianguang Zuo,Jianyu Wei,Jiebao Xiao,Jinhao Dong,Jun Shi,Junhao Hu,Kainan Bao,Kang Zhou,Linghao Zhang,Meng Chen,Nuo Chen,Peng Zhang,Qianli Chen,Qiantong Wang,Rang Li,Shaohui Liu,Shengfan Wang,Shicheng Li,Shihua Yu,Shijie Cao,Shimao Chen,Shuhao Gu,Weikun Wang,Wenhan Ma,Xiangwei Deng,Xing Yong,Xing Zhang,Xu Wang,Yifan Song,Yihao Zhao,Yingbo Zhao,Yizhao Gao,Yu Cheng,Yu Tu,Yudong Wang,Zhaojun Huang,Zhengju Tang,Zhenru Lin,Zhichao Song,Zhipeng Xu,Zhixian Zheng,Zihan Jiang*

Main category: cs.CL

TL;DR: MiMo-Audio通过大规模音频预训练（超过1亿小时）实现了音频任务的少样本学习能力，在语音理解和音频生成任务上达到开源SOTA，并能泛化到训练数据中未见的任务。


<details>
  <summary>Details</summary>
Motivation: 现有音频语言模型通常需要任务特定的微调来完成特定任务，而人类可以通过少量示例或简单指令泛化到新任务。研究者认为GPT-3展示的缩放预训练范式同样适用于音频领域，通过大规模音频预训练数据可以实现类似的人类泛化能力。

Method: 1. 将MiMo-Audio的预训练数据扩展到超过1亿小时音频；2. 开发系统评估框架验证少样本学习能力；3. 在后期训练阶段，策划多样化的指令调优语料库；4. 在音频理解和生成中引入思维机制；5. 在7B参数基座模型上进行指令调优。

Result: 1. MiMo-Audio-7B-Base在语音智能和音频理解基准测试中达到开源SOTA；2. 模型能够泛化到训练数据中未见的任务（语音转换、风格迁移、语音编辑）；3. 展示强大的语音延续能力，能生成高度逼真的谈话节目、朗诵、直播和辩论；4. MiMo-Audio-7B-Instruct在音频理解、口语对话和指令TTS评估中达到开源SOTA，接近或超越闭源模型。

Conclusion: 通过大规模音频预训练，音频语言模型能够展现出类似人类的少样本学习能力，在多种音频任务上达到最先进性能。该方法为构建更通用的音频AI系统提供了有前景的路径。

Abstract: Existing audio language models typically rely on task-specific fine-tuning to accomplish particular audio tasks. In contrast, humans are able to generalize to new audio tasks with only a few examples or simple instructions. GPT-3 has shown that scaling next-token prediction pretraining enables strong generalization capabilities in text, and we believe this paradigm is equally applicable to the audio domain. By scaling MiMo-Audio's pretraining data to over one hundred million of hours, we observe the emergence of few-shot learning capabilities across a diverse set of audio tasks. We develop a systematic evaluation of these capabilities and find that MiMo-Audio-7B-Base achieves SOTA performance on both speech intelligence and audio understanding benchmarks among open-source models. Beyond standard metrics, MiMo-Audio-7B-Base generalizes to tasks absent from its training data, such as voice conversion, style transfer, and speech editing. MiMo-Audio-7B-Base also demonstrates powerful speech continuation capabilities, capable of generating highly realistic talk shows, recitations, livestreaming and debates. At the post-training stage, we curate a diverse instruction-tuning corpus and introduce thinking mechanisms into both audio understanding and generation. MiMo-Audio-7B-Instruct achieves open-source SOTA on audio understanding benchmarks (MMSU, MMAU, MMAR, MMAU-Pro), spoken dialogue benchmarks (Big Bench Audio, MultiChallenge Audio) and instruct-TTS evaluations, approaching or surpassing closed-source models. Model checkpoints and full evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-Audio.

</details>


### [13] [StressRoBERTa: Cross-Condition Transfer Learning from Depression, Anxiety, and PTSD to Stress Detection](https://arxiv.org/abs/2512.23813)
*Amal Alqahtani,Efsun Kayi,Mona Diab*

Main category: cs.CL

TL;DR: StressRoBERTa：基于跨条件迁移学习的慢性压力检测模型，通过临床相关精神障碍数据预训练，在推特文本上实现82% F1值，优于现有最佳系统。


<details>
  <summary>Details</summary>
Motivation: 慢性压力是重大公共卫生问题，社交媒体成为人们分享压力体验的重要平台。现有通用语言模型和宽泛心理健康模型在慢性压力检测上可能不够精准，而临床相关精神障碍（抑郁、焦虑、PTSD）与慢性压力高度共病，可能提供更有效的表征学习。

Method: 提出StressRoBERTa跨条件迁移学习方法：1）使用Stress-SMHD语料库（来自自我报告抑郁、焦虑、PTSD用户的1.08亿词）对RoBERTa进行持续训练；2）在SMM4H 2022 Task 8数据集上进行微调；3）评估与通用语言模型和宽泛心理健康模型的对比。

Result: StressRoBERTa在SMM4H 2022 Task 8数据集上达到82% F1值，比最佳共享任务系统（79% F1）提高3个百分点。与原始RoBERTa相比，来自压力相关障碍的跨条件迁移带来1% F1提升。在Dreaddit数据集上达到81% F1值，证明从临床心理健康到情境压力讨论的有效迁移。

Conclusion: 针对压力相关精神障碍的跨条件迁移学习比通用心理健康训练提供更强的表征能力，能有效检测社交媒体中的慢性压力表达。该方法展示了从临床心理健康语境到日常压力讨论的迁移潜力。

Abstract: The prevalence of chronic stress represents a significant public health concern, with social media platforms like Twitter serving as important venues for individuals to share their experiences. This paper introduces StressRoBERTa, a cross-condition transfer learning approach for automatic detection of self-reported chronic stress in English tweets. The investigation examines whether continual training on clinically related conditions (depression, anxiety, PTSD), disorders with high comorbidity with chronic stress, improves stress detection compared to general language models and broad mental health models. RoBERTa is continually trained on the Stress-SMHD corpus (108M words from users with self-reported diagnoses of depression, anxiety, and PTSD) and fine-tuned on the SMM4H 2022 Task 8 dataset. StressRoBERTa achieves 82% F1-score, outperforming the best shared task system (79% F1) by 3 percentage points. The results demonstrate that focused cross-condition transfer from stress-related disorders (+1% F1 over vanilla RoBERTa) provides stronger representations than general mental health training. Evaluation on Dreaddit (81% F1) further demonstrates transfer from clinical mental health contexts to situational stress discussions.

</details>


### [14] [AdaGReS:Adaptive Greedy Context Selection via Redundancy-Aware Scoring for Token-Budgeted RAG](https://arxiv.org/abs/2512.25052)
*Chao Peng,Bin Wang,Zhilei Long,Jinfang Sheng*

Main category: cs.CL

TL;DR: AdaGReS是一个针对token预算约束的RAG框架，通过优化集级目标函数来减少冗余，自动校准相关性与冗余性的权衡参数，并提供理论近似最优性保证。


<details>
  <summary>Details</summary>
Motivation: 传统RAG中的top-k检索经常返回冗余或近重复的文本块，浪费token预算并降低下游生成质量。

Method: 提出AdaGReS框架，使用贪心选择算法在token预算约束下优化集级目标函数（结合查询-块相关性和集合内冗余惩罚），并引入闭式、实例自适应的参数校准方法。

Result: 在开放域问答和生物医学领域实验中，AdaGReS在冗余控制和上下文质量方面表现一致提升，转化为更好的端到端答案质量和鲁棒性。

Conclusion: AdaGReS通过冗余感知的上下文选择框架有效解决了RAG中的冗余问题，提供了无需手动调参的解决方案，并在理论和实验上都证明了其优越性。

Abstract: Retrieval-augmented generation (RAG) is highly sensitive to the quality of selected context, yet standard top-k retrieval often returns redundant or near-duplicate chunks that waste token budget and degrade downstream generation. We present AdaGReS, a redundancy-aware context selection framework for token-budgeted RAG that optimizes a set-level objective combining query-chunk relevance and intra-set redundancy penalties. AdaGReS performs greedy selection under a token-budget constraint using marginal gains derived from the objective, and introduces a closed-form, instance-adaptive calibration of the relevance-redundancy trade-off parameter to eliminate manual tuning and adapt to candidate-pool statistics and budget limits. We further provide a theoretical analysis showing that the proposed objective exhibits epsilon-approximate submodularity under practical embedding similarity conditions, yielding near-optimality guarantees for greedy selection. Experiments on open-domain question answering (Natural Questions) and a high-redundancy biomedical (drug) corpus demonstrate consistent improvements in redundancy control and context quality, translating to better end-to-end answer quality and robustness across settings.

</details>


### [15] [Explaining News Bias Detection: A Comparative SHAP Analysis of Transformer Model Decision Mechanisms](https://arxiv.org/abs/2512.23835)
*Himel Ghosh*

Main category: cs.CL

TL;DR: 比较两种基于Transformer的新闻偏见检测模型的解释性研究，发现领域自适应模型比专用偏见检测器表现更好，假阳性减少63%，揭示了模型误判源于话语层面的模糊性而非明确的偏见线索。


<details>
  <summary>Details</summary>
Motivation: 虽然自动化偏见检测广泛用于新闻分析和媒体问责，但人们对偏见检测模型如何做出决策以及为何失败知之甚少，需要理解不同模型架构如何操作化语言偏见。

Method: 使用SHAP解释方法对两种Transformer模型进行对比解释性研究：1）基于BABE数据集微调的偏见检测器；2）领域自适应预训练RoBERTa模型（同样在BABE上微调）。分析正确和错误预测的词级归因，探究不同模型架构如何操作化语言偏见。

Result: 1）两种模型都关注相似的评价性语言类别，但在如何整合这些信号到预测中存在显著差异；2）偏见检测器模型给假阳性分配比真阳性更强的内部证据，表明归因强度与预测正确性不匹配，导致系统性过度标记中性新闻内容；3）领域自适应模型的归因模式与预测结果更一致，假阳性减少63%；4）模型错误源于不同的语言机制，假阳性主要由话语层面的模糊性驱动而非明确的偏见线索。

Conclusion: 研究强调了可解释性评估对偏见检测系统的重要性，表明架构和训练选择会严重影响模型可靠性以及在新闻场景中的部署适用性。领域自适应方法比专用偏见检测器在减少误报方面表现更好。

Abstract: Automated bias detection in news text is heavily used to support journalistic analysis and media accountability, yet little is known about how bias detection models arrive at their decisions or why they fail. In this work, we present a comparative interpretability study of two transformer-based bias detection models: a bias detector fine-tuned on the BABE dataset and a domain-adapted pre-trained RoBERTa model fine-tuned on the BABE dataset, using SHAP-based explanations. We analyze word-level attributions across correct and incorrect predictions to characterize how different model architectures operationalize linguistic bias. Our results show that although both models attend to similar categories of evaluative language, they differ substantially in how these signals are integrated into predictions. The bias detector model assigns stronger internal evidence to false positives than to true positives, indicating a misalignment between attribution strength and prediction correctness and contributing to systematic over-flagging of neutral journalistic content. In contrast, the domain-adaptive model exhibits attribution patterns that better align with prediction outcomes and produces 63\% fewer false positives. We further demonstrate that model errors arise from distinct linguistic mechanisms, with false positives driven by discourse-level ambiguity rather than explicit bias cues. These findings highlight the importance of interpretability-aware evaluation for bias detection systems and suggest that architectural and training choices critically affect both model reliability and deployment suitability in journalistic contexts.

</details>


### [16] [Retrieval Augmented Question Answering: When Should LLMs Admit Ignorance?](https://arxiv.org/abs/2512.23836)
*Dingmin Wang,Ji Ma,Shankar Kumar*

Main category: cs.CL

TL;DR: 本文提出一种自适应提示策略，通过将检索信息分块并顺序提示LLM，在保持问答性能的同时减少token使用量。


<details>
  <summary>Details</summary>
Motivation: 尽管扩展上下文窗口使LLM更容易纳入目标知识，但同时也引入了更多无关信息，这会阻碍模型生成过程并降低性能。需要解决检索增强问答中长上下文带来的信息噪声问题。

Method: 设计自适应提示策略：将检索到的信息分割成较小的块，然后顺序提示LLM使用每个块来回答问题。通过调整块大小，可以在纳入相关信息与减少无关信息之间进行权衡。

Result: 在三个开放域问答数据集上的实验结果表明，自适应策略在保持标准提示性能的同时使用了更少的token。分析发现当遇到信息不足时，LLM往往生成错误答案而非拒绝回答，这是主要错误来源。

Conclusion: 自适应提示策略能有效平衡相关信息和噪声，提高效率。研究发现LLM在信息不足时倾向于生成错误答案而非拒绝回答，这突显了需要进一步研究增强LLM在信息不足时有效拒绝请求的能力。

Abstract: The success of expanded context windows in Large Language Models (LLMs) has driven increased use of broader context in retrieval-augmented generation. We investigate the use of LLMs for retrieval augmented question answering. While longer contexts make it easier to incorporate targeted knowledge, they introduce more irrelevant information that hinders the model's generation process and degrades its performance. To address the issue, we design an adaptive prompting strategy which involves splitting the retrieved information into smaller chunks and sequentially prompting a LLM to answer the question using each chunk. Adjusting the chunk size allows a trade-off between incorporating relevant information and reducing irrelevant information. Experimental results on three open-domain question answering datasets demonstrate that the adaptive strategy matches the performance of standard prompting while using fewer tokens. Our analysis reveals that when encountering insufficient information, the LLM often generates incorrect answers instead of declining to respond, which constitutes a major source of error. This finding highlights the need for further research into enhancing LLMs' ability to effectively decline requests when faced with inadequate information.

</details>


### [17] [Adversarial Lens: Exploiting Attention Layers to Generate Adversarial Examples for Evaluation](https://arxiv.org/abs/2512.23837)
*Kaustubh Dhole*

Main category: cs.CL

TL;DR: 该研究利用大语言模型中间注意力层的token分布生成对抗样本，用于压力测试LLM评估流程，发现这种方法能有效降低评估性能但存在语法退化问题。


<details>
  <summary>Details</summary>
Motivation: 基于机械可解释性研究发现中间注意力层编码了逐步优化的token级假设，研究者希望利用这一特性从注意力层token分布直接生成对抗样本，以更原则性的方式测试LLM评估流程的鲁棒性。

Method: 提出从中间注意力层提取token作为对抗扰动的方法，利用模型内部的token预测生成既合理又与模型生成过程内部一致的对抗样本。在ArgQuality数据集上进行论证质量评估实验，使用LLaMA-3.1-Instruct-8B同时作为生成器和评估器。

Result: 注意力层生成的对抗样本能显著降低评估性能，同时保持与原始输入的语义相似性。但某些层和token位置提取的替换会导致语法退化，限制了实际效果。

Conclusion: 利用中间层表征作为对抗样本来源具有前景，但当前存在语法退化等限制。这为压力测试LLM评估流程提供了一种新的原则性方法，同时也揭示了模型内部表示与生成质量之间的复杂关系。

Abstract: Recent advances in mechanistic interpretability suggest that intermediate attention layers encode token-level hypotheses that are iteratively refined toward the final output. In this work, we exploit this property to generate adversarial examples directly from attention-layer token distributions. Unlike prompt-based or gradient-based attacks, our approach leverages model-internal token predictions, producing perturbations that are both plausible and internally consistent with the model's own generation process. We evaluate whether tokens extracted from intermediate layers can serve as effective adversarial perturbations for downstream evaluation tasks. We conduct experiments on argument quality assessment using the ArgQuality dataset, with LLaMA-3.1-Instruct-8B serving as both the generator and evaluator. Our results show that attention-based adversarial examples lead to measurable drops in evaluation performance while remaining semantically similar to the original inputs. However, we also observe that substitutions drawn from certain layers and token positions can introduce grammatical degradation, limiting their practical effectiveness. Overall, our findings highlight both the promise and current limitations of using intermediate-layer representations as a principled source of adversarial examples for stress-testing LLM-based evaluation pipelines.

</details>


### [18] [Integrating Domain Knowledge for Financial QA: A Multi-Retriever RAG Approach with LLMs](https://arxiv.org/abs/2512.23848)
*Yukun Zhang,Stefan Elbl Droguett,Samyak Jain*

Main category: cs.CL

TL;DR: 该研究通过多检索器RAG系统结合最新LLM，显著提升了金融数值推理问答任务的性能，在FinQA基准上取得了SOTA结果，但仍低于人类专家水平。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在金融数值推理问答任务中表现不佳，主要原因是缺乏金融领域专业知识，而金融数值问题需要特定领域知识和复杂的多步骤数值推理能力。

Method: 采用多检索器检索增强生成(RAG)系统，同时检索外部领域知识和内部问题上下文，利用最新LLM处理任务。通过消融实验和错误分析，验证了领域特定训练（使用SecBERT编码器）的有效性。

Result: 领域特定训练的神经符号模型超越了FinQA论文的顶级模型；基于提示的LLM生成器取得了SOTA性能，提升超过7%，但仍低于人类专家水平。研究揭示了小模型中幻觉损失与外部知识增益的权衡，而大模型中外部事实的收益通常超过幻觉损失。

Conclusion: 领域特定训练对金融数值推理至关重要；最新LLM在少样本学习方面具有增强的数值推理能力；外部知识检索与大模型结合能显著提升性能，但需要平衡幻觉问题。

Abstract: This research project addresses the errors of financial numerical reasoning Question Answering (QA) tasks due to the lack of domain knowledge in finance. Despite recent advances in Large Language Models (LLMs), financial numerical questions remain challenging because they require specific domain knowledge in finance and complex multi-step numeric reasoning. We implement a multi-retriever Retrieval Augmented Generators (RAG) system to retrieve both external domain knowledge and internal question contexts, and utilize the latest LLM to tackle these tasks. Through comprehensive ablation experiments and error analysis, we find that domain-specific training with the SecBERT encoder significantly contributes to our best neural symbolic model surpassing the FinQA paper's top model, which serves as our baseline. This suggests the potential superior performance of domain-specific training. Furthermore, our best prompt-based LLM generator achieves the state-of-the-art (SOTA) performance with significant improvement (>7%), yet it is still below the human expert performance. This study highlights the trade-off between hallucinations loss and external knowledge gains in smaller models and few-shot examples. For larger models, the gains from external facts typically outweigh the hallucination loss. Finally, our findings confirm the enhanced numerical reasoning capabilities of the latest LLM, optimized for few-shot learning.

</details>


### [19] [Disentangling Learning from Judgment: Representation Learning for Open Response Analytics](https://arxiv.org/abs/2512.23941)
*Conrad Borchers,Manit Patel,Seiyon M. Lee,Anthony F. Botelho*

Main category: cs.CL

TL;DR: 论文提出了一种分析优先的框架，将学生回答的内容信号与评分者倾向分离，通过建模教师历史作为动态先验并结合文本嵌入，实现可审计的自动化评分分析。


<details>
  <summary>Details</summary>
Motivation: 传统的自动化评分方法往往将学生回答内容与教师评分倾向混为一谈，缺乏透明度和可审计性。研究者希望开发一个能够分离内容信号和评分者倾向的框架，使评分判断变得可见和可审计。

Method: 使用ASSISTments数学回答数据，将教师评分历史建模为动态先验，从句子嵌入中提取文本表示，采用中心化和残差化技术减轻提示和教师混杂因素的影响。通过时间验证的线性模型量化每个信号的贡献，并通过投影展示模型分歧供定性检查。

Result: 教师先验对成绩预测影响很大；先验与内容嵌入结合时效果最佳（AUC~0.815），而仅基于内容的模型效果显著较弱（AUC~0.626）。调整评分者效应能锐化残差内容表示，保留更多信息维度，揭示语义证据支持理解而非表面回答差异的情况。

Conclusion: 该框架提供了一个实用的分析流程，将嵌入从单纯的特征转化为用于反思的学习分析工具，使教师和研究人员能够检查评分实践与学生推理和学习证据的一致性（或冲突）。

Abstract: Open-ended responses are central to learning, yet automated scoring often conflates what students wrote with how teachers grade. We present an analytics-first framework that separates content signals from rater tendencies, making judgments visible and auditable via analytics. Using de-identified ASSISTments mathematics responses, we model teacher histories as dynamic priors and derive text representations from sentence embeddings, incorporating centering and residualization to mitigate prompt and teacher confounds. Temporally-validated linear models quantify the contributions of each signal, and a projection surfaces model disagreements for qualitative inspection. Results show that teacher priors heavily influence grade predictions; the strongest results arise when priors are combined with content embeddings (AUC~0.815), while content-only models remain above chance but substantially weaker (AUC~0.626). Adjusting for rater effects sharpens the residual content representation, retaining more informative embedding dimensions and revealing cases where semantic evidence supports understanding as opposed to surface-level differences in how students respond. The contribution presents a practical pipeline that transforms embeddings from mere features into learning analytics for reflection, enabling teachers and researchers to examine where grading practices align (or conflict) with evidence of student reasoning and learning.

</details>


### [20] [Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling](https://arxiv.org/abs/2512.23959)
*Chulun Zhou,Chunkang Zhang,Guoxin Yu,Fandong Meng,Jie Zhou,Wai Lam,Mo Yu*

Main category: cs.CL

TL;DR: HGMem提出了一种基于超图的动态记忆机制，通过高阶交互增强多步检索增强生成（RAG）的全局理解和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统中的工作记忆模块主要作为被动存储，积累孤立事实用于输入压缩和生成子查询。这种静态设计忽略了原始事实间的高阶相关性，而这些组合能为后续步骤提供更强指导，导致推理碎片化和弱全局理解能力。

Method: 引入HGMem——基于超图的记忆机制，将记忆表示为超图，其中超边对应不同的记忆单元。这种方法能够逐步形成记忆内的高阶交互，将事实和思考围绕核心问题连接起来，演变为集成的情境化知识结构。

Result: 在多个为全局理解设计的挑战性数据集上进行评估，实验和分析表明HGMem持续改进多步RAG，在各种任务上显著优于强基线系统。

Conclusion: HGMem通过将记忆从简单存储扩展到动态表达结构，增强了复杂推理和全局理解能力，为多步RAG提供了更强大的命题支持。

Abstract: Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.

</details>


### [21] [Efficient Context Scaling with LongCat ZigZag Attention](https://arxiv.org/abs/2512.23966)
*Chen Zhang,Yang Bai,Jiahuan Li,Anchun Gui,Keheng Wang,Feifan Liu,Guanyu Wu,Yuwei Jiang,Defei Bu,Li Wei,Haihang Jing,Hongyin Tang,Xin Chen,Xiangzhou Huang,Fengcun Li,Rongxiang Weng,Yulei Qian,Yifan Lu,Yerui Sun,Jingang Wang,Yuchen Xie,Xunliang Cai*

Main category: cs.CL

TL;DR: LoZA是一种稀疏注意力方案，可将现有全注意力模型转换为稀疏版本，在有限计算预算下实现长上下文场景的显著加速。


<details>
  <summary>Details</summary>
Motivation: 解决长上下文场景中全注意力模型计算成本高的问题，特别是在预填充密集型（如检索增强生成）和解码密集型（如工具集成推理）任务中。

Method: 提出LongCat ZigZag Attention (LoZA)稀疏注意力方案，通过将其应用于LongCat-Flash模型进行中期训练，创建LongCat-Flash-Exp长上下文基础模型。

Result: LoZA能在长上下文场景中实现显著加速，LongCat-Flash-Exp模型可高效处理高达100万token的输入，支持高效长期推理和长视野智能体能力。

Conclusion: LoZA为现有全注意力模型提供了一种有效的稀疏化方案，在有限计算预算下显著提升了长上下文处理能力，为长期推理和智能体应用提供了高效基础。

Abstract: We introduce LongCat ZigZag Attention (LoZA), which is a sparse attention scheme designed to transform any existing full-attention models into sparse versions with rather limited compute budget. In long-context scenarios, LoZA can achieve significant speed-ups both for prefill-intensive (e.g., retrieval-augmented generation) and decode-intensive (e.g., tool-integrated reasoning) cases. Specifically, by applying LoZA to LongCat-Flash during mid-training, we serve LongCat-Flash-Exp as a long-context foundation model that can swiftly process up to 1 million tokens, enabling efficient long-term reasoning and long-horizon agentic capabilities.

</details>


### [22] [CEC-Zero: Zero-Supervision Character Error Correction with Self-Generated Rewards](https://arxiv.org/abs/2512.23971)
*Zhiming Lin,Kai Zhao,Sophie Zhang,Peilai Yu,Canran Xiao*

Main category: cs.CL

TL;DR: CEC-Zero：基于零监督强化学习的汉语拼写纠错框架，无需人工标注，通过LLM自我纠错实现高性能


<details>
  <summary>Details</summary>
Motivation: 现有大规模汉语拼写纠错方法面临两大问题：1）LLM和监督方法对新型错误的鲁棒性不足；2）依赖昂贵的标注数据。需要一种无需标注且能处理新型错误的鲁棒方法。

Method: 提出CEC-Zero框架：1）从干净文本合成错误输入；2）通过语义相似性和候选一致性计算聚类共识奖励；3）使用PPO优化策略；4）提供无偏奖励和收敛的理论保证。

Result: 在9个基准测试中：1）比监督基线高出10-13个F1点；2）比强LLM微调方法高出5-8个F1点；3）证明了无标注条件下LLM在噪声文本处理中的潜力。

Conclusion: CEC-Zero建立了一种无需标注的鲁棒、可扩展汉语拼写纠错范式，解锁了LLM在噪声文本处理流水线中的潜力，为大规模文本处理提供了新解决方案。

Abstract: Large-scale Chinese spelling correction (CSC) remains critical for real-world text processing, yet existing LLMs and supervised methods lack robustness to novel errors and rely on costly annotations. We introduce CEC-Zero, a zero-supervision reinforcement learning framework that addresses this by enabling LLMs to correct their own mistakes. CEC-Zero synthesizes errorful inputs from clean text, computes cluster-consensus rewards via semantic similarity and candidate agreement, and optimizes the policy with PPO. It outperforms supervised baselines by 10--13 F$_1$ points and strong LLM fine-tunes by 5--8 points across 9 benchmarks, with theoretical guarantees of unbiased rewards and convergence. CEC-Zero establishes a label-free paradigm for robust, scalable CSC, unlocking LLM potential in noisy text pipelines.

</details>


### [23] [Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process](https://arxiv.org/abs/2512.23988)
*Zhenyu Zhang,Shujian Zhang,John Lambert,Wenxuan Zhou,Zhangyang Wang,Mingqing Chen,Andrew Hard,Rajiv Mathews,Lun Wang*

Main category: cs.CL

TL;DR: 本文提出了一种无监督框架RISE，通过稀疏自编码器在激活空间中自动发现编码不同推理行为的"推理向量"，无需人工定义概念即可解释和控制大语言模型的推理过程。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型的推理能力不断增强，但其推理过程的内部机制仍未被充分探索。现有方法通常依赖人工定义的概念（如过度思考、反思）在词汇层面进行分析，但这种方法存在局限性，无法捕捉所有潜在的推理行为，且许多行为难以在词元空间中定义。

Method: 提出RISE无监督框架，将思维链轨迹分割为句子级别的"步骤"，在步骤级激活上训练稀疏自编码器（SAEs），发现编码不同推理行为的解耦特征（如反思和回溯）。通过可视化和聚类分析显示这些行为在解码器列空间中占据可分离区域。

Result: 研究发现：1）SAEs能够发现可解释的推理行为（如反思、回溯）；2）这些行为在解码器空间中占据可分离区域；3）对SAE衍生向量进行针对性干预可以可控地放大或抑制特定推理行为，改变推理轨迹而无需重新训练；4）SAEs还能捕捉结构特性（如响应长度）；5）能够发现超越人工监督的新行为；6）通过在SAE解码器空间中识别置信度相关向量，能够控制响应置信度。

Conclusion: 无监督潜在发现方法在解释和可控引导大语言模型的推理方面具有巨大潜力，RISE框架能够自动发现可解释的推理向量，并为模型推理行为的分析和控制提供了新途径。

Abstract: Despite the growing reasoning capabilities of recent large language models (LLMs), their internal mechanisms during the reasoning process remain underexplored. Prior approaches often rely on human-defined concepts (e.g., overthinking, reflection) at the word level to analyze reasoning in a supervised manner. However, such methods are limited, as it is infeasible to capture the full spectrum of potential reasoning behaviors, many of which are difficult to define in token space. In this work, we propose an unsupervised framework (namely, RISE: Reasoning behavior Interpretability via Sparse auto-Encoder) for discovering reasoning vectors, which we define as directions in the activation space that encode distinct reasoning behaviors. By segmenting chain-of-thought traces into sentence-level 'steps' and training sparse auto-encoders (SAEs) on step-level activations, we uncover disentangled features corresponding to interpretable behaviors such as reflection and backtracking. Visualization and clustering analyses show that these behaviors occupy separable regions in the decoder column space. Moreover, targeted interventions on SAE-derived vectors can controllably amplify or suppress specific reasoning behaviors, altering inference trajectories without retraining. Beyond behavior-specific disentanglement, SAEs capture structural properties such as response length, revealing clusters of long versus short reasoning traces. More interestingly, SAEs enable the discovery of novel behaviors beyond human supervision. We demonstrate the ability to control response confidence by identifying confidence-related vectors in the SAE decoder space. These findings underscore the potential of unsupervised latent discovery for both interpreting and controllably steering reasoning in LLMs.

</details>


### [24] [WISE: Web Information Satire and Fakeness Evaluation](https://arxiv.org/abs/2512.24000)
*Gaurab Chhetri,Subasish Das,Tausif Islam Chowdhury*

Main category: cs.CL

TL;DR: 本文开发了WISE框架，在2万条平衡数据集上评估了8个轻量级Transformer模型与2个基线模型，发现MiniLM在准确率上表现最佳（87.58%），RoBERTa-base在ROC-AUC上最优（95.42%），DistilBERT提供了最佳的效率-准确率权衡。


<details>
  <summary>Details</summary>
Motivation: 区分虚假新闻与讽刺幽默内容具有独特挑战性，因为它们在语言特征上重叠但意图不同。需要在资源受限的实际环境中部署有效的错误信息检测系统。

Method: 开发WISE框架，在Fakeddit的2万个标注样本（假新闻vs讽刺）上，使用分层5折交叉验证评估8个轻量级Transformer模型和2个基线模型。评估指标包括准确率、精确率、召回率、F1分数、ROC-AUC、PR-AUC、MCC、Brier分数和期望校准误差。

Result: MiniLM获得最高准确率87.58%，RoBERTa-base获得最高ROC-AUC 95.42%和强准确率87.36%。DistilBERT在效率-准确率权衡上表现优异（准确率86.28%，ROC-AUC 93.90%）。统计检验证实了模型间的显著性能差异。

Conclusion: 轻量级模型可以匹配或超越基线性能，为在资源受限的实际环境中部署错误信息检测系统提供了可行方案。

Abstract: Distinguishing fake or untrue news from satire or humor poses a unique challenge due to their overlapping linguistic features and divergent intent. This study develops WISE (Web Information Satire and Fakeness Evaluation) framework which benchmarks eight lightweight transformer models alongside two baseline models on a balanced dataset of 20,000 samples from Fakeddit, annotated as either fake news or satire. Using stratified 5-fold cross-validation, we evaluate models across comprehensive metrics including accuracy, precision, recall, F1-score, ROC-AUC, PR-AUC, MCC, Brier score, and Expected Calibration Error. Our evaluation reveals that MiniLM, a lightweight model, achieves the highest accuracy (87.58%) among all models, while RoBERTa-base achieves the highest ROC-AUC (95.42%) and strong accuracy (87.36%). DistilBERT offers an excellent efficiency-accuracy trade-off with 86.28\% accuracy and 93.90\% ROC-AUC. Statistical tests confirm significant performance differences between models, with paired t-tests and McNemar tests providing rigorous comparisons. Our findings highlight that lightweight models can match or exceed baseline performance, offering actionable insights for deploying misinformation detection systems in real-world, resource-constrained settings.

</details>


### [25] [iCLP: Large Language Model Reasoning with Implicit Cognition Latent Planning](https://arxiv.org/abs/2512.24014)
*Sijia Chen,Di Niu*

Main category: cs.CL

TL;DR: iCLP框架让大语言模型在隐空间进行规划，在语言空间推理，通过向量量化自编码器学习紧凑的隐式计划表示，显著提升推理准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于显式文本规划的方法面临LLM幻觉和任务问题多样性高的挑战，需要更紧凑有效的规划方式。受人类内隐认知启发，希望让LLM像人类一样通过从经验中学习的紧凑模式进行潜意识规划。

Method: 提出iCLP框架：1) 从现有逐步推理轨迹中提取显式计划；2) 通过向量量化自编码器和码本学习这些计划的离散表示；3) 在成对的隐式计划和对应推理步骤上微调LLM，使模型学会在推理过程中进行隐式规划。

Result: 在数学推理和代码生成任务上的实验表明，iCLP使LLM能在隐空间规划同时在语言空间推理，在准确性和效率上都有显著提升，并展现出强大的跨领域泛化能力，同时保持了思维链推理的可解释性。

Conclusion: iCLP框架成功地将人类内隐认知原理应用于LLM规划，通过隐式计划表示解决了显式文本规划的局限性，为LLM推理提供了更高效、准确且可泛化的规划机制。

Abstract: Large language models (LLMs), when guided by explicit textual plans, can perform reliable step-by-step reasoning during problem-solving. However, generating accurate and effective textual plans remains challenging due to LLM hallucinations and the high diversity of task-specific questions. To address this, we draw inspiration from human Implicit Cognition (IC), the subconscious process by which decisions are guided by compact, generalized patterns learned from past experiences without requiring explicit verbalization. We propose iCLP, a novel framework that enables LLMs to adaptively generate latent plans (LPs), which are compact encodings of effective reasoning instructions. iCLP first distills explicit plans from existing step-by-step reasoning trajectories. It then learns discrete representations of these plans via a vector-quantized autoencoder coupled with a codebook. Finally, by fine-tuning LLMs on paired latent plans and corresponding reasoning steps, the models learn to perform implicit planning during reasoning. Experimental results on mathematical reasoning and code generation tasks demonstrate that, with iCLP, LLMs can plan in latent space while reasoning in language space. This approach yields significant improvements in both accuracy and efficiency and, crucially, demonstrates strong cross-domain generalization while preserving the interpretability of chain-of-thought reasoning.

</details>


### [26] [Beyond Hallucinations: A Composite Score for Measuring Reliability in Open-Source Large Language Models](https://arxiv.org/abs/2512.24058)
*Rohit Kumar Salla,Manoj Saravanan,Shrikar Reddy Kota*

Main category: cs.CL

TL;DR: 提出CRS框架统一评估LLM可靠性，整合校准、鲁棒性和不确定性量化，通过实验发现最可靠系统是平衡准确性、鲁棒性和校准不确定性的模型。


<details>
  <summary>Details</summary>
Motivation: LLM在医疗、法律、金融等关键决策领域应用日益广泛，但其可靠性仍不确定。现有评估方法碎片化，只关注孤立方面，无法全面评估LLM的可靠性问题。

Method: 提出Composite Reliability Score (CRS)框架，将校准、鲁棒性和不确定性量化整合为单一可解释指标。在5个QA数据集上对10个领先开源LLM进行实验，评估基线性能、扰动影响和校准方法。

Result: CRS提供了稳定的模型排名，发现了单一指标遗漏的隐藏故障模式，并显示最可靠的系统是那些平衡准确性、鲁棒性和校准不确定性的模型。

Conclusion: CRS是一个有效的统一框架，能够全面评估LLM在关键决策应用中的可靠性，为模型选择和部署提供重要指导。

Abstract: Large Language Models (LLMs) like LLaMA, Mistral, and Gemma are increasingly used in decision-critical domains such as healthcare, law, and finance, yet their reliability remains uncertain. They often make overconfident errors, degrade under input shifts, and lack clear uncertainty estimates. Existing evaluations are fragmented, addressing only isolated aspects. We introduce the Composite Reliability Score (CRS), a unified framework that integrates calibration, robustness, and uncertainty quantification into a single interpretable metric. Through experiments on ten leading open-source LLMs across five QA datasets, we assess performance under baselines, perturbations, and calibration methods. CRS delivers stable model rankings, uncovers hidden failure modes missed by single metrics, and highlights that the most dependable systems balance accuracy, robustness, and calibrated uncertainty.

</details>


### [27] [HY-MT1.5 Technical Report](https://arxiv.org/abs/2512.24092)
*Mao Zheng,Zheng Li,Tao Chen,Mingyang Song,Di Wang*

Main category: cs.CL

TL;DR: HY-MT1.5系列翻译模型通过多层次训练框架，在1.8B和7B参数规模下实现了卓越性能，超越多数开源和商业模型，接近甚至在某些任务上超越超大规模专有模型。


<details>
  <summary>Details</summary>
Motivation: 开发参数高效的高性能机器翻译模型，通过优化训练框架来超越现有开源和商业翻译系统的性能，同时支持高级翻译约束功能。

Method: 采用多层次训练框架：包括通用预训练、MT导向预训练、监督微调、策略蒸馏和强化学习。该框架整合了多种训练阶段，专门针对翻译任务优化。

Result: 1.8B参数模型在标准中英外翻译任务中超越Tower-Plus-72B、Qwen3-32B等大型开源模型及微软翻译、豆包翻译等商业API，达到Gemini-3.0-Pro约90%性能。7B参数模型在Flores-200上达到Gemini-3.0-Pro的95%性能，并在WMT25和少数民族语言测试集上超越Gemini-3.0-Pro。

Conclusion: HY-MT1.5系列模型在各自参数规模下提供了高度竞争力和鲁棒性的翻译解决方案，支持术语干预、上下文感知翻译和格式保留等高级功能，为通用和专业翻译任务设定了新的性能标准。

Abstract: In this report, we introduce our latest translation models, HY-MT1.5-1.8B and HY-MT1.5-7B, a new family of machine translation models developed through a holistic training framework tailored for high-performance translation. Our methodology orchestrates a multi-stage pipeline that integrates general and MT-oriented pre-training, supervised fine-tuning, on-policy distillation, and reinforcement learning. HY-MT1.5-1.8B, the 1.8B-parameter model demonstrates remarkable parameter efficiency, comprehensively outperforming significantly larger open-source baselines (e.g., Tower-Plus-72B, Qwen3-32B) and mainstream commercial APIs (e.g., Microsoft Translator, Doubao Translator) in standard Chinese-foreign and English-foreign tasks. It achieves approximately 90% of the performance of ultra-large proprietary models such as Gemini-3.0-Pro, while marginally trailing Gemini-3.0-Pro on WMT25 and Mandarin-minority language benchmarks, it maintains a substantial lead over other competing models. Furthermore, HY-MT1.5-7B establishes a new state-of-the-art for its size class, achieving 95% of Gemini-3.0-Pro's performance on Flores-200 and surpassing it on the challenging WMT25 and Mandarin-minority language test sets. Beyond standard translation, the HY-MT1.5 series supports advanced constraints, including terminology intervention, context-aware translation, and format preservation. Extensive empirical evaluations confirm that both models offer highly competitive, robust solutions for general and specialized translation tasks within their respective parameter scales.

</details>


### [28] [Training a Huggingface Model on AWS Sagemaker (Without Tears)](https://arxiv.org/abs/2512.24098)
*Liling Tan*

Main category: cs.CL

TL;DR: 为研究人员提供一站式指南，帮助他们在AWS SageMaker上从零开始训练首个Hugging Face模型，降低云平台学习门槛


<details>
  <summary>Details</summary>
Motivation: 大语言模型开发主要被资源丰富的研究机构和行业伙伴主导，许多研究人员缺乏本地计算资源而转向AWS SageMaker等云服务，但云平台的陡峭学习曲线和分散的文档信息阻碍了研究人员的使用

Method: 通过演示论文形式，集中整理研究人员在AWS SageMaker上训练首个Hugging Face模型所需的关键信息，提供一站式指南

Result: 创建了一个全面的资源指南，帮助研究人员克服云平台的学习障碍，成功在AWS SageMaker上训练他们的第一个Hugging Face模型

Conclusion: 该演示论文通过提供集中化的必要信息，有助于降低云平台采用门槛，促进更广泛的研究群体参与大语言模型开发

Abstract: The development of Large Language Models (LLMs) has primarily been driven by resource-rich research groups and industry partners. Due to the lack of on-premise computing resources required for increasingly complex models, many researchers are turning to cloud services like AWS SageMaker to train Hugging Face models. However, the steep learning curve of cloud platforms often presents a barrier for researchers accustomed to local environments. Existing documentation frequently leaves knowledge gaps, forcing users to seek fragmented information across the web. This demo paper aims to democratize cloud adoption by centralizing the essential information required for researchers to successfully train their first Hugging Face model on AWS SageMaker from scratch.

</details>


### [29] [Activation Steering for Masked Diffusion Language Models](https://arxiv.org/abs/2512.24143)
*Adi Shnaidman,Erin Feiglin,Osher Yaari,Efrat Mentel,Amit Levi,Raz Lapid*

Main category: cs.CL

TL;DR: 提出了一个基于激活引导的框架，用于在掩码扩散语言模型中实现推理时控制，通过对比示例计算层间引导向量，无需模拟去噪轨迹。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散语言模型通过迭代去噪过程生成文本，具有掩码并行解码和与自回归大语言模型竞争的性能，但缺乏有效的推理时控制和引导机制。

Method: 使用对比示例通过单次前向传播计算层间引导向量，在反向扩散的每一步应用这些方向，实现高效的推理时控制。

Result: 在LLaDA-8B-Instruct上的实验展示了可靠的高层属性调制能力，通过消融实验检验了在Transformer子模块和token范围（提示vs.响应）上的引导效果。

Conclusion: 该激活引导框架为掩码扩散语言模型提供了一种有效的推理时控制机制，能够可靠地调制生成文本的属性，为MDLMs的控制和引导开辟了新途径。

Abstract: Masked diffusion language models (MDLMs) generate text through an iterative denoising process. They have recently gained attention due to mask-parallel decoding and competitive performance with autoregressive large language models. However, effective mechanisms for inference-time control and steering in MDLMs remain largely unexplored. We present an activation-steering framework for MDLMs that computes layer-wise steering vectors from a single forward pass using contrastive examples, without simulating the denoising trajectory. These directions are applied at every reverse-diffusion step, yielding an efficient inference-time control mechanism. Experiments on LLaDA-8B-Instruct demonstrate reliable modulation of high-level attributes, with ablations examining the effects of steering across transformer sub-modules and token scope (prompt vs.\ response).

</details>


### [30] [Large Emotional World Model](https://arxiv.org/abs/2512.24149)
*Changhao Song,Yazhou Zhang,Hui Gao,Chang Yang,Peng Zhang*

Main category: cs.CL

TL;DR: 本文提出大型情感世界模型（LEWM），通过构建情感-原因-方法（EWH）数据集，将情感因素整合到世界模型中，以更准确地预测情感驱动的社会行为。


<details>
  <summary>Details</summary>
Motivation: 情感作为世界知识的重要组成部分显著影响人类决策，但现有大型语言模型主要关注物理世界规律，缺乏对情感因素的系统探索。研究首先证明了情感信息对理解世界的重要性，移除情感相关信息会降低推理性能。

Method: 基于心智理论，提出大型情感世界模型（LEWM）。构建情感-原因-方法（EWH）数据集，将情感整合到因果关系中，支持推理行为发生原因和情感如何驱动未来世界状态。LEWM同时建模情感状态、视觉观察和动作，使世界模型能够预测未来状态和情感转变。

Result: 实验结果表明，LEWM在情感驱动的社会行为预测上更准确，同时在基础任务上保持与通用世界模型相当的性能。

Conclusion: 将情感因素系统整合到世界模型中对于理解人类决策和预测社会行为具有重要意义，LEWM展示了情感建模在提升世界模型能力方面的潜力。

Abstract: World Models serve as tools for understanding the current state of the world and predicting its future dynamics, with broad application potential across numerous fields. As a key component of world knowledge, emotion significantly influences human decision-making. While existing Large Language Models (LLMs) have shown preliminary capability in capturing world knowledge, they primarily focus on modeling physical-world regularities and lack systematic exploration of emotional factors. In this paper, we first demonstrate the importance of emotion in understanding the world by showing that removing emotionally relevant information degrades reasoning performance. Inspired by theory of mind, we further propose a Large Emotional World Model (LEWM). Specifically, we construct the Emotion-Why-How (EWH) dataset, which integrates emotion into causal relationships and enables reasoning about why actions occur and how emotions drive future world states. Based on this dataset, LEWM explicitly models emotional states alongside visual observations and actions, allowing the world model to predict both future states and emotional transitions. Experimental results show that LEWM more accurately predicts emotion-driven social behaviors while maintaining comparable performance to general world models on basic tasks.

</details>


### [31] [Training Report of TeleChat3-MoE](https://arxiv.org/abs/2512.24157)
*Xinzhang Liu,Chao Wang,Zhihao Yang,Zhuo Jiang,Xuncheng Zhao,Haoran Wang,Lei Li,Dongdong He,Luobin Liu,Kaizhe Yuan,Han Gao,Zihan Wang,Yitong Yao,Sishi Xiong,Wenmin Deng,Haowei He,Kaidong Yu,Yu Zhao,Ruiyu Fang,Yuhao Jiang,Yingyan Li,Xiaohui Hu,Xi Yu,Jingqi Li,Yanwei Liu,Qingli Li,Xinyu Shi,Junhao Niu,Chengnuo Huang,Yao Xiao,Ruiwen Wang,Fengkai Li,Luwen Pu,Kaipeng Jia,Fubei Yao,Yuyao Huang,Xuewei He,Zhuoru Jiang,Ruiting Song,Rui Xue,Qiyi Xie,Jie Zhang,Zilu Huang,Zhaoxi Zhang,Zhilong Lu,Yanhan Zhang,Yin Zhang,Yanlei Xue,Zhu Yuan,Teng Su,Xin Jiang,Shuangyong Song,Yongxiang Li,Xuelong Li*

Main category: cs.CL

TL;DR: TeleChat3-MoE是采用MoE架构的大语言模型系列，本报告重点介绍了支持其千亿到万亿参数规模训练的基础设施优化方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模扩展到千亿至万亿参数级别，需要可靠且高效的训练基础设施来支持大规模分布式训练，确保硬件平台间的一致性和训练效率。

Method: 1) 系统化的算子级和端到端数值精度验证方法；2) 性能优化技术：交错流水线调度、长序列训练的注意力感知数据调度、专家并行的分层重叠通信、DVM算子融合；3) 基于分析估计和整数线性规划的系统化并行化框架；4) 集群级优化方法，解决主机和设备端瓶颈。

Result: 这些基础设施改进在数千个设备的集群上实现了显著的吞吐量提升和接近线性的扩展性，为硬件生态系统上的大规模语言模型开发提供了坚实基础。

Conclusion: 报告展示了TeleChat3-MoE系列模型训练基础设施的系统化优化方法，为大规模MoE架构语言模型的可靠高效训练提供了可复制的技术框架和实践经验。

Abstract: TeleChat3-MoE is the latest series of TeleChat large language models, featuring a Mixture-of-Experts (MoE) architecture with parameter counts ranging from 105 billion to over one trillion,trained end-to-end on Ascend NPU cluster. This technical report mainly presents the underlying training infrastructure that enables reliable and efficient scaling to frontier model sizes. We detail systematic methodologies for operator-level and end-to-end numerical accuracy verification, ensuring consistency across hardware platforms and distributed parallelism strategies. Furthermore, we introduce a suite of performance optimizations, including interleaved pipeline scheduling, attention-aware data scheduling for long-sequence training,hierarchical and overlapped communication for expert parallelism, and DVM-based operator fusion. A systematic parallelization framework, leveraging analytical estimation and integer linear programming, is also proposed to optimize multi-dimensional parallelism configurations. Additionally, we present methodological approaches to cluster-level optimizations, addressing host- and device-bound bottlenecks during large-scale training tasks. These infrastructure advancements yield significant throughput improvements and near-linear scaling on clusters comprising thousands of devices, providing a robust foundation for large-scale language model development on hardware ecosystems.

</details>


### [32] [MedKGI: Iterative Differential Diagnosis with Medical Knowledge Graphs and Information-Guided Inquiring](https://arxiv.org/abs/2512.24181)
*Qipeng Wang,Rui Sheng,Yafei Li,Huamin Qu,Yushi Sun,Min Zhu*

Main category: cs.CL

TL;DR: MedKGI：基于医学知识图谱的临床诊断框架，解决LLM在诊断推理中的幻觉、冗余和一致性不足问题，显著提升诊断准确性和对话效率。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在临床诊断中存在三个关键局限：1）基于弱验证知识产生医学幻觉内容；2）提出冗余而非判别性问题阻碍诊断进展；3）多轮对话中失去连贯性导致矛盾结论。需要模拟真实临床场景的迭代诊断假设驱动推理。

Method: 提出MedKGI诊断框架，整合医学知识图谱约束推理于验证医学本体，基于信息增益选择问题以最大化诊断效率，采用OSCE格式结构化状态保持跨轮次证据追踪一致性。

Result: 在临床基准测试中，MedKGI在诊断准确性和询问效率上均优于强LLM基线，平均提升对话效率30%，同时保持最先进的准确性。

Conclusion: MedKGI通过知识图谱约束、信息增益驱动的问题选择和结构化状态追踪，有效解决了LLM在临床诊断中的核心问题，为医疗AI提供了更可靠的诊断推理框架。

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated significant promise in clinical diagnosis. However, current models struggle to emulate the iterative, diagnostic hypothesis-driven reasoning of real clinical scenarios. Specifically, current LLMs suffer from three critical limitations: (1) generating hallucinated medical content due to weak grounding in verified knowledge, (2) asking redundant or inefficient questions rather than discriminative ones that hinder diagnostic progress, and (3) losing coherence over multi-turn dialogues, leading to contradictory or inconsistent conclusions. To address these challenges, we propose MedKGI, a diagnostic framework grounded in clinical practices. MedKGI integrates a medical knowledge graph (KG) to constrain reasoning to validated medical ontologies, selects questions based on information gain to maximize diagnostic efficiency, and adopts an OSCE-format structured state to maintain consistent evidence tracking across turns. Experiments on clinical benchmarks show that MedKGI outperforms strong LLM baselines in both diagnostic accuracy and inquiry efficiency, improving dialogue efficiency by 30% on average while maintaining state-of-the-art accuracy.

</details>


### [33] [LAILA: A Large Trait-Based Dataset for Arabic Automated Essay Scoring](https://arxiv.org/abs/2512.24235)
*May Bashendy,Walid Massoud,Sohaila Eltanbouly,Salam Albatarni,Marwan Sayed,Abrar Abir,Houda Bouamor,Tamer Elsayed*

Main category: cs.CL

TL;DR: 本文介绍了LAILA——迄今为止最大的公开阿拉伯语自动作文评分数据集，包含7,859篇作文，标注了整体分数和七个维度的特质分数，填补了阿拉伯语AES研究的数据空白。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语自动作文评分研究因缺乏公开可用数据集而受限，现有研究不足，需要高质量数据集来支持该领域的发展。

Method: 构建LAILA数据集，包含7,859篇阿拉伯语作文，标注整体分数和七个维度的特质分数（相关性、组织结构、词汇、风格、发展、格式、语法），并设计了数据集的设计、收集和标注流程。

Result: 提供了基准测试结果，使用最先进的阿拉伯语和英语模型在特定提示和跨提示设置下进行评估，展示了数据集的实用性和有效性。

Conclusion: LAILA数据集填补了阿拉伯语自动作文评分研究的关键空白，为开发鲁棒的评分系统提供了重要支持。

Abstract: Automated Essay Scoring (AES) has gained increasing attention in recent years, yet research on Arabic AES remains limited due to the lack of publicly available datasets. To address this, we introduce LAILA, the largest publicly available Arabic AES dataset to date, comprising 7,859 essays annotated with holistic and trait-specific scores on seven dimensions: relevance, organization, vocabulary, style, development, mechanics, and grammar. We detail the dataset design, collection, and annotations, and provide benchmark results using state-of-the-art Arabic and English models in prompt-specific and cross-prompt settings. LAILA fills a critical need in Arabic AES research, supporting the development of robust scoring systems.

</details>


### [34] [Tracing the Flow of Knowledge From Science to Technology Using Deep Learning](https://arxiv.org/abs/2512.24259)
*Michael E. Rose,Mainak Ghosh,Sebastian Erhardt,Cheng Li,Erik Buunk,Dietmar Harhoff*

Main category: cs.CL

TL;DR: 开发了适用于专利和科学文献的语言相似性模型Pat-SPECTER，在预测专利-论文引用方面表现最佳，并用于分析不同司法管辖区引用模式的差异。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够同时处理专利和科学文献的语言相似性模型，以评估专利与论文之间的语义关联性，特别是在预测可信的专利-论文引用关系方面。

Method: 开发了Pat-SPECTER模型（基于SPECTER2在专利数据上的微调版本），通过"赛马式"评估比较了8种语言相似性模型在预测专利-论文引用方面的性能。

Result: Pat-SPECTER模型在预测可信专利-论文引用方面表现最佳；在两种实际场景（分离和预测专利-论文对）中展示了其能力；验证了美国专利因"诚信义务"而引用语义相似度较低论文的假设。

Conclusion: Pat-SPECTER是处理专利和科学文献相似性分析的有效工具，模型已开源供学术界和从业者使用，有助于理解不同司法管辖区专利引用行为的差异。

Abstract: We develop a language similarity model suitable for working with patents and scientific publications at the same time. In a horse race-style evaluation, we subject eight language (similarity) models to predict credible Patent-Paper Citations. We find that our Pat-SPECTER model performs best, which is the SPECTER2 model fine-tuned on patents. In two real-world scenarios (separating patent-paper-pairs and predicting patent-paper-pairs) we demonstrate the capabilities of the Pat-SPECTER. We finally test the hypothesis that US patents cite papers that are semantically less similar than in other large jurisdictions, which we posit is because of the duty of candor. The model is open for the academic community and practitioners alike.

</details>


### [35] [Joint Selection for Large-Scale Pre-Training Data via Policy Gradient-based Mask Learning](https://arxiv.org/abs/2512.24265)
*Ziqing Fan,Yuqiao Xian,Yan Sun,Li Shen*

Main category: cs.CL

TL;DR: DATAMASK提出了一种高效的大规模预训练数据选择框架，能够联合优化质量和多样性指标，在万亿token数据集上实现高效选择，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法存在局限性：基于质量指标的选择在长期预训练中会出现收益递减，而基于多样性指标的选择会移除过多高质量样本，两者都限制了预训练LLM的能力。由于计算成本高，现有方法很少在万亿级数据集上联合考虑多种指标。

Method: 将数据选择过程建模为掩码学习问题，通过迭代采样数据掩码、基于预定义目标计算策略梯度、更新掩码采样逻辑，使用基于策略梯度的优化和各种加速增强技术，比贪心算法减少98.9%的选择时间。

Result: 从15万亿token的FineWeb数据集中选择约10%的子集FineWeb-Mask，在12个不同任务上评估，1.5B密集模型获得3.2%的显著提升，7B MoE模型获得1.9%的提升。

Conclusion: DATAMASK框架能够高效地在万亿规模token数据集上联合优化质量和多样性指标，显著提升预训练效率并改善模型性能，为大规模预训练数据选择提供了有效解决方案。

Abstract: A fine-grained data recipe is crucial for pre-training large language models, as it can significantly enhance training efficiency and model performance. One important ingredient in the recipe is to select samples based on scores produced by defined rules, LLM judgment, or statistical information in embeddings, which can be roughly categorized into quality and diversity metrics. Due to the high computational cost when applied to trillion-scale token pre-training datasets such as FineWeb and DCLM, these two or more types of metrics are rarely considered jointly in a single selection process. However, in our empirical study, selecting samples based on quality metrics exhibit severe diminishing returns during long-term pre-training, while selecting on diversity metrics removes too many valuable high-quality samples, both of which limit pre-trained LLMs' capabilities. Therefore, we introduce DATAMASK, a novel and efficient joint learning framework designed for large-scale pre-training data selection that can simultaneously optimize multiple types of metrics in a unified process, with this study focusing specifically on quality and diversity metrics. DATAMASK approaches the selection process as a mask learning problem, involving iterative sampling of data masks, computation of policy gradients based on predefined objectives with sampled masks, and updating of mask sampling logits. Through policy gradient-based optimization and various acceleration enhancements, it significantly reduces selection time by 98.9% compared to greedy algorithm, enabling our study to explore joint learning within trillion-scale tokens. With DATAMASK, we select a subset of about 10% from the 15 trillion-token FineWeb dataset, termed FineWeb-Mask. Evaluated across 12 diverse tasks, we achieves significant improvements of 3.2% on a 1.5B dense model and 1.9% on a 7B MoE model.

</details>


### [36] [Automated Analysis of Sustainability Reports: Using Large Language Models for the Extraction and Prediction of EU Taxonomy-Compliant KPIs](https://arxiv.org/abs/2512.24289)
*Jonathan Schmoll,Adam Jatowt*

Main category: cs.CL

TL;DR: 该研究首次系统评估了大型语言模型在欧盟分类法合规工作流程中的表现，发现模型在定性任务上表现中等，但在定量任务上完全失败，表明LLM目前只能作为辅助工具而非完全自动化解决方案。


<details>
  <summary>Details</summary>
Motivation: 欧盟分类法合规流程是手动且资源密集的，虽然LLM提供了自动化路径，但缺乏公开基准数据集阻碍了研究进展。为了解决这一空白，研究需要创建结构化数据集并进行系统评估。

Method: 从190份企业报告中构建了包含真实经济活动和经济绩效指标的结构化数据集，使用该数据集对LLM在核心合规工作流程中进行首次系统评估，包括定性（识别经济活动）和定量（预测财务KPI）任务。

Result: LLM在定性任务中表现中等，多步代理框架能适度提高精度；但在零样本设置下的定量任务中完全失败。还发现了一个悖论：简洁的元数据通常比完整的非结构化报告表现更好，且模型置信度分数校准较差。

Conclusion: 虽然LLM尚未准备好实现完全自动化，但可以作为人类专家的强大辅助工具。研究提供的数据集为未来研究提供了公开基准。

Abstract: The manual, resource-intensive process of complying with the EU Taxonomy presents a significant challenge for companies. While Large Language Models (LLMs) offer a path to automation, research is hindered by a lack of public benchmark datasets. To address this gap, we introduce a novel, structured dataset from 190 corporate reports, containing ground-truth economic activities and quantitative Key Performance Indicators (KPIs). We use this dataset to conduct the first systematic evaluation of LLMs on the core compliance workflow. Our results reveal a clear performance gap between qualitative and quantitative tasks. LLMs show moderate success in the qualitative task of identifying economic activities, with a multi-step agentic framework modestly enhancing precision. Conversely, the models comprehensively fail at the quantitative task of predicting financial KPIs in a zero-shot setting. We also discover a paradox, where concise metadata often yields superior performance to full, unstructured reports, and find that model confidence scores are poorly calibrated. We conclude that while LLMs are not ready for full automation, they can serve as powerful assistive tools for human experts. Our dataset provides a public benchmark for future research.

</details>


### [37] [Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking](https://arxiv.org/abs/2512.24297)
*Meiqi Chen,Fandong Meng,Jie Zhou*

Main category: cs.CL

TL;DR: FIGR通过端到端强化学习将主动视觉思考融入多轮推理，构建视觉表示来外化中间结构假设，提升复杂推理任务的稳定性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 复杂推理问题常涉及隐式的空间、几何和结构关系，这些关系在纯文本中难以充分编码。现有的文本推理模型在捕捉全局结构约束方面存在局限。

Method: FIGR通过端到端强化学习将主动视觉思考集成到多轮推理中，在问题解决过程中动态构建视觉表示来外化中间结构假设，并自适应地调节视觉推理的调用时机与方式。

Result: 在数学推理基准测试中，FIGR优于强文本链式思维基线模型，在AIME 2025上提升13.12%，在BeyondAIME上提升11.00%，证明了视觉引导多模态推理的有效性。

Conclusion: FIGR通过将主动视觉思考融入推理过程，显著提升了复杂推理任务的稳定性和可靠性，特别是在处理全局结构约束方面，为多模态推理提供了有效解决方案。

Abstract: Complex reasoning problems often involve implicit spatial, geometric, and structural relationships that are not explicitly encoded in text. While recent reasoning models have achieved strong performance across many domains, purely text-based reasoning struggles to represent global structural constraints in complex settings. In this paper, we introduce FIGR, which integrates active visual thinking into multi-turn reasoning via end-to-end reinforcement learning. FIGR externalizes intermediate structural hypotheses by constructing visual representations during problem solving. By adaptively regulating when and how visual reasoning should be invoked, FIGR enables more stable and coherent reasoning over global structural properties that are difficult to capture from text alone. Experiments on challenging mathematical reasoning benchmarks demonstrate that FIGR outperforms strong text-only chain-of-thought baselines. In particular, FIGR improves the base model by 13.12% on AIME 2025 and 11.00% on BeyondAIME, highlighting the effectiveness of figure-guided multimodal reasoning in enhancing the stability and reliability of complex reasoning.

</details>


### [38] [QianfanHuijin Technical Report: A Novel Multi-Stage Training Paradigm for Finance Industrial LLMs](https://arxiv.org/abs/2512.24314)
*Shupeng Li,Weipeng Lu,Linyun Liu,Chen Lin,Shaofei Li,Zhendong Tan,Hanjun Zhong,Yucheng Zeng,Chenghao Zhu,Mengyue Liu,Daxiang Dong,Jianmin Wu,Yunting Xiao,Annan Li,Danyu Liu,Jingnan Zhang,Licen Liu,Dawei Yin,Dou Shen*

Main category: cs.CL

TL;DR: QianfanHuijin是一个金融领域大语言模型，采用多阶段训练范式，包括持续预训练和细粒度后训练，在金融推理和代理能力方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 金融领域对LLM的需求从单纯的知识增强转向需要更强的金融推理和代理能力，以应对日益复杂的金融服务需求。

Method: 提出可泛化的多阶段训练范式：1) 金融语料持续预训练；2) 细粒度后训练（金融SFT→金融推理RL→金融代理RL→通用RL与真实业务对齐）。

Result: QianfanHuijin在多个权威金融基准测试中表现优异，消融研究证实推理RL和代理RL阶段显著提升了相应能力。

Conclusion: 这种细粒度渐进式后训练方法有望成为工业增强LLM的主流范式，验证了研究动机的有效性。

Abstract: Domain-specific enhancement of Large Language Models (LLMs) within the financial context has long been a focal point of industrial application. While previous models such as BloombergGPT and Baichuan-Finance primarily focused on knowledge enhancement, the deepening complexity of financial services has driven a growing demand for models that possess not only domain knowledge but also robust financial reasoning and agentic capabilities. In this paper, we present QianfanHuijin, a financial domain LLM, and propose a generalizable multi-stage training paradigm for industrial model enhancement.
  Our approach begins with Continual Pre-training (CPT) on financial corpora to consolidate the knowledge base. This is followed by a fine-grained Post-training pipeline designed with increasing specificity: starting with Financial SFT, progressing to Finance Reasoning RL and Finance Agentic RL, and culminating in General RL aligned with real-world business scenarios. Empirical results demonstrate that QianfanHuijin achieves superior performance across various authoritative financial benchmarks. Furthermore, ablation studies confirm that the targeted Reasoning RL and Agentic RL stages yield significant gains in their respective capabilities. These findings validate our motivation and suggest that this fine-grained, progressive post-training methodology is poised to become a mainstream paradigm for various industrial-enhanced LLMs.

</details>


### [39] [World model inspired sarcasm reasoning with large language model agents](https://arxiv.org/abs/2512.24329)
*Keito Inoshita,Shinnosuke Mizuno*

Main category: cs.CL

TL;DR: WM-SAR：将讽刺理解重新定义为世界模型启发的推理过程，通过分解字面意义、上下文、规范期望和意图为专门的LLM代理，结合不一致性评分和意图评分，实现可解释的讽刺检测。


<details>
  <summary>Details</summary>
Motivation: 现有讽刺理解方法大多依赖单一模型的黑盒预测，难以结构化解释讽刺背后的认知因素。虽然讽刺常表现为语义评估与规范期望或意图之间的不匹配，但能明确分解和建模这些组件的框架仍然有限。

Method: 提出WM-SAR框架，将讽刺理解重新定义为世界模型启发的推理过程。分解为四个专门LLM代理：字面意义、上下文、规范期望和意图。量化字面评估与规范期望之间的不一致性评分，结合意图评分，通过轻量级逻辑回归模型集成这些信号来推断最终讽刺概率。

Result: 在代表性讽刺检测基准测试中，WM-SAR一致优于现有的深度学习和LLM方法。消融研究和案例分析进一步证明，整合语义不一致性和意图推理对于有效的讽刺检测至关重要，实现了强大的性能和高度可解释性。

Conclusion: WM-SAR通过世界模型启发的推理方法，在保持可解释数值决策结构的同时，利用LLM的推理能力，为讽刺理解提供了既有强大性能又有高度可解释性的解决方案。

Abstract: Sarcasm understanding is a challenging problem in natural language processing, as it requires capturing the discrepancy between the surface meaning of an utterance and the speaker's intentions as well as the surrounding social context. Although recent advances in deep learning and Large Language Models (LLMs) have substantially improved performance, most existing approaches still rely on black-box predictions of a single model, making it difficult to structurally explain the cognitive factors underlying sarcasm. Moreover, while sarcasm often emerges as a mismatch between semantic evaluation and normative expectations or intentions, frameworks that explicitly decompose and model these components remain limited. In this work, we reformulate sarcasm understanding as a world model inspired reasoning process and propose World Model inspired SArcasm Reasoning (WM-SAR), which decomposes literal meaning, context, normative expectation, and intention into specialized LLM-based agents. The discrepancy between literal evaluation and normative expectation is explicitly quantified as a deterministic inconsistency score, and together with an intention score, these signals are integrated by a lightweight Logistic Regression model to infer the final sarcasm probability. This design leverages the reasoning capability of LLMs while maintaining an interpretable numerical decision structure. Experiments on representative sarcasm detection benchmarks show that WM-SAR consistently outperforms existing deep learning and LLM-based methods. Ablation studies and case analyses further demonstrate that integrating semantic inconsistency and intention reasoning is essential for effective sarcasm detection, achieving both strong performance and high interpretability.

</details>


### [40] [Skim-Aware Contrastive Learning for Efficient Document Representation](https://arxiv.org/abs/2512.24373)
*Waheed Ahmed Abro,Zied Bouraoui*

Main category: cs.CL

TL;DR: 提出基于人类略读策略的自监督对比学习框架，通过随机掩码文档片段并使用NLI对比目标来增强长文档表示，在法医文本上取得显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长文档表示方面存在不足：Transformer模型难以处理长文档，稀疏注意力机制资源消耗大且难以捕获完整文档上下文，分层Transformer效率较高但缺乏清晰的跨片段关系解释。受人类略读策略启发，需要更有效的长文档表示方法。

Method: 提出自监督对比学习框架：1）随机掩码文档片段；2）使用基于自然语言推理(NLI)的对比学习目标，将掩码片段与相关部分对齐，同时与不相关部分保持距离；3）模仿人类信息整合过程，生成更丰富且计算高效的文档表示。

Result: 在法学和生物医学文本上的实验表明，该方法在准确性和效率方面都取得了显著提升。

Conclusion: 受人类略读策略启发的自监督对比学习框架能够有效提升长文档表示的质量和计算效率，特别适用于法学和医学等专业领域的长文档处理。

Abstract: Although transformer-based models have shown strong performance in word- and sentence-level tasks, effectively representing long documents, especially in fields like law and medicine, remains difficult. Sparse attention mechanisms can handle longer inputs, but are resource-intensive and often fail to capture full-document context. Hierarchical transformer models offer better efficiency but do not clearly explain how they relate different sections of a document. In contrast, humans often skim texts, focusing on important sections to understand the overall message. Drawing from this human strategy, we introduce a new self-supervised contrastive learning framework that enhances long document representation. Our method randomly masks a section of the document and uses a natural language inference (NLI)-based contrastive objective to align it with relevant parts while distancing it from unrelated ones. This mimics how humans synthesize information, resulting in representations that are both richer and more computationally efficient. Experiments on legal and biomedical texts confirm significant gains in both accuracy and efficiency.

</details>


### [41] [Comparing Approaches to Automatic Summarization in Less-Resourced Languages](https://arxiv.org/abs/2512.24410)
*Chester Palen-Michel,Constantine Lignos*

Main category: cs.CL

TL;DR: 本研究比较了在低资源语言文本摘要中的多种方法，包括大模型零样本提示、小模型微调、数据增强、多语言迁移和翻译管道，发现微调mT5模型在多数指标上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 虽然自动文本摘要在英语等高资源语言中已取得高性能，但对低资源语言的摘要研究相对较少，需要探索适用于这些语言的有效方法。

Method: 1) 大语言模型零样本提示（不同规模模型）；2) 微调较小模型如mT5（包含三种数据增强方法）；3) 多语言迁移；4) 翻译管道方法（源语言→英语→摘要→翻译回源语言）。使用五种不同指标进行评估。

Result: 1) 相似参数规模的大语言模型在性能上存在差异；2) 多语言微调的mT5基线模型在大多数指标上优于其他方法，包括大语言模型零样本性能；3) 大语言模型作为评估者在低资源语言上可能不够可靠。

Conclusion: 对于低资源语言文本摘要，微调多语言模型（如mT5）比依赖大语言模型零样本提示或翻译管道方法更有效，且需要注意大语言模型评估在低资源语言上的可靠性问题。

Abstract: Automatic text summarization has achieved high performance in high-resourced languages like English, but comparatively less attention has been given to summarization in less-resourced languages. This work compares a variety of different approaches to summarization from zero-shot prompting of LLMs large and small to fine-tuning smaller models like mT5 with and without three data augmentation approaches and multilingual transfer. We also explore an LLM translation pipeline approach, translating from the source language to English, summarizing and translating back. Evaluating with five different metrics, we find that there is variation across LLMs in their performance across similar parameter sizes, that our multilingual fine-tuned mT5 baseline outperforms most other approaches including zero-shot LLM performance for most metrics, and that LLM as judge may be less reliable on less-resourced languages.

</details>


### [42] [Cleaning English Abstracts of Scientific Publications](https://arxiv.org/abs/2512.24459)
*Michael E. Rose,Nils A. Herrmann,Sebastian Erhardt*

Main category: cs.CL

TL;DR: 开发了一个开源语言模型，用于自动清理科学摘要中的无关信息，以提高文本分析和嵌入的质量。


<details>
  <summary>Details</summary>
Motivation: 科学摘要常被用作研究内容分析，但许多摘要包含版权声明、章节标题、作者注释等无关信息，这些会扭曲下游分析，特别是文档相似性和文本嵌入分析。

Method: 开发了一个开源、易于集成的语言模型，专门用于自动识别和清理英文科学摘要中的无关信息。

Result: 该模型既保守又精确，能改变清理后摘要的相似性排名，并提高标准长度嵌入的信息含量。

Conclusion: 提出的模型能有效清理科学摘要中的干扰信息，为基于摘要的文本分析提供更准确的数据基础。

Abstract: Scientific abstracts are often used as proxies for the content and thematic focus of research publications. However, a significant share of published abstracts contains extraneous information-such as publisher copyright statements, section headings, author notes, registrations, and bibliometric or bibliographic metadata-that can distort downstream analyses, particularly those involving document similarity or textual embeddings. We introduce an open-source, easy-to-integrate language model designed to clean English-language scientific abstracts by automatically identifying and removing such clutter. We demonstrate that our model is both conservative and precise, alters similarity rankings of cleaned abstracts and improves information content of standard-length embeddings.

</details>


### [43] [IELTS Writing Revision Platform with Automated Essay Scoring and Adaptive Feedback](https://arxiv.org/abs/2512.24460)
*Titas Ramancauskas,Kotryna Ramancauske*

Main category: cs.CL

TL;DR: 本文开发了一个用于雅思写作备考的智能修订平台，通过设计研究法迭代优化，最终基于DistilBERT的自动评分系统实现了显著改进，并提供个性化反馈。


<details>
  <summary>Details</summary>
Motivation: 传统雅思备考方法缺乏针对雅思写作评分标准的个性化反馈，需要开发能够提供针对性指导的智能平台。

Method: 采用设计研究法（DBR）进行迭代开发，从基于规则的方法过渡到基于DistilBERT变换器模型加回归头的自动评分系统，并实现自适应反馈功能。

Result: DistilBERT模型显著提升了评分准确性（MAE=0.66，正R²值），自适应反馈使雅思写作分数平均提高0.060个分数段（p=0.011），效果因修订策略而异。

Conclusion: 自动反馈功能最适合作为人工指导的补充，保守的表面修正比激进的结构干预更可靠。未来需要针对高分作文评估进行改进，并开展长期研究和官方考官验证。

Abstract: This paper presents the design, development, and evaluation of a proposed revision platform assisting candidates for the International English Language Testing System (IELTS) writing exam. Traditional IELTS preparation methods lack personalised feedback, catered to the IELTS writing rubric. To address these shortcomings, the platform features an attractive user interface (UI), an Automated Essay Scoring system (AES), and targeted feedback tailored to candidates and the IELTS writing rubric. The platform architecture separates conversational guidance from a dedicated writing interface to reduce cognitive load and simulate exam conditions. Through iterative, Design-Based Research (DBR) cycles, the study progressed from rule-based to transformer-based with a regression head scoring, mounted with adaptive feedback.
  Early cycles (2-3) revealed fundamental limitations of rule-based approaches: mid-band compression, low accuracy, and negative $R^2$ values. DBR Cycle 4 implemented a DistilBERT transformer model with a regression head, yielding substantial improvements with MAE of 0.66 and positive $R^2$. This enabled Cycle 5's adaptive feedback implementation, which demonstrated statistically significant score improvements (mean +0.060 bands, p = 0.011, Cohen's d = 0.504), though effectiveness varied by revision strategy. Findings suggest automated feedback functions are most suited as a supplement to human instruction, with conservative surface-level corrections proving more reliable than aggressive structural interventions for IELTS preparation contexts. Challenges remain in assessing higher-band essays, and future work should incorporate longitudinal studies with real IELTS candidates and validation from official examiners.

</details>


### [44] [Paragraph Segmentation Revisited: Towards a Standard Task for Structuring Speech](https://arxiv.org/abs/2512.24517)
*Fabian Retkowski,Alexander Waibel*

Main category: cs.CL

TL;DR: 该论文提出了首个语音段落分割基准数据集（TEDPara和YTSegPara），并提出约束解码方法和紧凑模型MiniSeg，实现了语音转录本的段落结构化。


<details>
  <summary>Details</summary>
Motivation: 自动语音转录通常产生无结构的词流，影响可读性和重用性。当前语音处理领域缺乏段落分割这一关键后处理步骤，且文本分割领域也缺乏鲁棒、自然的基准数据集。

Method: 1. 建立两个基准数据集：人工标注的TEDPara和合成标注的YTSegPara；2. 提出约束解码方法，让大语言模型插入段落断点同时保留原始转录本；3. 开发紧凑模型MiniSeg，并扩展为分层模型联合预测章节和段落。

Result: MiniSeg模型在段落分割任务上达到最先进的准确率，分层扩展后能以最小计算成本同时预测章节和段落。提出的资源和方将段落分割确立为标准化的实用语音处理任务。

Conclusion: 该研究填补了语音处理和文本分割交叉领域的空白，通过建立基准数据集、提出有效方法，将段落分割确立为语音处理中的标准化实用任务，提升了语音转录本的结构化和可读性。

Abstract: Automatic speech transcripts are often delivered as unstructured word streams that impede readability and repurposing. We recast paragraph segmentation as the missing structuring step and fill three gaps at the intersection of speech processing and text segmentation. First, we establish TEDPara (human-annotated TED talks) and YTSegPara (YouTube videos with synthetic labels) as the first benchmarks for the paragraph segmentation task. The benchmarks focus on the underexplored speech domain, where paragraph segmentation has traditionally not been part of post-processing, while also contributing to the wider text segmentation field, which still lacks robust and naturalistic benchmarks. Second, we propose a constrained-decoding formulation that lets large language models insert paragraph breaks while preserving the original transcript, enabling faithful, sentence-aligned evaluation. Third, we show that a compact model (MiniSeg) attains state-of-the-art accuracy and, when extended hierarchically, jointly predicts chapters and paragraphs with minimal computational cost. Together, our resources and methods establish paragraph segmentation as a standardized, practical task in speech processing.

</details>


### [45] [Safe in the Future, Dangerous in the Past: Dissecting Temporal and Linguistic Vulnerabilities in LLMs](https://arxiv.org/abs/2512.24556)
*Muhammad Abdullahi Said,Muhammad Sammani Sani*

Main category: cs.CL

TL;DR: 本研究系统审计了GPT-5.1、Gemini 3 Pro和Claude 4.5 Opus三种大语言模型在英语和豪萨语中的安全性表现，发现安全性不是固定属性而是取决于语言和时态框架的交互作用，存在复杂的干扰机制而非简单的低资源语言性能退化。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型融入全球关键基础设施，人们通常假设安全性对齐能够从英语零样本迁移到其他语言，但这种假设存在危险盲点。研究者希望系统评估模型在多语言环境中的真实安全性表现。

Method: 使用基于西非威胁场景（如雅虎-雅虎诈骗、丹恩枪制造）构建的新型对抗性数据集HausaSafety，采用2×4因子设计（语言×时态框架）对三种最先进模型进行1,440次评估，分析语言与时态框架的非线性交互作用。

Result: 研究发现：1）存在复杂干扰机制而非简单的低资源语言性能退化；2）Claude 4.5 Opus在豪萨语中安全性（45.0%）显著高于英语（36.7%），出现反向语言安全现象；3）存在深刻的时间不对称性，过去时框架绕过防御（15.6%安全），未来时场景触发过度保守拒绝（57.2%安全）；4）最安全与最脆弱配置间存在9.2倍差异。

Conclusion: 当前模型依赖表面启发式而非稳健的语义理解，形成了安全漏洞，使全球南方用户面临本地化危害。需要向不变对齐范式转变，确保跨语言和时态变化的安全性稳定性。

Abstract: As Large Language Models (LLMs) integrate into critical global infrastructure, the assumption that safety alignment transfers zero-shot from English to other languages remains a dangerous blind spot. This study presents a systematic audit of three state of the art models (GPT-5.1, Gemini 3 Pro, and Claude 4.5 Opus) using HausaSafety, a novel adversarial dataset grounded in West African threat scenarios (e.g., Yahoo-Yahoo fraud, Dane gun manufacturing). Employing a 2 x 4 factorial design across 1,440 evaluations, we tested the non-linear interaction between language (English vs. Hausa) and temporal framing. Our results challenge the prevailing multilingual safety gap narrative. Instead of a simple degradation in low-resource settings, we identified a mechanism of Complex Interference where safety is determined by the intersection of variables. While models exhibited a Reverse Linguistic with Claude 4.5 Opus proving significantly safer in Hausa (45.0%) than in English (36.7%) due to uncertainty-driven refusal they suffered catastrophic failures in temporal reasoning. We report a profound Temporal Asymmetry, where past-tense framing bypassed defenses (15.6% safe) while future-tense scenarios triggered hyper-conservative refusals (57.2% safe). The magnitude of this volatility is illustrated by a 9.2x disparity between the safest and most vulnerable configurations, proving that safety is not a fixed property but a context-dependent state. We conclude that current models rely on superficial heuristics rather than robust semantic understanding, creating Safety Pockets that leave Global South users exposed to localized harms. We propose Invariant Alignment as a necessary paradigm shift to ensure safety stability across linguistic and temporal shifts.

</details>


### [46] [HaluNet: Multi-Granular Uncertainty Modeling for Efficient Hallucination Detection in LLM Question Answering](https://arxiv.org/abs/2512.24562)
*Chaodong Tong,Qi Zhang,Jiayang Gao,Lei Jiang,Yanbing Liu,Nannan Sun*

Main category: cs.CL

TL;DR: HaluNet是一个轻量级可训练的神经网络框架，通过整合多粒度标记级不确定性（结合语义嵌入与概率置信度和分布不确定性）来检测LLM幻觉，实现高效的单次检测。


<details>
  <summary>Details</summary>
Motivation: LLM在问答任务中经常产生幻觉（事实错误或虚构内容），现有方法通常只关注单一类型的不确定性，忽视了不同来源（特别是标记级概率不确定性和内部语义表示不确定性）之间的互补性。

Method: 提出HaluNet框架，采用多分支架构自适应地融合模型已知信息与其输出中表达的不确定性，整合语义嵌入与概率置信度和分布不确定性，实现高效的单次幻觉检测。

Result: 在SQuAD、TriviaQA和Natural Questions数据集上的实验表明，HaluNet在检测性能和计算效率方面表现优异，无论是否访问上下文都能有效工作。

Conclusion: HaluNet展示了在基于LLM的问答系统中进行实时幻觉检测的潜力，通过利用不同不确定性来源的互补性实现了强检测性能和良好计算效率。

Abstract: Large Language Models (LLMs) excel at question answering (QA) but often generate hallucinations, including factual errors or fabricated content. Detecting hallucinations from internal uncertainty signals is attractive due to its scalability and independence from external resources. Existing methods often aim to accurately capture a single type of uncertainty while overlooking the complementarity among different sources, particularly between token-level probability uncertainty and the uncertainty conveyed by internal semantic representations, which provide complementary views on model reliability. We present \textbf{HaluNet}, a lightweight and trainable neural framework that integrates multi granular token level uncertainties by combining semantic embeddings with probabilistic confidence and distributional uncertainty. Its multi branch architecture adaptively fuses what the model knows with the uncertainty expressed in its outputs, enabling efficient one pass hallucination detection. Experiments on SQuAD, TriviaQA, and Natural Questions show that HaluNet delivers strong detection performance and favorable computational efficiency, with or without access to context, highlighting its potential for real time hallucination detection in LLM based QA systems.

</details>


### [47] [Korean Canonical Legal Benchmark: Toward Knowledge-Independent Evaluation of LLMs' Legal Reasoning Capabilities](https://arxiv.org/abs/2512.24572)
*Hongseok Oh,Wonseok Hwang,Kyoung-Woon On*

Main category: cs.CL

TL;DR: 韩国规范法律基准(KCL)：评估语言模型法律推理能力的新基准，包含选择题和开放式问答两个部分，通过提供案例支持来分离推理能力与领域知识。


<details>
  <summary>Details</summary>
Motivation: 现有法律评估基准往往将推理能力与领域特定知识混淆，难以准确评估语言模型的法律推理能力。需要创建一个能够独立评估推理能力、并提供问题级案例支持的基准。

Method: 构建KCL基准，包含两个组件：KCL-MCQA（283道选择题，1,103个对齐案例）和KCL-Essay（169道开放式问题，550个对齐案例，2,739个实例级评分标准）。提供问题级案例支持以分离推理能力与参数化知识。

Result: 评估30多个模型显示，在法律推理能力上仍存在较大差距，特别是在KCL-Essay部分。专门用于推理的模型始终优于通用模型。所有资源（基准数据集和评估代码）已公开发布。

Conclusion: KCL基准能够有效评估语言模型的法律推理能力，分离推理能力与领域知识。结果显示专门化模型优于通用模型，为未来法律AI研究提供了有价值的评估工具。

Abstract: We introduce the Korean Canonical Legal Benchmark (KCL), a benchmark designed to assess language models' legal reasoning capabilities independently of domain-specific knowledge. KCL provides question-level supporting precedents, enabling a more faithful disentanglement of reasoning ability from parameterized knowledge. KCL consists of two components: (1) KCL-MCQA, multiple-choice problems of 283 questions with 1,103 aligned precedents, and (2) KCL-Essay, open-ended generation problems of 169 questions with 550 aligned precedents and 2,739 instance-level rubrics for automated evaluation. Our systematic evaluation of 30+ models shows large remaining gaps, particularly in KCL-Essay, and that reasoning-specialized models consistently outperform their general-purpose counterparts. We release all resources, including the benchmark dataset and evaluation code, at https://github.com/lbox-kr/kcl.

</details>


### [48] [Understanding and Steering the Cognitive Behaviors of Reasoning Models at Test-Time](https://arxiv.org/abs/2512.24574)
*Zhenyu Zhang,Xiaoxia Wu,Zhongzhu Zhou,Qingyang Wu,Yineng Zhang,Pragaash Ponnusamy,Harikaran Subbaraj,Jue Wang,Shuaiwen Leon Song,Ben Athiwaratkun*

Main category: cs.CL

TL;DR: CREST是一种无需训练的方法，通过干预特定注意力头来引导LLM推理，提高准确率同时减少计算成本。


<details>
  <summary>Details</summary>
Motivation: LLM在复杂任务中依赖长链思维推理，但这些推理轨迹通常效率低下，存在延迟高、推理不稳定（思考不足或过度思考）的问题。

Method: 提出CREST方法：1）离线校准步骤识别与特定认知行为相关的注意力头并推导头特定的引导向量；2）推理时旋转隐藏表示以抑制这些向量分量。

Result: 在多样化推理基准测试中，CREST将准确率提升高达17.5%，同时减少37.6%的token使用量。

Conclusion: CREST提供了一种简单有效的途径，通过干预认知头来引导LLM推理，实现更快、更可靠的推理性能。

Abstract: Large Language Models (LLMs) often rely on long chain-of-thought (CoT) reasoning to solve complex tasks. While effective, these trajectories are frequently inefficient, leading to high latency from excessive token generation, or unstable reasoning that alternates between underthinking (shallow, inconsistent steps) and overthinking (repetitive, verbose reasoning). In this work, we study the structure of reasoning trajectories and uncover specialized attention heads that correlate with distinct cognitive behaviors such as verification and backtracking. By lightly intervening on these heads at inference time, we can steer the model away from inefficient modes. Building on this insight, we propose CREST, a training-free method for Cognitive REasoning Steering at Test-time. CREST has two components: (1) an offline calibration step that identifies cognitive heads and derives head-specific steering vectors, and (2) an inference-time procedure that rotates hidden representations to suppress components along those vectors. CREST adaptively suppresses unproductive reasoning behaviors, yielding both higher accuracy and lower computational cost. Across diverse reasoning benchmarks and models, CREST improves accuracy by up to 17.5% while reducing token usage by 37.6%, offering a simple and effective pathway to faster, more reliable LLM reasoning.

</details>


### [49] [Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models](https://arxiv.org/abs/2512.24618)
*Junru Lu,Jiarui Qin,Lingfeng Qiao,Yinghui Li,Xinyi Dai,Bo Ke,Jianfeng He,Ruizhi Qiao,Di Yin,Xing Sun,Yunsheng Wu,Yinsong Liu,Shuangyin Liu,Mingkong Tang,Haodong Lin,Jiayi Kuang,Fanxu Meng,Xiaojuan Tang,Yunjia Xi,Junjie Huang,Haotong Yang,Zhenyi Shen,Yangning Li,Qianwen Zhang,Yifei Yu,Siyu An,Junnan Dong,Qiufeng Wang,Jie Wang,Keyu Chen,Wei Wen,Taian Guo,Zhifeng Shen,Daohai Yu,Jiahao Li,Ke Li,Zongyi Li,Xiaoyu Tan*

Main category: cs.CL

TL;DR: Youtu-LLM是一个1.96B参数的语言模型，通过从零预训练而非蒸馏获得推理和规划能力，支持128k上下文，在子20亿参数模型中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有小型模型通常依赖蒸馏技术，缺乏真正的智能代理能力。Youtu-LLM旨在构建一个轻量级但具有原生代理智能的语言模型，能够在有限计算资源下实现强大的推理和规划能力。

Method: 1. 采用密集多潜在注意力架构和STEM导向词汇表，支持128k上下文窗口；2. 设计"常识-STEM-代理"三阶段课程学习策略，使用约11T token的大规模语料进行渐进式训练；3. 针对代理任务进行可扩展的中期训练，通过多样化数据构建方案合成数学、编程和工具使用领域的轨迹数据。

Result: 在通用基准测试中，Youtu-LLM与更大模型竞争性能相当；在代理特定任务中，显著超越现有SOTA基线，证明了轻量级模型可以具备强大的内在代理能力。

Conclusion: Youtu-LLM展示了通过从零预训练和系统化课程学习，轻量级语言模型能够获得真正的代理智能，为资源受限环境下的智能代理应用提供了新可能。

Abstract: We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled "Commonsense-STEM-Agent" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.

</details>


### [50] [Do Large Language Models Know What They Are Capable Of?](https://arxiv.org/abs/2512.24661)
*Casey O. Barkan,Sid Black,Oliver Sourbut*

Main category: cs.CL

TL;DR: 大语言模型普遍过度自信，但在预测任务成功率方面有优于随机的辨别能力；模型越大越新不一定辨别能力更强；在多步任务中过度自信会加剧；部分模型能从失败经验中学习改善决策，但所有模型都因过度乐观估计导致决策不佳。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型能否预测自身在任务中的成功率，以及这种预测能力是否会随着多步任务的推进而改善。同时探索LLMs能否从上下文经验中学习，在失败代价高昂的场景中做出更好的决策选择。

Method: 测试多个LLM的自我预测能力，包括新旧不同规模模型。在多步代理任务中评估模型随着任务进展的预测变化。通过提供失败经验上下文来观察模型是否能学习并改善决策。

Result: 所有测试的LLM都表现出过度自信，但大多数具有优于随机的辨别能力；模型大小和更新程度与辨别能力没有正相关（Claude除外）；在多步任务中，多个前沿LLM的过度自信会加剧；部分LLM能从失败经验中减少过度自信并改善决策，但所有模型都因过度乐观估计导致决策不佳。

Conclusion: 当前LLM代理缺乏对自身能力的准确认知，这阻碍了其决策表现。研究结果对AI滥用和错位风险有重要启示，需要关注LLM的能力意识问题。

Abstract: We investigate whether large language models (LLMs) can predict whether they will succeed on a given task and whether their predictions improve as they progress through multi-step tasks. We also investigate whether LLMs can learn from in-context experiences to make better decisions about whether to pursue a task in scenarios where failure is costly. All LLMs we tested are overconfident, but most predict their success with better-than-random discriminatory power. We find that newer and larger LLMs generally do not have greater discriminatory power, though Claude models do show such a trend. On multi-step agentic tasks, the overconfidence of several frontier LLMs worsens as they progress through the tasks, and reasoning LLMs perform comparably to or worse than non-reasoning LLMs. With in-context experiences of failure, some but not all LLMs reduce their overconfidence leading to significantly improved decision making, while others do not. Interestingly, all LLMs' decisions are approximately rational given their estimated probabilities of success, yet their overly-optimistic estimates result in poor decision making. These results suggest that current LLM agents are hindered by their lack of awareness of their own capabilities. We discuss the implications of LLMs' awareness of their capabilities for AI misuse and misalignment risks.

</details>


### [51] [R-Debater: Retrieval-Augmented Debate Generation through Argumentative Memory](https://arxiv.org/abs/2512.24684)
*Maoyuan Li,Zhongsheng Wang,Haoyuan Li,Jiamou Liu*

Main category: cs.CL

TL;DR: R-Debater是一个基于论据记忆的智能辩论框架，通过检索先前的辩论知识和证据来生成多轮一致且连贯的辩论内容。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在辩论任务中难以保持立场一致性、有效回应对手论点以及提供可靠证据支持。需要结合辩论理论与记忆机制来创建更忠实、连贯的多轮辩论系统。

Method: 基于修辞学和记忆研究，构建包含辩论知识库的检索系统，结合基于角色的智能体框架。知识库存储案例证据和先前辩论动作，智能体负责跨轮次生成连贯话语。

Result: 在ORCHID标准辩论数据集上评估，包含1000项检索语料和32个跨7个领域的保留辩论。在单轮话语生成（InspireScore）和多轮对抗模拟（Debatrix）任务中均优于强LLM基线。人类评估也证实了其一致性和证据使用能力。

Conclusion: 将检索基础与结构化规划相结合，能够产生更忠实、立场一致且跨轮次连贯的辩论。这为构建更可靠的辩论AI系统提供了有效途径。

Abstract: We present R-Debater, an agentic framework for generating multi-turn debates built on argumentative memory. Grounded in rhetoric and memory studies, the system views debate as a process of recalling and adapting prior arguments to maintain stance consistency, respond to opponents, and support claims with evidence. Specifically, R-Debater integrates a debate knowledge base for retrieving case-like evidence and prior debate moves with a role-based agent that composes coherent utterances across turns. We evaluate on standardized ORCHID debates, constructing a 1,000-item retrieval corpus and a held-out set of 32 debates across seven domains. Two tasks are evaluated: next-utterance generation, assessed by InspireScore (subjective, logical, and factual), and adversarial multi-turn simulations, judged by Debatrix (argument, source, language, and overall). Compared with strong LLM baselines, R-Debater achieves higher single-turn and multi-turn scores. Human evaluation with 20 experienced debaters further confirms its consistency and evidence use, showing that combining retrieval grounding with structured planning yields more faithful, stance-aligned, and coherent debates across turns.

</details>


### [52] [MUSIC: MUlti-Step Instruction Contrast for Multi-Turn Reward Models](https://arxiv.org/abs/2512.24693)
*Wenzhe Li,Shujian Zhang,Wenxuan Zhou,John Lambert,Chi Jin,Andrew Hard,Rajiv Mathews,Lun Wang*

Main category: cs.CL

TL;DR: MUSIC是一种无监督数据增强策略，通过合成多轮对比对话对来训练更强大的多轮奖励模型，在保持单轮基准性能的同时，显著提升了多轮对话评估能力。


<details>
  <summary>Details</summary>
Motivation: 多轮对话质量评估对LLM发展至关重要，但现有方法依赖昂贵的人工评估。虽然多轮奖励模型提供了可扩展的替代方案，但现有偏好数据集通常只基于最终轮次对比，无法捕捉多轮交互的细微差别。

Method: 提出了MUSIC（多步指令对比）无监督数据增强策略，通过合成在多轮对话中表现出差异的对比对话对。基于Skywork偏好数据集，使用Gemma-2-9B-Instruct模型训练多轮奖励模型。

Result: 实验结果表明，MUSIC增强的奖励模型在基准方法中表现优异，在多轮对话评估上与高级专有LLM评判结果更一致，且不影响标准单轮奖励模型的基准性能。

Conclusion: 在多轮对话中纳入跨多个轮次的对比对构建稳健的多轮奖励模型至关重要。MUSIC方法有效解决了多轮评估的挑战，为LLM训练提供了更可靠的评估信号。

Abstract: Evaluating the quality of multi-turn conversations is crucial for developing capable Large Language Models (LLMs), yet remains a significant challenge, often requiring costly human evaluation. Multi-turn reward models (RMs) offer a scalable alternative and can provide valuable signals for guiding LLM training. While recent work has advanced multi-turn \textit{training} techniques, effective automated \textit{evaluation} specifically for multi-turn interactions lags behind. We observe that standard preference datasets, typically contrasting responses based only on the final conversational turn, provide insufficient signal to capture the nuances of multi-turn interactions. Instead, we find that incorporating contrasts spanning \textit{multiple} turns is critical for building robust multi-turn RMs. Motivated by this finding, we propose \textbf{MU}lti-\textbf{S}tep \textbf{I}nstruction \textbf{C}ontrast (MUSIC), an unsupervised data augmentation strategy that synthesizes contrastive conversation pairs exhibiting differences across multiple turns. Leveraging MUSIC on the Skywork preference dataset, we train a multi-turn RM based on the Gemma-2-9B-Instruct model. Empirical results demonstrate that our MUSIC-augmented RM outperforms baseline methods, achieving higher alignment with judgments from advanced proprietary LLM judges on multi-turn conversations, crucially, without compromising performance on standard single-turn RM benchmarks.

</details>


### [53] [BIOME-Bench: A Benchmark for Biomolecular Interaction Inference and Multi-Omics Pathway Mechanism Elucidation from Scientific Literature](https://arxiv.org/abs/2512.24733)
*Sibo Wei,Peng Chen,Lifeng Dong,Yin Luo,Lei Wang,Peng Zhang,Wenpeng Lu,Jianbin Guo,Hongjun Yang,Dajun Zeng*

Main category: cs.CL

TL;DR: 研究人员开发了BIOME-Bench基准测试，用于评估大语言模型在多组学分析中的生物分子相互作用推断和通路机制阐明能力，发现现有模型在这些任务上仍存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 多组学研究依赖通路富集分析，但现有方法受限于通路数据库的结构性缺陷（如更新滞后、功能冗余、对分子状态和干预敏感性有限）。虽然已有研究探索使用大语言模型改进通路富集分析，但缺乏标准化的端到端多组学通路机制阐明基准测试，限制了可重复性进展。

Method: 研究人员通过四阶段工作流程构建了BIOME-Bench基准测试，用于评估大语言模型在多组学分析中的两个核心能力：生物分子相互作用推断和端到端多组学通路机制阐明。他们为这两项任务开发了评估协议，并在多个当代强模型上进行了全面实验。

Result: 实验结果表明，现有模型在多组学分析中仍存在显著缺陷：难以可靠地区分细粒度的生物分子关系类型，也无法生成忠实、稳健的通路水平机制解释。

Conclusion: BIOME-Bench基准测试的引入填补了多组学分析中标准化评估的空白，揭示了当前大语言模型在生物分子相互作用推断和通路机制阐明方面的局限性，为未来模型改进提供了重要参考框架。

Abstract: Multi-omics studies often rely on pathway enrichment to interpret heterogeneous molecular changes, but pathway enrichment (PE)-based workflows inherit structural limitations of pathway resources, including curation lag, functional redundancy, and limited sensitivity to molecular states and interventions. Although recent work has explored using large language models (LLMs) to improve PE-based interpretation, the lack of a standardized benchmark for end-to-end multi-omics pathway mechanism elucidation has largely confined evaluation to small, manually curated datasets or ad hoc case studies, hindering reproducible progress. To address this issue, we introduce BIOME-Bench, constructed via a rigorous four-stage workflow, to evaluate two core capabilities of LLMs in multi-omics analysis: Biomolecular Interaction Inference and end-to-end Multi-Omics Pathway Mechanism Elucidation. We develop evaluation protocols for both tasks and conduct comprehensive experiments across multiple strong contemporary models. Experimental results demonstrate that existing models still exhibit substantial deficiencies in multi-omics analysis, struggling to reliably distinguish fine-grained biomolecular relation types and to generate faithful, robust pathway-level mechanistic explanations.

</details>


### [54] [Uncertainty-aware Semi-supervised Ensemble Teacher Framework for Multilingual Depression Detection](https://arxiv.org/abs/2512.24772)
*Mohammad Zia Ur Rehman,Velpuru Navya,Sanskar,Shuja Uddin Qureshi,Nagendra Kumar*

Main category: cs.CL

TL;DR: Semi-SMDNet：一个结合教师-学生伪标注、集成学习和数据增强的半监督多语言抑郁症检测网络，在资源有限的多语言场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 社交媒体抑郁症检测面临语言风格多样、表达非正式、多语言标注数据缺乏等挑战，需要开发能在标注资源有限情况下有效工作的多语言解决方案。

Method: 提出Semi-SMDNet框架，结合教师-学生伪标注、集成学习和数据增强。使用教师模型组通过软投票生成预测，基于不确定性的阈值过滤低置信度伪标签，采用置信度加权训练方法聚焦可靠样本。

Result: 在阿拉伯语、孟加拉语、英语和西班牙语数据集上测试，该方法持续超越强基线模型，显著缩小了资源丰富与资源匮乏设置之间的性能差距。

Conclusion: 该框架在标注资源有限的情况下有效且适用性广，适合用于可扩展的跨语言心理健康监测系统。

Abstract: Detecting depression from social media text is still a challenging task. This is due to different language styles, informal expression, and the lack of annotated data in many languages. To tackle these issues, we propose, Semi-SMDNet, a strong Semi-Supervised Multilingual Depression detection Network. It combines teacher-student pseudo-labelling, ensemble learning, and augmentation of data. Our framework uses a group of teacher models. Their predictions come together through soft voting. An uncertainty-based threshold filters out low-confidence pseudo-labels to reduce noise and improve learning stability. We also use a confidence-weighted training method that focuses on reliable pseudo-labelled samples. This greatly boosts robustness across languages. Tests on Arabic, Bangla, English, and Spanish datasets show that our approach consistently beats strong baselines. It significantly reduces the performance gap between settings that have plenty of resources and those that do not. Detailed experiments and studies confirm that our framework is effective and can be used in various situations. This shows that it is suitable for scalable, cross-language mental health monitoring where labelled resources are limited.

</details>


### [55] [Compute-Accuracy Pareto Frontiers for Open-Source Reasoning Large Language Models](https://arxiv.org/abs/2512.24776)
*Ákos Prucs,Márton Csutora,Mátyás Antal,Márk Marosi*

Main category: cs.CL

TL;DR: 论文通过测试时计算感知评估，发现混合专家架构在平衡性能与效率方面表现优异，并揭示推理计算存在饱和点，超出阈值后准确率增益递减。


<details>
  <summary>Details</summary>
Motivation: 当前文献往往忽视生成长推理序列带来的显著计算负担，而工业应用中的模型选择不仅取决于原始准确率，还受资源约束和推理成本的影响。

Method: 对当代和早期开源大语言模型进行测试时计算感知评估，绘制它们在数学和推理密集型基准测试中的帕累托前沿，并追踪帕累托效率随时间的变化轨迹。

Result: 混合专家架构在评估环境中表现出良好的性能与效率平衡；推理计算存在饱和点，超过特定阈值后准确率增益显著减少；可以推导出单位计算带来的准确率增益涌现趋势。

Conclusion: 虽然扩展推理能力有益，但无法克服模型在特定复杂性方面的内在限制；混合专家架构是平衡性能与效率的有力候选方案；推理计算存在收益递减点，对工业部署具有重要指导意义。

Abstract: Large Language Models (LLMs) are demonstrating rapid improvements on complex reasoning benchmarks, particularly when allowed to utilize intermediate reasoning steps before converging on a final solution. However, current literature often overlooks the significant computational burden associated with generating long reasoning sequences. For industrial applications, model selection depends not only on raw accuracy but also on resource constraints and inference costs. In this work, we conduct a test-time-compute aware evaluation of both contemporary and older open-source LLMs, mapping their Pareto frontiers across math- and reasoning-intensive benchmarks. Our findings identify the Mixture of Experts (MoE) architecture as a strong candidate to balance performance and efficiency in our evaluation setting. Furthermore, we trace the trajectory of Pareto efficiency over time to derive an emergent trend regarding accuracy gain per unit of compute. Finally, we demonstrate that there is a saturation point for inference-time compute. Beyond a certain threshold, accuracy gains diminish, indicating that while extended reasoning capabilities are beneficial, they cannot overcome intrinsic model limitations regarding specific complexities.

</details>


### [56] [Practising responsibility: Ethics in NLP as a hands-on course](https://arxiv.org/abs/2512.24825)
*Malvina Nissim,Viviana Patti,Beatrice Savoldi*

Main category: cs.CL

TL;DR: 论文介绍了NLP伦理教育课程的设计与实施方法，通过主动学习、互动教学等方式培养学生在NLP领域的伦理意识和批判性思维。


<details>
  <summary>Details</summary>
Motivation: 随着NLP系统日益普及，将伦理考量融入NLP教育变得至关重要。然而，课程开发面临两大挑战：该领域从学术界到工业界的快速演变，以及超越传统技术培训培养批判性思维的需求。

Method: 提出了基于主动学习的教学方法，包括互动式课堂、实践操作活动以及"以教促学"的方式。课程经过四年发展，在不同机构、教育层次和跨学科背景下不断优化调整。

Result: 课程产生了许多可重复使用的成果，包括教学材料和学生自主开发的面向不同受众的实际教育产品。课程已成功适应不同机构、教育层次和跨学科背景。

Conclusion: 通过分享教学方法和经验，希望为寻求将社会影响考量融入课程的教育工作者提供启发，促进NLP伦理教育的发展。

Abstract: As Natural Language Processing (NLP) systems become more pervasive, integrating ethical considerations into NLP education has become essential. However, this presents inherent challenges in curriculum development: the field's rapid evolution from both academia and industry, and the need to foster critical thinking beyond traditional technical training. We introduce our course on Ethical Aspects in NLP and our pedagogical approach, grounded in active learning through interactive sessions, hands-on activities, and "learning by teaching" methods. Over four years, the course has been refined and adapted across different institutions, educational levels, and interdisciplinary backgrounds; it has also yielded many reusable products, both in the form of teaching materials and in the form of actual educational products aimed at diverse audiences, made by the students themselves. By sharing our approach and experience, we hope to provide inspiration for educators seeking to incorporate social impact considerations into their curricula.

</details>


### [57] [Triangulation as an Acceptance Rule for Multilingual Mechanistic Interpretability](https://arxiv.org/abs/2512.24842)
*Yanan Long*

Main category: cs.CL

TL;DR: 提出了一种名为"三角化"的因果验证方法，用于评估多语言模型中机制解释的可信度，通过跨语言变体测试电路的必要性、充分性和不变性。


<details>
  <summary>Details</summary>
Motivation: 多语言模型在总体表现上很强，但在不同语言、文字和文化间的行为不可预测。现有的机制解释方法缺乏因果标准，无法确保解释在不同表面形式变化但意义保持的环境中的稳定性。

Method: 1. 形式化"参考家族"作为谓词保持变体；2. 提出"三角化"接受规则，要求同时满足必要性（消融电路会降低目标行为）、充分性（修补激活可以转移行为）和不变性（这些效应在参考家族中保持方向稳定和足够幅度）；3. 采用自动电路发现生成候选子图，通过三角化接受或拒绝这些候选；4. 将三角化建立在因果抽象框架中，作为交换干预分布上的近似变换分数。

Result: 三角化提供了一个可证伪的标准，用于过滤那些通过单环境测试但在跨语言不变性测试中失败的虚假电路。该方法通过跨模型家族、语言对和任务的比较实验协议进行了验证。

Conclusion: 三角化为机制解释提供了一个严格的因果验证框架，确保解释在不同语言变体中的稳定性，推动了实用可解释性议程的发展，并为多语言模型的机制分析提供了可靠标准。

Abstract: Multilingual language models achieve strong aggregate performance yet often behave unpredictably across languages, scripts, and cultures. We argue that mechanistic explanations for such models should satisfy a \emph{causal} standard: claims must survive causal interventions and must \emph{cross-reference} across environments that perturb surface form while preserving meaning. We formalize \emph{reference families} as predicate-preserving variants and introduce \emph{triangulation}, an acceptance rule requiring necessity (ablating the circuit degrades the target behavior), sufficiency (patching activations transfers the behavior), and invariance (both effects remain directionally stable and of sufficient magnitude across the reference family). To supply candidate subgraphs, we adopt automatic circuit discovery and \emph{accept or reject} those candidates by triangulation. We ground triangulation in causal abstraction by casting it as an approximate transformation score over a distribution of interchange interventions, connect it to the pragmatic interpretability agenda, and present a comparative experimental protocol across multiple model families, language pairs, and tasks. Triangulation provides a falsifiable standard for mechanistic claims that filters spurious circuits passing single-environment tests but failing cross-lingual invariance.

</details>


### [58] [PrivacyBench: A Conversational Benchmark for Evaluating Privacy in Personalized AI](https://arxiv.org/abs/2512.24848)
*Srija Mukhopadhyay,Sathwik Reddy,Shruthi Muthukumar,Jisun An,Ponnurangam Kumaraguru*

Main category: cs.CL

TL;DR: PrivacyBench基准测试显示RAG助手在高达26.56%的交互中泄露用户隐私，现有隐私提示仅部分缓解，架构存在单点故障风险。


<details>
  <summary>Details</summary>
Motivation: 个性化AI代理需要访问用户数字足迹，但这会带来隐私风险。缺乏社交情境感知的系统可能无意中暴露用户秘密，威胁数字福祉。

Method: 引入PrivacyBench基准，包含社会情境数据集和嵌入式秘密，采用多轮对话评估来测量秘密保护程度。测试了检索增强生成(RAG)助手。

Result: RAG助手在高达26.56%的交互中泄露秘密。使用隐私意识提示可将泄露率降至5.12%，但这只是部分缓解。检索机制仍无差别访问敏感数据。

Conclusion: 当前架构将隐私保护负担完全放在生成器上，形成单点故障，不适合大规模部署。需要基于设计的结构性隐私保护措施来确保伦理和包容的网络环境。

Abstract: Personalized AI agents rely on access to a user's digital footprint, which often includes sensitive data from private emails, chats and purchase histories. Yet this access creates a fundamental societal and privacy risk: systems lacking social-context awareness can unintentionally expose user secrets, threatening digital well-being. We introduce PrivacyBench, a benchmark with socially grounded datasets containing embedded secrets and a multi-turn conversational evaluation to measure secret preservation. Testing Retrieval-Augmented Generation (RAG) assistants reveals that they leak secrets in up to 26.56% of interactions. A privacy-aware prompt lowers leakage to 5.12%, yet this measure offers only partial mitigation. The retrieval mechanism continues to access sensitive data indiscriminately, which shifts the entire burden of privacy preservation onto the generator. This creates a single point of failure, rendering current architectures unsafe for wide-scale deployment. Our findings underscore the urgent need for structural, privacy-by-design safeguards to ensure an ethical and inclusive web for everyone.

</details>


### [59] [Big AI is accelerating the metacrisis: What can we do?](https://arxiv.org/abs/2512.24863)
*Steven Bird*

Main category: cs.CL

TL;DR: 本文批判当前大AI技术加剧生态、意义和语言危机，呼吁NLP领域转向以人类繁荣和生态健康为中心的价值导向。


<details>
  <summary>Details</summary>
Motivation: 当前世界面临生态危机、意义危机和语言危机的三重交汇（元危机），而大型AI技术正在加速这些危机。语言工程师们固守规模化叙事，为富豪和腐败政权提供关键技术支持，将技术创新视为价值中立，这正在损害人类福祉。

Method: 本文采用批判性分析和呼吁性论述，通过揭示当前NLP领域的问题，提出需要探索替代方案，并呼吁集体智慧参与设计新的发展路径。

Result: 文章指出当前NLP领域的发展模式正在加剧人类面临的元危机，强调需要从根本上转变技术发展方向，从价值中立转向以人类繁荣和生态健康为中心的价值导向。

Conclusion: NLP领域迫切需要探索替代发展路径，应用集体智慧设计以人类在地球上繁荣发展为目标的、生命肯定的未来，建立真正服务于人类福祉的技术体系。

Abstract: The world is in the grip of ecological, meaning, and language crises which are converging into a metacrisis. Big AI is accelerating them all. Language engineers are playing a central role, persisting with a scalability story that is failing humanity, supplying critical talent to plutocrats and kleptocrats, and creating new technologies as if the whole endeavour was value-free. We urgently need to explore alternatives, applying our collective intelligence to design a life-affirming future for NLP that is centered on human flourishing on a living planet.

</details>


### [60] [Encyclo-K: Evaluating LLMs with Dynamically Composed Knowledge Statements](https://arxiv.org/abs/2512.24867)
*Yiming Liang,Yizhi Li,Yantao Du,Ge Zhang,Jiayi Zhou,Yuchen Wu,Yinzhu Piao,Denghui Cao,Tong Sun,Ziniu Li,Li Du,Bo Lei,Jiaheng Liu,Chenghua Lin,Zhaoxiang Zhang,Wenhao Huang,Jiajun Zhang*

Main category: cs.CL

TL;DR: Encyclo-K是一个基于知识陈述的动态基准测试框架，通过从权威教科书中提取独立知识陈述，在测试时随机组合生成评估问题，解决了现有基准测试的数据污染、单知识点评估和专家标注成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准测试存在三个根本性局限：1) 易受数据污染影响，2) 仅限于单知识点评估，3) 依赖昂贵的领域专家标注。需要一种新的基准测试构建方法来克服这些限制。

Method: 将知识陈述而非问题作为基准构建的基本单位。从权威教科书中提取独立的知识陈述，在测试时通过随机抽样动态组合成评估问题。每个问题聚合8-10个陈述进行综合多知识评估。

Result: 在50多个LLM上的实验表明，Encyclo-K具有强区分力和挑战性。最佳模型OpenAI-GPT-5.1仅达到62.07%准确率。推理模型准确率在16.04%-62.07%之间，聊天模型在9.71%-50.40%之间，显示出明显的梯度分布。

Conclusion: Encyclo-K为动态评估LLM对多个细粒度学科知识陈述的综合理解能力提供了一个可扩展的框架，验证了动态评估和多陈述综合理解带来的挑战。

Abstract: Benchmarks play a crucial role in tracking the rapid advancement of large language models (LLMs) and identifying their capability boundaries. However, existing benchmarks predominantly curate questions at the question level, suffering from three fundamental limitations: vulnerability to data contamination, restriction to single-knowledge-point assessment, and reliance on costly domain expert annotation. We propose Encyclo-K, a statement-based benchmark that rethinks benchmark construction from the ground up. Our key insight is that knowledge statements, not questions, can serve as the unit of curation, and questions can then be constructed from them. We extract standalone knowledge statements from authoritative textbooks and dynamically compose them into evaluation questions through random sampling at test time. This design directly addresses all three limitations: the combinatorial space is too vast to memorize, and model rankings remain stable across dynamically generated question sets, enabling reliable periodic dataset refresh; each question aggregates 8-10 statements for comprehensive multi-knowledge assessment; annotators only verify formatting compliance without requiring domain expertise, substantially reducing annotation costs. Experiments on over 50 LLMs demonstrate that Encyclo-K poses substantial challenges with strong discriminative power. Even the top-performing OpenAI-GPT-5.1 achieves only 62.07% accuracy, and model performance displays a clear gradient distribution--reasoning models span from 16.04% to 62.07%, while chat models range from 9.71% to 50.40%. These results validate the challenges introduced by dynamic evaluation and multi-statement comprehensive understanding. These findings establish Encyclo-K as a scalable framework for dynamic evaluation of LLMs' comprehensive understanding over multiple fine-grained disciplinary knowledge statements.

</details>


### [61] [mHC: Manifold-Constrained Hyper-Connections](https://arxiv.org/abs/2512.24880)
*Zhenda Xie,Yixuan Wei,Huanqi Cao,Chenggang Zhao,Chengqi Deng,Jiashi Li,Damai Dai,Huazuo Gao,Jiang Chang,Liang Zhao,Shangyan Zhou,Zhean Xu,Zhengyan Zhang,Wangding Zeng,Shengding Hu,Yuqing Wang,Jingyang Yuan,Lean Wang,Wenfeng Liang*

Main category: cs.CL

TL;DR: 提出Manifold-Constrained Hyper-Connections (mHC)框架，通过将超连接的残差空间投影到特定流形来恢复恒等映射特性，同时优化基础设施以确保效率，解决了超连接训练不稳定和可扩展性受限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有超连接（HC）方法通过扩展残差流宽度和多样化连接模式获得了性能提升，但破坏了残差连接固有的恒等映射特性，导致严重的训练不稳定性、受限的可扩展性以及显著的内存访问开销。

Method: 提出mHC框架，将超连接的残差连接空间投影到特定流形上以恢复恒等映射特性，同时结合严格的基础设施优化来确保效率。

Result: 实证实验表明mHC在大规模训练中有效，提供了实际的性能改进和优越的可扩展性。

Conclusion: mHC作为超连接的灵活实用扩展，将有助于更深入理解拓扑架构设计，并为基础模型的演进提供有前景的方向。

Abstract: Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.

</details>


### [62] [BEDA: Belief Estimation as Probabilistic Constraints for Performing Strategic Dialogue Acts](https://arxiv.org/abs/2512.24885)
*Hengli Li,Zhaoxin Yu,Qi Shen,Chenxi Li,Mengmeng Wang,Tinglang Wu,Yipeng Kang,Yuxuan Wang,Song-Chun Zhu,Zixia Jia,Zilong Zheng*

Main category: cs.CL

TL;DR: BEDA框架通过将信念估计转化为对话生成的概率约束，在战略对话中显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有方法在战略对话中能准确估计信念，但缺乏使用这些信念进行生成的机制。需要桥接信念估计与对话生成之间的鸿沟。

Method: 1. 形式化两种核心对话行为：对抗性(Adversarial)和对齐性(Alignment)
2. 通过概率约束操作化这些行为，限制代理可能生成的内容
3. 提出BEDA框架：包含世界集合、信念估计器和条件生成器
4. 条件生成器根据推断的信念选择行为并生成一致的话语

Result: 在三个设置中均显著优于基线：
- CKBG（对抗性）：所有骨干模型上成功率至少提升5.0点，GPT-4.1-nano上提升20.6点
- Mutual Friends（合作性）：平均提升9.3点
- CaSiNo（谈判）：达到相对于所有基线的最优交易

Conclusion: 将信念估计转化为约束为战略对话提供了简单、通用的可靠机制，证明了BEDA框架的有效性和泛化能力。

Abstract: Strategic dialogue requires agents to execute distinct dialogue acts, for which belief estimation is essential. While prior work often estimates beliefs accurately, it lacks a principled mechanism to use those beliefs during generation. We bridge this gap by first formalizing two core acts Adversarial and Alignment, and by operationalizing them via probabilistic constraints on what an agent may generate. We instantiate this idea in BEDA, a framework that consists of the world set, the belief estimator for belief estimation, and the conditional generator that selects acts and realizes utterances consistent with the inferred beliefs. Across three settings, Conditional Keeper Burglar (CKBG, adversarial), Mutual Friends (MF, cooperative), and CaSiNo (negotiation), BEDA consistently outperforms strong baselines: on CKBG it improves success rate by at least 5.0 points across backbones and by 20.6 points with GPT-4.1-nano; on Mutual Friends it achieves an average improvement of 9.3 points; and on CaSiNo it achieves the optimal deal relative to all baselines. These results indicate that casting belief estimation as constraints provides a simple, general mechanism for reliable strategic dialogue.

</details>


### [63] [Adaptive Dependency-aware Prompt Optimization Framework for Multi-Step LLM Pipeline](https://arxiv.org/abs/2512.24933)
*Minjun Zhao,Xinyu Zhang,Shuai Zhang,Deyang Li,Ruifeng Shi*

Main category: cs.CL

TL;DR: ADOPT框架通过显式建模LLM多步骤管道中各步骤与最终任务结果的依赖关系，实现类似计算解析导数的文本梯度估计，有效优化多步骤提示词。


<details>
  <summary>Details</summary>
Motivation: 多步骤LLM管道的性能严重依赖于每个步骤使用的提示词，但由于缺乏步骤级监督和步骤间依赖关系，联合优化这些提示词非常困难。现有的端到端提示优化方法在这些条件下表现不佳，常常产生次优或不稳定的更新。

Method: ADOPT框架包含三个核心组件：1）显式建模每个LLM步骤与最终任务结果的依赖关系，实现精确的文本梯度估计；2）将文本梯度估计与梯度更新解耦，将多提示优化简化为灵活的单提示优化步骤；3）使用基于Shapley值的机制自适应分配优化资源。

Result: 在真实世界数据集和多样化管道结构上的实验表明，ADOPT是有效且鲁棒的，一致优于最先进的提示优化基线方法。

Conclusion: ADOPT为多步骤LLM管道提供了一种自适应、依赖感知的提示优化框架，通过精确的梯度估计和资源分配机制，解决了现有方法在复杂多步骤场景下的优化挑战。

Abstract: Multi-step LLM pipelines invoke large language models multiple times in a structured sequence and can effectively solve complex tasks, but their performance heavily depends on the prompts used at each step. Jointly optimizing these prompts is difficult due to missing step-level supervision and inter-step dependencies. Existing end-to-end prompt optimization methods struggle under these conditions and often yield suboptimal or unstable updates. We propose ADOPT, an Adaptive Dependency-aware Prompt Optimization framework for multi-step LLM pipelines. ADOPT explicitly models the dependency between each LLM step and the final task outcome, enabling precise text-gradient estimation analogous to computing analytical derivatives. It decouples textual gradient estimation from gradient updates, reducing multi-prompt optimization to flexible single-prompt optimization steps, and employs a Shapley-based mechanism to adaptively allocate optimization resources. Experiments on real-world datasets and diverse pipeline structures show that ADOPT is effective and robust, consistently outperforming state-of-the-art prompt optimization baselines.

</details>


### [64] [Classifying long legal documents using short random chunks](https://arxiv.org/abs/2512.24997)
*Luis Adrián Cabrera-Diego*

Main category: cs.CL

TL;DR: 基于DeBERTa V3和LSTM的法律文档分类器，使用随机选择的48个短文本块（最多128个词元）作为输入，并通过Temporal部署了可靠的流程管道。


<details>
  <summary>Details</summary>
Motivation: 法律文档分类面临两个主要挑战：一是专业词汇，二是文档可能非常长。将完整文档输入基于Transformer的模型进行分类可能不可行、昂贵或缓慢。

Method: 提出基于DeBERTa V3和LSTM的法律文档分类器，输入是从文档中随机选择的48个短文本块（每个最多128个词元）。此外，使用Temporal（一个持久执行解决方案）部署了处理管道。

Result: 最佳模型的加权F分数为0.898。在CPU上运行的管道处理100个文件的平均时间为498秒。

Conclusion: 该方法有效解决了长法律文档分类的挑战，通过使用短文本块作为输入，结合DeBERTa V3和LSTM模型，并利用Temporal实现了可靠的处理流程。

Abstract: Classifying legal documents is a challenge, besides their specialized vocabulary, sometimes they can be very long. This means that feeding full documents to a Transformers-based models for classification might be impossible, expensive or slow. Thus, we present a legal document classifier based on DeBERTa V3 and a LSTM, that uses as input a collection of 48 randomly-selected short chunks (max 128 tokens). Besides, we present its deployment pipeline using Temporal, a durable execution solution, which allow us to have a reliable and robust processing workflow. The best model had a weighted F-score of 0.898, while the pipeline running on CPU had a processing median time of 498 seconds per 100 files.

</details>


### [65] [MAMA-Memeia! Multi-Aspect Multi-Agent Collaboration for Depressive Symptoms Identification in Memes](https://arxiv.org/abs/2512.25015)
*Siddhant Agarwal,Adya Dhuler,Polly Ruhnke,Melvin Speisman,Md Shad Akhtar,Shweta Yadav*

Main category: cs.CL

TL;DR: 论文提出了RESTOREx数据集和MAMAMemeia框架，用于检测社交媒体表情包中的抑郁症状，基于LLM生成和人工标注的解释，在CAT临床心理学方法基础上构建多智能体多维度讨论框架，性能提升7.55%成为新基准。


<details>
  <summary>Details</summary>
Motivation: 表情包已从单纯的幽默表达演变为用户自由表达各种情感（包括抑郁情绪）的媒介。随着表情包在表达抑郁情绪中的使用日益增长，需要研究识别社交媒体平台上用户分享的表情包中表现的抑郁症状。

Method: 1. 引入RESTOREx数据集：通过大语言模型生成和人工标注的解释来检测社交媒体表情包中的抑郁症状；2. 提出MAMAMemeia框架：基于临床心理学中的认知分析疗法（CAT）能力，构建协作式多智能体多维度讨论框架。

Result: MAMAMemeia框架相比现有30多种方法，在macro-F1指标上提升了7.55%，被确立为新的基准方法。

Conclusion: 该研究为检测社交媒体表情包中的抑郁症状提供了有效的资源和框架，通过结合LLM生成解释和多智能体讨论机制，显著提升了检测性能，为心理健康监测提供了新工具。

Abstract: Over the past years, memes have evolved from being exclusively a medium of humorous exchanges to one that allows users to express a range of emotions freely and easily. With the ever-growing utilization of memes in expressing depressive sentiments, we conduct a study on identifying depressive symptoms exhibited by memes shared by users of online social media platforms. We introduce RESTOREx as a vital resource for detecting depressive symptoms in memes on social media through the Large Language Model (LLM) generated and human-annotated explanations. We introduce MAMAMemeia, a collaborative multi-agent multi-aspect discussion framework grounded in the clinical psychology method of Cognitive Analytic Therapy (CAT) Competencies. MAMAMemeia improves upon the current state-of-the-art by 7.55% in macro-F1 and is established as the new benchmark compared to over 30 methods.

</details>


### [66] [Modeling Language as a Sequence of Thoughts](https://arxiv.org/abs/2512.25026)
*Nasim Borazjanizadeh,James McClelland*

Main category: cs.CL

TL;DR: 本文提出了Thought Gestalt模型，这是一个在token和句子层面双重建模的循环Transformer，通过记忆前文句子表示来提升语言生成的一致性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer语言模型主要依赖表层共现统计，缺乏对实体和事件的全局一致潜在表示，导致关系方向脆弱（如反转诅咒）、上下文错误和数据效率低下。而人类理解语言时会将其转换为紧凑的事件表示并持久记忆。

Method: 提出Thought Gestalt模型：一个循环Transformer，在两个抽象层次建模语言——token和句子级"思想"状态。模型每次生成一个句子的token，同时交叉关注先前句子表示的记忆。token和句子表示使用同一组参数生成，通过单一的下一个token交叉熵目标训练。

Result: 在扩展实验中，TG相比匹配的GPT-2基线持续提升效率，缩放拟合表明GPT-2需要多5-8%的数据和33-42%的参数才能达到TG的损失水平。TG还在父子反转诅咒探针上减少了关系方向泛化错误。

Conclusion: Thought Gestalt模型通过结合token级生成和句子级记忆表示，在保持Transformer架构优势的同时，解决了现有模型在全局一致性、关系推理和数据效率方面的局限性，为语言建模提供了更接近人类认知的处理方式。

Abstract: Transformer language models can generate strikingly natural text by modeling language as a sequence of tokens. Yet, by relying primarily on surface-level co-occurrence statistics, they fail to form globally consistent latent representations of entities and events, lack of which contributes to brittleness in relational direction (e.g., reversal curse), contextualization errors, and data inefficiency. On the other hand, cognitive science shows that human comprehension involves converting the input linguistic stream into compact, event-like representations that persist in memory while verbatim form is short-lived. Motivated by this view, we introduce Thought Gestalt (TG) model, a recurrent Transformer that models language at two levels of abstraction - tokens and sentence-level "thought" states. TG generates the tokens of one sentence at a time while cross-attending to a memory of prior sentence representations. In TG, token and sentence representations are generated using the same set of model parameters and trained with a single objective, the next-token cross-entropy: by retaining the computation graph of sentence representations written to memory, gradients from future token losses flow backward through cross-attention to optimize the parameters generating earlier sentence vectors. In scaling experiments, TG consistently improves efficiency over matched GPT-2 runs, among other baselines, with scaling fits indicating GPT-2 requires ~5-8% more data and ~33-42% more parameters to match TG's loss. TG also reduces errors on relational direction generalization on a father-son reversal curse probe.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [67] [An Comparative Analysis about KYC on a Recommendation System Toward Agentic Recommendation System](https://arxiv.org/abs/2512.23961)
*Junjie H. Xu*

Main category: cs.IR

TL;DR: 本研究提出了一种基于智能体AI的KYC推荐系统，并在五个内容垂直领域（广告、新闻、八卦、用户生成内容、技术）进行评估，通过nDCG指标与行业基准对比分析系统性能。


<details>
  <summary>Details</summary>
Motivation: 金融领域KYC（了解你的客户）流程需要更智能的推荐系统来提升用户体验和业务效率，现有系统在个性化推荐方面存在不足，需要结合智能体AI技术进行改进。

Method: 1. 开发基于智能体AI的KYC推荐系统；2. 在五个内容垂直领域（广告、新闻、八卦、用户生成内容、技术）进行评估；3. 按KYC使用强度分为四个实验组；4. 使用nDCG@k（k=1,3,5）作为评估指标；5. 结合百度、小红书等平台的行业基准进行对比分析。

Result: 通过实验数据与理论框架、行业基准的综合分析，展示了大规模智能体推荐系统的工程化实验结果，为金融领域KYC推荐系统的优化提供了实证依据。

Conclusion: 智能体AI技术在KYC推荐系统中具有显著应用价值，能够提升推荐准确性和用户体验，该研究为金融科技领域的推荐系统开发提供了可行的技术路径和实践参考。

Abstract: This research presents a cutting-edge recommendation system utilizing agentic AI for KYC (Know Your Customer in the financial domain), and its evaluation across five distinct content verticals: Advertising (Ad), News, Gossip, Sharing (User-Generated Content), and Technology (Tech). The study compares the performance of four experimental groups, grouping by the intense usage of KYC, benchmarking them against the Normalized Discounted Cumulative Gain (nDCG) metric at truncation levels of $k=1$, $k=3$, and $k=5$. By synthesizing experimental data with theoretical frameworks and industry benchmarks from platforms such as Baidu and Xiaohongshu, this research provides insight by showing experimental results for engineering a large-scale agentic recommendation system.

</details>


### [68] [Time-Aware Adaptive Side Information Fusion for Sequential Recommendation](https://arxiv.org/abs/2512.24246)
*Jie Luo,Wenyu Zhang,Xinming Zhang,Yuan Fang*

Main category: cs.IR

TL;DR: TASIF框架通过时间感知、自适应频率过滤和高效属性融合三个组件，系统解决了时序推荐中忽略时间动态、对噪声敏感和计算效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于商品侧信息（如类别、品牌）的序列推荐模型存在三个关键挑战：忽视时间戳的细粒度时间动态、对用户交互序列噪声敏感、依赖计算昂贵的融合架构。

Method: 提出TASIF框架，包含三个协同组件：1）简单即插即用的时间跨度划分机制捕捉全局时间模式；2）自适应频率过滤器使用可学习门控自适应去噪特征序列；3）高效自适应侧信息融合层采用"引导而非混合"架构，让属性引导注意力机制但不混入商品嵌入。

Result: 在四个公共数据集上的大量实验表明，TASIF显著优于最先进的基线方法，同时在训练中保持优异的效率。

Conclusion: TASIF框架通过系统解决时间动态建模、噪声鲁棒性和计算效率三个关键挑战，为时序推荐中的商品侧信息融合提供了有效的解决方案。

Abstract: Incorporating item-side information, such as category and brand, into sequential recommendation is a well-established and effective approach for improving performance. However, despite significant advancements, current models are generally limited by three key challenges: they often overlook the fine-grained temporal dynamics inherent in timestamps, exhibit vulnerability to noise in user interaction sequences, and rely on computationally expensive fusion architectures. To systematically address these challenges, we propose the Time-Aware Adaptive Side Information Fusion (TASIF) framework. TASIF integrates three synergistic components: (1) a simple, plug-and-play time span partitioning mechanism to capture global temporal patterns; (2) an adaptive frequency filter that leverages a learnable gate to denoise feature sequences adaptively, thereby providing higher-quality inputs for subsequent fusion modules; and (3) an efficient adaptive side information fusion layer, this layer employs a "guide-not-mix" architecture, where attributes guide the attention mechanism without being mixed into the content-representing item embeddings, ensuring deep interaction while ensuring computational efficiency. Extensive experiments on four public datasets demonstrate that TASIF significantly outperforms state-of-the-art baselines while maintaining excellent efficiency in training. Our source code is available at https://github.com/jluo00/TASIF.

</details>


### [69] [RAGPart & RAGMask: Retrieval-Stage Defenses Against Corpus Poisoning in Retrieval-Augmented Generation](https://arxiv.org/abs/2512.24268)
*Pankayaraj Pathmanathan,Michael-Andrei Panaitescu-Liess,Cho-Yu Jason Chiang,Furong Huang*

Main category: cs.IR

TL;DR: 提出RAGPart和RAGMask两种检索阶段防御方法，对抗RAG系统中的语料库中毒攻击，保持良性条件下效用同时显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: RAG系统虽然能增强大语言模型的外部知识并减少幻觉，但存在语料库中毒漏洞，攻击者可通过注入恶意文档操纵模型输出，需要有效防御机制。

Method: 提出两种互补的检索阶段防御：1) RAGPart利用密集检索器训练动态，通过文档分区缓解中毒点影响；2) RAGMask基于目标token掩码下的显著相似性偏移识别可疑token。两种方法直接在检索器上操作，计算轻量且无需修改生成模型。

Result: 在两个基准测试、四种中毒策略和四种最先进检索器上，防御方法能一致降低攻击成功率，同时在良性条件下保持效用。还引入了可解释攻击来压力测试防御。

Conclusion: 研究展示了检索阶段防御的潜力和局限性，为稳健的RAG部署提供了实用见解，证明在检索阶段防御语料库中毒攻击是可行且有效的。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising paradigm to enhance large language models (LLMs) with external knowledge, reducing hallucinations and compensating for outdated information. However, recent studies have exposed a critical vulnerability in RAG pipelines corpus poisoning where adversaries inject malicious documents into the retrieval corpus to manipulate model outputs. In this work, we propose two complementary retrieval-stage defenses: RAGPart and RAGMask. Our defenses operate directly on the retriever, making them computationally lightweight and requiring no modification to the generation model. RAGPart leverages the inherent training dynamics of dense retrievers, exploiting document partitioning to mitigate the effect of poisoned points. In contrast, RAGMask identifies suspicious tokens based on significant similarity shifts under targeted token masking. Across two benchmarks, four poisoning strategies, and four state-of-the-art retrievers, our defenses consistently reduce attack success rates while preserving utility under benign conditions. We further introduce an interpretable attack to stress-test our defenses. Our findings highlight the potential and limitations of retrieval-stage defenses, providing practical insights for robust RAG deployments.

</details>


### [70] [MaRCA: Multi-Agent Reinforcement Learning for Dynamic Computation Allocation in Large-Scale Recommender Systems](https://arxiv.org/abs/2512.24325)
*Wan Jiang,Xinyi Zang,Yudong Zhao,Yusi Zou,Yunfei Lu,Junbo Tong,Yang Liu,Ming Li,Jiani Shi,Xin Yang*

Main category: cs.IR

TL;DR: MaRCA是一个用于大规模推荐系统端到端计算资源分配的多智能体强化学习框架，通过协同优化各阶段资源分配，在现有计算资源下实现16.67%的收入提升。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统面临模型复杂性和流量规模增长带来的计算挑战，现有方法通常简化多阶段计算资源分配，忽略阶段间依赖关系，限制了全局最优性。

Method: 提出MaRCA框架，将推荐系统各阶段建模为协同智能体，采用集中训练分散执行（CTDE）策略，引入AutoBucket TestBench进行精确计算成本估计，以及基于模型预测控制（MPC）的收入成本平衡器来预测流量负载并调整收入成本权衡。

Result: 自2024年11月在领先全球电商平台的广告管道端到端部署以来，MaRCA每天稳定处理数千亿广告请求，在使用现有计算资源的情况下实现了16.67%的收入提升。

Conclusion: MaRCA通过多智能体强化学习框架有效解决了大规模推荐系统中的计算资源分配问题，实现了全局最优的资源分配和显著的商业价值提升。

Abstract: Modern recommender systems face significant computational challenges due to growing model complexity and traffic scale, making efficient computation allocation critical for maximizing business revenue. Existing approaches typically simplify multi-stage computation resource allocation, neglecting inter-stage dependencies, thus limiting global optimality. In this paper, we propose MaRCA, a multi-agent reinforcement learning framework for end-to-end computation resource allocation in large-scale recommender systems. MaRCA models the stages of a recommender system as cooperative agents, using Centralized Training with Decentralized Execution (CTDE) to optimize revenue under computation resource constraints. We introduce an AutoBucket TestBench for accurate computation cost estimation, and a Model Predictive Control (MPC)-based Revenue-Cost Balancer to proactively forecast traffic loads and adjust the revenue-cost trade-off accordingly. Since its end-to-end deployment in the advertising pipeline of a leading global e-commerce platform in November 2024, MaRCA has consistently handled hundreds of billions of ad requests per day and has delivered a 16.67% revenue uplift using existing computation resources.

</details>


### [71] [On the Factual Consistency of Text-based Explainable Recommendation Models](https://arxiv.org/abs/2512.24366)
*Ben Kabongo,Vincent Guigue*

Main category: cs.IR

TL;DR: 本文提出一个评估文本解释推荐系统事实一致性的框架，发现现有模型虽然语义相似度高，但事实一致性表现很差。


<details>
  <summary>Details</summary>
Motivation: 尽管基于LLM的文本解释推荐系统能生成流畅的解释，但这些解释是否与可用证据事实一致的问题尚未充分探索，需要建立评估框架来检测事实一致性。

Method: 设计基于提示的流水线，使用LLM从评论中提取原子解释语句构建事实基础；在亚马逊评论数据集上创建增强基准；提出结合LLM和NLI的语句级对齐指标来评估事实一致性和相关性。

Result: 对6个最先进的解释推荐模型进行实验发现：虽然模型在语义相似度上得分很高（BERTScore F1: 0.81-0.90），但所有事实性指标都显示性能极低（LLM-based语句级精度：4.38%-32.88%）。

Conclusion: 当前解释推荐系统存在严重的事实一致性问题，需要事实感知的评估方法，这为开发更可信的解释系统提供了基础。

Abstract: Text-based explainable recommendation aims to generate natural-language explanations that justify item recommendations, to improve user trust and system transparency. Although recent advances leverage LLMs to produce fluent outputs, a critical question remains underexplored: are these explanations factually consistent with the available evidence? We introduce a comprehensive framework for evaluating the factual consistency of text-based explainable recommenders. We design a prompting-based pipeline that uses LLMs to extract atomic explanatory statements from reviews, thereby constructing a ground truth that isolates and focuses on their factual content. Applying this pipeline to five categories from the Amazon Reviews dataset, we create augmented benchmarks for fine-grained evaluation of explanation quality. We further propose statement-level alignment metrics that combine LLM- and NLI-based approaches to assess both factual consistency and relevance of generated explanations. Across extensive experiments on six state-of-the-art explainable recommendation models, we uncover a critical gap: while models achieve high semantic similarity scores (BERTScore F1: 0.81-0.90), all our factuality metrics reveal alarmingly low performance (LLM-based statement-level precision: 4.38%-32.88%). These findings underscore the need for factuality-aware evaluation in explainable recommendation and provide a foundation for developing more trustworthy explanation systems.

</details>


### [72] [MEIC-DT: Memory-Efficient Incremental Clustering for Long-Text Coreference Resolution with Dual-Threshold Constraints](https://arxiv.org/abs/2512.24711)
*Kangyang Luo,Shuzheng Si,Yuzhuo Bai,Cheng Gao,Zhitong Wang,Cheng Huang,Yingli Shen,Yufeng Han,Wenhao Li,Cunliang Kong,Maosong Sun*

Main category: cs.IR

TL;DR: MEIC-DT是一种基于轻量级Transformer的双阈值内存高效增量聚类方法，用于共指消解任务，在严格内存约束下保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 尽管监督神经方法在LLM时代仍是共指消解的SOTA，但其在增量聚类方面的潜力尚未充分探索。增量聚类面临平衡长文本处理效率与性能的关键挑战，现有方法在内存使用上存在局限。

Method: 提出了MEIC-DT方法，包括：1）双阈值约束机制，精确控制Transformer输入规模在预定义内存预算内；2）统计感知驱逐策略（SAES），利用训练和推理阶段的统计特征进行智能缓存管理；3）内部正则化策略（IRP），通过选择最具代表性的提及来战略性地压缩聚类，保持语义完整性。

Result: 在常见基准测试上的广泛实验表明，MEIC-DT在严格内存约束下实现了高度竞争力的共指消解性能。

Conclusion: MEIC-DT成功解决了增量聚类中内存效率与性能平衡的挑战，为长文本共指消解提供了一种有效的解决方案，证明了在有限资源下仍能保持SOTA水平。

Abstract: In the era of large language models (LLMs), supervised neural methods remain the state-of-the-art (SOTA) for Coreference Resolution. Yet, their full potential is underexplored, particularly in incremental clustering, which faces the critical challenge of balancing efficiency with performance for long texts. To address the limitation, we propose \textbf{MEIC-DT}, a novel dual-threshold, memory-efficient incremental clustering approach based on a lightweight Transformer. MEIC-DT features a dual-threshold constraint mechanism designed to precisely control the Transformer's input scale within a predefined memory budget. This mechanism incorporates a Statistics-Aware Eviction Strategy (\textbf{SAES}), which utilizes distinct statistical profiles from the training and inference phases for intelligent cache management. Furthermore, we introduce an Internal Regularization Policy (\textbf{IRP}) that strategically condenses clusters by selecting the most representative mentions, thereby preserving semantic integrity. Extensive experiments on common benchmarks demonstrate that MEIC-DT achieves highly competitive coreference performance under stringent memory constraints.

</details>


### [73] [MDiffFR: Modality-Guided Diffusion Generation for Cold-start Items in Federated Recommendation](https://arxiv.org/abs/2512.24715)
*Kang Fu,Honglei Zhang,Xuechao Zou,Yidong Li*

Main category: cs.IR

TL;DR: 提出MDiffFR方法，利用模态引导的扩散模型为联邦推荐中的冷启动项目生成嵌入表示，解决了传统属性到嵌入映射方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 联邦推荐系统在保护用户隐私的同时，由于严格的隐私约束限制了跨客户端的数据访问，使得学习冷启动项目的全局有效表示变得困难。现有的属性到嵌入映射方法存在一对一的固定对应关系，无法适应不同的数据分布，容易导致嵌入错位。

Method: 提出MDiffFR方法：1）在服务器端使用定制的扩散模型生成冷启动项目的嵌入表示；2）利用预训练的模态编码器提取模态特征作为条件信号，指导反向去噪过程以对齐项目语义；3）将生成的嵌入分发给客户端进行冷启动推理。

Result: 在四个真实数据集上的广泛实验表明，该方法在联邦推荐场景中始终优于所有基线方法。理论分析验证了该方法相比现有基于映射的方法具有更强的隐私保证。

Conclusion: MDiffFR通过扩散模型生成冷启动项目嵌入，解决了联邦推荐中项目冷启动的挑战，提供了更好的性能表现和更强的隐私保护。

Abstract: Federated recommendations (FRs) provide personalized services while preserving user privacy by keeping user data on local clients, which has attracted significant attention in recent years. However, due to the strict privacy constraints inherent in FRs, access to user-item interaction data and user profiles across clients is highly restricted, making it difficult to learn globally effective representations for new (cold-start) items. Consequently, the item cold-start problem becomes even more challenging in FRs. Existing solutions typically predict embeddings for new items through the attribute-to-embedding mapping paradigm, which establishes a fixed one-to-one correspondence between item attributes and their embeddings. However, this one-to-one mapping paradigm often fails to model varying data distributions and tends to cause embedding misalignment, as verified by our empirical studies. To this end, we propose MDiffFR, a novel generation-based modality-guided diffusion method for cold-start items in FRs. In this framework, we employ a tailored diffusion model on the server to generate embeddings for new items, which are then distributed to clients for cold-start inference. To align item semantics, we deploy a pre-trained modality encoder to extract modality features as conditional signals to guide the reverse denoising process. Furthermore, our theoretical analysis verifies that the proposed method achieves stronger privacy guarantees compared to existing mapping-based approaches. Extensive experiments on four real datasets demonstrate that our method consistently outperforms all baselines in FRs.

</details>


### [74] [OpenOneRec Technical Report](https://arxiv.org/abs/2512.24762)
*Guorui Zhou,Honghui Bao,Jiaming Huang,Jiaxin Deng,Jinghao Zhang,Junda She,Kuo Cai,Lejian Ren,Lu Ren,Qiang Luo,Qianqian Wang,Qigen Hu,Rongzhou Zhang,Ruiming Tang,Shiyao Wang,Wuchao Li,Xiangyu Wu,Xinchen Luo,Xingmei Wang,Yifei Hu,Yunfan Wu,Zhanyu Liu,Zhiyang Zhang,Zixing Zhang,Bo Chen,Bin Wen,Chaoyi Ma,Chengru Song,Chenglong Chu,Defu Lian,Fan Yang,Feng Jiang,Hongtao Cheng,Huanjie Wang,Kun Gai,Pengfei Zheng,Qiang Wang,Rui Huang,Siyang Mao,Tingting Gao,Wei Yuan,Yan Wang,Yang Zhou,Yi Su,Zexuan Cheng,Zhixin Ling,Ziming Li*

Main category: cs.IR

TL;DR: 该论文提出RecIF-Bench基准测试、大规模训练数据集和OneRec-Foundation模型系列，旨在弥合推荐系统与通用智能之间的差距，在多个任务上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有OneRec系列虽然将推荐流程统一为端到端生成框架，但仍存在显著局限：受限于孤立数据，成为领域专家但缺乏世界知识、推理能力和指令跟随能力；同时缺乏评估这些综合能力的全面基准。

Method: 1) 提出RecIF-Bench基准，涵盖8个多样化任务以全面评估从基础预测到复杂推理的能力；2) 发布包含9600万交互的大规模训练数据集；3) 开源完整训练流水线（数据处理、协同预训练、后训练）；4) 开发OneRec-Foundation模型系列（1.7B和8B参数）。

Result: OneRec-Foundation模型在RecIF-Bench所有任务上取得新的SOTA结果；在Amazon基准测试中，相比最强基线在10个数据集上Recall@10平均提升26.8%；展示了推荐能力可扩展性同时缓解通用知识的灾难性遗忘。

Conclusion: 这项工作向构建真正智能推荐系统迈出了一步，但实现这一愿景仍面临显著的技术和理论挑战，需要更广泛的研究参与。

Abstract: While the OneRec series has successfully unified the fragmented recommendation pipeline into an end-to-end generative framework, a significant gap remains between recommendation systems and general intelligence. Constrained by isolated data, they operate as domain specialists-proficient in pattern matching but lacking world knowledge, reasoning capabilities, and instruction following. This limitation is further compounded by the lack of a holistic benchmark to evaluate such integrated capabilities. To address this, our contributions are: 1) RecIF Bench & Open Data: We propose RecIF-Bench, a holistic benchmark covering 8 diverse tasks that thoroughly evaluate capabilities from fundamental prediction to complex reasoning. Concurrently, we release a massive training dataset comprising 96 million interactions from 160,000 users to facilitate reproducible research. 2) Framework & Scaling: To ensure full reproducibility, we open-source our comprehensive training pipeline, encompassing data processing, co-pretraining, and post-training. Leveraging this framework, we demonstrate that recommendation capabilities can scale predictably while mitigating catastrophic forgetting of general knowledge. 3) OneRec-Foundation: We release OneRec Foundation (1.7B and 8B), a family of models establishing new state-of-the-art (SOTA) results across all tasks in RecIF-Bench. Furthermore, when transferred to the Amazon benchmark, our models surpass the strongest baselines with an average 26.8% improvement in Recall@10 across 10 diverse datasets (Figure 1). This work marks a step towards building truly intelligent recommender systems. Nonetheless, realizing this vision presents significant technical and theoretical challenges, highlighting the need for broader research engagement in this promising direction.

</details>


### [75] [HiGR: Efficient Generative Slate Recommendation via Hierarchical Planning and Multi-Objective Preference Alignment](https://arxiv.org/abs/2512.24787)
*Yunsheng Pang,Zijian Liu,Yudong Li,Shaojie Zhu,Zijian Luo,Chenyun Yu,Sikai Wu,Shichen Shen,Cong Xu,Bin Wang,Kai Jiang,Hongyong Yu,Chengxiang Zhuo,Zang Li*

Main category: cs.IR

TL;DR: HiGR提出了一种高效生成式列表推荐框架，通过分层规划和列表级偏好对齐解决现有自回归方法的语义纠缠和解码效率问题，在商业媒体平台上取得了显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 现有自回归方法在列表推荐中存在两个主要问题：1）项目标记化导致语义纠缠，难以进行可控生成；2）顺序解码缺乏整体列表规划，效率低下。需要一种既能进行全局规划又能高效解码的框架。

Method: HiGR采用三层设计：1）使用残差量化和对比约束的自动编码器将项目转换为语义结构化的ID；2）将生成过程解耦为列表级规划阶段（全局意图）和项目级解码阶段（具体选择）；3）引入列表级偏好对齐目标，利用隐式用户反馈直接优化列表质量。

Result: 在大型商业媒体平台上的实验表明，HiGR在离线评估和在线部署中都取得了一致改进。具体来说，在离线推荐质量上超过最先进方法10%以上，推理速度提升5倍；在线A/B测试中，平均观看时间和平均视频观看量分别提升1.22%和1.73%。

Conclusion: HiGR通过分层规划和列表级偏好对齐有效解决了生成式列表推荐中的语义纠缠和解码效率问题，为实际部署提供了高效且高质量的解决方案。

Abstract: Slate recommendation, where users are presented with a ranked list of items simultaneously, is widely adopted in online platforms. Recent advances in generative models have shown promise in slate recommendation by modeling sequences of discrete semantic IDs autoregressively. However, existing autoregressive approaches suffer from semantically entangled item tokenization and inefficient sequential decoding that lacks holistic slate planning. To address these limitations, we propose HiGR, an efficient generative slate recommendation framework that integrates hierarchical planning with listwise preference alignment. First, we propose an auto-encoder utilizing residual quantization and contrastive constraints to tokenize items into semantically structured IDs for controllable generation. Second, HiGR decouples generation into a list-level planning stage for global slate intent, followed by an item-level decoding stage for specific item selection. Third, we introduce a listwise preference alignment objective to directly optimize slate quality using implicit user feedback. Experiments on our large-scale commercial media platform demonstrate that HiGR delivers consistent improvements in both offline evaluations and online deployment. Specifically, it outperforms state-of-the-art methods by over 10% in offline recommendation quality with a 5x inference speedup, while further achieving a 1.22% and 1.73% increase in Average Watch Time and Average Video Views in online A/B tests.

</details>


### [76] [RAIR: A Rule-Aware Benchmark Uniting Challenging Long-Tail and Visual Salience Subset for E-commerce Relevance Assessment](https://arxiv.org/abs/2512.24943)
*Chenji Lu,Zhuo Chen,Hui Zhao,Zhenyi Wang,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.IR

TL;DR: RAIR是一个中文电商搜索相关性评估基准，包含通用、长尾困难和视觉显著性三个子集，旨在提供标准化的相关性评估框架，挑战当前LLM和VLM的能力极限。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在电商搜索相关性任务中表现出色，但缺乏足够复杂度的标准化评估基准和统一评估指标，导致行业内无法进行有效的模型比较和评估。

Method: 构建基于真实场景的中文数据集RAIR，建立标准化评估框架和通用规则集，包含三个子集：行业平衡采样的通用子集、聚焦挑战案例的长尾困难子集、评估多模态理解能力的视觉显著性子集。

Result: 在14个开源和闭源模型上的实验表明，RAIR对包括GPT-5在内的先进模型都构成足够挑战，GPT-5表现最佳但仍有提升空间，验证了基准的有效性和挑战性。

Conclusion: RAIR为电商搜索相关性评估提供了行业标准化基准，不仅适用于相关性模型评估，也为通用LLM和VLM评估提供了新见解，相关数据已公开可用。

Abstract: Search relevance plays a central role in web e-commerce. While large language models (LLMs) have shown significant results on relevance task, existing benchmarks lack sufficient complexity for comprehensive model assessment, resulting in an absence of standardized relevance evaluation metrics across the industry. To address this limitation, we propose Rule-Aware benchmark with Image for Relevance assessment(RAIR), a Chinese dataset derived from real-world scenarios. RAIR established a standardized framework for relevance assessment and provides a set of universal rules, which forms the foundation for standardized evaluation. Additionally, RAIR analyzes essential capabilities required for current relevance models and introduces a comprehensive dataset consists of three subset: (1) a general subset with industry-balanced sampling to evaluate fundamental model competencies; (2) a long-tail hard subset focus on challenging cases to assess performance limits; (3) a visual salience subset for evaluating multimodal understanding capabilities. We conducted experiments on RAIR using 14 open and closed-source models. The results demonstrate that RAIR presents sufficient challenges even for GPT-5, which achieved the best performance. RAIR data are now available, serving as an industry benchmark for relevance assessment while providing new insights into general LLM and Visual Language Model(VLM) evaluation.

</details>
