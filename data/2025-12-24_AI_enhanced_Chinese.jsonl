{"id": "2512.19958", "categories": ["cs.IR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.19958", "abs": "https://arxiv.org/abs/2512.19958", "authors": ["Sneha Oommen", "Gabby Sanchez", "Cassandra T. Britto", "Di Wang", "Jordan Chiou", "Maria Spichkova"], "title": "Towards Analysing Invoices and Receipts with Amazon Textract", "comment": null, "summary": "This paper presents an evaluation of the AWS Textract in the context of extracting data from receipts. We analyse Textract functionalities using a dataset that includes receipts of varied formats and conditions. Our analysis provided a qualitative view of Textract strengths and limitations. While the receipts totals were consistently detected, we also observed typical issues and irregularities that were often influenced by image quality and layout. Based on the analysis of the observations, we propose mitigation strategies.", "AI": {"tldr": "\u5bf9AWS Textract\u4ece\u6536\u636e\u4e2d\u63d0\u53d6\u6570\u636e\u7684\u8bc4\u4f30\uff0c\u5206\u6790\u4e86\u5176\u529f\u80fd\u8868\u73b0\u3001\u5178\u578b\u95ee\u9898\u53ca\u7f13\u89e3\u7b56\u7565", "motivation": "\u8bc4\u4f30AWS Textract\u5728\u6536\u636e\u6570\u636e\u63d0\u53d6\u573a\u666f\u4e2d\u7684\u5b9e\u9645\u8868\u73b0\uff0c\u4e86\u89e3\u5176\u5728\u4e0d\u540c\u683c\u5f0f\u548c\u6761\u4ef6\u4e0b\u7684\u529f\u80fd\u8868\u73b0", "method": "\u4f7f\u7528\u5305\u542b\u591a\u79cd\u683c\u5f0f\u548c\u6761\u4ef6\u7684\u6536\u636e\u6570\u636e\u96c6\uff0c\u5bf9Textract\u529f\u80fd\u8fdb\u884c\u5b9a\u6027\u5206\u6790\uff0c\u89c2\u5bdf\u5178\u578b\u95ee\u9898\u548c\u5f02\u5e38\u60c5\u51b5", "result": "\u6536\u636e\u603b\u989d\u80fd\u4e00\u81f4\u5730\u88ab\u68c0\u6d4b\u5230\uff0c\u4f46\u56fe\u50cf\u8d28\u91cf\u548c\u5e03\u5c40\u4f1a\u5f71\u54cd\u63d0\u53d6\u6548\u679c\uff0c\u5b58\u5728\u5178\u578b\u95ee\u9898\u548c\u5f02\u5e38\u60c5\u51b5", "conclusion": "\u57fa\u4e8e\u89c2\u5bdf\u5206\u6790\u63d0\u51fa\u4e86\u7f13\u89e3\u7b56\u7565\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6539\u8fdb\u65b9\u5411"}}
{"id": "2512.19983", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.19983", "abs": "https://arxiv.org/abs/2512.19983", "authors": ["Ziyuan Guo", "Jie Guo", "Zhenghao Chen", "Bin Song", "Fei Richard Yu"], "title": "IGDMRec: Behavior Conditioned Item Graph Diffusion for Multimodal Recommendation", "comment": "12 pages, 6 figures. This paper has been accepted for publication in IEEE Transactions on Multimedia. The final published version will be available via IEEE Xplore", "summary": "Multimodal recommender systems (MRSs) are critical for various online platforms, offering users more accurate personalized recommendations by incorporating multimodal information of items. Structure-based MRSs have achieved state-of-the-art performance by constructing semantic item graphs, which explicitly model relationships between items based on modality feature similarity. However, such semantic item graphs are often noisy due to 1) inherent noise in multimodal information and 2) misalignment between item semantics and user-item co-occurrence relationships, which introduces false links and leads to suboptimal recommendations. To address this challenge, we propose Item Graph Diffusion for Multimodal Recommendation (IGDMRec), a novel method that leverages a diffusion model with classifier-free guidance to denoise the semantic item graph by integrating user behavioral information. Specifically, IGDMRec introduces a Behavior-conditioned Graph Diffusion (BGD) module, incorporating interaction data as conditioning information to guide the denoising of the semantic item graph. Additionally, a Conditional Denoising Network (CD-Net) is designed to implement the denoising process with manageable complexity. Finally, we propose a contrastive representation augmentation scheme that leverages both the denoised item graph and the original item graph to enhance item representations. \\LL{Extensive experiments on four real-world datasets demonstrate the superiority of IGDMRec over competitive baselines, with robustness analysis validating its denoising capability and ablation studies verifying the effectiveness of its key components.", "AI": {"tldr": "IGDMRec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u6269\u6563\u6a21\u578b\u548c\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u6765\u964d\u566a\u8bed\u4e49\u7269\u54c1\u56fe\u7684\u591a\u6a21\u6001\u63a8\u8350\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u7528\u6237\u884c\u4e3a\u4fe1\u606f\u63d0\u5347\u63a8\u8350\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u7ed3\u6784\u7684MRS\u901a\u8fc7\u6784\u5efa\u8bed\u4e49\u7269\u54c1\u56fe\u53d6\u5f97\u4e86SOTA\u6027\u80fd\uff0c\u4f46\u8fd9\u7c7b\u56fe\u5b58\u5728\u566a\u58f0\u95ee\u9898\uff1a1)\u591a\u6a21\u6001\u4fe1\u606f\u672c\u8eab\u5b58\u5728\u566a\u58f0\uff1b2)\u7269\u54c1\u8bed\u4e49\u4e0e\u7528\u6237-\u7269\u54c1\u5171\u73b0\u5173\u7cfb\u4e4b\u95f4\u7684\u4e0d\u5bf9\u9f50\u5bfc\u81f4\u865a\u5047\u94fe\u63a5\uff0c\u4ece\u800c\u5f71\u54cd\u63a8\u8350\u6548\u679c\u3002", "method": "IGDMRec\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1)\u884c\u4e3a\u6761\u4ef6\u56fe\u6269\u6563\u6a21\u5757\uff0c\u5c06\u4ea4\u4e92\u6570\u636e\u4f5c\u4e3a\u6761\u4ef6\u4fe1\u606f\u6765\u6307\u5bfc\u8bed\u4e49\u7269\u54c1\u56fe\u7684\u964d\u566a\uff1b2)\u6761\u4ef6\u964d\u566a\u7f51\u7edc\uff0c\u4ee5\u53ef\u7ba1\u7406\u590d\u6742\u5ea6\u5b9e\u73b0\u964d\u566a\u8fc7\u7a0b\uff1b3)\u5bf9\u6bd4\u8868\u793a\u589e\u5f3a\u65b9\u6848\uff0c\u5229\u7528\u964d\u566a\u540e\u7684\u7269\u54c1\u56fe\u548c\u539f\u59cb\u7269\u54c1\u56fe\u6765\u589e\u5f3a\u7269\u54c1\u8868\u793a\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cIGDMRec\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\uff0c\u9c81\u68d2\u6027\u5206\u6790\u9a8c\u8bc1\u4e86\u5176\u964d\u566a\u80fd\u529b\uff0c\u6d88\u878d\u7814\u7a76\u786e\u8ba4\u4e86\u5176\u5173\u952e\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u3002", "conclusion": "IGDMRec\u901a\u8fc7\u6269\u6563\u6a21\u578b\u6709\u6548\u964d\u566a\u8bed\u4e49\u7269\u54c1\u56fe\uff0c\u7ed3\u5408\u7528\u6237\u884c\u4e3a\u4fe1\u606f\u63d0\u5347\u4e86\u591a\u6a21\u6001\u63a8\u8350\u7684\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u8bed\u4e49\u7269\u54c1\u56fe\u4e2d\u7684\u566a\u58f0\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.20022", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.20022", "abs": "https://arxiv.org/abs/2512.20022", "authors": ["Kian Godhwani", "David Benrimoh"], "title": "LLM-Assisted Abstract Screening with OLIVER: Evaluating Calibration and Single-Model vs. Actor-Critic Configurations in Literature Reviews", "comment": null, "summary": "Introduction: Recent work suggests large language models (LLMs) can accelerate screening, but prior evaluations focus on earlier LLMs, standardized Cochrane reviews, single-model setups, and accuracy as the primary metric, leaving generalizability, configuration effects, and calibration largely unexamined.\n  Methods: We developed OLIVER (Optimized LLM-based Inclusion and Vetting Engine for Reviews), an open-source pipeline for LLM-assisted abstract screening. We evaluated multiple contemporary LLMs across two non-Cochrane systematic reviews and performance was assessed at both the full-text screening and final inclusion stages using accuracy, AUC, and calibration metrics. We further tested an actor-critic screening framework combining two lightweight models under three aggregation rules.\n  Results: Across individual models, performance varied widely. In the smaller Review 1 (821 abstracts, 63 final includes), several models achieved high sensitivity for final includes but at the cost of substantial false positives and poor calibration. In the larger Review 2 (7741 abstracts, 71 final includes), most models were highly specific but struggled to recover true includes, with prompt design influencing recall. Calibration was consistently weak across single-model configurations despite high overall accuracy. Actor-critic screening improved discrimination and markedly reduced calibration error in both reviews, yielding higher AUCs.\n  Discussion: LLMs may eventually accelerate abstract screening, but single-model performance is highly sensitive to review characteristics, prompting, and calibration is limited. An actor-critic framework improves classification quality and confidence reliability while remaining computationally efficient, enabling large-scale screening at low cost.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOLVER\u7ba1\u9053\u8bc4\u4f30LLM\u5728\u6587\u732e\u7b5b\u9009\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u5355\u6a21\u578b\u8868\u73b0\u53d7\u6587\u732e\u7efc\u8ff0\u7279\u6027\u5f71\u54cd\u5927\u4e14\u6821\u51c6\u5dee\uff0c\u800c\u6f14\u5458-\u8bc4\u8bba\u5bb6\u6846\u67b6\u80fd\u663e\u8457\u63d0\u5347\u7b5b\u9009\u8d28\u91cf\u548c\u7f6e\u4fe1\u5ea6\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709LLM\u8f85\u52a9\u6587\u732e\u7b5b\u9009\u7684\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u65e9\u671f\u6a21\u578b\u3001\u6807\u51c6\u5316Cochrane\u7efc\u8ff0\u3001\u5355\u6a21\u578b\u8bbe\u7f6e\u548c\u51c6\u786e\u6027\u6307\u6807\uff0c\u800c\u5ffd\u89c6\u4e86\u901a\u7528\u6027\u3001\u914d\u7f6e\u6548\u5e94\u548c\u6821\u51c6\u7b49\u91cd\u8981\u65b9\u9762\u3002", "method": "\u5f00\u53d1\u4e86\u5f00\u6e90\u7ba1\u9053OLVER\uff0c\u8bc4\u4f30\u591a\u4e2a\u5f53\u4ee3LLM\u5728\u4e24\u4e2a\u975eCochrane\u7cfb\u7edf\u7efc\u8ff0\u4e2d\u7684\u8868\u73b0\uff0c\u4f7f\u7528\u51c6\u786e\u6027\u3001AUC\u548c\u6821\u51c6\u6307\u6807\uff0c\u5e76\u6d4b\u8bd5\u4e86\u7ed3\u5408\u4e24\u4e2a\u8f7b\u91cf\u7ea7\u6a21\u578b\u7684\u6f14\u5458-\u8bc4\u8bba\u5bb6\u7b5b\u9009\u6846\u67b6\u53ca\u4e09\u79cd\u805a\u5408\u89c4\u5219\u3002", "result": "\u5355\u6a21\u578b\u8868\u73b0\u5dee\u5f02\u5927\uff1a\u5c0f\u7efc\u8ff0\u4e2d\u654f\u611f\u6027\u9ad8\u4f46\u5047\u9633\u6027\u591a\u3001\u6821\u51c6\u5dee\uff1b\u5927\u7efc\u8ff0\u4e2d\u7279\u5f02\u6027\u9ad8\u4f46\u53ec\u56de\u7387\u4f4e\uff0c\u63d0\u793a\u8bbe\u8ba1\u5f71\u54cd\u53ec\u56de\u3002\u5355\u6a21\u578b\u6821\u51c6\u666e\u904d\u8f83\u5f31\u3002\u6f14\u5458-\u8bc4\u8bba\u5bb6\u6846\u67b6\u5728\u4e24\u4e2a\u7efc\u8ff0\u4e2d\u90fd\u6539\u5584\u4e86\u533a\u5206\u5ea6\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u6821\u51c6\u8bef\u5dee\uff0c\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684AUC\u3002", "conclusion": "LLM\u6700\u7ec8\u53ef\u80fd\u52a0\u901f\u6587\u732e\u7b5b\u9009\uff0c\u4f46\u5355\u6a21\u578b\u8868\u73b0\u5bf9\u7efc\u8ff0\u7279\u6027\u3001\u63d0\u793a\u8bbe\u8ba1\u9ad8\u5ea6\u654f\u611f\u4e14\u6821\u51c6\u6709\u9650\u3002\u6f14\u5458-\u8bc4\u8bba\u5bb6\u6846\u67b6\u80fd\u63d0\u5347\u5206\u7c7b\u8d28\u91cf\u548c\u7f6e\u4fe1\u5ea6\u53ef\u9760\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\uff0c\u5b9e\u73b0\u4f4e\u6210\u672c\u7684\u5927\u89c4\u6a21\u7b5b\u9009\u3002"}}
{"id": "2512.20034", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.20034", "abs": "https://arxiv.org/abs/2512.20034", "authors": ["Xian Wu", "Ming Zhang", "Zhiyu Fang", "Fei Li", "Bin Wang", "Yong Jiang", "Hao Zhou"], "title": "VSA:Visual-Structural Alignment for UI-to-Code", "comment": null, "summary": "The automation of user interface development has the potential to accelerate software delivery by mitigating intensive manual implementation. Despite the advancements in Large Multimodal Models for design-to-code translation, existing methodologies predominantly yield unstructured, flat codebases that lack compatibility with component-oriented libraries such as React or Angular. Such outputs typically exhibit low cohesion and high coupling, complicating long-term maintenance. In this paper, we propose \\textbf{VSA (VSA)}, a multi-stage paradigm designed to synthesize organized frontend assets through visual-structural alignment. Our approach first employs a spatial-aware transformer to reconstruct the visual input into a hierarchical tree representation. Moving beyond basic layout extraction, we integrate an algorithmic pattern-matching layer to identify recurring UI motifs and encapsulate them into modular templates. These templates are then processed via a schema-driven synthesis engine, ensuring the Large Language Model generates type-safe, prop-drilled components suitable for production environments. Experimental results indicate that our framework yields a substantial improvement in code modularity and architectural consistency over state-of-the-art benchmarks, effectively bridging the gap between raw pixels and scalable software engineering.", "AI": {"tldr": "VSA\u662f\u4e00\u4e2a\u591a\u9636\u6bb5\u89c6\u89c9-\u7ed3\u6784\u5bf9\u9f50\u8303\u5f0f\uff0c\u80fd\u591f\u4ece\u89c6\u89c9\u8bbe\u8ba1\u56fe\u751f\u6210\u6a21\u5757\u5316\u3001\u7c7b\u578b\u5b89\u5168\u7684\u524d\u7aef\u7ec4\u4ef6\u4ee3\u7801\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7801\u6a21\u5757\u5316\u548c\u67b6\u6784\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u6a21\u578b\u7684\u8bbe\u8ba1\u5230\u4ee3\u7801\u8f6c\u6362\u65b9\u6cd5\u4e3b\u8981\u751f\u6210\u975e\u7ed3\u6784\u5316\u3001\u6241\u5e73\u7684\u4ee3\u7801\uff0c\u7f3a\u4e4f\u4e0eReact\u6216Angular\u7b49\u7ec4\u4ef6\u5316\u5e93\u7684\u517c\u5bb9\u6027\uff0c\u5bfc\u81f4\u4ee3\u7801\u4f4e\u5185\u805a\u9ad8\u8026\u5408\uff0c\u96be\u4ee5\u957f\u671f\u7ef4\u62a4\u3002", "method": "1. \u4f7f\u7528\u7a7a\u95f4\u611f\u77e5transformer\u5c06\u89c6\u89c9\u8f93\u5165\u91cd\u6784\u4e3a\u5c42\u6b21\u6811\u8868\u793a\uff1b2. \u96c6\u6210\u7b97\u6cd5\u6a21\u5f0f\u5339\u914d\u5c42\u8bc6\u522b\u91cd\u590dUI\u6a21\u5f0f\u5e76\u5c01\u88c5\u4e3a\u6a21\u5757\u5316\u6a21\u677f\uff1b3. \u901a\u8fc7\u6a21\u5f0f\u9a71\u52a8\u5408\u6210\u5f15\u64ce\uff0c\u786e\u4fdd\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7c7b\u578b\u5b89\u5168\u3001\u652f\u6301\u5c5e\u6027\u4f20\u9012\u7684\u751f\u4ea7\u7ea7\u7ec4\u4ef6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u8be5\u6846\u67b6\u5728\u4ee3\u7801\u6a21\u5757\u5316\u548c\u67b6\u6784\u4e00\u81f4\u6027\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\uff0c\u6709\u6548\u5f25\u5408\u4e86\u539f\u59cb\u50cf\u7d20\u4e0e\u53ef\u6269\u5c55\u8f6f\u4ef6\u5de5\u7a0b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "VSA\u901a\u8fc7\u89c6\u89c9-\u7ed3\u6784\u5bf9\u9f50\u7684\u591a\u9636\u6bb5\u8303\u5f0f\uff0c\u80fd\u591f\u4ece\u89c6\u89c9\u8bbe\u8ba1\u751f\u6210\u7ec4\u7ec7\u826f\u597d\u7684\u524d\u7aef\u8d44\u4ea7\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bbe\u8ba1\u5230\u4ee3\u7801\u8f6c\u6362\u65b9\u6cd5\u751f\u6210\u975e\u7ed3\u6784\u5316\u4ee3\u7801\u7684\u95ee\u9898\uff0c\u4e3a\u81ea\u52a8\u5316UI\u5f00\u53d1\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.19864", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19864", "abs": "https://arxiv.org/abs/2512.19864", "authors": ["Shashi Kant Gupta", "Arijeet Pramanik", "Jerrin John Thomas", "Regina Schwind", "Lauren Wiener", "Avi Raju", "Jeremy Kornbluth", "Yanshan Wang", "Zhaohui Su", "Hrituraj Singh"], "title": "HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data", "comment": "39 Pages, Supplementary Included", "summary": "Unstructured notes within the electronic health record (EHR) contain rich clinical information vital for cancer treatment decision making and research, yet reliably extracting structured oncology data remains challenging due to extensive variability, specialized terminology, and inconsistent document formats. Manual abstraction, although accurate, is prohibitively costly and unscalable. Existing automated approaches typically address narrow scenarios - either using synthetic datasets, restricting focus to document-level extraction, or isolating specific clinical variables (e.g., staging, biomarkers, histology) - and do not adequately handle patient-level synthesis across the large number of clinical documents containing contradictory information. In this study, we propose an agentic framework that systematically decomposes complex oncology data extraction into modular, adaptive tasks. Specifically, we use large language models (LLMs) as reasoning agents, equipped with context-sensitive retrieval and iterative synthesis capabilities, to exhaustively and comprehensively extract structured clinical variables from real-world oncology notes. Evaluated on a large-scale dataset of over 400,000 unstructured clinical notes and scanned PDF reports spanning 2,250 cancer patients, our method achieves an average F1-score of 0.93, with 100 out of 103 oncology-specific clinical variables exceeding 0.85, and critical variables (e.g., biomarkers and medications) surpassing 0.95. Moreover, integration of the agentic system into a data curation workflow resulted in 0.94 direct manual approval rate, significantly reducing annotation costs. To our knowledge, this constitutes the first exhaustive, end-to-end application of LLM-based agents for structured oncology data extraction at scale", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u4ece\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u81ea\u52a8\u63d0\u53d6\u7ed3\u6784\u5316\u80bf\u7624\u5b66\u6570\u636e\uff0c\u572840\u4e07\u4efd\u4e34\u5e8a\u7b14\u8bb0\u4e0a\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u63d0\u53d6\uff0c\u663e\u8457\u964d\u4f4e\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u975e\u7ed3\u6784\u5316\u7b14\u8bb0\u5305\u542b\u4e30\u5bcc\u7684\u4e34\u5e8a\u4fe1\u606f\uff0c\u4f46\u624b\u52a8\u63d0\u53d6\u6210\u672c\u9ad8\u4e14\u4e0d\u53ef\u6269\u5c55\u3002\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u901a\u5e38\u5c40\u9650\u4e8e\u7279\u5b9a\u573a\u666f\uff0c\u65e0\u6cd5\u5904\u7406\u60a3\u8005\u7ea7\u522b\u7684\u4fe1\u606f\u6574\u5408\uff0c\u4e14\u96be\u4ee5\u5e94\u5bf9\u4e34\u5e8a\u6587\u6863\u4e2d\u7684\u77db\u76fe\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u590d\u6742\u80bf\u7624\u6570\u636e\u63d0\u53d6\u5206\u89e3\u4e3a\u6a21\u5757\u5316\u3001\u81ea\u9002\u5e94\u7684\u4efb\u52a1\u3002\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u63a8\u7406\u667a\u80fd\u4f53\uff0c\u914d\u5907\u4e0a\u4e0b\u6587\u654f\u611f\u68c0\u7d22\u548c\u8fed\u4ee3\u5408\u6210\u80fd\u529b\uff0c\u4ece\u771f\u5b9e\u4e16\u754c\u80bf\u7624\u5b66\u7b14\u8bb0\u4e2d\u5168\u9762\u63d0\u53d6\u7ed3\u6784\u5316\u4e34\u5e8a\u53d8\u91cf\u3002", "result": "\u5728\u5305\u542b40\u4e07\u4efd\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u7b14\u8bb0\u548c\u626b\u63cfPDF\u62a5\u544a\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u5e73\u5747F1\u5206\u6570\u8fbe\u52300.93\uff0c103\u4e2a\u80bf\u7624\u5b66\u7279\u5b9a\u4e34\u5e8a\u53d8\u91cf\u4e2d\u6709100\u4e2a\u8d85\u8fc70.85\uff0c\u5173\u952e\u53d8\u91cf\uff08\u5982\u751f\u7269\u6807\u5fd7\u7269\u548c\u836f\u7269\uff09\u8d85\u8fc70.95\u3002\u7cfb\u7edf\u96c6\u6210\u5230\u6570\u636e\u7ba1\u7406\u6d41\u7a0b\u4e2d\u83b7\u5f97\u4e860.94\u7684\u76f4\u63a5\u4eba\u5de5\u6279\u51c6\u7387\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u57fa\u4e8eLLM\u667a\u80fd\u4f53\u7684\u7aef\u5230\u7aef\u7ed3\u6784\u5316\u80bf\u7624\u6570\u636e\u63d0\u53d6\u5e94\u7528\uff0c\u80fd\u591f\u5927\u89c4\u6a21\u5904\u7406\u771f\u5b9e\u4e16\u754c\u4e34\u5e8a\u6570\u636e\uff0c\u663e\u8457\u63d0\u9ad8\u63d0\u53d6\u51c6\u786e\u6027\u5e76\u964d\u4f4e\u6210\u672c\u3002"}}
{"id": "2512.20172", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.20172", "abs": "https://arxiv.org/abs/2512.20172", "authors": ["Yan Zhang", "Li Deng", "Lixin Duan", "Ivor W. Tsang", "Guowu Yang"], "title": "Collaborative Group-Aware Hashing for Fast Recommender Systems", "comment": null, "summary": "The fast online recommendation is critical for applications with large-scale databases; meanwhile, it is challenging to provide accurate recommendations in sparse scenarios. Hash technique has shown its superiority for speeding up the online recommendation by bit operations on Hamming distance computations. However, existing hashing-based recommendations suffer from low accuracy, especially with sparse settings, due to the limited representation capability of each bit and neglected inherent relations among users and items. To this end, this paper lodges a Collaborative Group-Aware Hashing (CGAH) method for both collaborative filtering (namely CGAH-CF) and content-aware recommendations (namely CGAH) by integrating the inherent group information to alleviate the sparse issue. Firstly, we extract inherent group affinities of users and items by classifying their latent vectors into different groups. Then, the preference is formulated as the inner product of the group affinity and the similarity of hash codes. By learning hash codes with the inherent group information, CGAH obtains more effective hash codes than other discrete methods with sparse interactive data. Extensive experiments on three public datasets show the superior performance of our proposed CGAH and CGAH-CF over the state-of-the-art discrete collaborative filtering methods and discrete content-aware recommendations under different sparse settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u534f\u4f5c\u7fa4\u7ec4\u611f\u77e5\u54c8\u5e0c\uff08CGAH\uff09\u7684\u63a8\u8350\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u56fa\u6709\u7fa4\u7ec4\u4fe1\u606f\u89e3\u51b3\u7a00\u758f\u573a\u666f\u4e0b\u54c8\u5e0c\u63a8\u8350\u7cbe\u5ea6\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u54c8\u5e0c\u63a8\u8350\u65b9\u6cd5\u5728\u7a00\u758f\u573a\u666f\u4e0b\u7cbe\u5ea6\u8f83\u4f4e\uff0c\u4e3b\u8981\u56e0\u4e3a\u6bcf\u4e2a\u6bd4\u7279\u8868\u793a\u80fd\u529b\u6709\u9650\uff0c\u4e14\u5ffd\u7565\u4e86\u7528\u6237\u548c\u7269\u54c1\u4e4b\u95f4\u7684\u56fa\u6709\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u5c06\u7528\u6237\u548c\u7269\u54c1\u7684\u6f5c\u5728\u5411\u91cf\u5206\u7c7b\u5230\u4e0d\u540c\u7fa4\u7ec4\u6765\u63d0\u53d6\u56fa\u6709\u7fa4\u7ec4\u4eb2\u548c\u5ea6\uff0c\u7136\u540e\u5c06\u504f\u597d\u8868\u793a\u4e3a\u7fa4\u7ec4\u4eb2\u548c\u5ea6\u4e0e\u54c8\u5e0c\u7801\u76f8\u4f3c\u5ea6\u7684\u5185\u79ef\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCGAH\u548cCGAH-CF\u5728\u4e0d\u540c\u7a00\u758f\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u79bb\u6563\u534f\u540c\u8fc7\u6ee4\u548c\u79bb\u6563\u5185\u5bb9\u611f\u77e5\u63a8\u8350\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u96c6\u6210\u56fa\u6709\u7fa4\u7ec4\u4fe1\u606f\u5b66\u4e60\u54c8\u5e0c\u7801\uff0cCGAH\u80fd\u591f\u5728\u7a00\u758f\u4ea4\u4e92\u6570\u636e\u4e0b\u83b7\u5f97\u6bd4\u73b0\u6709\u79bb\u6563\u65b9\u6cd5\u66f4\u6709\u6548\u7684\u54c8\u5e0c\u7801\uff0c\u663e\u8457\u63d0\u5347\u63a8\u8350\u7cbe\u5ea6\u3002"}}
{"id": "2512.19903", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19903", "abs": "https://arxiv.org/abs/2512.19903", "authors": ["Kirk Vanacore", "Rene F. Kizilcec"], "title": "How well do Large Language Models Recognize Instructional Moves? Establishing Baselines for Foundation Models in Educational Discourse", "comment": null, "summary": "Large language models (LLMs) are increasingly adopted in educational technologies for a variety of tasks, from generating instructional materials and assisting with assessment design to tutoring. While prior work has investigated how models can be adapted or optimized for specific tasks, far less is known about how well LLMs perform at interpreting authentic educational scenarios without significant customization. As LLM-based systems become widely adopted by learners and educators in everyday academic contexts, understanding their out-of-the-box capabilities is increasingly important for setting expectations and benchmarking. We compared six LLMs to estimate their baseline performance on a simple but important task: classifying instructional moves in authentic classroom transcripts. We evaluated typical prompting methods: zero-shot, one-shot, and few-shot prompting. We found that while zero-shot performance was moderate, providing comprehensive examples (few-shot prompting) significantly improved performance for state-of-the-art models, with the strongest configuration reaching Cohen's Kappa = 0.58 against expert-coded annotations. At the same time, improvements were neither uniform nor complete: performance varied considerably by instructional move, and higher recall frequently came at the cost of increased false positives. Overall, these findings indicate that foundation models demonstrate meaningful yet limited capacity to interpret instructional discourse, with prompt design helping to surface capability but not eliminating fundamental reliability constraints.", "AI": {"tldr": "LLMs\u5728\u6559\u80b2\u4efb\u52a1\u4e2d\u7684\u57fa\u7ebf\u6027\u80fd\u8bc4\u4f30\uff1a\u901a\u8fc7\u591a\u79cd\u63d0\u793a\u65b9\u6cd5\u6d4b\u8bd5LLMs\u5bf9\u771f\u5b9e\u8bfe\u5802\u6307\u4ee4\u52a8\u4f5c\u7684\u5206\u7c7b\u80fd\u529b\uff0c\u53d1\u73b0few-shot\u63d0\u793a\u663e\u8457\u63d0\u5347\u6027\u80fd\u4f46\u4ecd\u6709\u53ef\u9760\u6027\u9650\u5236\u3002", "motivation": "\u968f\u7740LLMs\u5728\u6559\u80b2\u6280\u672f\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u4e86\u89e3\u5176\u5728\u672a\u7ecf\u5927\u91cf\u5b9a\u5236\u7684\u60c5\u51b5\u4e0b\u89e3\u91ca\u771f\u5b9e\u6559\u80b2\u573a\u666f\u7684\u80fd\u529b\uff0c\u8fd9\u5bf9\u8bbe\u5b9a\u671f\u671b\u548c\u57fa\u51c6\u6d4b\u8bd5\u5f88\u91cd\u8981\u3002", "method": "\u6bd4\u8f83\u516d\u4e2aLLMs\u5728\u8bfe\u5802\u6307\u4ee4\u52a8\u4f5c\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u57fa\u7ebf\u6027\u80fd\uff0c\u8bc4\u4f30\u96f6\u6837\u672c\u3001\u5355\u6837\u672c\u548c\u5c11\u6837\u672c\u63d0\u793a\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e13\u5bb6\u7f16\u7801\u6807\u6ce8\u4f5c\u4e3a\u57fa\u51c6\u3002", "result": "\u96f6\u6837\u672c\u8868\u73b0\u4e2d\u7b49\uff0c\u5c11\u6837\u672c\u63d0\u793a\u663e\u8457\u63d0\u5347\u4e86\u6700\u5148\u8fdb\u6a21\u578b\u7684\u6027\u80fd\uff08Cohen's Kappa = 0.58\uff09\uff0c\u4f46\u6027\u80fd\u56e0\u6307\u4ee4\u52a8\u4f5c\u7c7b\u578b\u800c\u5f02\uff0c\u9ad8\u53ec\u56de\u7387\u5e38\u4f34\u968f\u5047\u9633\u6027\u589e\u52a0\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u5728\u89e3\u91ca\u6559\u5b66\u8bdd\u8bed\u65b9\u9762\u8868\u73b0\u51fa\u6709\u610f\u4e49\u4f46\u6709\u9650\u7684\u80fd\u529b\uff0c\u63d0\u793a\u8bbe\u8ba1\u6709\u52a9\u4e8e\u5c55\u73b0\u80fd\u529b\u4f46\u65e0\u6cd5\u6d88\u9664\u6839\u672c\u7684\u53ef\u9760\u6027\u9650\u5236\u3002"}}
{"id": "2512.20458", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.20458", "abs": "https://arxiv.org/abs/2512.20458", "authors": ["Shuting Wang", "Qiaolin Xia", "Hao Wang", "Yu Lu", "Bobsimons", "Zhicheng Dou"], "title": "Laser: Governing Long-Horizon Agentic Search via Structured Protocol and Context Register", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) and Large Reasoning Models (LRMs) have enabled agentic search systems that interleave multi-step reasoning with external tool use. However, existing frameworks largely rely on unstructured natural-language reasoning and accumulate raw intermediate traces in the context, which often leads to unstable reasoning trajectories, context overflow, and degraded performance on complex multi-hop queries. In this study, we introduce Laser, a general framework for stabilizing and scaling agentic search. Laser defines a symbolic action protocol that organizes agent behaviors into three spaces: planning, task-solving, and retrospection. Each action is specified with explicit semantics and a deterministic execution format, enabling structured and logical reasoning processes and reliable action parsing. This design makes intermediate decisions interpretable and traceable, enhancing explicit retrospection and fine-grained control over reasoning trajectories. In coordination with parsable actions, Laser further maintains a compact context register that stores only essential states of the reasoning process, allowing the agent to reason over long horizons without uncontrolled context expansion. Experiments on Qwen2.5/3-series models across challenging multi-hop QA datasets show that Laser consistently outperforms existing agentic search baselines under both prompting-only and fine-tuning settings, demonstrating that Laser provides a principled and effective foundation for robust, scalable agentic search.", "AI": {"tldr": "Laser\u662f\u4e00\u4e2a\u7528\u4e8e\u7a33\u5b9a\u548c\u6269\u5c55\u667a\u80fd\u4f53\u641c\u7d22\u7684\u901a\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u7b26\u53f7\u5316\u52a8\u4f5c\u534f\u8bae\u548c\u7d27\u51d1\u4e0a\u4e0b\u6587\u5bc4\u5b58\u5668\u89e3\u51b3\u73b0\u6709\u7cfb\u7edf\u63a8\u7406\u8f68\u8ff9\u4e0d\u7a33\u5b9a\u3001\u4e0a\u4e0b\u6587\u6ea2\u51fa\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5927\u578b\u63a8\u7406\u6a21\u578b\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u641c\u7d22\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u975e\u7ed3\u6784\u5316\u7684\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff0c\u5728\u4e0a\u4e0b\u6587\u4e2d\u79ef\u7d2f\u539f\u59cb\u4e2d\u95f4\u75d5\u8ff9\uff0c\u5bfc\u81f4\u63a8\u7406\u8f68\u8ff9\u4e0d\u7a33\u5b9a\u3001\u4e0a\u4e0b\u6587\u6ea2\u51fa\uff0c\u4ee5\u53ca\u5728\u590d\u6742\u591a\u8df3\u67e5\u8be2\u4e0a\u6027\u80fd\u4e0b\u964d\u3002", "method": "Laser\u5b9a\u4e49\u4e86\u4e00\u4e2a\u7b26\u53f7\u5316\u52a8\u4f5c\u534f\u8bae\uff0c\u5c06\u667a\u80fd\u4f53\u884c\u4e3a\u7ec4\u7ec7\u5230\u4e09\u4e2a\u7a7a\u95f4\uff1a\u89c4\u5212\u3001\u4efb\u52a1\u89e3\u51b3\u548c\u53cd\u601d\u3002\u6bcf\u4e2a\u52a8\u4f5c\u90fd\u6709\u660e\u786e\u7684\u8bed\u4e49\u548c\u786e\u5b9a\u6027\u6267\u884c\u683c\u5f0f\uff0c\u5b9e\u73b0\u7ed3\u6784\u5316\u903b\u8f91\u63a8\u7406\u548c\u53ef\u9760\u7684\u52a8\u4f5c\u89e3\u6790\u3002\u914d\u5408\u53ef\u89e3\u6790\u52a8\u4f5c\uff0cLaser\u8fd8\u7ef4\u62a4\u4e00\u4e2a\u7d27\u51d1\u7684\u4e0a\u4e0b\u6587\u5bc4\u5b58\u5668\uff0c\u53ea\u5b58\u50a8\u63a8\u7406\u8fc7\u7a0b\u7684\u57fa\u672c\u72b6\u6001\u3002", "result": "\u5728Qwen2.5/3\u7cfb\u5217\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLaser\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u591a\u8df3QA\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u7684\u667a\u80fd\u4f53\u641c\u7d22\u57fa\u7ebf\u65b9\u6cd5\uff0c\u65e0\u8bba\u662f\u5728\u4ec5\u63d0\u793a\u8fd8\u662f\u5fae\u8c03\u8bbe\u7f6e\u4e0b\u90fd\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "Laser\u4e3a\u7a33\u5065\u3001\u53ef\u6269\u5c55\u7684\u667a\u80fd\u4f53\u641c\u7d22\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u548c\u6709\u6548\u7684\u57fa\u7840\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u548c\u7d27\u51d1\u72b6\u6001\u7ba1\u7406\u89e3\u51b3\u4e86\u73b0\u6709\u6846\u67b6\u7684\u5173\u952e\u9650\u5236\u3002"}}
{"id": "2512.19908", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.19908", "abs": "https://arxiv.org/abs/2512.19908", "authors": ["Jingyi Qiu", "Hong Chen", "Zongyi Li"], "title": "Counterfactual LLM-based Framework for Measuring Rhetorical Style", "comment": null, "summary": "The rise of AI has fueled growing concerns about ``hype'' in machine learning papers, yet a reliable way to quantify rhetorical style independently of substantive content has remained elusive. Because bold language can stem from either strong empirical results or mere rhetorical style, it is often difficult to distinguish between the two. To disentangle rhetorical style from substantive content, we introduce a counterfactual, LLM-based framework: multiple LLM rhetorical personas generate counterfactual writings from the same substantive content, an LLM judge compares them through pairwise evaluations, and the outcomes are aggregated using a Bradley--Terry model. Applying this method to 8,485 ICLR submissions sampled from 2017 to 2025, we generate more than 250,000 counterfactual writings and provide a large-scale quantification of rhetorical style in ML papers. We find that visionary framing significantly predicts downstream attention, including citations and media attention, even after controlling for peer-review evaluations. We also observe a sharp rise in rhetorical strength after 2023, and provide empirical evidence showing that this increase is largely driven by the adoption of LLM-based writing assistance. The reliability of our framework is validated by its robustness to the choice of personas and the high correlation between LLM judgments and human annotations. Our work demonstrates that LLMs can serve as instruments to measure and improve scientific evaluation.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u53cd\u4e8b\u5b9e\u6846\u67b6\u6765\u91cf\u5316\u673a\u5668\u5b66\u4e60\u8bba\u6587\u4e2d\u7684\u4fee\u8f9e\u98ce\u683c\uff0c\u53d1\u73b0\u613f\u666f\u5f0f\u8868\u8fbe\u80fd\u663e\u8457\u9884\u6d4b\u8bba\u6587\u7684\u5f15\u7528\u548c\u5a92\u4f53\u5173\u6ce8\u5ea6\uff0c\u4e142023\u5e74\u540e\u4fee\u8f9e\u5f3a\u5ea6\u6025\u5267\u4e0a\u5347\u4e3b\u8981\u7531LLM\u5199\u4f5c\u8f85\u52a9\u5de5\u5177\u9a71\u52a8\u3002", "motivation": "AI\u9886\u57df\u5bf9\u673a\u5668\u5b66\u4e60\u8bba\u6587\u4e2d\"\u7092\u4f5c\"\u73b0\u8c61\u7684\u62c5\u5fe7\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5c06\u4fee\u8f9e\u98ce\u683c\u4e0e\u5b9e\u8d28\u6027\u5185\u5bb9\u5206\u79bb\u3002\u7531\u4e8e\u5927\u80c6\u8bed\u8a00\u53ef\u80fd\u6e90\u4e8e\u5f3a\u5927\u7684\u5b9e\u8bc1\u7ed3\u679c\u6216\u4ec5\u4ec5\u662f\u4fee\u8f9e\u98ce\u683c\uff0c\u533a\u5206\u4e8c\u8005\u4e00\u76f4\u662f\u4e2a\u6311\u6218\u3002", "method": "\u5f15\u5165\u57fa\u4e8eLLM\u7684\u53cd\u4e8b\u5b9e\u6846\u67b6\uff1a1) \u591a\u4e2aLLM\u4fee\u8f9e\u89d2\u8272\u4ece\u76f8\u540c\u5b9e\u8d28\u6027\u5185\u5bb9\u751f\u6210\u53cd\u4e8b\u5b9e\u5199\u4f5c\uff1b2) LLM\u8bc4\u5224\u5458\u901a\u8fc7\u6210\u5bf9\u6bd4\u8f83\u8bc4\u4f30\u8fd9\u4e9b\u5199\u4f5c\uff1b3) \u4f7f\u7528Bradley-Terry\u6a21\u578b\u805a\u5408\u7ed3\u679c\u3002\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8e2017-2025\u5e74\u95f4\u76848,485\u7bc7ICLR\u6295\u7a3f\uff0c\u751f\u6210\u4e86\u8d85\u8fc725\u4e07\u7bc7\u53cd\u4e8b\u5b9e\u5199\u4f5c\u3002", "result": "1) \u613f\u666f\u5f0f\u8868\u8fbe\u80fd\u663e\u8457\u9884\u6d4b\u4e0b\u6e38\u5173\u6ce8\u5ea6\uff08\u5305\u62ec\u5f15\u7528\u548c\u5a92\u4f53\u5173\u6ce8\uff09\uff0c\u5373\u4f7f\u63a7\u5236\u4e86\u540c\u884c\u8bc4\u5ba1\u8bc4\u4f30\uff1b2) 2023\u5e74\u540e\u4fee\u8f9e\u5f3a\u5ea6\u6025\u5267\u4e0a\u5347\uff1b3) \u5b9e\u8bc1\u8bc1\u636e\u8868\u660e\u8fd9\u79cd\u4e0a\u5347\u4e3b\u8981\u7531LLM\u5199\u4f5c\u8f85\u52a9\u5de5\u5177\u7684\u91c7\u7528\u9a71\u52a8\uff1b4) \u6846\u67b6\u53ef\u9760\u6027\u5f97\u5230\u9a8c\u8bc1\uff08\u5bf9\u89d2\u8272\u9009\u62e9\u7684\u9c81\u68d2\u6027\u548cLLM\u5224\u65ad\u4e0e\u4eba\u5de5\u6807\u6ce8\u7684\u9ad8\u76f8\u5173\u6027\uff09\u3002", "conclusion": "LLM\u53ef\u4ee5\u4f5c\u4e3a\u6d4b\u91cf\u548c\u6539\u8fdb\u79d1\u5b66\u8bc4\u4f30\u7684\u5de5\u5177\uff0c\u672c\u7814\u7a76\u6846\u67b6\u80fd\u591f\u53ef\u9760\u5730\u91cf\u5316\u4fee\u8f9e\u98ce\u683c\uff0c\u63ed\u793a\u4e86\u4fee\u8f9e\u98ce\u683c\u5bf9\u8bba\u6587\u5f71\u54cd\u529b\u7684\u91cd\u8981\u4f5c\u7528\uff0c\u4ee5\u53caLLM\u5199\u4f5c\u8f85\u52a9\u5bf9\u5b66\u672f\u5199\u4f5c\u98ce\u683c\u7684\u663e\u8457\u5f71\u54cd\u3002"}}
{"id": "2512.20612", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20612", "abs": "https://arxiv.org/abs/2512.20612", "authors": ["Yibin Lei", "Shwai He", "Ang Li", "Andrew Yates"], "title": "Making Large Language Models Efficient Dense Retrievers", "comment": null, "summary": "Recent work has shown that directly fine-tuning large language models (LLMs) for dense retrieval yields strong performance, but their substantial parameter counts make them computationally inefficient. While prior studies have revealed significant layer redundancy in LLMs for generative tasks, it remains unclear whether similar redundancy exists when these models are adapted for retrieval tasks, which require encoding entire sequences into fixed representations rather than generating tokens iteratively. To this end, we conduct a comprehensive analysis of layer redundancy in LLM-based dense retrievers. We find that, in contrast to generative settings, MLP layers are substantially more prunable, while attention layers remain critical for semantic aggregation. Building on this insight, we propose EffiR, a framework for developing efficient retrievers that performs large-scale MLP compression through a coarse-to-fine strategy (coarse-grained depth reduction followed by fine-grained width reduction), combined with retrieval-specific fine-tuning. Across diverse BEIR datasets and LLM backbones, EffiR achieves substantial reductions in model size and inference cost while preserving the performance of full-size models.", "AI": {"tldr": "EffiR\u6846\u67b6\u901a\u8fc7\u5206\u6790LLM\u5728\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u5c42\u7ea7\u5197\u4f59\u6027\uff0c\u53d1\u73b0MLP\u5c42\u6bd4\u6ce8\u610f\u529b\u5c42\u66f4\u5bb9\u6613\u526a\u679d\uff0c\u5e76\u63d0\u51fa\u7c97\u5230\u7ec6\u7684MLP\u538b\u7f29\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\u548c\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u5c3d\u7ba1\u76f4\u63a5\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u7528\u4e8e\u7a20\u5bc6\u68c0\u7d22\u6548\u679c\u5f88\u597d\uff0c\u4f46\u53c2\u6570\u91cf\u5927\u5bfc\u81f4\u8ba1\u7b97\u6548\u7387\u4f4e\u3002\u73b0\u6709\u7814\u7a76\u663e\u793aLLM\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u5b58\u5728\u5c42\u7ea7\u5197\u4f59\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u5728\u68c0\u7d22\u4efb\u52a1\u4e2d\u662f\u5426\u4e5f\u5b58\u5728\u7c7b\u4f3c\u5197\u4f59\uff0c\u56e0\u4e3a\u68c0\u7d22\u4efb\u52a1\u9700\u8981\u5c06\u6574\u4e2a\u5e8f\u5217\u7f16\u7801\u4e3a\u56fa\u5b9a\u8868\u793a\u800c\u975e\u8fed\u4ee3\u751f\u6210\u6807\u8bb0\u3002", "method": "1) \u5168\u9762\u5206\u6790LLM\u57fa\u7a20\u5bc6\u68c0\u7d22\u5668\u7684\u5c42\u7ea7\u5197\u4f59\u6027\uff1b2) \u53d1\u73b0MLP\u5c42\u6bd4\u6ce8\u610f\u529b\u5c42\u66f4\u5bb9\u6613\u526a\u679d\uff1b3) \u63d0\u51faEffiR\u6846\u67b6\uff0c\u91c7\u7528\u7c97\u5230\u7ec6\u7b56\u7565\u8fdb\u884c\u5927\u89c4\u6a21MLP\u538b\u7f29\uff1a\u5148\u7c97\u7c92\u5ea6\u6df1\u5ea6\u51cf\u5c11\uff0c\u518d\u7ec6\u7c92\u5ea6\u5bbd\u5ea6\u51cf\u5c11\uff1b4) \u7ed3\u5408\u68c0\u7d22\u7279\u5b9a\u7684\u5fae\u8c03\u3002", "result": "\u5728\u591a\u79cdBEIR\u6570\u636e\u96c6\u548cLLM\u9aa8\u5e72\u7f51\u7edc\u4e0a\uff0cEffiR\u5728\u4fdd\u6301\u5168\u5c3a\u5bf8\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u578b\u5927\u5c0f\u548c\u63a8\u7406\u6210\u672c\u3002", "conclusion": "LLM\u5728\u68c0\u7d22\u4efb\u52a1\u4e2d\u5b58\u5728\u72ec\u7279\u7684\u5c42\u7ea7\u5197\u4f59\u6a21\u5f0f\uff08MLP\u5c42\u6bd4\u6ce8\u610f\u529b\u5c42\u66f4\u6613\u526a\u679d\uff09\uff0cEffiR\u6846\u67b6\u901a\u8fc7\u9488\u5bf9\u6027\u7684\u538b\u7f29\u7b56\u7565\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u68c0\u7d22\u5668\uff0c\u5e73\u8861\u4e86\u6027\u80fd\u4e0e\u6548\u7387\u3002"}}
{"id": "2512.19933", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19933", "abs": "https://arxiv.org/abs/2512.19933", "authors": ["Zhixiang Lu", "Xueyuan Deng", "Yiran Liu", "Yulong Li", "Qiang Yan", "Imran Razzak", "Jionglong Su"], "title": "PRISM: A Personality-Driven Multi-Agent Framework for Social Media Simulation", "comment": null, "summary": "Traditional agent-based models (ABMs) of opinion dynamics often fail to capture the psychological heterogeneity driving online polarization due to simplistic homogeneity assumptions. This limitation obscures the critical interplay between individual cognitive biases and information propagation, thereby hindering a mechanistic understanding of how ideological divides are amplified. To address this challenge, we introduce the Personality-Refracted Intelligent Simulation Model (PRISM), a hybrid framework coupling stochastic differential equations (SDE) for continuous emotional evolution with a personality-conditional partially observable Markov decision process (PC-POMDP) for discrete decision-making. In contrast to continuous trait approaches, PRISM assigns distinct Myers-Briggs Type Indicator (MBTI) based cognitive policies to multimodal large language model (MLLM) agents, initialized via data-driven priors from large-scale social media datasets. PRISM achieves superior personality consistency aligned with human ground truth, significantly outperforming standard homogeneous and Big Five benchmarks. This framework effectively replicates emergent phenomena such as rational suppression and affective resonance, offering a robust tool for analyzing complex social media ecosystems.", "AI": {"tldr": "\u63d0\u51fa\u4e86PRISM\u6a21\u578b\uff0c\u7ed3\u5408\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u548c\u57fa\u4e8eMBTI\u7684\u4eba\u683c\u6761\u4ef6\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u7528\u4e8e\u6a21\u62df\u5728\u7ebf\u610f\u89c1\u52a8\u6001\u4e2d\u7684\u5fc3\u7406\u5f02\u8d28\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4ee3\u7406\u7684\u6a21\u578b\u5728\u6a21\u62df\u5728\u7ebf\u6781\u5316\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u4e3a\u5b83\u4eec\u901a\u5e38\u5047\u8bbe\u540c\u8d28\u6027\uff0c\u5ffd\u7565\u4e86\u9a71\u52a8\u6781\u5316\u7684\u5fc3\u7406\u5f02\u8d28\u6027\u3002\u8fd9\u79cd\u7b80\u5316\u963b\u788d\u4e86\u5bf9\u4e2a\u4f53\u8ba4\u77e5\u504f\u89c1\u4e0e\u4fe1\u606f\u4f20\u64ad\u4e4b\u95f4\u76f8\u4e92\u4f5c\u7528\u7684\u7406\u89e3\uff0c\u4ece\u800c\u96be\u4ee5\u4ece\u673a\u5236\u4e0a\u7406\u89e3\u610f\u8bc6\u5f62\u6001\u5206\u6b67\u5982\u4f55\u88ab\u653e\u5927\u3002", "method": "\u63d0\u51fa\u4e86\u4eba\u683c\u6298\u5c04\u667a\u80fd\u4eff\u771f\u6a21\u578b\uff08PRISM\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u6df7\u5408\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff08SDE\uff09\u6a21\u62df\u8fde\u7eed\u7684\u60c5\u7eea\u6f14\u5316\uff1b2\uff09\u4f7f\u7528\u57fa\u4e8e\u4eba\u683c\u6761\u4ef6\u7684\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08PC-POMDP\uff09\u6a21\u62df\u79bb\u6563\u51b3\u7b56\u3002\u4e0e\u8fde\u7eed\u7279\u8d28\u65b9\u6cd5\u4e0d\u540c\uff0cPRISM\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5206\u914d\u4e86\u57fa\u4e8eMBTI\u7684\u8ba4\u77e5\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u4ece\u5927\u89c4\u6a21\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u96c6\u83b7\u5f97\u7684\u6570\u636e\u9a71\u52a8\u5148\u9a8c\u8fdb\u884c\u521d\u59cb\u5316\u3002", "result": "PRISM\u5728\u4eba\u683c\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e0e\u4eba\u7c7b\u771f\u5b9e\u6570\u636e\u76f8\u7b26\uff0c\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u7684\u540c\u8d28\u6027\u6a21\u578b\u548cBig Five\u57fa\u51c6\u6a21\u578b\u3002\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u590d\u73b0\u7406\u6027\u6291\u5236\u548c\u60c5\u611f\u5171\u9e23\u7b49\u6d8c\u73b0\u73b0\u8c61\uff0c\u4e3a\u5206\u6790\u590d\u6742\u7684\u793e\u4ea4\u5a92\u4f53\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u5de5\u5177\u3002", "conclusion": "PRISM\u6846\u67b6\u901a\u8fc7\u6574\u5408\u8fde\u7eed\u60c5\u7eea\u6f14\u5316\u548c\u57fa\u4e8e\u4eba\u683c\u7684\u79bb\u6563\u51b3\u7b56\uff0c\u6210\u529f\u6355\u6349\u4e86\u5728\u7ebf\u610f\u89c1\u52a8\u6001\u4e2d\u7684\u5fc3\u7406\u5f02\u8d28\u6027\uff0c\u4e3a\u7406\u89e3\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u6781\u5316\u673a\u5236\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u5efa\u6a21\u5de5\u5177\uff0c\u5e76\u80fd\u6a21\u62df\u590d\u6742\u7684\u6d8c\u73b0\u793e\u4f1a\u73b0\u8c61\u3002"}}
{"id": "2512.20145", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.20145", "abs": "https://arxiv.org/abs/2512.20145", "authors": ["Xiang Chen", "Yixin Ou", "Quan Feng", "Lei Li", "Piji Li", "Haibo Ye", "Sheng-Jun Huang", "Shuofei Qiao", "Shumin Deng", "Huajun Chen", "Ningyu Zhang"], "title": "Retrieval-augmented Prompt Learning for Pre-trained Foundation Models", "comment": "IEEE/ACM Transactions on Audio, Speech and Language Processing", "summary": "The pre-trained foundation models (PFMs) have become essential for facilitating large-scale multimodal learning. Researchers have effectively employed the ``pre-train, prompt, and predict'' paradigm through prompt learning to induce improved few-shot performance. However, prompt learning approaches for PFMs still follow a parametric learning paradigm. As such, the stability of generalization in memorization and rote learning can be compromised. More specifically, conventional prompt learning might face difficulties in fully utilizing atypical instances and avoiding overfitting to shallow patterns with limited data during the process of fully-supervised training. To overcome these constraints, we present our approach, named RetroPrompt, which aims to achieve a balance between memorization and generalization by decoupling knowledge from mere memorization. Unlike traditional prompting methods, RetroPrompt leverages a publicly accessible knowledge base generated from the training data and incorporates a retrieval mechanism throughout the input, training, and inference stages. This enables the model to actively retrieve relevant contextual information from the corpus, thereby enhancing the available cues. We conduct comprehensive experiments on a variety of datasets across natural language processing and computer vision tasks to demonstrate the superior performance of our proposed approach, RetroPrompt, in both zero-shot and few-shot scenarios. Through detailed analysis of memorization patterns, we observe that RetroPrompt effectively reduces the reliance on rote memorization, leading to enhanced generalization.", "AI": {"tldr": "RetroPrompt\u662f\u4e00\u79cd\u65b0\u578b\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u7d22\u673a\u5236\u4ece\u77e5\u8bc6\u5e93\u4e2d\u83b7\u53d6\u76f8\u5173\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u51cf\u5c11\u6b7b\u8bb0\u786c\u80cc\u4f9d\u8d56\uff0c\u63d0\u5347\u6a21\u578b\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u4ecd\u9075\u5faa\u53c2\u6570\u5316\u5b66\u4e60\u8303\u5f0f\uff0c\u5728\u5b8c\u5168\u76d1\u7763\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u96be\u4ee5\u5145\u5206\u5229\u7528\u975e\u5178\u578b\u5b9e\u4f8b\uff0c\u5e76\u5bb9\u6613\u5bf9\u6d45\u5c42\u6a21\u5f0f\u8fc7\u62df\u5408\uff0c\u5bfc\u81f4\u8bb0\u5fc6\u4e0e\u6cdb\u5316\u4e4b\u95f4\u7684\u5e73\u8861\u4e0d\u4f73\u3002", "method": "\u63d0\u51faRetroPrompt\u65b9\u6cd5\uff0c\u5229\u7528\u8bad\u7ec3\u6570\u636e\u751f\u6210\u7684\u516c\u5f00\u77e5\u8bc6\u5e93\uff0c\u5728\u8f93\u5165\u3001\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u96c6\u6210\u68c0\u7d22\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u4e3b\u52a8\u4ece\u8bed\u6599\u5e93\u4e2d\u68c0\u7d22\u76f8\u5173\u4e0a\u4e0b\u6587\u4fe1\u606f\u6765\u589e\u5f3a\u53ef\u7528\u7ebf\u7d22\u3002", "result": "\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u7684\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5168\u9762\u5b9e\u9a8c\uff0c\u8bc1\u660eRetroPrompt\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u573a\u666f\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u5bf9\u6b7b\u8bb0\u786c\u80cc\u7684\u4f9d\u8d56\u3002", "conclusion": "RetroPrompt\u901a\u8fc7\u89e3\u8026\u77e5\u8bc6\u548c\u8bb0\u5fc6\uff0c\u5b9e\u73b0\u4e86\u8bb0\u5fc6\u4e0e\u6cdb\u5316\u4e4b\u95f4\u7684\u66f4\u597d\u5e73\u8861\uff0c\u4e3a\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u7684\u63d0\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u8303\u5f0f\u3002"}}
{"id": "2512.19950", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.19950", "abs": "https://arxiv.org/abs/2512.19950", "authors": ["Heet Bodara", "Md Masum Mushfiq", "Isma Farah Siddiqui"], "title": "Bias Beneath the Tone: Empirical Characterisation of Tone Bias in LLM-Driven UX Systems", "comment": null, "summary": "Large Language Models are increasingly used in conversational systems such as digital personal assistants, shaping how people interact with technology through language. While their responses often sound fluent and natural, they can also carry subtle tone biases such as sounding overly polite, cheerful, or cautious even when neutrality is expected. These tendencies can influence how users perceive trust, empathy, and fairness in dialogue. In this study, we explore tone bias as a hidden behavioral trait of large language models. The novelty of this research lies in the integration of controllable large language model based dialogue synthesis with tone classification models, enabling robust and ethical emotion recognition in personal assistant interactions. We created two synthetic dialogue datasets, one generated from neutral prompts and another explicitly guided to produce positive or negative tones. Surprisingly, even the neutral set showed consistent tonal skew, suggesting that bias may stem from the model's underlying conversational style. Using weak supervision through a pretrained DistilBERT model, we labeled tones and trained several classifiers to detect these patterns. Ensemble models achieved macro F1 scores up to 0.92, showing that tone bias is systematic, measurable, and relevant to designing fair and trustworthy conversational AI.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53ef\u63a7LLM\u5bf9\u8bdd\u751f\u6210\u4e0e\u8bed\u8c03\u5206\u7c7b\u7684\u65b9\u6cd5\uff0c\u7cfb\u7edf\u6027\u68c0\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u8bdd\u4e2d\u9690\u542b\u7684\u8bed\u8c03\u504f\u89c1\uff0c\u53d1\u73b0\u5373\u4f7f\u4e2d\u6027\u63d0\u793a\u4e5f\u4f1a\u4ea7\u751f\u7cfb\u7edf\u6027\u8bed\u8c03\u504f\u5dee\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5176\u56de\u5e94\u5e38\u5e26\u6709\u5fae\u5999\u7684\u8bed\u8c03\u504f\u89c1\uff08\u5982\u8fc7\u5ea6\u793c\u8c8c\u3001\u6109\u60a6\u6216\u8c28\u614e\uff09\uff0c\u8fd9\u4e9b\u504f\u89c1\u4f1a\u5f71\u54cd\u7528\u6237\u5bf9\u4fe1\u4efb\u3001\u540c\u7406\u5fc3\u548c\u516c\u5e73\u6027\u7684\u611f\u77e5\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u68c0\u6d4b\u3002", "method": "1) \u521b\u5efa\u4e24\u4e2a\u5408\u6210\u5bf9\u8bdd\u6570\u636e\u96c6\uff1a\u4e2d\u6027\u63d0\u793a\u751f\u6210\u7684\u6570\u636e\u96c6\u548c\u660e\u786e\u5f15\u5bfc\u751f\u6210\u79ef\u6781/\u6d88\u6781\u8bed\u8c03\u7684\u6570\u636e\u96c6\uff1b2) \u4f7f\u7528\u9884\u8bad\u7ec3\u7684DistilBERT\u6a21\u578b\u8fdb\u884c\u5f31\u76d1\u7763\u6807\u6ce8\uff1b3) \u8bad\u7ec3\u591a\u4e2a\u5206\u7c7b\u5668\u68c0\u6d4b\u8bed\u8c03\u6a21\u5f0f\uff1b4) \u91c7\u7528\u96c6\u6210\u6a21\u578b\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "result": "\u5373\u4f7f\u4e2d\u6027\u6570\u636e\u96c6\u4e5f\u663e\u793a\u51fa\u4e00\u81f4\u7684\u8bed\u8c03\u504f\u5dee\uff0c\u8868\u660e\u504f\u89c1\u6e90\u4e8e\u6a21\u578b\u5e95\u5c42\u5bf9\u8bdd\u98ce\u683c\u3002\u96c6\u6210\u6a21\u578b\u5728\u8bed\u8c03\u68c0\u6d4b\u4e0a\u8fbe\u52300.92\u7684\u5b8f\u89c2F1\u5206\u6570\uff0c\u8bc1\u660e\u8bed\u8c03\u504f\u89c1\u662f\u7cfb\u7edf\u6027\u3001\u53ef\u6d4b\u91cf\u4e14\u4e0e\u8bbe\u8ba1\u516c\u5e73\u53ef\u4fe1\u5bf9\u8bddAI\u76f8\u5173\u7684\u91cd\u8981\u95ee\u9898\u3002", "conclusion": "\u8bed\u8c03\u504f\u89c1\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9690\u85cf\u884c\u4e3a\u7279\u5f81\uff0c\u53ef\u901a\u8fc7\u53ef\u63a7\u5bf9\u8bdd\u5408\u6210\u4e0e\u8bed\u8c03\u5206\u7c7b\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u8fdb\u884c\u7cfb\u7edf\u6027\u68c0\u6d4b\u3002\u8fd9\u4e00\u53d1\u73b0\u5bf9\u8bbe\u8ba1\u516c\u5e73\u3001\u53ef\u4fe1\u8d56\u7684\u5bf9\u8bddAI\u7cfb\u7edf\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2512.19995", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.19995", "abs": "https://arxiv.org/abs/2512.19995", "authors": ["Ming Li", "Chenrui Fan", "Yize Cheng", "Soheil Feizi", "Tianyi Zhou"], "title": "Schoenfeld's Anatomy of Mathematical Reasoning by Language Models", "comment": null, "summary": "Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faThinkARM\u6846\u67b6\uff0c\u4f7f\u7528Schoenfeld\u7684Episode\u7406\u8bba\u5c06LLM\u63a8\u7406\u8f68\u8ff9\u62bd\u8c61\u4e3a\u529f\u80fd\u6b65\u9aa4\uff0c\u63ed\u793a\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u601d\u7ef4\u52a8\u6001\u548c\u7ed3\u6784\u5dee\u5f02\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u5c55\u793a\u63a8\u7406\u8f68\u8ff9\uff0c\u4f46\u5176\u5e95\u5c42\u7684\u8ba4\u77e5\u7ed3\u6784\u548c\u6b65\u9aa4\u96be\u4ee5\u901a\u8fc7\u8868\u5c42\u7edf\u8ba1\u6765\u8bc6\u522b\u548c\u5206\u6790\u3002\u9700\u8981\u4e00\u79cd\u4e2d\u95f4\u5c3a\u5ea6\u7684\u6846\u67b6\u6765\u660e\u786e\u62bd\u8c61\u63a8\u7406\u6b65\u9aa4\u3002", "method": "\u91c7\u7528Schoenfeld\u7684Episode\u7406\u8bba\u4f5c\u4e3a\u5f52\u7eb3\u6027\u4e2d\u95f4\u5c3a\u5ea6\u89c6\u89d2\uff0c\u63d0\u51faThinkARM\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u8f68\u8ff9\u660e\u786e\u62bd\u8c61\u4e3a\u5206\u6790\u3001\u63a2\u7d22\u3001\u5b9e\u65bd\u3001\u9a8c\u8bc1\u7b49\u529f\u80fd\u6027\u63a8\u7406\u6b65\u9aa4\u3002", "result": "\u5e94\u7528\u4e8e\u4e0d\u540c\u6a21\u578b\u7684\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u65f6\uff0c\u8be5\u62bd\u8c61\u63ed\u793a\u4e86\u53ef\u91cd\u590d\u7684\u601d\u7ef4\u52a8\u6001\u548c\u63a8\u7406\u6a21\u578b\u4e0e\u975e\u63a8\u7406\u6a21\u578b\u4e4b\u95f4\u7684\u7ed3\u6784\u5dee\u5f02\uff0c\u8fd9\u4e9b\u5dee\u5f02\u5728token\u7ea7\u89c6\u56fe\u4e2d\u4e0d\u660e\u663e\u3002\u8bca\u65ad\u6848\u4f8b\u7814\u7a76\u663e\u793a\u63a2\u7d22\u529f\u80fd\u662f\u5173\u952e\u5206\u652f\u6b65\u9aa4\u4e0e\u6b63\u786e\u6027\u76f8\u5173\uff0c\u6548\u7387\u5bfc\u5411\u65b9\u6cd5\u9009\u62e9\u6027\u5730\u6291\u5236\u8bc4\u4f30\u53cd\u9988\u6b65\u9aa4\u800c\u975e\u5747\u5300\u7f29\u77ed\u54cd\u5e94\u3002", "conclusion": "Episode\u7ea7\u8868\u793a\u4f7f\u63a8\u7406\u6b65\u9aa4\u660e\u786e\u5316\uff0c\u80fd\u591f\u7cfb\u7edf\u5206\u6790\u73b0\u4ee3\u8bed\u8a00\u6a21\u578b\u4e2d\u63a8\u7406\u7684\u7ed3\u6784\u5316\u3001\u7a33\u5b9a\u5316\u548c\u53d8\u5316\u65b9\u5f0f\u3002"}}
{"id": "2512.20092", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20092", "abs": "https://arxiv.org/abs/2512.20092", "authors": ["Yiming Du", "Baojun Wang", "Yifan Xiang", "Zhaowei Wang", "Wenyu Huang", "Boyang Xue", "Bin Liang", "Xingshan Zeng", "Fei Mi", "Haoli Bai", "Lifeng Shang", "Jeff Z. Pan", "Yuxin Jiang", "Kam-Fai Wong"], "title": "Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents", "comment": null, "summary": "Temporal reasoning over long, multi-session dialogues is a critical capability for conversational agents. However, existing works and our pilot study have shown that as dialogue histories grow in length and accumulate noise, current long-context models struggle to accurately identify temporally pertinent information, significantly impairing reasoning performance. To address this, we introduce Memory-T1, a framework that learns a time-aware memory selection policy using reinforcement learning (RL). It employs a coarse-to-fine strategy, first pruning the dialogue history into a candidate set using temporal and relevance filters, followed by an RL agent that selects the precise evidence sessions. The RL training is guided by a multi-level reward function optimizing (i) answer accuracy, (ii) evidence grounding, and (iii) temporal consistency. In particular, the temporal consistency reward provides a dense signal by evaluating alignment with the query time scope at both the session-level (chronological proximity) and the utterance-level (chronological fidelity), enabling the agent to resolve subtle chronological ambiguities. On the Time-Dialog benchmark, Memory-T1 boosts a 7B model to an overall score of 67.0\\%, establishing a new state-of-the-art performance for open-source models and outperforming a 14B baseline by 10.2\\%. Ablation studies show temporal consistency and evidence grounding rewards jointly contribute to a 15.0\\% performance gain. Moreover, Memory-T1 maintains robustness up to 128k tokens, where baseline models collapse, proving effectiveness against noise in extensive dialogue histories. The code and datasets are publicly available at https://github.com/Elvin-Yiming-Du/Memory-T1/", "AI": {"tldr": "Memory-T1\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u65f6\u95f4\u611f\u77e5\u8bb0\u5fc6\u9009\u62e9\u7b56\u7565\uff0c\u901a\u8fc7\u7c97\u5230\u7ec6\u7684\u65b9\u6cd5\u4ece\u957f\u5bf9\u8bdd\u5386\u53f2\u4e2d\u7cbe\u786e\u9009\u62e9\u8bc1\u636e\u4f1a\u8bdd\uff0c\u663e\u8457\u63d0\u5347\u65f6\u5e8f\u63a8\u7406\u6027\u80fd", "motivation": "\u73b0\u6709\u957f\u4e0a\u4e0b\u6587\u6a21\u578b\u5728\u5904\u7406\u5197\u957f\u3001\u591a\u4f1a\u8bdd\u5bf9\u8bdd\u65f6\uff0c\u96be\u4ee5\u51c6\u786e\u8bc6\u522b\u65f6\u5e8f\u76f8\u5173\u4fe1\u606f\uff0c\u5bfc\u81f4\u63a8\u7406\u6027\u80fd\u663e\u8457\u4e0b\u964d", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u9996\u5148\u4f7f\u7528\u65f6\u5e8f\u548c\u76f8\u5173\u6027\u8fc7\u6ee4\u5668\u5c06\u5bf9\u8bdd\u5386\u53f2\u4fee\u526a\u4e3a\u5019\u9009\u96c6\uff0c\u7136\u540e\u7531RL\u4ee3\u7406\u9009\u62e9\u7cbe\u786e\u7684\u8bc1\u636e\u4f1a\u8bdd\u3002RL\u8bad\u7ec3\u91c7\u7528\u591a\u7ea7\u5956\u52b1\u51fd\u6570\u4f18\u5316\u7b54\u6848\u51c6\u786e\u6027\u3001\u8bc1\u636e\u57fa\u7840\u548c\u65f6\u5e8f\u4e00\u81f4\u6027", "result": "\u5728Time-Dialog\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMemory-T1\u5c067B\u6a21\u578b\u7684\u603b\u4f53\u5f97\u5206\u63d0\u5347\u81f367.0%\uff0c\u521b\u4e0b\u5f00\u6e90\u6a21\u578b\u7684\u65b0SOTA\uff0c\u4f18\u4e8e14B\u57fa\u7ebf10.2%\u3002\u5728128k token\u7684\u957f\u5bf9\u8bdd\u4e2d\u4ecd\u4fdd\u6301\u9c81\u68d2\u6027", "conclusion": "Memory-T1\u901a\u8fc7\u65f6\u95f4\u611f\u77e5\u8bb0\u5fc6\u9009\u62e9\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u957f\u5bf9\u8bdd\u65f6\u5e8f\u63a8\u7406\u95ee\u9898\uff0c\u65f6\u5e8f\u4e00\u81f4\u6027\u548c\u8bc1\u636e\u57fa\u7840\u5956\u52b1\u5171\u540c\u8d21\u732e\u4e8615.0%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u6846\u67b6\u5728\u566a\u58f0\u957f\u5bf9\u8bdd\u5386\u53f2\u4e2d\u7684\u6709\u6548\u6027"}}
{"id": "2512.20097", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20097", "abs": "https://arxiv.org/abs/2512.20097", "authors": ["Zuo Wang", "Ye Yuan"], "title": "A Novel Graph-Sequence Learning Model for Inductive Text Classification", "comment": null, "summary": "Text classification plays an important role in various downstream text-related tasks, such as sentiment analysis, fake news detection, and public opinion analysis. Recently, text classification based on Graph Neural Networks (GNNs) has made significant progress due to their strong capabilities of structural relationship learning. However, these approaches still face two major limitations. First, these approaches fail to fully consider the diverse structural information across word pairs, e.g., co-occurrence, syntax, and semantics. Furthermore, they neglect sequence information in the text graph structure information learning module and can not classify texts with new words and relations. In this paper, we propose a Novel Graph-Sequence Learning Model for Inductive Text Classification (TextGSL) to address the previously mentioned issues. More specifically, we construct a single text-level graph for all words in each text and establish different edge types based on the diverse relationships between word pairs. Building upon this, we design an adaptive multi-edge message-passing paradigm to aggregate diverse structural information between word pairs. Additionally, sequential information among text data can be captured by the proposed TextGSL through the incorporation of Transformer layers. Therefore, TextGSL can learn more discriminative text representations. TextGSL has been comprehensively compared with several strong baselines. The experimental results on diverse benchmarking datasets demonstrate that TextGSL outperforms these baselines in terms of accuracy.", "AI": {"tldr": "TextGSL\u662f\u4e00\u79cd\u65b0\u9896\u7684\u56fe-\u5e8f\u5217\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u5f52\u7eb3\u5f0f\u6587\u672c\u5206\u7c7b\uff0c\u901a\u8fc7\u6784\u5efa\u591a\u8fb9\u7c7b\u578b\u6587\u672c\u56fe\u5e76\u7ed3\u5408Transformer\u5c42\uff0c\u6709\u6548\u878d\u5408\u591a\u6837\u7ed3\u6784\u4fe1\u606f\u548c\u5e8f\u5217\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u6587\u672c\u5206\u7c7b\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff1a1) \u672a\u80fd\u5145\u5206\u8003\u8651\u8bcd\u5bf9\u95f4\u7684\u591a\u6837\u5316\u7ed3\u6784\u4fe1\u606f\uff08\u5982\u5171\u73b0\u3001\u53e5\u6cd5\u3001\u8bed\u4e49\uff09\uff1b2) \u5728\u6587\u672c\u56fe\u7ed3\u6784\u5b66\u4e60\u6a21\u5757\u4e2d\u5ffd\u7565\u4e86\u5e8f\u5217\u4fe1\u606f\uff0c\u4e14\u65e0\u6cd5\u5904\u7406\u5305\u542b\u65b0\u8bcd\u548c\u65b0\u5173\u7cfb\u7684\u6587\u672c\u3002", "method": "\u63d0\u51faTextGSL\u6a21\u578b\uff1a1) \u4e3a\u6bcf\u4e2a\u6587\u672c\u6784\u5efa\u5355\u6587\u672c\u7ea7\u56fe\uff0c\u57fa\u4e8e\u8bcd\u5bf9\u95f4\u7684\u591a\u6837\u5316\u5173\u7cfb\u5efa\u7acb\u4e0d\u540c\u7684\u8fb9\u7c7b\u578b\uff1b2) \u8bbe\u8ba1\u81ea\u9002\u5e94\u591a\u8fb9\u6d88\u606f\u4f20\u9012\u8303\u5f0f\u6765\u805a\u5408\u8bcd\u5bf9\u95f4\u7684\u591a\u6837\u7ed3\u6784\u4fe1\u606f\uff1b3) \u901a\u8fc7\u52a0\u5165Transformer\u5c42\u6765\u6355\u6349\u6587\u672c\u6570\u636e\u7684\u5e8f\u5217\u4fe1\u606f\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTextGSL\u5728\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u591a\u4e2a\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "TextGSL\u901a\u8fc7\u540c\u65f6\u8003\u8651\u591a\u6837\u5316\u7684\u7ed3\u6784\u4fe1\u606f\u548c\u5e8f\u5217\u4fe1\u606f\uff0c\u80fd\u591f\u5b66\u4e60\u66f4\u5177\u533a\u5206\u6027\u7684\u6587\u672c\u8868\u793a\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\u6587\u672c\u5206\u7c7b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2512.20111", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.20111", "abs": "https://arxiv.org/abs/2512.20111", "authors": ["Aly Lidayan", "Jakob Bjorner", "Satvik Golechha", "Kartik Goyal", "Alane Suhr"], "title": "ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language", "comment": null, "summary": "As the length of sequential decision-making tasks increases, it becomes computationally impractical to keep full interaction histories in context. We introduce a general framework for LLM agents to maintain concise contexts through multi-step interaction: Acting through Belief Bottlenecks Expressed in Language (ABBEL), and methods to further improve ABBEL agents with RL post-training. ABBEL replaces long multi-step interaction history by a belief state, i.e., a natural language summary of what has been discovered about task-relevant unknowns. Under ABBEL, at each step the agent first updates a prior belief with the most recent observation from the environment to form a posterior belief, then uses only the posterior to select an action. We systematically evaluate frontier models under ABBEL across six diverse multi-step environments, finding that ABBEL supports generating interpretable beliefs while maintaining near-constant memory use over interaction steps. However, bottleneck approaches are generally prone to error propagation, which we observe causing inferior performance when compared to the full context setting due to errors in belief updating. Therefore, we train LLMs to generate and act on beliefs within the ABBEL framework via reinforcement learning (RL). We experiment with belief grading, to reward higher quality beliefs, as well as belief length penalties to reward more compressed beliefs. Our experiments demonstrate the ability of RL to improve ABBEL's performance beyond the full context setting, while using less memory than contemporaneous approaches.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.20136", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20136", "abs": "https://arxiv.org/abs/2512.20136", "authors": ["Hyeongcheol Park", "Jiyoung Seo", "Jaewon Mun", "Hogun Park", "Wonmin Byeon", "Sung June Kim", "Hyeonsoo Im", "JeungSub Lee", "Sangpil Kim"], "title": "M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has recently been extended to multimodal settings, connecting multimodal large language models (MLLMs) with vast corpora of external knowledge such as multimodal knowledge graphs (MMKGs). Despite their recent success, multimodal RAG in the audio-visual domain remains challenging due to 1) limited modality coverage and multi-hop connectivity of existing MMKGs, and 2) retrieval based solely on similarity in a shared multimodal embedding space, which fails to filter out off-topic or redundant knowledge. To address these limitations, we propose M$^3$KG-RAG, a Multi-hop Multimodal Knowledge Graph-enhanced RAG that retrieves query-aligned audio-visual knowledge from MMKGs, improving reasoning depth and answer faithfulness in MLLMs. Specifically, we devise a lightweight multi-agent pipeline to construct multi-hop MMKG (M$^3$KG), which contains context-enriched triplets of multimodal entities, enabling modality-wise retrieval based on input queries. Furthermore, we introduce GRASP (Grounded Retrieval And Selective Pruning), which ensures precise entity grounding to the query, evaluates answer-supporting relevance, and prunes redundant context to retain only knowledge essential for response generation. Extensive experiments across diverse multimodal benchmarks demonstrate that M$^3$KG-RAG significantly enhances MLLMs' multimodal reasoning and grounding over existing approaches.", "AI": {"tldr": "M\u00b3KG-RAG\uff1a\u4e00\u4e2a\u591a\u8df3\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u591a\u8df3MMKG\u548cGRASP\u673a\u5236\uff0c\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u97f3\u9891-\u89c6\u89c9\u9886\u57df\u7684\u63a8\u7406\u6df1\u5ea6\u548c\u7b54\u6848\u5fe0\u5b9e\u5ea6\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001RAG\u5728\u97f3\u9891-\u89c6\u89c9\u9886\u57df\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a1\uff09\u73b0\u6709MMKG\u6a21\u6001\u8986\u76d6\u6709\u9650\u4e14\u7f3a\u4e4f\u591a\u8df3\u8fde\u63a5\u6027\uff1b2\uff09\u4ec5\u57fa\u4e8e\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u7684\u76f8\u4f3c\u6027\u68c0\u7d22\u65e0\u6cd5\u8fc7\u6ee4\u65e0\u5173\u6216\u5197\u4f59\u77e5\u8bc6\u3002", "method": "\u63d0\u51faM\u00b3KG-RAG\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u8f7b\u91cf\u7ea7\u591a\u667a\u80fd\u4f53\u6d41\u6c34\u7ebf\u6784\u5efa\u591a\u8df3MMKG\uff08M\u00b3KG\uff09\uff0c\u5305\u542b\u4e30\u5bcc\u7684\u591a\u6a21\u6001\u5b9e\u4f53\u4e09\u5143\u7ec4\uff1b2\uff09\u5f15\u5165GRASP\u673a\u5236\uff0c\u5b9e\u73b0\u67e5\u8be2\u7684\u7cbe\u786e\u5b9e\u4f53\u63a5\u5730\u3001\u7b54\u6848\u652f\u6301\u76f8\u5173\u6027\u8bc4\u4f30\u548c\u5197\u4f59\u4e0a\u4e0b\u6587\u526a\u679d\u3002", "result": "\u5728\u591a\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cM\u00b3KG-RAG\u663e\u8457\u63d0\u5347\u4e86MLLMs\u7684\u591a\u6a21\u6001\u63a8\u7406\u548c\u63a5\u5730\u80fd\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "M\u00b3KG-RAG\u901a\u8fc7\u6784\u5efa\u591a\u8df3\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u548c\u5f15\u5165GRASP\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001RAG\u5728\u97f3\u9891-\u89c6\u89c9\u9886\u57df\u7684\u6311\u6218\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u548c\u7b54\u6848\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2512.20144", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20144", "abs": "https://arxiv.org/abs/2512.20144", "authors": ["Yuxin Wang", "Shicheng Fang", "Bo Wang", "Qi Luo", "Xuanjing Huang", "Yining Zheng", "Xipeng Qiu"], "title": "Multi-hop Reasoning via Early Knowledge Alignment", "comment": "16 pages", "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for Large Language Models (LLMs) to address knowledge-intensive queries requiring domain-specific or up-to-date information. To handle complex multi-hop questions that are challenging for single-step retrieval, iterative RAG approaches incorporating reinforcement learning have been proposed. However, existing iterative RAG systems typically plan to decompose questions without leveraging information about the available retrieval corpus, leading to inefficient retrieval and reasoning chains that cascade into suboptimal performance. In this paper, we introduce Early Knowledge Alignment (EKA), a simple but effective module that aligns LLMs with retrieval set before planning in iterative RAG systems with contextually relevant retrieved knowledge. Extensive experiments on six standard RAG datasets demonstrate that by establishing a stronger reasoning foundation, EKA significantly improves retrieval precision, reduces cascading errors, and enhances both performance and efficiency. Our analysis from an entropy perspective demonstrate that incorporating early knowledge reduces unnecessary exploration during the reasoning process, enabling the model to focus more effectively on relevant information subsets. Moreover, EKA proves effective as a versatile, training-free inference strategy that scales seamlessly to large models. Generalization tests across diverse datasets and retrieval corpora confirm the robustness of our approach. Overall, EKA advances the state-of-the-art in iterative RAG systems while illuminating the critical interplay between structured reasoning and efficient exploration in reinforcement learning-augmented frameworks. The code is released at \\href{https://github.com/yxzwang/EarlyKnowledgeAlignment}{Github}.", "AI": {"tldr": "EKA\uff08\u65e9\u671f\u77e5\u8bc6\u5bf9\u9f50\uff09\u901a\u8fc7\u5728\u8fed\u4ee3RAG\u7cfb\u7edf\u4e2d\u5f15\u5165\u68c0\u7d22\u77e5\u8bc6\u5bf9\u9f50\u6a21\u5757\uff0c\u5728\u89c4\u5212\u524d\u5c06LLM\u4e0e\u68c0\u7d22\u96c6\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u591a\u8df3\u95ee\u7b54\u7684\u68c0\u7d22\u7cbe\u5ea6\u548c\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u8fed\u4ee3RAG\u7cfb\u7edf\u5728\u89c4\u5212\u95ee\u9898\u5206\u89e3\u65f6\u672a\u5145\u5206\u5229\u7528\u68c0\u7d22\u8bed\u6599\u5e93\u4fe1\u606f\uff0c\u5bfc\u81f4\u68c0\u7d22\u6548\u7387\u4f4e\u4e0b\u548c\u63a8\u7406\u94fe\u9519\u8bef\u7ea7\u8054\uff0c\u5f71\u54cd\u591a\u8df3\u95ee\u7b54\u6027\u80fd\u3002", "method": "\u63d0\u51faEarly Knowledge Alignment (EKA)\u6a21\u5757\uff0c\u5728\u89c4\u5212\u524d\u5c06LLM\u4e0e\u68c0\u7d22\u8bed\u6599\u5e93\u77e5\u8bc6\u5bf9\u9f50\uff0c\u5229\u7528\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u68c0\u7d22\u77e5\u8bc6\u5efa\u7acb\u66f4\u5f3a\u7684\u63a8\u7406\u57fa\u7840\u3002", "result": "\u5728\u516d\u4e2a\u6807\u51c6RAG\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEKA\u663e\u8457\u63d0\u5347\u68c0\u7d22\u7cbe\u5ea6\u3001\u51cf\u5c11\u7ea7\u8054\u9519\u8bef\uff0c\u540c\u65f6\u63d0\u9ad8\u6027\u80fd\u548c\u6548\u7387\u3002\u4ece\u71b5\u7684\u89d2\u5ea6\u5206\u6790\u663e\u793a\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u63a2\u7d22\u3002", "conclusion": "EKA\u4f5c\u4e3a\u65e0\u9700\u8bad\u7ec3\u7684\u63a8\u7406\u7b56\u7565\uff0c\u53ef\u6269\u5c55\u5230\u5927\u578b\u6a21\u578b\uff0c\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u68c0\u7d22\u8bed\u6599\u5e93\u4e0a\u8868\u73b0\u51fa\u7a33\u5065\u6027\uff0c\u63a8\u52a8\u4e86\u8fed\u4ee3RAG\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.20156", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.20156", "abs": "https://arxiv.org/abs/2512.20156", "authors": ["Qian Chen", "Luyao Cheng", "Chong Deng", "Xiangang Li", "Jiaqing Liu", "Chao-Hong Tan", "Wen Wang", "Junhao Xu", "Jieping Ye", "Qinglin Zhang", "Qiquan Zhang", "Jingren Zhou"], "title": "Fun-Audio-Chat Technical Report", "comment": "21 pages, https://github.com/FunAudioLLM/Fun-Audio-Chat", "summary": "Recent advancements in joint speech-text models show great potential for seamless voice interactions. However, existing models face critical challenges: temporal resolution mismatch between speech tokens (25Hz) and text tokens (~3Hz) dilutes semantic information, incurs high computational costs, and causes catastrophic forgetting of text LLM knowledge. We introduce Fun-Audio-Chat, a Large Audio Language Model addressing these limitations via two innovations from our previous work DrVoice. First, Dual-Resolution Speech Representations (DRSR): the Shared LLM processes audio at efficient 5Hz (via token grouping), while the Speech Refined Head generates high-quality tokens at 25Hz, balancing efficiency (~50% GPU reduction) and quality. Second, Core-Cocktail Training, a two-stage fine-tuning with intermediate merging that mitigates catastrophic forgetting. We then apply Multi-Task DPO Training to enhance robustness, audio understanding, instruction-following and voice empathy. This multi-stage post-training enables Fun-Audio-Chat to retain text LLM knowledge while gaining powerful audio understanding, reasoning, and generation. Unlike recent LALMs requiring large-scale audio-text pre-training, Fun-Audio-Chat leverages pre-trained models and extensive post-training. Fun-Audio-Chat 8B and MoE 30B-A3B achieve competitive performance on Speech-to-Text and Speech-to-Speech tasks, ranking top among similar-scale models on Spoken QA benchmarks. They also achieve competitive to superior performance on Audio Understanding, Speech Function Calling, Instruction-Following and Voice Empathy. We develop Fun-Audio-Chat-Duplex, a full-duplex variant with strong performance on Spoken QA and full-duplex interactions. We open-source Fun-Audio-Chat-8B with training and inference code, and provide an interactive demo.", "AI": {"tldr": "Fun-Audio-Chat\u662f\u4e00\u4e2a\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u5206\u8fa8\u7387\u8bed\u97f3\u8868\u793a\u548c\u6838\u5fc3\u9e21\u5c3e\u9152\u8bad\u7ec3\u89e3\u51b3\u4e86\u73b0\u6709\u8054\u5408\u8bed\u97f3\u6587\u672c\u6a21\u578b\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u4fdd\u6301\u6587\u672cLLM\u77e5\u8bc6\u7684\u540c\u65f6\u83b7\u5f97\u5f3a\u5927\u7684\u97f3\u9891\u7406\u89e3\u3001\u63a8\u7406\u548c\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8054\u5408\u8bed\u97f3\u6587\u672c\u6a21\u578b\u9762\u4e34\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a1\uff09\u8bed\u97f3\u6807\u8bb0\uff0825Hz\uff09\u548c\u6587\u672c\u6807\u8bb0\uff08~3Hz\uff09\u4e4b\u95f4\u7684\u65f6\u95f4\u5206\u8fa8\u7387\u4e0d\u5339\u914d\u4f1a\u7a00\u91ca\u8bed\u4e49\u4fe1\u606f\uff1b2\uff09\u9ad8\u8ba1\u7b97\u6210\u672c\uff1b3\uff09\u5bfc\u81f4\u6587\u672cLLM\u77e5\u8bc6\u7684\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "1\uff09\u53cc\u5206\u8fa8\u7387\u8bed\u97f3\u8868\u793a\uff1a\u5171\u4eabLLM\u4ee5\u9ad8\u6548\u76845Hz\u5904\u7406\u97f3\u9891\uff08\u901a\u8fc7\u6807\u8bb0\u5206\u7ec4\uff09\uff0c\u800c\u8bed\u97f3\u7cbe\u70bc\u5934\u4ee525Hz\u751f\u6210\u9ad8\u8d28\u91cf\u6807\u8bb0\uff0c\u5e73\u8861\u6548\u7387\u548c\u8d28\u91cf\uff1b2\uff09\u6838\u5fc3\u9e21\u5c3e\u9152\u8bad\u7ec3\uff1a\u4e24\u9636\u6bb5\u5fae\u8c03\u4e0e\u4e2d\u95f4\u5408\u5e76\uff0c\u51cf\u8f7b\u707e\u96be\u6027\u9057\u5fd8\uff1b3\uff09\u591a\u4efb\u52a1DPO\u8bad\u7ec3\uff1a\u589e\u5f3a\u9c81\u68d2\u6027\u3001\u97f3\u9891\u7406\u89e3\u3001\u6307\u4ee4\u8ddf\u968f\u548c\u8bed\u97f3\u5171\u60c5\u3002", "result": "Fun-Audio-Chat 8B\u548cMoE 30B-A3B\u5728\u8bed\u97f3\u8f6c\u6587\u672c\u548c\u8bed\u97f3\u8f6c\u8bed\u97f3\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u53e3\u8bed\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5728\u7c7b\u4f3c\u89c4\u6a21\u6a21\u578b\u4e2d\u6392\u540d\u9760\u524d\u3002\u5728\u97f3\u9891\u7406\u89e3\u3001\u8bed\u97f3\u529f\u80fd\u8c03\u7528\u3001\u6307\u4ee4\u8ddf\u968f\u548c\u8bed\u97f3\u5171\u60c5\u65b9\u9762\u8fbe\u5230\u7ade\u4e89\u6027\u751a\u81f3\u4f18\u8d8a\u6027\u80fd\u3002\u8fd8\u5f00\u53d1\u4e86\u5168\u53cc\u5de5\u53d8\u4f53Fun-Audio-Chat-Duplex\u3002", "conclusion": "Fun-Audio-Chat\u901a\u8fc7\u521b\u65b0\u7684\u53cc\u5206\u8fa8\u7387\u8bed\u97f3\u8868\u793a\u548c\u6838\u5fc3\u9e21\u5c3e\u9152\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u8054\u5408\u8bed\u97f3\u6587\u672c\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5728\u4fdd\u6301\u6587\u672cLLM\u77e5\u8bc6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u97f3\u9891\u80fd\u529b\uff0c\u4e14\u65e0\u9700\u5927\u89c4\u6a21\u97f3\u9891\u6587\u672c\u9884\u8bad\u7ec3\u3002"}}
{"id": "2512.20164", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20164", "abs": "https://arxiv.org/abs/2512.20164", "authors": ["Honglin Mu", "Jinghao Liu", "Kaiyang Wan", "Rui Xing", "Xiuying Chen", "Timothy Baldwin", "Wanxiang Che"], "title": "AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications", "comment": null, "summary": "Large Language Models (LLMs) excel at text comprehension and generation, making them ideal for automated tasks like code review and content moderation. However, our research identifies a vulnerability: LLMs can be manipulated by \"adversarial instructions\" hidden in input data, such as resumes or code, causing them to deviate from their intended task. Notably, while defenses may exist for mature domains such as code review, they are often absent in other common applications such as resume screening and peer review. This paper introduces a benchmark to assess this vulnerability in resume screening, revealing attack success rates exceeding 80% for certain attack types. We evaluate two defense mechanisms: prompt-based defenses achieve 10.1% attack reduction with 12.5% false rejection increase, while our proposed FIDS (Foreign Instruction Detection through Separation) using LoRA adaptation achieves 15.4% attack reduction with 10.4% false rejection increase. The combined approach provides 26.3% attack reduction, demonstrating that training-time defenses outperform inference-time mitigations in both security and utility preservation.", "AI": {"tldr": "LLMs\u5728\u7b80\u5386\u7b5b\u9009\u4e2d\u5b58\u5728\u5bf9\u6297\u6307\u4ee4\u653b\u51fb\u6f0f\u6d1e\uff0c\u653b\u51fb\u6210\u529f\u7387\u8d8580%\uff0c\u672c\u6587\u63d0\u51fa\u8bad\u7ec3\u65f6\u9632\u5fa1\u65b9\u6cd5FIDS\u4f18\u4e8e\u63a8\u7406\u65f6\u9632\u5fa1", "motivation": "LLMs\u5728\u4ee3\u7801\u5ba1\u67e5\u3001\u5185\u5bb9\u5ba1\u6838\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u5176\u5b58\u5728\u88ab\"\u5bf9\u6297\u6307\u4ee4\"\u64cd\u7eb5\u7684\u6f0f\u6d1e\uff0c\u7279\u522b\u662f\u5728\u7b80\u5386\u7b5b\u9009\u7b49\u6210\u719f\u9632\u5fa1\u673a\u5236\u7f3a\u5931\u7684\u9886\u57df\uff0c\u653b\u51fb\u8005\u53ef\u901a\u8fc7\u9690\u85cf\u6307\u4ee4\u4f7fLLM\u504f\u79bb\u539f\u4efb\u52a1", "method": "1) \u5efa\u7acb\u7b80\u5386\u7b5b\u9009\u9886\u57df\u7684\u5bf9\u6297\u6307\u4ee4\u653b\u51fb\u57fa\u51c6\u6d4b\u8bd5\uff1b2) \u8bc4\u4f30\u4e24\u79cd\u9632\u5fa1\u673a\u5236\uff1a\u57fa\u4e8e\u63d0\u793a\u7684\u63a8\u7406\u65f6\u9632\u5fa1\u548c\u63d0\u51fa\u7684FIDS\u8bad\u7ec3\u65f6\u9632\u5fa1\u65b9\u6cd5\uff08\u4f7f\u7528LoRA\u9002\u5e94\u6280\u672f\uff09\uff1b3) \u7ed3\u5408\u4e24\u79cd\u9632\u5fa1\u65b9\u6cd5", "result": "1) \u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u67d0\u4e9b\u653b\u51fb\u7c7b\u578b\u6210\u529f\u7387\u8d85\u8fc780%\uff1b2) \u63d0\u793a\u9632\u5fa1\u5b9e\u73b010.1%\u653b\u51fb\u964d\u4f4e\u4f46\u5e26\u676512.5%\u8bef\u62d2\u589e\u52a0\uff1b3) FIDS\u5b9e\u73b015.4%\u653b\u51fb\u964d\u4f4e\u548c10.4%\u8bef\u62d2\u589e\u52a0\uff1b4) \u7ec4\u5408\u65b9\u6cd5\u5b9e\u73b026.3%\u653b\u51fb\u964d\u4f4e\uff0c\u8bad\u7ec3\u65f6\u9632\u5fa1\u5728\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u4fdd\u62a4\u4e0a\u90fd\u4f18\u4e8e\u63a8\u7406\u65f6\u9632\u5fa1", "conclusion": "\u8bad\u7ec3\u65f6\u9632\u5fa1\u65b9\u6cd5\uff08\u5982FIDS\uff09\u6bd4\u63a8\u7406\u65f6\u9632\u5fa1\u5728\u5bf9\u6297\u6307\u4ee4\u653b\u51fb\u9632\u62a4\u65b9\u9762\u66f4\u6709\u6548\uff0c\u65e2\u80fd\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\u53c8\u80fd\u66f4\u597d\u5730\u4fdd\u6301\u7cfb\u7edf\u5b9e\u7528\u6027\uff0c\u4e3aLLMs\u5728\u7b80\u5386\u7b5b\u9009\u7b49\u654f\u611f\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc"}}
{"id": "2512.20182", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20182", "abs": "https://arxiv.org/abs/2512.20182", "authors": ["Shuzheng Si", "Qingyi Wang", "Haozhe Zhao", "Yuzhuo Bai", "Guanqiao Chen", "Kangyang Luo", "Gang Chen", "Fanchao Qi", "Minjia Zhang", "Baobao Chang", "Maosong Sun"], "title": "FaithLens: Detecting and Explaining Faithfulness Hallucination", "comment": null, "summary": "Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.", "AI": {"tldr": "FaithLens\u662f\u4e00\u4e2a\u7528\u4e8e\u68c0\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u4e2d\u5fe0\u5b9e\u6027\u5e7b\u89c9\u76848B\u53c2\u6570\u6a21\u578b\uff0c\u80fd\u540c\u65f6\u63d0\u4f9b\u4e8c\u5143\u9884\u6d4b\u548c\u89e3\u91ca\uff0c\u572812\u4e2a\u4efb\u52a1\u4e0a\u8d85\u8d8aGPT-4\u7b49\u5148\u8fdb\u6a21\u578b\u3002", "motivation": "\u5728\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\uff08\u5982\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u6458\u8981\uff09\uff0c\u8bc6\u522b\u8f93\u51fa\u4e2d\u662f\u5426\u5305\u542b\u5fe0\u5b9e\u6027\u5e7b\u89c9\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u63d0\u9ad8\u68c0\u6d4b\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "1) \u4f7f\u7528\u5148\u8fdbLLM\u5408\u6210\u5e26\u89e3\u91ca\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u8fc7\u6ee4\u786e\u4fdd\u6807\u7b7e\u6b63\u786e\u6027\u3001\u89e3\u91ca\u8d28\u91cf\u548c\u6570\u636e\u591a\u6837\u6027\uff1b2) \u5728\u7cbe\u5fc3\u7b56\u5212\u7684\u6570\u636e\u4e0a\u8fdb\u884c\u51b7\u542f\u52a8\u5fae\u8c03\uff1b3) \u4f7f\u7528\u57fa\u4e8e\u89c4\u5219\u7684\u5f3a\u5316\u5b66\u4e60\u8fdb\u4e00\u6b65\u4f18\u5316\uff0c\u5956\u52b1\u9884\u6d4b\u6b63\u786e\u6027\u548c\u89e3\u91ca\u8d28\u91cf\u3002", "result": "\u572812\u4e2a\u591a\u6837\u5316\u4efb\u52a1\u4e0a\uff0c8B\u53c2\u6570\u7684FaithLens\u8d85\u8d8a\u4e86GPT-4.1\u548co3\u7b49\u5148\u8fdb\u6a21\u578b\uff0c\u5e76\u80fd\u4ea7\u751f\u9ad8\u8d28\u91cf\u7684\u89e3\u91ca\uff0c\u5728\u53ef\u4fe1\u5ea6\u3001\u6548\u7387\u548c\u6548\u679c\u4e4b\u95f4\u5b9e\u73b0\u4e86\u72ec\u7279\u5e73\u8861\u3002", "conclusion": "FaithLens\u662f\u4e00\u4e2a\u6210\u672c\u6548\u76ca\u9ad8\u4e14\u6709\u6548\u7684\u5fe0\u5b9e\u6027\u5e7b\u89c9\u68c0\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u8054\u5408\u9884\u6d4b\u548c\u89e3\u91ca\u63d0\u9ad8\u4e86\u53ef\u4fe1\u5ea6\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.20204", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20204", "abs": "https://arxiv.org/abs/2512.20204", "authors": ["Marko \u010cechovi\u010d", "Nat\u00e1lia Komorn\u00edkov\u00e1", "Dominik Mach\u00e1\u010dek", "Ond\u0159ej Bojar"], "title": "Corpus of Cross-lingual Dialogues with Minutes and Detection of Misunderstandings", "comment": "12 pages, 2 figures, 6 tables, published as a conference paper in Text, Speech, and Dialogue 28th International Conference, TSD 2025, Erlangen, Germany, August 25-28, 2025, Proceedings, Part II. This version published here on arXiv.org is before review comments and seedings of the TSD conference staff", "summary": "Speech processing and translation technology have the potential to facilitate meetings of individuals who do not share any common language. To evaluate automatic systems for such a task, a versatile and realistic evaluation corpus is needed. Therefore, we create and present a corpus of cross-lingual dialogues between individuals without a common language who were facilitated by automatic simultaneous speech translation. The corpus consists of 5 hours of speech recordings with ASR and gold transcripts in 12 original languages and automatic and corrected translations into English. For the purposes of research into cross-lingual summarization, our corpus also includes written summaries (minutes) of the meetings.\n  Moreover, we propose automatic detection of misunderstandings. For an overview of this task and its complexity, we attempt to quantify misunderstandings in cross-lingual meetings. We annotate misunderstandings manually and also test the ability of current large language models to detect them automatically. The results show that the Gemini model is able to identify text spans with misunderstandings with recall of 77% and precision of 47%.", "AI": {"tldr": "\u8bba\u6587\u521b\u5efa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u8de8\u8bed\u8a00\u5bf9\u8bdd\u7cfb\u7edf\u7684\u8bed\u6599\u5e93\uff0c\u5305\u542b12\u79cd\u8bed\u8a00\u76845\u5c0f\u65f6\u8bed\u97f3\u6570\u636e\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bef\u89e3\u81ea\u52a8\u68c0\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u771f\u5b9e\u4e14\u591a\u529f\u80fd\u7684\u8bc4\u4f30\u8bed\u6599\u5e93\uff0c\u4ee5\u8bc4\u4f30\u5728\u65e0\u5171\u540c\u8bed\u8a00\u573a\u666f\u4e0b\u7531\u81ea\u52a8\u8bed\u97f3\u7ffb\u8bd1\u8f85\u52a9\u7684\u8de8\u8bed\u8a00\u5bf9\u8bdd\u7cfb\u7edf\u3002", "method": "1) \u521b\u5efa\u5305\u542b12\u79cd\u8bed\u8a00\u30015\u5c0f\u65f6\u8bed\u97f3\u7684\u8de8\u8bed\u8a00\u5bf9\u8bdd\u8bed\u6599\u5e93\uff0c\u5305\u542bASR\u8f6c\u5f55\u3001\u9ec4\u91d1\u8f6c\u5f55\u548c\u82f1\u8bd1\u7248\u672c\uff1b2) \u63d0\u51fa\u81ea\u52a8\u68c0\u6d4b\u8bef\u89e3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4eba\u5de5\u6807\u6ce8\u8bef\u89e3\u5e76\u6d4b\u8bd5\u5927\u8bed\u8a00\u6a21\u578b\uff08Gemini\uff09\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "result": "Gemini\u6a21\u578b\u5728\u8bc6\u522b\u8bef\u89e3\u6587\u672c\u7247\u6bb5\u65b9\u9762\u53d6\u5f97\u4e8677%\u7684\u53ec\u56de\u7387\u548c47%\u7684\u7cbe\u786e\u7387\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u8de8\u8bed\u8a00\u4f1a\u8bae\u4e2d\u81ea\u52a8\u68c0\u6d4b\u8bef\u89e3\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8de8\u8bed\u8a00\u5bf9\u8bdd\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8bed\u6599\u8d44\u6e90\uff0c\u5e76\u5c55\u793a\u4e86\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u68c0\u6d4b\u8de8\u8bed\u8a00\u4ea4\u6d41\u4e2d\u8bef\u89e3\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2512.20292", "categories": ["cs.CL", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.20292", "abs": "https://arxiv.org/abs/2512.20292", "authors": ["Wenzheng Zeng", "Mingyu Ouyang", "Langyuan Cui", "Hwee Tou Ng"], "title": "SlideTailor: Personalized Presentation Slide Generation for Scientific Papers", "comment": "AAAI 2026 (with appendix)", "summary": "Automatic presentation slide generation can greatly streamline content creation. However, since preferences of each user may vary, existing under-specified formulations often lead to suboptimal results that fail to align with individual user needs. We introduce a novel task that conditions paper-to-slides generation on user-specified preferences. We propose a human behavior-inspired agentic framework, SlideTailor, that progressively generates editable slides in a user-aligned manner. Instead of requiring users to write their preferences in detailed textual form, our system only asks for a paper-slides example pair and a visual template - natural and easy-to-provide artifacts that implicitly encode rich user preferences across content and visual style. Despite the implicit and unlabeled nature of these inputs, our framework effectively distills and generalizes the preferences to guide customized slide generation. We also introduce a novel chain-of-speech mechanism to align slide content with planned oral narration. Such a design significantly enhances the quality of generated slides and enables downstream applications like video presentations. To support this new task, we construct a benchmark dataset that captures diverse user preferences, with carefully designed interpretable metrics for robust evaluation. Extensive experiments demonstrate the effectiveness of our framework.", "AI": {"tldr": "SlideTailor\uff1a\u57fa\u4e8e\u7528\u6237\u504f\u597d\u7684\u4e2a\u6027\u5316\u8bba\u6587\u5230\u5e7b\u706f\u7247\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8bba\u6587-\u5e7b\u706f\u7247\u793a\u4f8b\u5bf9\u548c\u89c6\u89c9\u6a21\u677f\u6765\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u5e76\u5f15\u5165\u8bed\u97f3\u94fe\u673a\u5236\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5e7b\u706f\u7247\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u5355\u4e00\u6807\u51c6\uff0c\u5ffd\u7565\u4e86\u7528\u6237\u504f\u597d\u7684\u591a\u6837\u6027\uff0c\u5bfc\u81f4\u751f\u6210\u7ed3\u679c\u4e0e\u4e2a\u4f53\u9700\u6c42\u4e0d\u7b26\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6839\u636e\u7528\u6237\u7279\u5b9a\u504f\u597d\u5b9a\u5236\u5e7b\u706f\u7247\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faSlideTailor\u6846\u67b6\uff1a1) \u4ec5\u9700\u7528\u6237\u63d0\u4f9b\u8bba\u6587-\u5e7b\u706f\u7247\u793a\u4f8b\u5bf9\u548c\u89c6\u89c9\u6a21\u677f\uff0c\u4ece\u8fd9\u4e9b\u81ea\u7136\u6613\u5f97\u7684\u8f93\u5165\u4e2d\u9690\u5f0f\u5b66\u4e60\u7528\u6237\u7684\u5185\u5bb9\u548c\u89c6\u89c9\u98ce\u683c\u504f\u597d\uff1b2) \u5f15\u5165\u94fe\u5f0f\u8bed\u97f3\u673a\u5236\uff0c\u4f7f\u5e7b\u706f\u7247\u5185\u5bb9\u4e0e\u8ba1\u5212\u7684\u53e3\u5934\u53d9\u8ff0\u5bf9\u9f50\uff1b3) \u6784\u5efa\u5305\u542b\u591a\u6837\u5316\u7528\u6237\u504f\u597d\u7684\u57fa\u51c6\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6846\u67b6\u80fd\u6709\u6548\u4ece\u9690\u5f0f\u3001\u65e0\u6807\u7b7e\u7684\u8f93\u5165\u4e2d\u63d0\u53d6\u548c\u6cdb\u5316\u7528\u6237\u504f\u597d\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4e2a\u6027\u5316\u5e7b\u706f\u7247\uff0c\u5e76\u80fd\u652f\u6301\u89c6\u9891\u6f14\u793a\u7b49\u4e0b\u6e38\u5e94\u7528\u3002", "conclusion": "SlideTailor\u901a\u8fc7\u7528\u6237\u53cb\u597d\u7684\u8f93\u5165\u65b9\u5f0f\u5b9e\u73b0\u4e86\u4e2a\u6027\u5316\u7684\u8bba\u6587\u5230\u5e7b\u706f\u7247\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u7528\u6237\u504f\u597d\u7684\u95ee\u9898\uff0c\u4e3a\u81ea\u52a8\u5185\u5bb9\u521b\u5efa\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.20293", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20293", "abs": "https://arxiv.org/abs/2512.20293", "authors": ["Jaykumar Kasundra", "Anjaneya Praharaj", "Sourabh Surana", "Lakshmi Sirisha Chodisetty", "Sourav Sharma", "Abhigya Verma", "Abhishek Bhardwaj", "Debasish Kanhar", "Aakash Bhagat", "Khalil Slimi", "Seganrasan Subramanian", "Sathwik Tejaswi Madhusudhan", "Ranga Prasad Chenna", "Srinivas Sunkara"], "title": "AprielGuard", "comment": null, "summary": "Safeguarding large language models (LLMs) against unsafe or adversarial behavior is critical as they are increasingly deployed in conversational and agentic settings. Existing moderation tools often treat safety risks (e.g. toxicity, bias) and adversarial threats (e.g. prompt injections, jailbreaks) as separate problems, limiting their robustness and generalizability. We introduce AprielGuard, an 8B parameter safeguard model that unify these dimensions within a single taxonomy and learning framework. AprielGuard is trained on a diverse mix of open and synthetic data covering standalone prompts, multi-turn conversations, and agentic workflows, augmented with structured reasoning traces to improve interpretability. Across multiple public and proprietary benchmarks, AprielGuard achieves strong performance in detecting harmful content and adversarial manipulations, outperforming existing opensource guardrails such as Llama-Guard and Granite Guardian, particularly in multi-step and reasoning intensive scenarios. By releasing the model, we aim to advance transparent and reproducible research on reliable safeguards for LLMs.", "AI": {"tldr": "AprielGuard\u662f\u4e00\u4e2a8B\u53c2\u6570\u7684\u5b89\u5168\u9632\u62a4\u6a21\u578b\uff0c\u7edf\u4e00\u5904\u7406\u5b89\u5168\u98ce\u9669\uff08\u5982\u6bd2\u6027\u3001\u504f\u89c1\uff09\u548c\u5bf9\u6297\u5a01\u80c1\uff08\u5982\u63d0\u793a\u6ce8\u5165\u3001\u8d8a\u72f1\uff09\uff0c\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u9632\u62a4\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u9632\u62a4\u5de5\u5177\u901a\u5e38\u5c06\u5b89\u5168\u98ce\u9669\u548c\u5bf9\u6297\u5a01\u80c1\u89c6\u4e3a\u72ec\u7acb\u95ee\u9898\u5904\u7406\uff0c\u9650\u5236\u4e86\u5176\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u8bdd\u548c\u4ee3\u7406\u573a\u666f\u4e2d\u7684\u90e8\u7f72\u589e\u52a0\uff0c\u9700\u8981\u66f4\u5168\u9762\u3001\u7edf\u4e00\u7684\u5b89\u5168\u9632\u62a4\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e868B\u53c2\u6570\u7684AprielGuard\u6a21\u578b\uff0c\u91c7\u7528\u7edf\u4e00\u5206\u7c7b\u548c\u5b66\u4e60\u6846\u67b6\u3002\u5728\u5305\u542b\u72ec\u7acb\u63d0\u793a\u3001\u591a\u8f6e\u5bf9\u8bdd\u548c\u4ee3\u7406\u5de5\u4f5c\u6d41\u7684\u591a\u6837\u5316\u5f00\u6e90\u548c\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5e76\u52a0\u5165\u7ed3\u6784\u5316\u63a8\u7406\u8f68\u8ff9\u4ee5\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u548c\u4e13\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAprielGuard\u5728\u68c0\u6d4b\u6709\u5bb3\u5185\u5bb9\u548c\u5bf9\u6297\u64cd\u4f5c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u591a\u6b65\u9aa4\u548c\u63a8\u7406\u5bc6\u96c6\u578b\u573a\u666f\u4e2d\uff0c\u8d85\u8d8a\u4e86Llama-Guard\u548cGranite Guardian\u7b49\u73b0\u6709\u5f00\u6e90\u9632\u62a4\u6a21\u578b\u3002", "conclusion": "AprielGuard\u901a\u8fc7\u7edf\u4e00\u7684\u5b89\u5168\u9632\u62a4\u6846\u67b6\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6a21\u578b\u7684\u5f00\u6e90\u53d1\u5e03\u65e8\u5728\u63a8\u52a8LLM\u53ef\u9760\u9632\u62a4\u7684\u900f\u660e\u548c\u53ef\u590d\u73b0\u7814\u7a76\u3002"}}
{"id": "2512.20298", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.20298", "abs": "https://arxiv.org/abs/2512.20298", "authors": ["Karolina Dro\u017cd\u017c", "Kacper Dudzic", "Anna Sterna", "Marcin Moskalewicz"], "title": "Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives", "comment": null, "summary": "Growing reliance on LLMs for psychiatric self-assessment raises questions about their ability to interpret qualitative patient narratives. We present the first direct comparison between state-of-the-art LLMs and mental health professionals in diagnosing Borderline (BPD) and Narcissistic (NPD) Personality Disorders utilizing Polish-language first-person autobiographical accounts. We show that the top-performing Gemini Pro models surpassed human professionals in overall diagnostic accuracy by 21.91 percentage points (65.48% vs. 43.57%). While both models and human experts excelled at identifying BPD (F1 = 83.4 & F1 = 80.0, respectively), models severely underdiagnosed NPD (F1 = 6.7 vs. 50.0), showing a reluctance toward the value-laden term \"narcissism.\" Qualitatively, models provided confident, elaborate justifications focused on patterns and formal categories, while human experts remained concise and cautious, emphasizing the patient's sense of self and temporal experience. Our findings demonstrate that while LLMs are highly competent at interpreting complex first-person clinical data, they remain subject to critical reliability and bias issues.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86LLM\u4e0e\u7cbe\u795e\u5065\u5eb7\u4e13\u5bb6\u5728\u8bca\u65ad\u8fb9\u7f18\u578b\u548c\u81ea\u604b\u578b\u4eba\u683c\u969c\u788d\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0LLM\u6574\u4f53\u51c6\u786e\u7387\u66f4\u9ad8\u4f46\u5b58\u5728\u4e25\u91cd\u504f\u89c1\uff0c\u5c24\u5176\u5728\u81ea\u604b\u578b\u4eba\u683c\u969c\u788d\u8bca\u65ad\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u968f\u7740LLM\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u7cbe\u795e\u79d1\u81ea\u6211\u8bc4\u4f30\uff0c\u9700\u8981\u4e86\u89e3\u5b83\u4eec\u89e3\u91ca\u5b9a\u6027\u60a3\u8005\u53d9\u8ff0\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u8bca\u65ad\u590d\u6742\u4eba\u683c\u969c\u788d\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u6ce2\u5170\u8bed\u7b2c\u4e00\u4eba\u79f0\u81ea\u4f20\u4f53\u53d9\u8ff0\uff0c\u5bf9\u6700\u5148\u8fdb\u7684LLM\uff08\u5982Gemini Pro\uff09\u4e0e\u7cbe\u795e\u5065\u5eb7\u4e13\u4e1a\u4eba\u58eb\u5728\u8bca\u65ad\u8fb9\u7f18\u578b\u548c\u81ea\u604b\u578b\u4eba\u683c\u969c\u788d\u65b9\u9762\u8fdb\u884c\u4e86\u9996\u6b21\u76f4\u63a5\u6bd4\u8f83\u3002", "result": "Gemini Pro\u6a21\u578b\u6574\u4f53\u8bca\u65ad\u51c6\u786e\u7387\u6bd4\u4eba\u7c7b\u4e13\u5bb6\u9ad821.91\u4e2a\u767e\u5206\u70b9\uff0865.48% vs. 43.57%\uff09\u3002\u867d\u7136\u4e24\u8005\u5728\u8fb9\u7f18\u578b\u4eba\u683c\u969c\u788d\u8bca\u65ad\u4e0a\u90fd\u8868\u73b0\u826f\u597d\uff08F1\u5206\u522b\u4e3a83.4\u548c80.0\uff09\uff0c\u4f46\u6a21\u578b\u4e25\u91cd\u4f4e\u4f30\u81ea\u604b\u578b\u4eba\u683c\u969c\u788d\uff08F1=6.7 vs. 50.0\uff09\uff0c\u663e\u793a\u5bf9\"\u81ea\u604b\"\u8fd9\u4e00\u4ef7\u503c\u8d1f\u8f7d\u672f\u8bed\u7684\u56de\u907f\u3002", "conclusion": "LLM\u5728\u89e3\u91ca\u590d\u6742\u7684\u7b2c\u4e00\u4eba\u79f0\u4e34\u5e8a\u6570\u636e\u65b9\u9762\u80fd\u529b\u5f88\u5f3a\uff0c\u4f46\u4ecd\u5b58\u5728\u5173\u952e\u7684\u53ef\u9760\u6027\u548c\u504f\u89c1\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4ef7\u503c\u8d1f\u8f7d\u7684\u8bca\u65ad\u6807\u7b7e\u4e0a\u8868\u73b0\u51fa\u7cfb\u7edf\u6027\u7684\u56de\u907f\u503e\u5411\u3002"}}
{"id": "2512.20308", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.20308", "abs": "https://arxiv.org/abs/2512.20308", "authors": ["Maxime Poli", "Mahi Luthra", "Youssef Benchekroun", "Yosuke Higuchi", "Martin Gleize", "Jiayi Shen", "Robin Algayres", "Yu-An Chung", "Mido Assran", "Juan Pino", "Emmanuel Dupoux"], "title": "SpidR: Learning Fast and Stable Linguistic Units for Spoken Language Models Without Supervision", "comment": "30 pages, 16 figures", "summary": "The parallel advances in language modeling and speech representation learning have raised the prospect of learning language directly from speech without textual intermediates. This requires extracting semantic representations directly from speech. Our contributions are threefold. First, we introduce SpidR, a self-supervised speech representation model that efficiently learns representations with highly accessible phonetic information, which makes it particularly suited for textless spoken language modeling. It is trained on raw waveforms using a masked prediction objective combined with self-distillation and online clustering. The intermediate layers of the student model learn to predict assignments derived from the teacher's intermediate layers. This learning objective stabilizes the online clustering procedure compared to previous approaches, resulting in higher quality codebooks. SpidR outperforms wav2vec 2.0, HuBERT, WavLM, and DinoSR on downstream language modeling benchmarks (sWUGGY, sBLIMP, tSC). Second, we systematically evaluate across models and layers the correlation between speech unit quality (ABX, PNMI) and language modeling performance, validating these metrics as reliable proxies. Finally, SpidR significantly reduces pretraining time compared to HuBERT, requiring only one day of pretraining on 16 GPUs, instead of a week. This speedup is enabled by the pretraining method and an efficient codebase, which allows faster iteration and easier experimentation. We open-source the training code and model checkpoints at https://github.com/facebookresearch/spidr.", "AI": {"tldr": "SpidR\u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u8bed\u97f3\u8868\u793a\u6a21\u578b\uff0c\u901a\u8fc7\u63a9\u7801\u9884\u6d4b\u76ee\u6807\u7ed3\u5408\u81ea\u84b8\u998f\u548c\u5728\u7ebf\u805a\u7c7b\uff0c\u5728\u539f\u59cb\u6ce2\u5f62\u4e0a\u8bad\u7ec3\uff0c\u80fd\u9ad8\u6548\u5b66\u4e60\u5305\u542b\u4e30\u5bcc\u8bed\u97f3\u4fe1\u606f\u7684\u8868\u793a\uff0c\u7279\u522b\u9002\u5408\u65e0\u6587\u672c\u53e3\u8bed\u8bed\u8a00\u5efa\u6a21\u3002", "motivation": "\u8bed\u8a00\u5efa\u6a21\u548c\u8bed\u97f3\u8868\u793a\u5b66\u4e60\u7684\u5e76\u884c\u8fdb\u5c55\u4f7f\u5f97\u76f4\u63a5\u4ece\u8bed\u97f3\u5b66\u4e60\u8bed\u8a00\u6210\u4e3a\u53ef\u80fd\uff0c\u8fd9\u9700\u8981\u4ece\u8bed\u97f3\u4e2d\u76f4\u63a5\u63d0\u53d6\u8bed\u4e49\u8868\u793a\u3002\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u97f3\u5355\u5143\u8d28\u91cf\u548c\u8bad\u7ec3\u6548\u7387\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u63d0\u51faSpidR\u6a21\u578b\uff0c\u4f7f\u7528\u63a9\u7801\u9884\u6d4b\u76ee\u6807\u7ed3\u5408\u81ea\u84b8\u998f\u548c\u5728\u7ebf\u805a\u7c7b\u5728\u539f\u59cb\u6ce2\u5f62\u4e0a\u8bad\u7ec3\u3002\u5b66\u751f\u6a21\u578b\u7684\u4e2d\u95f4\u5c42\u5b66\u4e60\u9884\u6d4b\u6559\u5e08\u6a21\u578b\u4e2d\u95f4\u5c42\u7684\u5206\u914d\u7ed3\u679c\uff0c\u8fd9\u79cd\u5b66\u4e60\u76ee\u6807\u7a33\u5b9a\u4e86\u5728\u7ebf\u805a\u7c7b\u8fc7\u7a0b\uff0c\u4ea7\u751f\u66f4\u9ad8\u8d28\u91cf\u7684\u7801\u672c\u3002", "result": "SpidR\u5728\u8bed\u8a00\u5efa\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff08sWUGGY\u3001sBLIMP\u3001tSC\uff09\u4e0a\u4f18\u4e8ewav2vec 2.0\u3001HuBERT\u3001WavLM\u548cDinoSR\u3002\u7cfb\u7edf\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u8bed\u97f3\u5355\u5143\u8d28\u91cf\u6307\u6807\u4e0e\u8bed\u8a00\u5efa\u6a21\u6027\u80fd\u7684\u76f8\u5173\u6027\u3002\u76f8\u6bd4HuBERT\uff0cSpidR\u5c06\u9884\u8bad\u7ec3\u65f6\u95f4\u4ece\u4e00\u5468\u7f29\u77ed\u5230\u4e00\u5929\uff0816\u4e2aGPU\uff09\u3002", "conclusion": "SpidR\u901a\u8fc7\u6539\u8fdb\u7684\u8bad\u7ec3\u76ee\u6807\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u8bed\u97f3\u8868\u793a\u5b66\u4e60\uff0c\u4e3a\u65e0\u6587\u672c\u53e3\u8bed\u8bed\u8a00\u5efa\u6a21\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u9a8c\u8bc1\u4e86\u8bed\u97f3\u5355\u5143\u8d28\u91cf\u6307\u6807\u4f5c\u4e3a\u8bed\u8a00\u5efa\u6a21\u6027\u80fd\u53ef\u9760\u4ee3\u7406\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2512.20324", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20324", "abs": "https://arxiv.org/abs/2512.20324", "authors": ["Nurul Labib Sayeedi", "Md. Faiyaz Abdullah Sayeedi", "Khushnur Binte Jahangir", "Swakkhar Shatabda", "Sarah Masud Preum"], "title": "Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles", "comment": null, "summary": "Large Language Models (LLMs) show impressive performance on many NLP benchmarks, yet their ability to reason in figurative, culturally grounded, and low-resource settings remains underexplored. We address this gap for Bangla by introducing BanglaRiddleEval, a benchmark of 1,244 traditional Bangla riddles instantiated across four tasks (4,976 riddle-task artifacts in total). Using an LLM-based pipeline, we generate Chain-of-Thought explanations, semantically coherent distractors, and fine-grained ambiguity annotations, and evaluate a diverse suite of open-source and closed-source models under different prompting strategies. Models achieve moderate semantic overlap on generative QA but low correctness, MCQ accuracy peaks at only about 56% versus an 83% human baseline, and ambiguity resolution ranges from roughly 26% to 68%, with high-quality explanations confined to the strongest models. These results show that current LLMs capture some cues needed for Bangla riddle reasoning but remain far from human-level performance, establishing BanglaRiddleEval as a challenging new benchmark for low-resource figurative reasoning. All data, code, and evaluation scripts are available on GitHub: https://github.com/Labib1610/BanglaRiddleEval.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u521b\u5efa\u4e86BanglaRiddleEval\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1,244\u4e2a\u5b5f\u52a0\u62c9\u8bed\u8c1c\u8bed\uff0c\u8bc4\u4f30LLMs\u5728\u4f4e\u8d44\u6e90\u3001\u6bd4\u55bb\u6027\u548c\u6587\u5316\u60c5\u5883\u4e0b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u8868\u73b0\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u6c34\u5e73\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u8bb8\u591aNLP\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u6bd4\u55bb\u6027\u3001\u6587\u5316\u6839\u57fa\u548c\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u65e8\u5728\u586b\u8865\u5b5f\u52a0\u62c9\u8bed\u9886\u57df\u7684\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7LLM\u9a71\u52a8\u7684\u6d41\u6c34\u7ebf\u751f\u6210\u601d\u7ef4\u94fe\u89e3\u91ca\u3001\u8bed\u4e49\u8fde\u8d2f\u7684\u5e72\u6270\u9879\u548c\u7ec6\u7c92\u5ea6\u6b67\u4e49\u6807\u6ce8\uff0c\u6784\u5efa\u5305\u542b4,976\u4e2a\u8c1c\u8bed\u4efb\u52a1\u5de5\u4ef6\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u8bc4\u4f30\u591a\u79cd\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u5728\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u6a21\u578b\u5728\u751f\u6210\u5f0fQA\u4e2d\u8fbe\u5230\u4e2d\u7b49\u8bed\u4e49\u91cd\u53e0\u4f46\u6b63\u786e\u7387\u4f4e\uff0c\u591a\u9879\u9009\u62e9\u9898\u51c6\u786e\u7387\u6700\u9ad8\u4ec5\u7ea656%\uff08\u4eba\u7c7b\u57fa\u7ebf\u4e3a83%\uff09\uff0c\u6b67\u4e49\u89e3\u51b3\u7387\u572826%\u81f368%\u4e4b\u95f4\uff0c\u9ad8\u8d28\u91cf\u89e3\u91ca\u4ec5\u9650\u4e8e\u6700\u5f3a\u6a21\u578b\u3002", "conclusion": "\u5f53\u524dLLMs\u80fd\u591f\u6355\u6349\u5b5f\u52a0\u62c9\u8bed\u8c1c\u8bed\u63a8\u7406\u6240\u9700\u7684\u4e00\u4e9b\u7ebf\u7d22\uff0c\u4f46\u8ddd\u79bb\u4eba\u7c7b\u6c34\u5e73\u4ecd\u6709\u5f88\u5927\u5dee\u8ddd\uff0cBanglaRiddleEval\u6210\u4e3a\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4f4e\u8d44\u6e90\u6bd4\u55bb\u63a8\u7406\u65b0\u57fa\u51c6\u3002"}}
{"id": "2512.20352", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20352", "abs": "https://arxiv.org/abs/2512.20352", "authors": ["Nilesh Jain", "Seyi Adeyinka", "Leor Roseman", "Aza Allsop"], "title": "Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation", "comment": "11 pages, 1 figure, 3 tables", "summary": "Qualitative research faces a critical reliability challenge: traditional inter-rater agreement methods require multiple human coders, are time-intensive, and often yield moderate consistency. We present a multi-perspective validation framework for LLM-based thematic analysis that combines ensemble validation with dual reliability metrics: Cohen's Kappa ($\u03ba$) for inter-rater agreement and cosine similarity for semantic consistency. Our framework enables configurable analysis parameters (1-6 seeds, temperature 0.0-2.0), supports custom prompt structures with variable substitution, and provides consensus theme extraction across any JSON format. As proof-of-concept, we evaluate three leading LLMs (Gemini 2.5 Pro, GPT-4o, Claude 3.5 Sonnet) on a psychedelic art therapy interview transcript, conducting six independent runs per model. Results demonstrate Gemini achieves highest reliability ($\u03ba= 0.907$, cosine=95.3%), followed by GPT-4o ($\u03ba= 0.853$, cosine=92.6%) and Claude ($\u03ba= 0.842$, cosine=92.1%). All three models achieve a high agreement ($\u03ba> 0.80$), validating the multi-run ensemble approach. The framework successfully extracts consensus themes across runs, with Gemini identifying 6 consensus themes (50-83% consistency), GPT-4o identifying 5 themes, and Claude 4 themes. Our open-source implementation provides researchers with transparent reliability metrics, flexible configuration, and structure-agnostic consensus extraction, establishing methodological foundations for reliable AI-assisted qualitative research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8eLLM\u4e3b\u9898\u5206\u6790\u7684\u591a\u89c6\u89d2\u9a8c\u8bc1\u6846\u67b6\uff0c\u7ed3\u5408\u96c6\u6210\u9a8c\u8bc1\u548c\u53cc\u91cd\u53ef\u9760\u6027\u6307\u6807\uff0c\u5728\u4e09\u4e2a\u9886\u5148LLM\u4e0a\u9a8c\u8bc1\u4e86\u9ad8\u53ef\u9760\u6027\u3002", "motivation": "\u4f20\u7edf\u8d28\u6027\u7814\u7a76\u4e2d\u4eba\u5de5\u7f16\u7801\u8005\u4e4b\u95f4\u7684\u8bc4\u5206\u8005\u4e00\u81f4\u6027\u65b9\u6cd5\u9700\u8981\u591a\u4e2a\u7f16\u7801\u8005\u3001\u8017\u65f6\u4e14\u4e00\u81f4\u6027\u901a\u5e38\u4e2d\u7b49\uff0c\u5b58\u5728\u53ef\u9760\u6027\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u89c6\u89d2\u9a8c\u8bc1\u6846\u67b6\uff0c\u7ed3\u5408\u96c6\u6210\u9a8c\u8bc1\u4e0e\u53cc\u91cd\u53ef\u9760\u6027\u6307\u6807\uff1a\u7528\u4e8e\u8bc4\u5206\u8005\u4e00\u81f4\u6027\u7684Cohen's Kappa\u548c\u7528\u4e8e\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u3002\u6846\u67b6\u652f\u6301\u53ef\u914d\u7f6e\u7684\u5206\u6790\u53c2\u6570\uff081-6\u4e2a\u79cd\u5b50\uff0c\u6e29\u5ea60.0-2.0\uff09\u3001\u81ea\u5b9a\u4e49\u63d0\u793a\u7ed3\u6784\u548c\u53d8\u91cf\u66ff\u6362\uff0c\u63d0\u4f9b\u8de8\u4efb\u610fJSON\u683c\u5f0f\u7684\u5171\u8bc6\u4e3b\u9898\u63d0\u53d6\u3002", "result": "\u5728\u4e09\u4e2a\u9886\u5148LLM\uff08Gemini 2.5 Pro\u3001GPT-4o\u3001Claude 3.5 Sonnet\uff09\u4e0a\u8bc4\u4f30\uff0c\u6bcf\u4e2a\u6a21\u578b\u8fdb\u884c\u516d\u6b21\u72ec\u7acb\u8fd0\u884c\u3002Gemini\u83b7\u5f97\u6700\u9ad8\u53ef\u9760\u6027\uff08\u03ba=0.907\uff0c\u4f59\u5f26\u76f8\u4f3c\u5ea695.3%\uff09\uff0c\u5176\u6b21\u662fGPT-4o\uff08\u03ba=0.853\uff0c92.6%\uff09\u548cClaude\uff08\u03ba=0.842\uff0c92.1%\uff09\u3002\u6240\u6709\u6a21\u578b\u90fd\u8fbe\u5230\u9ad8\u4e00\u81f4\u6027\uff08\u03ba>0.80\uff09\u3002Gemini\u8bc6\u522b\u51fa6\u4e2a\u5171\u8bc6\u4e3b\u9898\uff0850-83%\u4e00\u81f4\u6027\uff09\uff0cGPT-4o\u8bc6\u522b5\u4e2a\uff0cClaude\u8bc6\u522b4\u4e2a\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u63d0\u4f9b\u900f\u660e\u7684\u53ef\u9760\u6027\u6307\u6807\u3001\u7075\u6d3b\u914d\u7f6e\u548c\u7ed3\u6784\u65e0\u5173\u7684\u5171\u8bc6\u63d0\u53d6\uff0c\u4e3a\u53ef\u9760\u7684AI\u8f85\u52a9\u8d28\u6027\u7814\u7a76\u5efa\u7acb\u4e86\u65b9\u6cd5\u8bba\u57fa\u7840\uff0c\u5f00\u6e90\u5b9e\u73b0\u53ef\u4f9b\u7814\u7a76\u4eba\u5458\u4f7f\u7528\u3002"}}
{"id": "2512.20404", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20404", "abs": "https://arxiv.org/abs/2512.20404", "authors": ["Junyi Liu", "Stanley Kok"], "title": "Sentiment-Aware Extractive and Abstractive Summarization for Unstructured Text Mining", "comment": "WITS 2025 (Workshop on Information Technologies and Systems 2025)", "summary": "With the rapid growth of unstructured data from social media, reviews, and forums, text mining has become essential in Information Systems (IS) for extracting actionable insights. Summarization can condense fragmented, emotion-rich posts, but existing methods-optimized for structured news-struggle with noisy, informal content. Emotional cues are critical for IS tasks such as brand monitoring and market analysis, yet few studies integrate sentiment modeling into summarization of short user-generated texts. We propose a sentiment-aware framework extending extractive (TextRank) and abstractive (UniLM) approaches by embedding sentiment signals into ranking and generation processes. This dual design improves the capture of emotional nuances and thematic relevance, producing concise, sentiment-enriched summaries that enhance timely interventions and strategic decision-making in dynamic online environments.", "AI": {"tldr": "\u63d0\u51fa\u60c5\u611f\u611f\u77e5\u6587\u672c\u6458\u8981\u6846\u67b6\uff0c\u7ed3\u5408\u62bd\u53d6\u5f0f\u548c\u751f\u6210\u5f0f\u65b9\u6cd5\uff0c\u6539\u8fdb\u5bf9\u7528\u6237\u751f\u6210\u77ed\u6587\u672c\u7684\u60c5\u611f\u5efa\u6a21\u548c\u6458\u8981\u8d28\u91cf\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u3001\u8bc4\u8bba\u548c\u8bba\u575b\u7b49\u975e\u7ed3\u6784\u5316\u6570\u636e\u5feb\u901f\u589e\u957f\uff0c\u6587\u672c\u6316\u6398\u5728\u4fe1\u606f\u7cfb\u7edf(IS)\u4e2d\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u6458\u8981\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u7ed3\u6784\u5316\u65b0\u95fb\u4f18\u5316\uff0c\u96be\u4ee5\u5904\u7406\u5608\u6742\u3001\u975e\u6b63\u5f0f\u7684\u7528\u6237\u751f\u6210\u5185\u5bb9\u3002\u60c5\u611f\u7ebf\u7d22\u5bf9\u4e8e\u54c1\u724c\u76d1\u63a7\u548c\u5e02\u573a\u5206\u6790\u7b49IS\u4efb\u52a1\u975e\u5e38\u5173\u952e\uff0c\u4f46\u5f88\u5c11\u6709\u7814\u7a76\u5c06\u60c5\u611f\u5efa\u6a21\u96c6\u6210\u5230\u77ed\u7528\u6237\u751f\u6210\u6587\u672c\u7684\u6458\u8981\u4e2d\u3002", "method": "\u63d0\u51fa\u60c5\u611f\u611f\u77e5\u6846\u67b6\uff0c\u6269\u5c55\u62bd\u53d6\u5f0f(TextRank)\u548c\u751f\u6210\u5f0f(UniLM)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u60c5\u611f\u4fe1\u53f7\u5d4c\u5165\u5230\u6392\u540d\u548c\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u6539\u8fdb\u5bf9\u60c5\u611f\u7ec6\u5fae\u5dee\u522b\u548c\u4e3b\u9898\u76f8\u5173\u6027\u7684\u6355\u6349\u3002", "result": "\u8be5\u53cc\u91cd\u8bbe\u8ba1\u80fd\u591f\u4ea7\u751f\u7b80\u6d01\u3001\u60c5\u611f\u4e30\u5bcc\u7684\u6458\u8981\uff0c\u589e\u5f3a\u4e86\u5728\u52a8\u6001\u5728\u7ebf\u73af\u5883\u4e2d\u53ca\u65f6\u5e72\u9884\u548c\u6218\u7565\u51b3\u7b56\u7684\u80fd\u529b\u3002", "conclusion": "\u60c5\u611f\u611f\u77e5\u6587\u672c\u6458\u8981\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u7528\u6237\u751f\u6210\u77ed\u6587\u672c\u7684\u6458\u8981\u95ee\u9898\uff0c\u901a\u8fc7\u96c6\u6210\u60c5\u611f\u5efa\u6a21\u63d0\u9ad8\u4e86\u6458\u8981\u8d28\u91cf\uff0c\u5bf9\u4fe1\u606f\u7cfb\u7edf\u4e2d\u7684\u54c1\u724c\u76d1\u63a7\u548c\u5e02\u573a\u5206\u6790\u7b49\u5e94\u7528\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2512.20491", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20491", "abs": "https://arxiv.org/abs/2512.20491", "authors": ["Chen Hu", "Haikuo Du", "Heng Wang", "Lin Lin", "Mingrui Chen", "Peng Liu", "Ruihang Miao", "Tianchi Yue", "Wang You", "Wei Ji", "Wei Yuan", "Wenjin Deng", "Xiaojian Yuan", "Xiaoyun Zhang", "Xiangyu Liu", "Xikai Liu", "Yanming Xu", "Yicheng Cao", "Yifei Zhang", "Yongyao Wang", "Yubo Shu", "Yurong Zhang", "Yuxiang Zhang", "Zheng Gong", "Zhichao Chang", "Binyan Li", "Dan Ma", "Furong Jia", "Hongyuan Wang", "Jiayu Liu", "Jing Bai", "Junlan Liu", "Manjiao Liu", "Na Wang", "Qiuping Wu", "Qinxin Du", "Shiwei Li", "Wen Sun", "Yifeng Gong", "Yonglin Chen", "Yuling Zhao", "Yuxuan Lin", "Ziqi Ren", "Zixuan Wang", "Aihu Zhang", "Brian Li", "Buyun Ma", "Kang An", "Li Xie", "Mingliang Li", "Pan Li", "Shidong Yang", "Xi Chen", "Xiaojia Liu", "Yuchu Luo", "Yuan Song", "YuanHao Ding", "Yuanwei Liang", "Zexi Li", "Zhaoning Zhang", "Zixin Zhang", "Binxing Jiao", "Daxin Jiang", "Jiansheng Chen", "Jing Li", "Xiangyu Zhang", "Yibo Zhu"], "title": "Step-DeepResearch Technical Report", "comment": null, "summary": "As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faStep-DeepResearch\uff0c\u4e00\u4e2a\u9762\u5411\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u7684\u7aef\u5230\u7aef\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u539f\u5b50\u80fd\u529b\u6570\u636e\u5408\u6210\u7b56\u7565\u548c\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u8def\u5f84\uff0c\u4f7f\u4e2d\u7b49\u89c4\u6a21\u6a21\u578b\u5728\u6210\u672c\u6548\u76ca\u4e0b\u8fbe\u5230\u4e13\u5bb6\u7ea7\u6df1\u5ea6\u7814\u7a76\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5b66\u672f\u57fa\u51c6\uff08\u5982BrowseComp\uff09\u65e0\u6cd5\u6ee1\u8db3\u73b0\u5b9e\u4e16\u754c\u5bf9\u5f00\u653e\u5f0f\u6df1\u5ea6\u7814\u7a76\u7684\u9700\u6c42\uff0c\u540e\u8005\u9700\u8981\u5f3a\u5927\u7684\u610f\u56fe\u8bc6\u522b\u3001\u957f\u65f6\u7a0b\u51b3\u7b56\u548c\u8de8\u6e90\u9a8c\u8bc1\u80fd\u529b\u3002\u540c\u65f6\uff0c\u4e2d\u6587\u9886\u57df\u7f3a\u4e4f\u9488\u5bf9\u6df1\u5ea6\u7814\u7a76\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "1. \u63d0\u51fa\u57fa\u4e8e\u539f\u5b50\u80fd\u529b\u7684\u6570\u636e\u5408\u6210\u7b56\u7565\uff0c\u5f3a\u5316\u89c4\u5212\u548c\u62a5\u544a\u64b0\u5199\u80fd\u529b\uff1b2. \u91c7\u7528\u4ece\u667a\u80fd\u4f53\u4e2d\u95f4\u8bad\u7ec3\u5230SFT\u548cRL\u7684\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u8def\u5f84\uff1b3. \u5f15\u5165\u6e05\u5355\u5f0f\u8bc4\u5224\u5668\u589e\u5f3a\u9c81\u68d2\u6027\uff1b4. \u5efa\u7acb\u4e2d\u6587\u6df1\u5ea6\u7814\u7a76\u8bc4\u4f30\u57fa\u51c6ADR-Bench\u3002", "result": "Step-DeepResearch\uff0832B\uff09\u5728Scale AI Research Rubrics\u4e0a\u83b7\u5f9761.4%\u7684\u5206\u6570\u3002\u5728ADR-Bench\u4e0a\uff0c\u663e\u8457\u4f18\u4e8e\u540c\u7c7b\u6a21\u578b\uff0c\u5e76\u4e0eOpenAI\u3001Gemini DeepResearch\u7b49\u95ed\u6e90SOTA\u6a21\u578b\u76f8\u5ab2\u7f8e\u3002", "conclusion": "\u7cbe\u7ec6\u5316\u8bad\u7ec3\u4f7f\u4e2d\u7b49\u89c4\u6a21\u6a21\u578b\u80fd\u591f\u4ee5\u884c\u4e1a\u9886\u5148\u7684\u6210\u672c\u6548\u76ca\u5b9e\u73b0\u4e13\u5bb6\u7ea7\u6df1\u5ea6\u7814\u7a76\u80fd\u529b\uff0c\u4e3a\u81ea\u4e3b\u667a\u80fd\u4f53\u7684\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.20569", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20569", "abs": "https://arxiv.org/abs/2512.20569", "authors": ["Yanhong Li", "Songlin Yang", "Shawn Tan", "Mayank Mishra", "Rameswar Panda", "Jiawei Zhou", "Yoon Kim"], "title": "Distilling to Hybrid Attention Models via KL-Guided Layer Selection", "comment": null, "summary": "Distilling pretrained softmax attention Transformers into more efficient hybrid architectures that interleave softmax and linear attention layers is a promising approach for improving the inference efficiency of LLMs without requiring expensive pretraining from scratch. A critical factor in the conversion process is layer selection, i.e., deciding on which layers to convert to linear attention variants. This paper describes a simple and efficient recipe for layer selection that uses layer importance scores derived from a small amount of training on generic text data. Once the layers have been selected we use a recent pipeline for the distillation process itself \\citep[RADLADS;][]{goldstein2025radlads}, which consists of attention weight transfer, hidden state alignment, KL-based distribution matching, followed by a small amount of finetuning. We find that this approach is more effective than existing approaches for layer selection, including heuristics that uniformly interleave linear attentions based on a fixed ratio, as well as more involved approaches that rely on specialized diagnostic datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5c42\u91cd\u8981\u6027\u5206\u6570\u7684\u7b80\u5355\u9ad8\u6548\u5c42\u9009\u62e9\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c06\u9884\u8bad\u7ec3Transformer\u84b8\u998f\u4e3a\u8f6f\u6ce8\u610f\u529b\u4e0e\u7ebf\u6027\u6ce8\u610f\u529b\u6df7\u5408\u67b6\u6784\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u6709\u6548\u3002", "motivation": "\u5c06\u9884\u8bad\u7ec3\u7684\u8f6f\u6ce8\u610f\u529bTransformer\u84b8\u998f\u4e3a\u6df7\u5408\u6ce8\u610f\u529b\u67b6\u6784\u53ef\u4ee5\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u4f46\u5173\u952e\u6311\u6218\u5728\u4e8e\u5982\u4f55\u9009\u62e9\u8981\u8f6c\u6362\u4e3a\u7ebf\u6027\u6ce8\u610f\u529b\u7684\u5c42\u3002", "method": "1) \u4f7f\u7528\u5c11\u91cf\u901a\u7528\u6587\u672c\u6570\u636e\u8bad\u7ec3\u5f97\u5230\u5c42\u91cd\u8981\u6027\u5206\u6570\u8fdb\u884c\u5c42\u9009\u62e9\uff1b2) \u91c7\u7528RADLADS\u84b8\u998f\u6d41\u7a0b\uff1a\u6ce8\u610f\u529b\u6743\u91cd\u8f6c\u79fb\u3001\u9690\u85cf\u72b6\u6001\u5bf9\u9f50\u3001KL\u5206\u5e03\u5339\u914d\u548c\u5c11\u91cf\u5fae\u8c03\u3002", "result": "\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709\u5c42\u9009\u62e9\u65b9\u6cd5\u66f4\u6709\u6548\uff0c\u5305\u62ec\u57fa\u4e8e\u56fa\u5b9a\u6bd4\u7387\u7684\u5747\u5300\u4ea4\u9519\u7ebf\u6027\u6ce8\u610f\u529b\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u4ee5\u53ca\u4f9d\u8d56\u4e13\u95e8\u8bca\u65ad\u6570\u636e\u96c6\u7684\u590d\u6742\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8e\u5c42\u91cd\u8981\u6027\u5206\u6570\u7684\u7b80\u5355\u5c42\u9009\u62e9\u65b9\u6cd5\u7ed3\u5408RADLADS\u84b8\u998f\u6d41\u7a0b\uff0c\u80fd\u591f\u6709\u6548\u5c06\u9884\u8bad\u7ec3Transformer\u8f6c\u6362\u4e3a\u9ad8\u6548\u6df7\u5408\u6ce8\u610f\u529b\u67b6\u6784\u3002"}}
{"id": "2512.20578", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20578", "abs": "https://arxiv.org/abs/2512.20578", "authors": ["Amirhosein Ghasemabadi", "Di Niu"], "title": "Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits", "comment": null, "summary": "Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision.", "AI": {"tldr": "Gnosis\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u81ea\u611f\u77e5\u673a\u5236\uff0c\u901a\u8fc7\u89e3\u7801LLM\u5185\u90e8\u72b6\u6001\uff08\u9690\u85cf\u72b6\u6001\u548c\u6ce8\u610f\u529b\u6a21\u5f0f\uff09\u6765\u9884\u6d4b\u81ea\u8eab\u9519\u8bef\uff0c\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u751f\u6210\u6d41\u7545\u590d\u6742\u7684\u8f93\u51fa\uff0c\u4f46\u5e38\u5e38\u65e0\u6cd5\u8bc6\u522b\u81ea\u5df1\u7684\u9519\u8bef\u548c\u5e7b\u89c9\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u8bc4\u5224\u8005\u3001\u591a\u6837\u672c\u4e00\u81f4\u6027\u6216\u57fa\u4e8e\u6587\u672c\u7684\u81ea\u6211\u6279\u5224\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u8981\u4e48\u589e\u52a0\u8ba1\u7b97\u6210\u672c\uff0c\u8981\u4e48\u4e0e\u771f\u5b9e\u6b63\u786e\u6027\u76f8\u5173\u6027\u8f83\u5f31\u3002", "method": "Gnosis\u901a\u8fc7\u88ab\u52a8\u89c2\u5bdfLLM\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u5185\u90e8\u75d5\u8ff9\uff08\u9690\u85cf\u72b6\u6001\u548c\u6ce8\u610f\u529b\u6a21\u5f0f\uff09\uff0c\u5c06\u5176\u538b\u7f29\u4e3a\u56fa\u5b9a\u9884\u7b97\u7684\u63cf\u8ff0\u7b26\uff0c\u7136\u540e\u9884\u6d4b\u6b63\u786e\u6027\u3002\u8be5\u65b9\u6cd5\u4ec5\u589e\u52a0\u7ea6500\u4e07\u4e2a\u53c2\u6570\uff0c\u4e0e\u5e8f\u5217\u957f\u5ea6\u65e0\u5173\uff0c\u63a8\u7406\u6210\u672c\u53ef\u5ffd\u7565\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u5f00\u653e\u57df\u95ee\u7b54\u548c\u5b66\u672f\u77e5\u8bc6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u57281.7B\u523020B\u53c2\u6570\u7684\u51bb\u7ed3\u9aa8\u5e72\u6a21\u578b\u4e0a\uff0cGnosis\u5728\u51c6\u786e\u6027\u548c\u6821\u51c6\u65b9\u9762\u5747\u4f18\u4e8e\u5f3a\u5927\u7684\u5185\u90e8\u57fa\u7ebf\u548c\u5927\u578b\u5916\u90e8\u8bc4\u5224\u8005\u3002\u6b64\u5916\uff0c\u5b83\u80fd\u96f6\u6837\u672c\u6cdb\u5316\u5230\u90e8\u5206\u751f\u6210\uff0c\u5b9e\u73b0\u65e9\u671f\u9519\u8bef\u68c0\u6d4b\u548c\u8ba1\u7b97\u611f\u77e5\u63a7\u5236\u3002", "conclusion": "\u53ef\u9760\u7684\u6b63\u786e\u6027\u7ebf\u7d22\u5185\u5728\u4e8e\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\u5373\u53ef\u9ad8\u6548\u63d0\u53d6\u3002\u8fd9\u8868\u660eLLM\u5177\u6709\u901a\u8fc7\u5185\u90e8\u72b6\u6001\u8fdb\u884c\u81ea\u6211\u9a8c\u8bc1\u7684\u5185\u5728\u80fd\u529b\u3002"}}
{"id": "2512.20595", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20595", "abs": "https://arxiv.org/abs/2512.20595", "authors": ["Dhruv Anand", "Ehsan Shareghi"], "title": "Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs", "comment": "27 pages, 5 figures, 9 tables. Cube available at https://github.com/dana-23/cube-bench", "summary": "We introduce Cube Bench, a Rubik's-cube benchmark for evaluating spatial and sequential reasoning in multimodal large language models (MLLMs). The benchmark decomposes performance into five skills: (i) reconstructing cube faces from images and text, (ii) choosing the optimal next move, (iii) predicting the outcome of a candidate move without applying it, (iv) executing multi-step plans while recovering from mistakes, and (v) detecting and revising one's own errors. Using a shared set of scrambled cube states, identical prompts and parsers, and a single distance-to-solved metric, we compare recent MLLMs side by side as a function of scramble depth. Across seven MLLMs, accuracy drops sharply with depth; once a trajectory stalls or diverges, models rarely recover, and high face-reconstruction accuracy does not guarantee competent action selection or multi-step execution. A pronounced closed- vs open-source gap emerges: the strongest closed model leads on both single-step perception tasks and multi-step control tasks, while open-weight models cluster near chance on the hardest settings; yet even the best MLLM degrades at higher cube complexity. A simple self-correction via reflective thinking yields modest gains but can also introduce overthinking. Cube Bench offers a compact, reproducible probe of sequential spatial reasoning in MLLMs.", "AI": {"tldr": "Cube Bench\u662f\u4e00\u4e2a\u57fa\u4e8e\u9b54\u65b9\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u7a7a\u95f4\u548c\u5e8f\u5217\u63a8\u7406\u80fd\u529b\uff0c\u5305\u542b\u4e94\u4e2a\u6280\u80fd\u7ef4\u5ea6\uff0c\u7ed3\u679c\u663e\u793a\u6a21\u578b\u6027\u80fd\u968f\u9b54\u65b9\u590d\u6742\u5ea6\u589e\u52a0\u800c\u6025\u5267\u4e0b\u964d\uff0c\u95ed\u6e90\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\u3002", "motivation": "\u73b0\u6709MLLM\u57fa\u51c6\u6d4b\u8bd5\u7f3a\u4e4f\u5bf9\u7a7a\u95f4\u548c\u5e8f\u5217\u63a8\u7406\u80fd\u529b\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u9700\u8981\u591a\u6b65\u51b3\u7b56\u548c\u9519\u8bef\u6062\u590d\u7684\u590d\u6742\u4efb\u52a1\u3002\u9b54\u65b9\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7d27\u51d1\u3001\u53ef\u91cd\u590d\u7684\u6d4b\u8bd5\u573a\u666f\u6765\u5168\u9762\u8bc4\u4f30\u8fd9\u4e9b\u80fd\u529b\u3002", "method": "\u521b\u5efaCube Bench\u57fa\u51c6\uff0c\u5c06\u9b54\u65b9\u89e3\u8c1c\u5206\u89e3\u4e3a\u4e94\u4e2a\u6280\u80fd\uff1a(1)\u4ece\u56fe\u50cf\u548c\u6587\u672c\u91cd\u5efa\u9b54\u65b9\u9762\uff0c(2)\u9009\u62e9\u6700\u4f18\u4e0b\u4e00\u6b65\u79fb\u52a8\uff0c(3)\u9884\u6d4b\u5019\u9009\u79fb\u52a8\u7ed3\u679c\u800c\u4e0d\u5b9e\u9645\u6267\u884c\uff0c(4)\u6267\u884c\u591a\u6b65\u8ba1\u5212\u5e76\u4ece\u4e2d\u9519\u8bef\u4e2d\u6062\u590d\uff0c(5)\u68c0\u6d4b\u548c\u4fee\u6b63\u81ea\u8eab\u9519\u8bef\u3002\u4f7f\u7528\u5171\u4eab\u7684\u9b54\u65b9\u6253\u4e71\u72b6\u6001\u3001\u76f8\u540c\u63d0\u793a\u548c\u89e3\u6790\u5668\uff0c\u4ee5\u53ca\u5355\u4e00\u7684\u8ddd\u79bb\u6c42\u89e3\u5ea6\u91cf\uff0c\u6bd4\u8f837\u4e2aMLLM\u5728\u4e0d\u540c\u6253\u4e71\u6df1\u5ea6\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u6240\u6709MLLM\u7684\u51c6\u786e\u7387\u968f\u6253\u4e71\u6df1\u5ea6\u589e\u52a0\u800c\u6025\u5267\u4e0b\u964d\uff1b\u4e00\u65e6\u8f68\u8ff9\u505c\u6ede\u6216\u504f\u79bb\uff0c\u6a21\u578b\u5f88\u5c11\u80fd\u6062\u590d\uff1b\u9ad8\u9762\u91cd\u5efa\u51c6\u786e\u7387\u4e0d\u80fd\u4fdd\u8bc1\u826f\u597d\u7684\u52a8\u4f5c\u9009\u62e9\u6216\u591a\u6b65\u6267\u884c\u80fd\u529b\u3002\u95ed\u6e90\u6a21\u578b\u5728\u5355\u6b65\u611f\u77e5\u4efb\u52a1\u548c\u591a\u6b65\u63a7\u5236\u4efb\u52a1\u4e0a\u90fd\u9886\u5148\uff0c\u800c\u5f00\u6e90\u6a21\u578b\u5728\u6700\u96be\u8bbe\u7f6e\u4e0b\u63a5\u8fd1\u968f\u673a\u6c34\u5e73\u3002\u5373\u4f7f\u6700\u597d\u7684MLLM\u5728\u66f4\u9ad8\u590d\u6742\u5ea6\u4e0b\u4e5f\u4f1a\u6027\u80fd\u4e0b\u964d\u3002\u7b80\u5355\u7684\u81ea\u6211\u6821\u6b63\u901a\u8fc7\u53cd\u601d\u6027\u601d\u8003\u5e26\u6765\u9002\u5ea6\u6539\u8fdb\uff0c\u4f46\u4e5f\u53ef\u80fd\u5f15\u5165\u8fc7\u5ea6\u601d\u8003\u3002", "conclusion": "Cube Bench\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7d27\u51d1\u3001\u53ef\u91cd\u590d\u7684\u57fa\u51c6\u6765\u63a2\u6d4bMLLM\u7684\u5e8f\u5217\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5f53\u524dMLLM\u5728\u590d\u6742\u7a7a\u95f4\u5e8f\u5217\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u591a\u6b65\u89c4\u5212\u548c\u9519\u8bef\u6062\u590d\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u6539\u8fdb\u63d0\u4f9b\u4e86\u660e\u786e\u65b9\u5411\u3002"}}
{"id": "2512.20604", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20604", "abs": "https://arxiv.org/abs/2512.20604", "authors": ["Alexandros Christoforos", "Chadbourne Davis"], "title": "MoE-DiffuSeq: Enhancing Long-Document Diffusion Models with Sparse Attention and Mixture of Experts", "comment": "Under submission", "summary": "We present MoE-DiffuSeq, a mixture of experts based framework for enhancing diffusion models in long document generation. Existing diffusion based text generation models, such as DiffuSeq, suffer from high computational cost and memory overhead when applied to extended sequences. To address these challenges, MoE-DiffuSeq integrates sparse attention with a mixture of experts architecture, enabling efficient and scalable long sequence modeling. Our approach introduces a customized sparse attention mechanism designed to reduce computational complexity while preserving text quality and coherence. In addition, we incorporate a soft absorbing state within the diffusion process to accelerate sequence reconstruction and improve generation precision. Extensive experiments demonstrate that MoE-DiffuSeq significantly improves training efficiency and sampling speed compared to existing diffusion models. These advantages are particularly effective for long document scenarios, including scientific article generation, code repository modeling, and long form dialogue generation. Benchmark results further show that MoE-DiffuSeq improves efficiency, speed, accuracy, and expressiveness, advancing the practical applicability of diffusion models for high quality long form text generation.", "AI": {"tldr": "MoE-DiffuSeq\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e13\u5bb6\u6df7\u5408\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u6269\u6563\u6a21\u578b\u5728\u957f\u6587\u6863\u751f\u6210\u4e2d\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u7a00\u758f\u6ce8\u610f\u529b\u548c\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u63d0\u9ad8\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6587\u672c\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u7684\u6587\u672c\u751f\u6210\u6a21\u578b\uff08\u5982DiffuSeq\uff09\u5728\u5904\u7406\u957f\u5e8f\u5217\u65f6\u5b58\u5728\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u5f00\u9500\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u957f\u6587\u6863\u751f\u6210\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u96c6\u6210\u7a00\u758f\u6ce8\u610f\u529b\u4e0e\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\uff0c\u5f15\u5165\u5b9a\u5236\u5316\u7684\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5e76\u5728\u6269\u6563\u8fc7\u7a0b\u4e2d\u52a0\u5165\u8f6f\u5438\u6536\u72b6\u6001\u4ee5\u52a0\u901f\u5e8f\u5217\u91cd\u5efa\u548c\u63d0\u9ad8\u751f\u6210\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMoE-DiffuSeq\u5728\u8bad\u7ec3\u6548\u7387\u548c\u91c7\u6837\u901f\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6269\u6563\u6a21\u578b\uff0c\u5c24\u5176\u5728\u957f\u6587\u6863\u573a\u666f\uff08\u5982\u79d1\u5b66\u6587\u7ae0\u751f\u6210\u3001\u4ee3\u7801\u5e93\u5efa\u6a21\u548c\u957f\u5bf9\u8bdd\u751f\u6210\uff09\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u63d0\u5347\u4e86\u6548\u7387\u3001\u901f\u5ea6\u3001\u51c6\u786e\u6027\u548c\u8868\u8fbe\u80fd\u529b\u3002", "conclusion": "MoE-DiffuSeq\u901a\u8fc7\u7ed3\u5408\u7a00\u758f\u6ce8\u610f\u529b\u548c\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u957f\u6587\u672c\u751f\u6210\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u63a8\u52a8\u4e86\u9ad8\u8d28\u91cf\u957f\u6587\u672c\u751f\u6210\u6269\u6563\u6a21\u578b\u7684\u5b9e\u7528\u5316\u5e94\u7528\u3002"}}
