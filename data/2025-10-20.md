<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 52]
- [cs.IR](#cs.IR) [Total: 11]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Rethinking Toxicity Evaluation in Large Language Models: A Multi-Label Perspective](https://arxiv.org/abs/2510.15007)
*Zhiqiang Kou,Junyang Chen,Xin-Qiang Cai,Ming-Kun Xie,Biao Liu,Changwei Wang,Lei Feng,Yuheng Jia,Gang Niu,Masashi Sugiyama,Xin Geng*

Main category: cs.CL

TL;DR: 提出了三个新的多标签毒性检测基准，通过伪标签训练方法显著提升了毒性检测性能，超越了GPT-4o和DeepSeek等先进基线。


<details>
  <summary>Details</summary>
Motivation: 当前毒性检测器主要依赖单标签基准，无法充分捕捉真实世界毒性提示的模糊性和多维度特性，导致有偏见的评估，包括漏检和误报，削弱了现有检测器的可靠性。

Method: 引入了三个多标签毒性检测基准（Q-A-MLL、R-A-MLL、H-X-MLL），基于详细的15类别分类法进行标注，并开发了基于伪标签的毒性检测方法。

Result: 实验结果表明，该方法显著超越了包括GPT-4o和DeepSeek在内的先进基线，能够更准确可靠地评估LLM生成内容中的多标签毒性。

Conclusion: 通过多标签基准和伪标签训练方法，有效解决了毒性检测中的模糊性和多维度问题，为LLM安全性评估提供了更可靠的解决方案。

Abstract: Large language models (LLMs) have achieved impressive results across a range
of natural language processing tasks, but their potential to generate harmful
content has raised serious safety concerns. Current toxicity detectors
primarily rely on single-label benchmarks, which cannot adequately capture the
inherently ambiguous and multi-dimensional nature of real-world toxic prompts.
This limitation results in biased evaluations, including missed toxic
detections and false positives, undermining the reliability of existing
detectors. Additionally, gathering comprehensive multi-label annotations across
fine-grained toxicity categories is prohibitively costly, further hindering
effective evaluation and development. To tackle these issues, we introduce
three novel multi-label benchmarks for toxicity detection: \textbf{Q-A-MLL},
\textbf{R-A-MLL}, and \textbf{H-X-MLL}, derived from public toxicity datasets
and annotated according to a detailed 15-category taxonomy. We further provide
a theoretical proof that, on our released datasets, training with pseudo-labels
yields better performance than directly learning from single-label supervision.
In addition, we develop a pseudo-label-based toxicity detection method.
Extensive experimental results show that our approach significantly surpasses
advanced baselines, including GPT-4o and DeepSeek, thus enabling more accurate
and reliable evaluation of multi-label toxicity in LLM-generated content.

</details>


### [2] [Can generative AI figure out figurative language? The influence of idioms on essay scoring by ChatGPT, Gemini, and Deepseek](https://arxiv.org/abs/2510.15009)
*Enis Oğuz*

Main category: cs.CL

TL;DR: 本研究评估了三种生成式AI模型（ChatGPT、Gemini和Deepseek）在评分含成语和不含成语学生作文时的表现，发现Gemini在与人评分者的一致性方面表现最佳，特别是在处理含成语作文时最接近人类评分模式。


<details>
  <summary>Details</summary>
Motivation: 生成式AI技术在教育领域的应用日益广泛，特别是在自动作文评分方面。考虑到AI在处理成语等修辞语言方面的潜在局限性，本研究旨在评估生成式AI模型在评分含成语和不含成语作文时的表现差异。

Method: 从348篇学生作文中创建两个相等的作文列表：一个包含多个成语，另一个不含成语。使用ChatGPT、Gemini和Deepseek三个模型，按照人类评分者使用的相同评分标准对两组作文各评分三次。

Result: 所有模型都表现出极好的一致性，但Gemini在与人评分者的评分者间信度方面优于其他模型。AI评估未检测到对任何人口群体的偏见。对于含多个成语的作文，Gemini的评分模式最接近人类评分者。

Conclusion: 虽然所有模型都显示出采用混合方法的潜力，但Gemini因其处理修辞语言的能力而成为最佳选择，并显示出未来可能独立处理作文评分任务的前景。

Abstract: The developments in Generative AI technologies have paved the way for
numerous innovations in different fields. Recently, Generative AI has been
proposed as a competitor to AES systems in evaluating student essays
automatically. Considering the potential limitations of AI in processing
idioms, this study assessed the scoring performances of Generative AI models
for essays with and without idioms by incorporating insights from Corpus
Linguistics and Computational Linguistics. Two equal essay lists were created
from 348 student essays taken from a corpus: one with multiple idioms present
in each essay and another with no idioms in essays. Three Generative AI models
(ChatGPT, Gemini, and Deepseek) were asked to score all essays in both lists
three times, using the same rubric used by human raters in assigning essay
scores. The results revealed excellent consistency for all models, but Gemini
outperformed its competitors in interrater reliability with human raters. There
was also no detectable bias for any demographic group in AI assessment. For
essays with multiple idioms, Gemini followed a the most similar pattern to
human raters. While the models in the study demonstrated potential for a hybrid
approach, Gemini was the best candidate for the task due to its ability to
handle figurative language and showed promise for handling essay-scoring tasks
alone in the future.

</details>


### [3] [A Generalizable Rhetorical Strategy Annotation Model Using LLM-based Debate Simulation and Labelling](https://arxiv.org/abs/2510.15081)
*Shiyu Ji,Farnoosh Hashemi,Joice Chen,Juanwen Pan,Weicheng Ma,Hefan Zhang,Sophia Pan,Ming Cheng,Shubham Mohole,Saeed Hassanpour,Soroush Vosoughi,Michael Macy*

Main category: cs.CL

TL;DR: 提出一个利用大语言模型自动生成和标注辩论数据的框架，基于四种修辞类型学（因果、经验、情感、道德）来训练修辞策略分类器，并在多个外部语料库上验证其性能。


<details>
  <summary>Details</summary>
Motivation: 修辞策略分析长期以来依赖人工标注，成本高、一致性差、难以扩展，且现有数据集通常局限于特定主题和策略，限制了稳健模型的发展。

Method: 利用大语言模型自动生成和标注合成辩论数据，基于四种修辞类型学（因果、经验、情感、道德），然后对基于Transformer的分类器进行微调，并在人类标注数据和多个外部语料库上进行验证。

Result: 模型实现了高性能和强大的跨领域泛化能力。应用表明：1）加入修辞策略标签能提高说服力预测；2）分析1960-2020年美国总统辩论显示情感论证使用增加而认知论证减少。

Conclusion: 该框架成功解决了修辞策略分析的扩展性问题，展示了在说服力预测和政治话语分析中的实际应用价值，揭示了美国政治辩论中修辞策略的演变趋势。

Abstract: Rhetorical strategies are central to persuasive communication, from political
discourse and marketing to legal argumentation. However, analysis of rhetorical
strategies has been limited by reliance on human annotation, which is costly,
inconsistent, difficult to scale. Their associated datasets are often limited
to specific topics and strategies, posing challenges for robust model
development. We propose a novel framework that leverages large language models
(LLMs) to automatically generate and label synthetic debate data based on a
four-part rhetorical typology (causal, empirical, emotional, moral). We
fine-tune transformer-based classifiers on this LLM-labeled dataset and
validate its performance against human-labeled data on this dataset and on
multiple external corpora. Our model achieves high performance and strong
generalization across topical domains. We illustrate two applications with the
fine-tuned model: (1) the improvement in persuasiveness prediction from
incorporating rhetorical strategy labels, and (2) analyzing temporal and
partisan shifts in rhetorical strategies in U.S. Presidential debates
(1960-2020), revealing increased use of affective over cognitive argument in
U.S. Presidential debates.

</details>


### [4] [Continual Learning via Sparse Memory Finetuning](https://arxiv.org/abs/2510.15103)
*Jessy Lin,Luke Zettlemoyer,Gargi Ghosh,Wen-Tau Yih,Aram Markosyan,Vincent-Pierre Berges,Barlas Oğuz*

Main category: cs.CL

TL;DR: 稀疏记忆微调通过仅更新被新知识高度激活的记忆槽，显著减少灾难性遗忘，在保持新知识学习能力的同时，遗忘率从全微调的89%降至11%。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型部署后通常是静态的，持续学习面临灾难性遗忘问题。由于可训练参数在所有任务间共享，缓解遗忘具有挑战性，因此研究稀疏参数更新是否能实现无灾难性遗忘的学习。

Method: 引入稀疏记忆微调，利用记忆层模型（Berges等人，2024），这些模型设计上就是稀疏更新的。通过仅更新被新知识相对于预训练数据使用高度激活的记忆槽，减少新知识与模型现有能力之间的干扰。

Result: 在两个问答任务上评估学习和遗忘，与全微调和LoRA参数高效微调相比，稀疏记忆微调在学习新知识的同时表现出显著更少的遗忘：NaturalQuestions F1在全微调新事实后下降89%，LoRA下降71%，而稀疏记忆微调仅下降11%，且新知识获取水平相同。

Conclusion: 记忆层中的稀疏性为大型语言模型的持续学习提供了一条有前景的路径。

Abstract: Modern language models are powerful, but typically static after deployment. A
major obstacle to building models that continually learn over time is
catastrophic forgetting, where updating on new data erases previously acquired
capabilities. Motivated by the intuition that mitigating forgetting is
challenging because trainable parameters are shared across all tasks, we
investigate whether sparse parameter updates can enable learning without
catastrophic forgetting. We introduce sparse memory finetuning, leveraging
memory layer models (Berges et al., 2024), which are sparsely updated by
design. By updating only the memory slots that are highly activated by a new
piece of knowledge relative to usage on pretraining data, we reduce
interference between new knowledge and the model's existing capabilities. We
evaluate learning and forgetting compared to full finetuning and
parameter-efficient finetuning with LoRA on two question answering tasks. We
find that sparse memory finetuning learns new knowledge while exhibiting
substantially less forgetting: while NaturalQuestions F1 drops by 89% after
full finetuning on new facts and 71% with LoRA, sparse memory finetuning yields
only an 11% drop with the same level of new knowledge acquisition. Our results
suggest sparsity in memory layers offers a promising path toward continual
learning in large language models.

</details>


### [5] [Measuring the Effect of Disfluency in Multilingual Knowledge Probing Benchmarks](https://arxiv.org/abs/2510.15115)
*Kirill Semenov,Rico Sennrich*

Main category: cs.CL

TL;DR: 本文研究发现，MLAMA等多语言事实知识评估基准使用模板翻译时忽略了命名实体的语法和语义信息，导致提示句不自然或错误。通过对比斯拉夫语系语言在原始MLAMA数据集与谷歌翻译、ChatGPT整句翻译版本上的知识检索得分，发现整句翻译能显著提高得分，建议社区控制多语言数据集的语法正确性以获得更可靠结果。


<details>
  <summary>Details</summary>
Motivation: 现有的多语言事实知识评估基准（如MLAMA）使用模板翻译方法，但这种方法没有考虑插入命名实体的语法和语义信息，导致最终提示句存在大量语法错误或措辞不当，特别是在形态丰富的语言中，这影响了得分的解释性。

Method: 从MLAMA数据集中选取4种斯拉夫语言，比较原始（模板化）MLAMA数据集与使用谷歌翻译和ChatGPT进行整句翻译版本的知识检索得分，并进行定性分析。额外分析了来自不同语系的5种语言以验证模式。

Result: 观察到知识检索得分显著提高，整句翻译方法能更好地处理语法和语义问题。对其他语系语言的额外分析也显示出类似模式。

Conclusion: 建议社区控制高度多语言数据集的语法正确性，以获得更高且更可解释的结果，这可以通过使用神经机器翻译或LLM系统进行整句翻译来近似实现。

Abstract: For multilingual factual knowledge assessment of LLMs, benchmarks such as
MLAMA use template translations that do not take into account the grammatical
and semantic information of the named entities inserted in the sentence. This
leads to numerous instances of ungrammaticality or wrong wording of the final
prompts, which complicates the interpretation of scores, especially for
languages that have a rich morphological inventory. In this work, we sample 4
Slavic languages from the MLAMA dataset and compare the knowledge retrieval
scores between the initial (templated) MLAMA dataset and its sentence-level
translations made by Google Translate and ChatGPT. We observe a significant
increase in knowledge retrieval scores, and provide a qualitative analysis for
possible reasons behind it. We also make an additional analysis of 5 more
languages from different families and see similar patterns. Therefore, we
encourage the community to control the grammaticality of highly multilingual
datasets for higher and more interpretable results, which is well approximated
by whole sentence translation with neural MT or LLM systems. The dataset and
all related code is published at the Github repository:
https://github.com/ZurichNLP/Fluent-mLAMA.

</details>


### [6] [Latent Topic Synthesis: Leveraging LLMs for Electoral Ad Analysis](https://arxiv.org/abs/2510.15125)
*Alexander Brady,Tunazzina Islam*

Main category: cs.CL

TL;DR: 提出了一个端到端的框架，通过无监督聚类和基于提示的标签，利用大语言模型自动从无标签语料库生成可解释的主题分类法，并应用于2024年美国总统选举前的Meta政治广告分析。


<details>
  <summary>Details</summary>
Motivation: 社交媒体平台在塑造政治话语中发挥关键作用，但分析其庞大且快速演变的内容仍然是一个重大挑战。需要一种无需种子集或领域专业知识的方法来理解政治传播结构。

Method: 结合无监督聚类与基于提示的标签方法，利用大语言模型迭代构建分类法，并标注道德框架维度。

Result: 发现投票和移民广告在支出和展示量中占主导地位，而堕胎和选举诚信话题获得了不成比例的关注。经济诉求主要由保守派PAC推动，堕胎信息在支持与反对权利联盟间分裂，犯罪与司法运动在地方委员会中分散。不同诉求的框架也存在差异。

Conclusion: 该框架支持对社交媒体政治信息进行可扩展、可解释的分析，使研究人员、政策制定者和公众能够更好地理解新兴叙事、极化动态以及数字政治传播的道德基础。

Abstract: Social media platforms play a pivotal role in shaping political discourse,
but analyzing their vast and rapidly evolving content remains a major
challenge. We introduce an end-to-end framework for automatically generating an
interpretable topic taxonomy from an unlabeled corpus. By combining
unsupervised clustering with prompt-based labeling, our method leverages large
language models (LLMs) to iteratively construct a taxonomy without requiring
seed sets or domain expertise. We apply this framework to a large corpus of
Meta (previously known as Facebook) political ads from the month ahead of the
2024 U.S. Presidential election. Our approach uncovers latent discourse
structures, synthesizes semantically rich topic labels, and annotates topics
with moral framing dimensions. We show quantitative and qualitative analyses to
demonstrate the effectiveness of our framework. Our findings reveal that voting
and immigration ads dominate overall spending and impressions, while abortion
and election-integrity achieve disproportionate reach. Funding patterns are
equally polarized: economic appeals are driven mainly by conservative PACs,
abortion messaging splits between pro- and anti-rights coalitions, and
crime-and-justice campaigns are fragmented across local committees. The framing
of these appeals also diverges--abortion ads emphasize liberty/oppression
rhetoric, while economic messaging blends care/harm, fairness/cheating, and
liberty/oppression narratives. Topic salience further reveals strong
correlations between moral foundations and issues. Demographic targeting also
emerges. This work supports scalable, interpretable analysis of political
messaging on social media, enabling researchers, policymakers, and the public
to better understand emerging narratives, polarization dynamics, and the moral
underpinnings of digital political communication.

</details>


### [7] [FarsiMCQGen: a Persian Multiple-choice Question Generation Framework](https://arxiv.org/abs/2510.15134)
*Mohammad Heydari Rad,Rezvan Afari,Saeedeh Momtazi*

Main category: cs.CL

TL;DR: FarsiMCQGen是一个用于生成波斯语多项选择题的创新方法，结合候选生成、过滤和排序技术，利用Transformer和知识图谱生成可信的干扰项。


<details>
  <summary>Details</summary>
Motivation: 在波斯语等低资源语言中生成高质量的多项选择题面临挑战，需要有效的方法来评估学习者知识。

Method: 结合候选生成、过滤和排序技术，利用Transformer和知识图谱与基于规则的方法集成，从维基百科数据构建波斯语MCQ数据集。

Result: 创建了包含10,289个问题的新波斯语MCQ数据集，经最先进的大型语言模型评估，证明了模型有效性和数据集质量。

Conclusion: FarsiMCQGen模型和生成的数据集在波斯语MCQ生成方面表现出色，有望推动该领域的进一步研究。

Abstract: Multiple-choice questions (MCQs) are commonly used in educational testing, as
they offer an efficient means of evaluating learners' knowledge. However,
generating high-quality MCQs, particularly in low-resource languages such as
Persian, remains a significant challenge. This paper introduces FarsiMCQGen, an
innovative approach for generating Persian-language MCQs. Our methodology
combines candidate generation, filtering, and ranking techniques to build a
model that generates answer choices resembling those in real MCQs. We leverage
advanced methods, including Transformers and knowledge graphs, integrated with
rule-based approaches to craft credible distractors that challenge test-takers.
Our work is based on data from Wikipedia, which includes general knowledge
questions. Furthermore, this study introduces a novel Persian MCQ dataset
comprising 10,289 questions. This dataset is evaluated by different
state-of-the-art large language models (LLMs). Our results demonstrate the
effectiveness of our model and the quality of the generated dataset, which has
the potential to inspire further research on MCQs.

</details>


### [8] [Structure-R1: Dynamically Leveraging Structural Knowledge in LLM Reasoning through Reinforcement Learning](https://arxiv.org/abs/2510.15191)
*Junlin Wu,Xianrui Zhong,Jiashuo Sun,Bolian Li,Bowen Jin,Jiawei Han,Qingkai Zeng*

Main category: cs.CL

TL;DR: Structure-R1是一个新颖的框架，通过将检索内容转化为结构化表示来增强大型语言模型的推理能力，使用强化学习动态生成任务特定的结构格式，并在多个知识密集型基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成(RAG)系统处理非结构化文本导致信息密度低和推理效果不佳，需要更有效的结构化知识表示来提升推理能力。

Method: 提出Structure-R1框架，使用强化学习学习内容表示策略，动态生成适应多步推理需求的结构化格式，并引入自奖励结构验证机制确保生成结构的质量和可靠性。

Result: 在七个知识密集型基准测试中，Structure-R1使用7B参数规模的骨干模型取得了具有竞争力的性能，并能与更大模型的表现相匹配。

Conclusion: 结构化表示通过提高信息密度和上下文清晰度来增强推理能力，Structure-R1为知识密集型任务提供了有效的解决方案。

Abstract: Large language models (LLMs) have demonstrated remarkable advances in
reasoning capabilities. However, their performance remains constrained by
limited access to explicit and structured domain knowledge. Retrieval-Augmented
Generation (RAG) addresses this by incorporating external information as
context to augment reasoning. Nevertheless, traditional RAG systems typically
operate over unstructured and fragmented text, resulting in low information
density and suboptimal reasoning. To overcome these limitations, we propose
\textsc{Structure-R1}, a novel framework that transforms retrieved content into
structured representations optimized for reasoning. Leveraging reinforcement
learning, \textsc{Structure-R1} learns a content representation policy that
dynamically generates and adapts structural formats based on the demands of
multi-step reasoning. Unlike prior methods that rely on fixed schemas, our
approach adopts a generative paradigm capable of producing task-specific
structures tailored to individual queries. To ensure the quality and
reliability of these representations, we introduce a self-reward structural
verification mechanism that checks whether the generated structures are both
correct and self-contained. Extensive experiments on seven knowledge-intensive
benchmarks show that \textsc{Structure-R1} consistently achieves competitive
performance with a 7B-scale backbone model and matches the performance of much
larger models. Additionally, our theoretical analysis demonstrates how
structured representations enhance reasoning by improving information density
and contextual clarity. Our code and data are available at:
https://github.com/jlwu002/sr1.

</details>


### [9] [Extending Audio Context for Long-Form Understanding in Large Audio-Language Models](https://arxiv.org/abs/2510.15231)
*Yuatyong Chaichana,Pittawat Taveekitworachai,Warit Sirichotedumrong,Potsawee Manakul,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: 提出了Partial YaRN和VLAT两种方法来解决大型音频语言模型的长上下文限制问题，前者是无需训练的位置编码扩展方法，后者是训练策略，能够显著提升模型对长音频的理解能力。


<details>
  <summary>Details</summary>
Motivation: 大型音频语言模型受限于短音频上下文窗口，即使其文本主干支持长上下文，这限制了长格式音频理解能力。

Method: 1. Partial YaRN：基于RoPE的无训练音频扩展方法，只修改音频token位置，保持文本位置不变以保留基础LLM的文本能力；2. VLAT：训练策略，将Partial YaRN扩展为训练时的位置增强，在训练中模拟不同音频长度。

Result: 在SALMONN和Qwen2-Audio上的实验表明，Partial YaRN在多种设置下优于原始模型，VLAT训练策略提供了显著改进，在未见长度的长音频上实现了强大性能。

Conclusion: Partial YaRN和VLAT方法有效解决了LALMs的长上下文限制问题，Partial YaRN作为无训练方法表现优异，VLAT进一步提升了模型对长音频的泛化能力和鲁棒性。

Abstract: Large Audio-Language Models (LALMs) are often constrained by short audio
context windows, even when their text backbones support long contexts, limiting
long-form audio understanding. Prior work has introduced context-extension
methods (e.g. YaRN) on unimodal LLMs, yet their application to LALMs remains
unexplored. First, building on RoPE-based context extension, we introduce
Partial YaRN, a training-free, audio-only extension method that modifies only
audio token positions, leaving text positions intact to preserve the base LLM's
text capabilities. Second, we propose Virtual Longform Audio Training (VLAT), a
training strategy that extends Partial YaRN into a training-time positional
augmentation. VLAT simulates diverse audio lengths during training, enabling
generalization to inputs far longer than those seen in training and improving
robustness for long-context audio understanding. Our experiments on SALMONN and
Qwen2-Audio show that Partial YaRN outperforms the original models across wide
range of settings, and VLAT training strategy provides substantial improvement,
achieving strong performance on long audio of unseen lengths.

</details>


### [10] [MCA: Modality Composition Awareness for Robust Composed Multimodal Retrieval](https://arxiv.org/abs/2510.15543)
*Qiyu Wu,Shuyang Cui,Satoshi Hayakawa,Wei-Yao Wang,Hiromi Wakaki,Yuki Mitsufuji*

Main category: cs.CL

TL;DR: 提出了一个模态组合感知框架，通过偏好损失和组合正则化目标来缓解统一编码器在对比学习中学习模态捷径的问题，提高多模态检索在分布变化下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管统一编码器在多模态大语言模型中提供了灵活性，但传统的对比学习容易导致模态捷径学习，从而在分布变化下表现不佳。

Method: 使用偏好损失强制多模态嵌入优于单模态对应物，同时通过组合正则化目标将多模态嵌入与其单模态部分组成的原型对齐，显式建模组合表示与单模态对应物之间的结构关系。

Result: 在各种基准测试中，该方法在分布外检索方面表现出改进，证明了模态组合感知是使用MLLMs作为统一编码器时实现鲁棒组合多模态检索的有效原则。

Conclusion: 模态组合感知框架通过显式建模多模态表示与其单模态组件之间的关系，有效缓解了统一编码器中的模态捷径问题，提高了多模态检索的鲁棒性。

Abstract: Multimodal retrieval, which seeks to retrieve relevant content across
modalities such as text or image, supports applications from AI search to
contents production. Despite the success of separate-encoder approaches like
CLIP align modality-specific embeddings with contrastive learning, recent
multimodal large language models (MLLMs) enable a unified encoder that directly
processes composed inputs. While flexible and advanced, we identify that
unified encoders trained with conventional contrastive learning are prone to
learn modality shortcut, leading to poor robustness under distribution shifts.
We propose a modality composition awareness framework to mitigate this issue.
Concretely, a preference loss enforces multimodal embeddings to outperform
their unimodal counterparts, while a composition regularization objective
aligns multimodal embeddings with prototypes composed from its unimodal parts.
These objectives explicitly model structural relationships between the composed
representation and its unimodal counterparts. Experiments on various benchmarks
show gains in out-of-distribution retrieval, highlighting modality composition
awareness as a effective principle for robust composed multimodal retrieval
when utilizing MLLMs as the unified encoder.

</details>


### [11] [Planner and Executor: Collaboration between Discrete Diffusion And Autoregressive Models in Reasoning](https://arxiv.org/abs/2510.15244)
*Lina Berrayana,Ahmed Heakl,Muhammad Abdullah Sohail,Thomas Hofmann,Salman Khan,Wei Chen*

Main category: cs.CL

TL;DR: 本研究探索了离散扩散语言模型（DDLM）与自回归语言模型（ARM）的混合架构，通过在文本空间和潜在空间中的协作，实现了显著的准确率提升和计算效率优化。


<details>
  <summary>Details</summary>
Motivation: 当前自回归语言模型虽然准确率高但需要长序列生成，成本高昂；离散扩散语言模型具有并行生成和固定步数的优势，在复杂推理任务中表现优异。研究旨在探索两者协作是否能带来互补效益。

Method: 首先在文本空间中探索协作，一个模型规划推理过程，另一个基于规划执行答案；然后扩展到潜在空间通信，引入学习投影器将DDLM潜在表示映射到ARM嵌入空间，绕过扩散模型的文本生成限制。

Result: 从文本空间转向潜在空间的DDLM-ARM通信带来显著准确率提升：DART-5从27.0%提升至54.0%，AIME24从0.0%提升至14.0%。混合架构还提供大幅计算节省，潜在空间管道使用64个token进行规划和约5个token执行，在DART-5和AIME上超越Qwen3.1-7B，而后者使用44倍更多token。

Conclusion: 研究为DDLM推理提供了新见解，并突显了其在混合架构中的潜力，展示了通过架构创新在保持准确性的同时大幅提升效率的可能性。

Abstract: Current autoregressive language models (ARMs) achieve high accuracy but
require long token sequences, making them costly. Discrete diffusion language
models (DDLMs) enable parallel and flexible generation within a fixed number of
steps and have recently emerged for their strong performance in complex
reasoning and long-term planning tasks. We present a study exploring hybrid
architectures that couple DDLMs with ARMs to assess whether their collaboration
can yield complementary benefits. We first examine collaboration in text space,
where one model plans the reasoning process and another executes the final
answer based on that plan. We then extend this setup to latent-space
communication, introducing a learned projector that maps DDLM latents into the
ARM's embedding space, potentially bypassing some of the text-generation
limitations of diffusion models. We find that shifting DDLM --> ARM
communication from text space to latent space yields significant accuracy
gains, for example increasing from 27.0% to 54.0% on DART-5 and from 0.0% to
14.0% on AIME24. We also find that combining a DDLM planner with an ARM
executor can provide substantial computational savings with little to no impact
on accuracy. For example, the latent-space pipeline, using 64 tokens for
planning and roughly 5 for execution, surpasses Qwen3.1-7B on DART-5 and AIME,
despite Qwen using 44 times more tokens. Overall, our study offers new insights
into reasoning with DDLMs and highlights their potential in hybrid
architectures.

</details>


### [12] [Cost-Aware Retrieval-Augmentation Reasoning Models with Adaptive Retrieval Depth](https://arxiv.org/abs/2510.15719)
*Helia Hashemi,Victor Rühle,Saravan Rajmohan*

Main category: cs.CL

TL;DR: 提出一种动态调整检索文档长度的检索增强推理模型，通过强化学习训练成本感知模型，在保持效果的同时显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型虽然性能强大，但检索和推理token都导致高昂计算成本，需要更高效的解决方案。

Method: 1) 动态调整检索文档长度；2) 开发成本感知优势函数用于强化学习训练；3) 探索内存和延迟约束下的实现方案。

Result: 在7个公开问答数据集上评估，模型延迟降低16-20%，同时精确匹配效果平均提升5%。

Conclusion: 该方法在保持甚至提升模型效果的同时，显著降低了计算成本，实现了效率与效果的平衡。

Abstract: Reasoning models have gained significant attention due to their strong
performance, particularly when enhanced with retrieval augmentation. However,
these models often incur high computational costs, as both retrieval and
reasoning tokens contribute substantially to the overall resource usage. In
this work, we make the following contributions: (1) we propose a
retrieval-augmented reasoning model that dynamically adjusts the length of the
retrieved document list based on the query and retrieval results; (2) we
develop a cost-aware advantage function for training of efficient
retrieval-augmented reasoning models through reinforcement learning; and (3) we
explore both memory- and latency-bound implementations of the proposed
cost-aware framework for both proximal and group relative policy optimization
algorithms. We evaluate our approach on seven public question answering
datasets and demonstrate significant efficiency gains, without compromising
effectiveness. In fact, we observed that the model latency decreases by ~16-20%
across datasets, while its effectiveness increases by ~5% on average, in terms
of exact match.

</details>


### [13] [Scaling Beyond Context: A Survey of Multimodal Retrieval-Augmented Generation for Document Understanding](https://arxiv.org/abs/2510.15253)
*Sensen Gao,Shanshan Zhao,Xu Jiang,Lunhao Duan,Yong Xien Chng,Qing-Guo Chen,Weihua Luo,Kaifu Zhang,Jia-Wang Bian,Mingming Gong*

Main category: cs.CL

TL;DR: 本文系统综述了多模态RAG在文档理解中的应用，提出了基于领域、检索模态和粒度的分类法，总结了关键数据集、基准和应用，并指出了效率、细粒度表示和鲁棒性等开放挑战。


<details>
  <summary>Details</summary>
Motivation: 当前文档理解方法存在局限性：OCR+LLM流水线会丢失结构细节，而原生多模态LLM在上下文建模方面表现不佳。文档的多模态特性（文本、表格、图表、布局）需要更先进的范式——多模态RAG。

Method: 提出基于领域、检索模态和粒度的分类法，回顾涉及图结构和代理框架的进展，总结关键数据集、基准和应用。

Result: 多模态RAG能够实现跨所有模态的整体检索和推理，解锁全面的文档智能。

Conclusion: 多模态RAG是文档理解的重要发展方向，本文提供了未来文档AI进展的路线图，重点关注效率、细粒度表示和鲁棒性等挑战。

Abstract: Document understanding is critical for applications from financial analysis
to scientific discovery. Current approaches, whether OCR-based pipelines
feeding Large Language Models (LLMs) or native Multimodal LLMs (MLLMs), face
key limitations: the former loses structural detail, while the latter struggles
with context modeling. Retrieval-Augmented Generation (RAG) helps ground models
in external data, but documents' multimodal nature, i.e., combining text,
tables, charts, and layout, demands a more advanced paradigm: Multimodal RAG.
This approach enables holistic retrieval and reasoning across all modalities,
unlocking comprehensive document intelligence. Recognizing its importance, this
paper presents a systematic survey of Multimodal RAG for document
understanding. We propose a taxonomy based on domain, retrieval modality, and
granularity, and review advances involving graph structures and agentic
frameworks. We also summarize key datasets, benchmarks, and applications, and
highlight open challenges in efficiency, fine-grained representation, and
robustness, providing a roadmap for future progress in document AI.

</details>


### [14] [TraceCoder: Towards Traceable ICD Coding via Multi-Source Knowledge Integration](https://arxiv.org/abs/2510.15267)
*Mucheng Ren,He Chen,Yuchen Yan,Danqing Hu,Jun Xu,Xian Zeng*

Main category: cs.CL

TL;DR: TraceCoder是一个集成多源外部知识的自动化ICD编码框架，通过动态整合UMLS、维基百科和大型语言模型来增强代码表示、弥合语义差距，并处理罕见和模糊代码，在多个数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动化ICD编码方法面临临床文本与ICD代码之间的语义差距、罕见和长尾代码性能差以及可解释性有限等挑战。

Method: 提出TraceCoder框架，动态整合多源外部知识（UMLS、维基百科、LLMs），引入混合注意力机制建模标签、临床上下文和知识之间的交互。

Result: 在MIMIC-III-ICD9、MIMIC-IV-ICD9和MIMIC-IV-ICD10数据集上的实验表明，TraceCoder实现了最先进的性能，消融研究验证了其组件的有效性。

Conclusion: TraceCoder为自动化ICD编码提供了一个可扩展且稳健的解决方案，符合临床对准确性、可解释性和可靠性的需求。

Abstract: Automated International Classification of Diseases (ICD) coding assigns
standardized diagnosis and procedure codes to clinical records, playing a
critical role in healthcare systems. However, existing methods face challenges
such as semantic gaps between clinical text and ICD codes, poor performance on
rare and long-tail codes, and limited interpretability. To address these
issues, we propose TraceCoder, a novel framework integrating multi-source
external knowledge to enhance traceability and explainability in ICD coding.
TraceCoder dynamically incorporates diverse knowledge sources, including UMLS,
Wikipedia, and large language models (LLMs), to enrich code representations,
bridge semantic gaps, and handle rare and ambiguous codes. It also introduces a
hybrid attention mechanism to model interactions among labels, clinical
context, and knowledge, improving long-tail code recognition and making
predictions interpretable by grounding them in external evidence. Experiments
on MIMIC-III-ICD9, MIMIC-IV-ICD9, and MIMIC-IV-ICD10 datasets demonstrate that
TraceCoder achieves state-of-the-art performance, with ablation studies
validating the effectiveness of its components. TraceCoder offers a scalable
and robust solution for automated ICD coding, aligning with clinical needs for
accuracy, interpretability, and reliability.

</details>


### [15] [TACL: Threshold-Adaptive Curriculum Learning Strategy for Enhancing Medical Text Understanding](https://arxiv.org/abs/2510.15269)
*Mucheng Ren,Yucheng Yan,He Chen,Danqing Hu,Jun Xu,Xian Zeng*

Main category: cs.CL

TL;DR: TACL是一种阈值自适应课程学习框架，通过动态调整训练过程，根据样本复杂度分类数据并优先训练简单案例，在医疗文本理解任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 医疗文本特别是电子病历的非结构化特性、领域特定语言和上下文变异性使得自动理解成为复杂挑战。现有方法将所有数据视为同等难度，忽略了临床记录中复杂度的内在差异，限制了模型在罕见或复杂案例上的泛化能力。

Method: TACL框架基于渐进学习原则，动态调整训练过程，将数据按复杂度分类，在训练早期优先处理简单案例，为模型建立坚实基础后再处理更复杂记录。

Result: 在包含英文和中文临床记录的多语言医疗数据上应用TACL，在自动ICD编码、再入院预测和中医证候鉴别等多样化临床任务中观察到显著改进。

Conclusion: TACL不仅提升了自动化系统的性能，还展示了统一不同医疗领域方法的潜力，为更准确、可扩展和全球适用的医疗文本理解解决方案铺平了道路。

Abstract: Medical texts, particularly electronic medical records (EMRs), are a
cornerstone of modern healthcare, capturing critical information about patient
care, diagnoses, and treatments. These texts hold immense potential for
advancing clinical decision-making and healthcare analytics. However, their
unstructured nature, domain-specific language, and variability across contexts
make automated understanding an intricate challenge. Despite the advancements
in natural language processing, existing methods often treat all data as
equally challenging, ignoring the inherent differences in complexity across
clinical records. This oversight limits the ability of models to effectively
generalize and perform well on rare or complex cases. In this paper, we present
TACL (Threshold-Adaptive Curriculum Learning), a novel framework designed to
address these challenges by rethinking how models interact with medical texts
during training. Inspired by the principle of progressive learning, TACL
dynamically adjusts the training process based on the complexity of individual
samples. By categorizing data into difficulty levels and prioritizing simpler
cases early in training, the model builds a strong foundation before tackling
more complex records. By applying TACL to multilingual medical data, including
English and Chinese clinical records, we observe significant improvements
across diverse clinical tasks, including automatic ICD coding, readmission
prediction and TCM syndrome differentiation. TACL not only enhances the
performance of automated systems but also demonstrates the potential to unify
approaches across disparate medical domains, paving the way for more accurate,
scalable, and globally applicable medical text understanding solutions.

</details>


### [16] [Exemplar-Guided Planing: Enhanced LLM Agent for KGQA](https://arxiv.org/abs/2510.15283)
*Jingao Xu,Shuoyoucheng Ma,Xin Song,Rong Jiang,Hongkui Tu,Bin Zhou*

Main category: cs.CL

TL;DR: 提出EGP框架，通过检索训练集中的成功推理路径作为范例，指导LLM在KGQA任务中的规划过程，包括任务分解和关系探索两个关键阶段，并引入智能前瞻机制提高效率。


<details>
  <summary>Details</summary>
Motivation: LLM在KGQA任务中面临自然语言查询与知识图谱结构化表示之间的语义鸿沟问题，导致规划不佳和探索效率低下，而无训练方法往往未能充分利用训练数据中的宝贵推理模式。

Method: EGP框架首先通过实体模板化预处理训练集问题以规范化语义变化，然后使用语义嵌入和FAISS索引检索高度相似的范例问题及其成功推理路径，这些范例动态指导LLM的规划过程，包括任务分解和关系探索两个阶段，并引入智能前瞻机制。

Result: 在WebQSP和CWQ两个真实世界KGQA数据集上的广泛实验表明，PoG-EGP显著优于基线PoG系统和其他对比方法。

Conclusion: EGP框架通过范例引导的规划有效提升了LLM在KGQA任务中的规划能力，解决了语义鸿沟问题并提高了推理效率。

Abstract: Large Language Models (LLMs) as interactive agents show significant promise
in Knowledge Graph Question Answering (KGQA) but often struggle with the
semantic gap between natural language queries and structured knowledge graph
(KG) representations. This leads to suboptimal planning and inefficient
exploration on KG, while training-free approaches often underutilize valuable
reasoning patterns in training data. To address these limitations, we propose a
novel framework, Exemplar-Guided Planning (EGP), which enhances the planning
capabilities of LLM agents for KGQA. EGP first preprocesses the training set
questions via entity templating to normalize semantic variations. It then
retrieves highly similar exemplary questions and their successful reasoning
paths from this preprocessed set using semantic embeddings and an efficient
FAISS index. These retrieved exemplars dynamically guide the LLM's planning
process in two key phases: (1) Task Decomposition, by aligning generated
sub-objectives with proven reasoning steps, and (2) Relation Exploration, by
providing high-quality auxiliary information to improve relation pruning
accuracy. Additionally, we introduce a Smart Lookahead mechanism during
relation exploration to improve efficiency by preemptively exploring promising
paths and potentially terminating exploration earlier. We apply EGP to the
Plan-on-Graph (PoG) framework, termed PoG-EGP. Extensive experiments on two
real-world KGQA datasets, WebQSP and CWQ, demonstrate that PoG-EGP
significantly improves over the baseline PoG system and other compared methods.

</details>


### [17] [Automatic essay scoring: leveraging Jaccard coefficient and Cosine similaritywith n-gram variation in vector space model approach](https://arxiv.org/abs/2510.15311)
*Andharini Dwi Cahyani,Moh. Wildan Fathoni,Fika Hastarita Rachman,Ari Basuki,Salman Amin,Bain Khusnul Khotimah*

Main category: cs.CL

TL;DR: 本研究比较了Jaccard系数和余弦相似度在向量空间模型中的表现，发现余弦相似度在自动作文评分中表现更优，且一元文法比二元和三元文法产生更低的RMSE。


<details>
  <summary>Details</summary>
Motivation: 自动作文评分(AES)旨在提供高效准确的写作评估工具，本研究探索在向量空间模型中使用不同相似度指标和n-gram表示的效果。

Method: 使用初中公民教育课程的作文数据，通过n-gram模型提取特征并向量化，然后分别计算Jaccard系数和余弦相似度，最后通过RMSE评估系统性能。

Result: 余弦相似度优于Jaccard系数，一元文法相比二元和三元文法产生更低的RMSE。

Conclusion: 在自动作文评分中，余弦相似度结合一元文法特征能够提供更准确的评分结果。

Abstract: Automated essay scoring (AES) is a vital area of research aiming to provide
efficient and accurate assessment tools for evaluating written content. This
study investigates the effectiveness of two popular similarity metrics, Jaccard
coefficient, and Cosine similarity, within the context of vector space
models(VSM)employing unigram, bigram, and trigram representations. The data
used in this research was obtained from the formative essay of the citizenship
education subject in a junior high school. Each essay undergoes preprocessing
to extract features using n-gram models, followed by vectorization to transform
text data into numerical representations. Then, similarity scores are computed
between essays using both Jaccard coefficient and Cosine similarity. The
performance of the system is evaluated by analyzing the root mean square error
(RMSE), which measures the difference between the scores given by human graders
and those generated by the system. The result shows that the Cosine similarity
outperformed the Jaccard coefficient. In terms of n-gram, unigrams have lower
RMSE compared to bigrams and trigrams.

</details>


### [18] [Accelerating Mobile Language Model Generation via Hybrid Context and Hardware Coordination](https://arxiv.org/abs/2510.15312)
*Zhiyang Chen,Daliang Xu,Haiyang Shen,Mengwei Xu,Shangguang Wang,Yun Ma*

Main category: cs.CL

TL;DR: CoordGen是一个移动推理框架，通过结合推测解码和动态硬件调度来加速移动设备上的上下文感知文本生成，实现了最高3.8倍的生成速度提升和4.7倍的能效提升。


<details>
  <summary>Details</summary>
Motivation: 移动设备上的大型语言模型虽然通过本地数据上下文信息实现了个性化和任务感知生成，但由于其内存受限的特性，逐令牌生成过程仍然存在高延迟和硬件利用率有限的问题。

Method: CoordGen框架包含三个协同组件：自适应执行调度（动态平衡预填充和解码阶段的计算图）、上下文对齐草稿生成（通过轻量级在线校准提高推测效率）、硬件高效草稿扩展（重用和扩展中间序列以提高处理并行性并降低验证成本）。

Result: 在多个智能手机和代表性工作负载上的实验显示，与现有移动推理解决方案相比，生成速度最高提升3.8倍，能效最高提升4.7倍。组件级分析进一步验证了每个优化的贡献。

Conclusion: CoordGen通过集成推测解码与动态硬件调度，有效解决了移动设备上LLM推理的内存瓶颈问题，显著提升了生成速度和能效，为移动智能助手和UI代理等应用提供了高效的推理解决方案。

Abstract: Enhancing on-device large language models (LLMs) with contextual information
from local data enables personalized and task-aware generation, powering use
cases such as intelligent assistants and UI agents. While recent developments
in neural processors have substantially improved the efficiency of prefill on
mobile devices, the token-by-token generation process still suffers from high
latency and limited hardware utilization due to its inherently memory-bound
characteristics. This work presents CoordGen, a mobile inference framework that
integrates speculative decoding with dynamic hardware scheduling to accelerate
context-aware text generation on mobile devices. The framework introduces three
synergistic components: (1) adaptive execution scheduling, which dynamically
balances compute graphs between prefill and decoding phases; (2)
context-aligned drafting, which improves speculative efficiency through
lightweight online calibration to current tasks; and (3) hardware-efficient
draft extension, which reuses and expands intermediate sequences to improve
processing parallelism and reduce verification cost. Experiments on multiple
smartphones and representative workloads show consistent improvements of up to
3.8x in generation speed and 4.7x in energy efficiency compared with existing
mobile inference solutions. Component-level analysis further validates the
contribution of each optimization.

</details>


### [19] [Capabilities and Evaluation Biases of Large Language Models in Classical Chinese Poetry Generation: A Case Study on Tang Poetry](https://arxiv.org/abs/2510.15313)
*Bolei Ma,Yina Yao,Anna-Carolina Haensch*

Main category: cs.CL

TL;DR: 本文提出了一个三步评估框架来评估大语言模型在古典诗歌生成和评估中的表现，发现LLM在评估创意质量时存在"回音室"效应和系统性偏见。


<details>
  <summary>Details</summary>
Motivation: 大语言模型越来越多地应用于创意领域，但它们在古典诗歌生成和评估方面的表现仍然缺乏深入理解，需要系统性的评估框架。

Method: 提出了一个三步评估框架，结合计算指标、LLM作为评判者的评估和人类专家验证，对六个最先进的LLM在诗歌质量多个维度上进行评估。

Result: 分析揭示了系统性的生成和评估偏见：LLM在评估创意质量时表现出"回音室"效应，常常收敛于与人类判断不同的有缺陷标准。

Conclusion: 这些发现突显了当前LLM作为文学生成代理的潜力和局限性，以及在文化和技术上复杂的创意任务中持续需要人类和模型的混合验证。

Abstract: Large Language Models (LLMs) are increasingly applied to creative domains,
yet their performance in classical Chinese poetry generation and evaluation
remains poorly understood. We propose a three-step evaluation framework that
combines computational metrics, LLM-as-a-judge assessment, and human expert
validation. Using this framework, we evaluate six state-of-the-art LLMs across
multiple dimensions of poetic quality, including themes, emotions, imagery,
form, and style. Our analysis reveals systematic generation and evaluation
biases: LLMs exhibit "echo chamber" effects when assessing creative quality,
often converging on flawed standards that diverge from human judgments. These
findings highlight both the potential and limitations of current capabilities
of LLMs as proxy for literacy generation and the limited evaluation practices,
thereby demonstrating the continued need of hybrid validation from both humans
and models in culturally and technically complex creative tasks.

</details>


### [20] [AutoGraph-R1: End-to-End Reinforcement Learning for Knowledge Graph Construction](https://arxiv.org/abs/2510.15339)
*Hong Ting Tsang,Jiaxin Bai,Haoyu Huang,Qiao Xiao,Tianshi Zheng,Baixuan Xu,Shujie Liu,Yangqiu Song*

Main category: cs.CL

TL;DR: AutoGraph-R1是首个使用强化学习直接优化知识图谱构建以提升任务性能的框架，通过将图生成视为策略学习问题，基于图谱在RAG管道中的功能效用设计奖励函数，显著提升了问答系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱构建过程与下游应用脱节，导致图结构不理想，限制了检索增强生成(RAG)在问答系统中的有效性。

Method: 使用强化学习训练LLM构造器，将图生成建模为策略学习问题，设计了两种任务感知的奖励函数：一种用于图谱作为知识载体，另一种用于图谱作为知识索引。

Result: 在多个问答基准测试中，AutoGraph-R1使图RAG方法相比任务无关的基线图谱获得了显著的性能提升。

Conclusion: 该工作表明可以在构建和应用之间建立闭环，将范式从构建内在'好'的图谱转变为构建可证明'有用'的图谱。

Abstract: Building effective knowledge graphs (KGs) for Retrieval-Augmented Generation
(RAG) is pivotal for advancing question answering (QA) systems. However, its
effectiveness is hindered by a fundamental disconnect: the knowledge graph (KG)
construction process is decoupled from its downstream application, yielding
suboptimal graph structures. To bridge this gap, we introduce AutoGraph-R1, the
first framework to directly optimize KG construction for task performance using
Reinforcement Learning (RL). AutoGraph-R1 trains an LLM constructor by framing
graph generation as a policy learning problem, where the reward is derived from
the graph's functional utility in a RAG pipeline. We design two novel,
task-aware reward functions, one for graphs as knowledge carriers and another
as knowledge indices. Across multiple QA benchmarks, AutoGraph-R1 consistently
enables graph RAG methods to achieve significant performance gains over using
task-agnostic baseline graphs. Our work shows it is possible to close the loop
between construction and application, shifting the paradigm from building
intrinsically ``good'' graphs to building demonstrably ``useful'' ones.

</details>


### [21] [Readability Reconsidered: A Cross-Dataset Analysis of Reference-Free Metrics](https://arxiv.org/abs/2510.15345)
*Catarina G Belem,Parker Glenn,Alfy Samuel,Anoop Kumar,Daben Liu*

Main category: cs.CL

TL;DR: 本文研究发现传统可读性评估指标与人类感知存在不匹配，基于模型的方法在评估文本可读性方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 自动可读性评估在确保有效和可访问的书面交流中起着关键作用，但该领域受到可读性定义不一致和依赖表层文本属性的测量方法的阻碍。

Method: 通过分析897个人类判断来研究影响人类可读性感知的因素，并在五个英语数据集上评估15个流行的可读性指标，与六个更细致的基于模型的指标进行对比。

Result: 研究发现信息内容和主题强烈影响文本可理解性；四个基于模型的指标在与人类判断的排名相关性中始终位列前四，而表现最佳的传统指标平均排名仅为8.6。

Conclusion: 当前可读性指标与人类感知存在不匹配，基于模型的方法是更有前景的方向。

Abstract: Automatic readability assessment plays a key role in ensuring effective and
accessible written communication. Despite significant progress, the field is
hindered by inconsistent definitions of readability and measurements that rely
on surface-level text properties. In this work, we investigate the factors
shaping human perceptions of readability through the analysis of 897 judgments,
finding that, beyond surface-level cues, information content and topic strongly
shape text comprehensibility. Furthermore, we evaluate 15 popular readability
metrics across five English datasets, contrasting them with six more nuanced,
model-based metrics. Our results show that four model-based metrics
consistently place among the top four in rank correlations with human
judgments, while the best performing traditional metric achieves an average
rank of 8.6. These findings highlight a mismatch between current readability
metrics and human perceptions, pointing to model-based approaches as a more
promising direction.

</details>


### [22] [When to Ensemble: Identifying Token-Level Points for Stable and Fast LLM Ensembling](https://arxiv.org/abs/2510.15346)
*Heecheol Yun,Kwangmin Ki,Junghyun Lee,Eunho Yang*

Main category: cs.CL

TL;DR: SAFE是一种选择性集成大语言模型的方法，通过考虑分词不匹配和概率分布一致性来确定集成位置，在长文本生成中优于现有方法，即使只集成不到1%的token也能提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM集成方法在短文本生成中有效，但在长文本生成中应用不足，标准做法在每个token处集成往往会降低性能。

Method: 提出SAFE框架，选择性集成LLM，考虑分词不匹配和概率分布一致性两个关键因素，并引入概率锐化策略来合并表示同一单词的多个子词token的概率。

Result: 在MATH500和BBH等基准测试中，SAFE在准确性和效率上都优于现有方法，即使集成少于1%的token也能获得性能提升。

Conclusion: SAFE通过选择性集成策略有效解决了长文本生成中的LLM集成问题，在保持效率的同时显著提升了性能。

Abstract: Ensembling Large Language Models (LLMs) has gained attention as a promising
approach to surpass the performance of individual models by leveraging their
complementary strengths. In particular, aggregating models' next-token
probability distributions to select the next token has been shown to be
effective in various tasks. However, while successful for short-form answers,
its application to long-form generation remains underexplored. In this paper,
we show that using existing ensemble methods in long-form generation requires a
careful choice of ensembling positions, since the standard practice of
ensembling at every token often degrades performance. We identify two key
factors for determining these positions: tokenization mismatch across models
and consensus in their next-token probability distributions. Based on this, we
propose SAFE, (Stable And Fast LLM Ensembling), a framework that selectively
ensembles by jointly considering these factors. To further improve stability,
we introduce a probability sharpening strategy that consolidates probabilities
spread across multiple sub-word tokens representing the same word into a single
representative token. Our experiments on diverse benchmarks, including MATH500
and BBH, demonstrate that SAFE outperforms existing methods in both accuracy
and efficiency, with gains achieved even when ensembling fewer than 1% of
tokens.

</details>


### [23] [Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing](https://arxiv.org/abs/2510.15349)
*Baode Wang,Biao Wu,Weizhen Li,Meng Fang,Zuming Huang,Jun Huang,Haozhe Wang,Yanjie Liang,Ling Chen,Wei Chu,Yuan Qi*

Main category: cs.CL

TL;DR: 提出了LayoutRL强化学习框架和Infinity-Parser模型，通过复合奖励机制在Infinity-Doc-400K数据集上训练，在多个文档解析基准测试中达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有监督微调方法在多样化文档类型上泛化能力差，特别是在分布外数据上表现不佳，且高质量布局感知解析训练数据有限

Method: 使用强化学习框架LayoutRL，通过整合归一化编辑距离、段落计数准确性和阅读顺序保持的复合奖励来优化布局理解，并构建Infinity-Doc-400K数据集训练Infinity-Parser视觉语言模型

Result: 在OmniDocBench、olmOCR-Bench、PubTabNet和FinTabNet等基准测试中，Infinity-Parser在多种文档类型、语言和结构复杂性上一致达到最先进性能，显著优于专业文档解析系统和通用视觉语言模型

Conclusion: LayoutRL框架和Infinity-Parser模型有效解决了文档解析中的泛化问题，将在代码、数据集和模型发布以促进可复现研究

Abstract: Document parsing from scanned images into structured formats remains a
significant challenge due to its complexly intertwined elements such as text
paragraphs, figures, formulas, and tables. Existing supervised fine-tuning
methods often struggle to generalize across diverse document types, leading to
poor performance, particularly on out-of-distribution data. This issue is
further exacerbated by the limited availability of high-quality training data
for layout-aware parsing tasks. To address these challenges, we introduce
LayoutRL, a reinforcement learning framework that optimizes layout
understanding through composite rewards integrating normalized edit distance,
paragraph count accuracy, and reading order preservation. To support this
training, we construct the Infinity-Doc-400K dataset, which we use to train
Infinity-Parser, a vision-language model demonstrating robust generalization
across various domains. Extensive evaluations on benchmarks including
OmniDocBench, olmOCR-Bench, PubTabNet, and FinTabNet show that Infinity-Parser
consistently achieves state-of-the-art performance across a broad range of
document types, languages, and structural complexities, substantially
outperforming both specialized document parsing systems and general-purpose
vision-language models. We will release our code, dataset, and model to
facilitate reproducible research in document parsing.

</details>


### [24] [VocalBench-DF: A Benchmark for Evaluating Speech LLM Robustness to Disfluency](https://arxiv.org/abs/2510.15406)
*Hongcheng Liu,Yixuan Hou,Heyang Liu,Yuhao Wang,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: Speech-LLMs在语音不流畅场景下表现不佳，特别是对帕金森等疾病相关的语音障碍处理能力有限，需要改进以构建真正包容的语音大模型。


<details>
  <summary>Details</summary>
Motivation: 现有Speech-LLMs评估多依赖理想化输入，忽视了常见的语音不流畅问题，特别是与帕金森等疾病相关的语音障碍，需要测试这些模型在与语音障碍用户交互时的性能表现。

Method: 提出了VocalBench-DF框架，用于系统评估多维度分类的语音不流畅问题，并对22个主流Speech-LLMs进行了评估。

Result: 评估显示Speech-LLMs在语音不流畅场景下性能显著下降，识别出音素级处理和长上下文建模是主要瓶颈，通过增强组件和管道的识别与推理能力可以显著提高鲁棒性。

Conclusion: 当前Speech-LLMs在实际应用中的准备度有限，迫切需要新方法来改进语音不流畅处理能力，构建真正包容的语音大模型。

Abstract: While Speech Large Language Models (Speech-LLMs) show strong performance in
many applications, their robustness is critically under-tested, especially to
speech disfluency. Existing evaluations often rely on idealized inputs,
overlooking common disfluencies, particularly those associated with conditions
like Parkinson's disease. This work investigates whether current Speech-LLMs
can maintain performance when interacting with users who have speech
impairments. To facilitate this inquiry, we introduce VocalBench-DF, a
framework for the systematic evaluation of disfluency across a
multi-dimensional taxonomy. Our evaluation of 22 mainstream Speech-LLMs reveals
substantial performance degradation, indicating that their real-world readiness
is limited. Further analysis identifies phoneme-level processing and
long-context modeling as primary bottlenecks responsible for these failures.
Strengthening recognition and reasoning capability from components and
pipelines can substantially improve robustness. These findings highlight the
urgent need for new methods to improve disfluency handling and build truly
inclusive Speech-LLMs

</details>


### [25] [Large-scale User Game Lifecycle Representation Learning](https://arxiv.org/abs/2510.15412)
*Yanjie Gou,Jiangming Liu,Kouying Xue,Yi Hua*

Main category: cs.CL

TL;DR: 为了解决游戏广告和推荐系统中的稀疏性和不平衡问题，本文提出了用户游戏生命周期（UGL）来丰富用户行为，并采用创新策略来提取用户兴趣，通过逆概率掩码策略处理游戏不平衡问题，在离线和在线实验中显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 在线游戏平台需要有效的广告和推荐系统，但现有的大规模推荐方法不适用于游戏场景，主要面临游戏稀疏性（仅数百款游戏）和游戏不平衡（用户行为集中在少数热门游戏）的问题。

Method: 引入用户游戏生命周期（UGL）来丰富用户行为，提出两种创新策略来提取用户短期和长期兴趣，并采用逆概率掩码策略进行UGL表示学习以解决游戏不平衡问题。

Result: 离线实验平均AUC提升1.83%，在线实验游戏广告CVR平均提升21.67%；游戏内物品推荐离线AUC提升0.5%，在线ARPU提升0.82%。

Conclusion: UGL表示学习方法有效解决了游戏广告和推荐中的稀疏性和不平衡问题，显著提升了模型性能，为游戏平台的广告和推荐系统提供了有效解决方案。

Abstract: The rapid expansion of video game production necessitates the development of
effective advertising and recommendation systems for online game platforms.
Recommending and advertising games to users hinges on capturing their interest
in games. However, existing representation learning methods crafted for
handling billions of items in recommendation systems are unsuitable for game
advertising and recommendation. This is primarily due to game sparsity, where
the mere hundreds of games fall short for large-scale user representation
learning, and game imbalance, where user behaviors are overwhelmingly dominated
by a handful of popular games. To address the sparsity issue, we introduce the
User Game Lifecycle (UGL), designed to enrich user behaviors in games.
Additionally, we propose two innovative strategies aimed at manipulating user
behaviors to more effectively extract both short and long-term interests. To
tackle the game imbalance challenge, we present an Inverse Probability Masking
strategy for UGL representation learning. The offline and online experimental
results demonstrate that the UGL representations significantly enhance model by
achieving a 1.83% AUC offline increase on average and a 21.67% CVR online
increase on average for game advertising and a 0.5% AUC offline increase and a
0.82% ARPU online increase for in-game item recommendation.

</details>


### [26] [Fine-Tuning MedGemma for Clinical Captioning to Enhance Multimodal RAG over Malaysia CPGs](https://arxiv.org/abs/2510.15418)
*Lee Qi Zun,Mohamad Zulhilmi Bin Abdul Halim,Goh Man Fye*

Main category: cs.CL

TL;DR: 该研究提出了一种专门化MedGemma模型的方法，用于生成高质量医学图像描述作为检索增强生成系统的查询，通过知识蒸馏创建合成数据集并使用QLoRA微调，显著提升了分类准确性和描述质量。


<details>
  <summary>Details</summary>
Motivation: 当前检索增强生成系统在处理基于图像的查询时效果有限，因为通用视觉语言模型的描述缺乏临床特异性和事实基础，需要专门化的医学视觉语言模型来生成高保真度的描述。

Method: 采用知识蒸馏管道创建皮肤病学、眼底和胸部X光领域的合成数据集，使用参数高效的QLoRA方法对MedGemma模型进行微调，并通过双重评估框架（分类准确性和RAGAS框架）进行性能评估。

Result: 微调后的模型在分类性能上取得显著提升，RAGAS评估确认了描述忠实度、相关性和正确性的显著改善，验证了模型生成可靠、事实基础描述的能力。

Conclusion: 本研究建立了一个专门化医学视觉语言模型的强大管道，验证了所得模型作为高质量查询生成器的有效性，为增强多模态检索增强生成系统在循证临床决策支持中的应用奠定了基础。

Abstract: Retrieval-Augmented Generation systems are essential for providing fact-based
guidance from Malaysian Clinical Practice Guidelines. However, their
effectiveness with image-based queries is limited, as general Vision-Language
Model captions often lack clinical specificity and factual grounding. This
study proposes and validates a framework to specialize the MedGemma model for
generating high-fidelity captions that serve as superior queries. To overcome
data scarcity, we employ a knowledge distillation pipeline to create a
synthetic dataset across dermatology, fundus, and chest radiography domains,
and fine-tune MedGemma using the parameter-efficient QLoRA method. Performance
was rigorously assessed through a dual framework measuring both classification
accuracy and, via a novel application of the RAGAS framework, caption
faithfulness, relevancy, and correctness. The fine-tuned model demonstrated
substantial improvements in classification performance, while RAGAS evaluation
confirmed significant gains in caption faithfulness and correctness, validating
the models ability to produce reliable, factually grounded descriptions. This
work establishes a robust pipeline for specializing medical VLMs and validates
the resulting model as a high-quality query generator, laying the groundwork
for enhancing multimodal RAG systems in evidence-based clinical decision
support.

</details>


### [27] [When Seeing Is not Enough: Revealing the Limits of Active Reasoning in MLLMs](https://arxiv.org/abs/2510.15421)
*Hongcheng Liu,Pingjie Wang,Yuhao Wang,Siqu Ou,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: 该论文提出了GuessBench基准测试，用于评估多模态大语言模型在信息不完整情况下的主动推理能力，发现现有模型在主动推理方面表现远落后于被动推理，并识别了细粒度感知和及时决策作为关键挑战。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM评估主要关注被动推理，即在完整信息下进行逐步推理，这与现实世界使用场景不符。现实世界中模型需要主动获取缺失证据并在不完整信息下迭代优化决策。

Method: 提出GuessBench基准测试，包含感知导向和知识导向的图像，要求模型在不具备任务特定先验的情况下从候选池中选择目标图像，以评估MLLM的主动推理能力。评估了20个优秀MLLM，并进行消融研究。

Result: 评估结果显示，MLLM在主动推理方面的表现远落后于被动设置，表明有巨大改进空间。感知增强对较小模型有益，而思维导向方法在不同模型规模上都能带来一致增益。

Conclusion: 细粒度感知和及时决策是主动推理的关键挑战，这些发现为多模态主动推理的未来研究指明了有前景的方向。

Abstract: Multimodal large language models (MLLMs) have shown strong capabilities
across a broad range of benchmarks. However, most existing evaluations focus on
passive inference, where models perform step-by-step reasoning under complete
information. This setup is misaligned with real-world use, where seeing is not
enough. This raises a fundamental question: Can MLLMs actively acquire missing
evidence under incomplete information? To bridge this gap, we require the MLLMs
to actively acquire missing evidence and iteratively refine decisions under
incomplete information, by selecting a target image from a candidate pool
without task-specific priors. To support systematic study, we propose
GuessBench, a benchmark with both perception-oriented and knowledge-oriented
images for evaluating active reasoning in MLLMs. We evaluate 20 superior MLLMs
and find that performance on active reasoning lags far behind it on passive
settings, indicating substantial room for improvement. Further analysis
identifies fine-grained perception and timely decision-making as key
challenges. Ablation studies show that perceptual enhancements benefit smaller
models, whereas thinking-oriented methods provide consistent gains across model
sizes. These results suggest promising directions for future research on
multimodal active reasoning.

</details>


### [28] [Controllable Abstraction in Summary Generation for Large Language Models via Prompt Engineering](https://arxiv.org/abs/2510.15436)
*Xiangchen Song,Yuchen Liu,Yaxuan Luan,Jinxu Guo,Xiaofan Guo*

Main category: cs.CL

TL;DR: 提出基于提示工程的可控摘要生成方法，通过多阶段提示框架处理不同抽象级别的摘要，分析提示长度、数据噪声和文本类型对摘要质量的影响。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在摘要质量和可控性方面的问题，探索如何通过提示工程改进大型语言模型的摘要生成能力。

Method: 设计多阶段提示生成框架，包括语义分析、主题建模和噪声控制，使用CNN/Daily Mail数据集进行实验。

Result: 提示长度对摘要质量有显著影响，过短或过长的提示都会降低质量；数据噪声增加会降低ROUGE-L分数；模型在新闻文本上表现最佳，学术文章表现较差。

Conclusion: 通过控制提示策略和优化文本预处理，可以提高大型语言模型生成摘要的准确性和可控性，为改进摘要生成提供了新视角。

Abstract: This study presents a controllable abstract summary generation method for
large language models based on prompt engineering. To address the issues of
summary quality and controllability in traditional methods, we design a
multi-stage prompt generation framework. This framework generates summaries
with varying levels of abstraction by performing semantic analysis, topic
modeling, and noise control on the input text. The experiment uses the
CNN/Daily Mail dataset and provides a detailed analysis of different prompt
lengths, data noise, and text types. The experimental results show that prompt
length has a significant impact on the quality of generated summaries. Both
very short and very long prompt tokens result in a decrease in summary quality.
Data noise also negatively affects the summary generation process. As noise
levels increase, the ROUGE-L score gradually decreases. Furthermore, different
text types have varying effects on the model's ability to generate summaries.
The model performs best when handling news texts, while its performance is
worse when processing academic articles. This research provides new insights
into improving summary generation using large language models, particularly in
how controlling prompt strategies and optimizing text preprocessing can enhance
summary accuracy and controllability.

</details>


### [29] [CORE: Reducing UI Exposure in Mobile Agents via Collaboration Between Cloud and Local LLMs](https://arxiv.org/abs/2510.15455)
*Gucongcong Fan,Chaoyue Niu,Chengfei Lyu,Fan Wu,Guihai Chen*

Main category: cs.CL

TL;DR: CORE是一个协作框架，结合云端和本地LLM的优势，在保持移动代理任务准确性的同时减少UI暴露。


<details>
  <summary>Details</summary>
Motivation: 云端LLM需要上传完整UI状态，暴露不必要信息；本地LLM容量有限，任务成功率低。需要平衡隐私保护和任务准确性。

Method: 包含三个关键组件：布局感知块分区、协同规划和协同决策，通过多轮累积机制缓解本地误判。

Result: 实验显示CORE能减少高达55.6%的UI暴露，同时任务成功率略低于纯云端代理，有效缓解不必要的隐私暴露。

Conclusion: CORE框架成功实现了在保持任务准确性的同时显著减少UI暴露，为移动代理提供了隐私保护的有效解决方案。

Abstract: Mobile agents rely on Large Language Models (LLMs) to plan and execute tasks
on smartphone user interfaces (UIs). While cloud-based LLMs achieve high task
accuracy, they require uploading the full UI state at every step, exposing
unnecessary and often irrelevant information. In contrast, local LLMs avoid UI
uploads but suffer from limited capacity, resulting in lower task success
rates. We propose $\textbf{CORE}$, a $\textbf{CO}$llaborative framework that
combines the strengths of cloud and local LLMs to $\textbf{R}$educe UI
$\textbf{E}$xposure, while maintaining task accuracy for mobile agents. CORE
comprises three key components: (1) $\textbf{Layout-aware block partitioning}$,
which groups semantically related UI elements based on the XML screen
hierarchy; (2) $\textbf{Co-planning}$, where local and cloud LLMs
collaboratively identify the current sub-task; and (3)
$\textbf{Co-decision-making}$, where the local LLM ranks relevant UI blocks,
and the cloud LLM selects specific UI elements within the top-ranked block.
CORE further introduces a multi-round accumulation mechanism to mitigate local
misjudgment or limited context. Experiments across diverse mobile apps and
tasks show that CORE reduces UI exposure by up to 55.6% while maintaining task
success rates slightly below cloud-only agents, effectively mitigating
unnecessary privacy exposure to the cloud. The code is available at
https://github.com/Entropy-Fighter/CORE.

</details>


### [30] [DeceptionBench: A Comprehensive Benchmark for AI Deception Behaviors in Real-world Scenarios](https://arxiv.org/abs/2510.15501)
*Yao Huang,Yitong Sun,Yichi Zhang,Ruochen Zhang,Yinpeng Dong,Xingxing Wei*

Main category: cs.CL

TL;DR: DeceptionBench是首个系统性评估大语言模型在不同社会领域中欺骗行为的基准测试，涵盖经济、医疗、教育、社交和娱乐五大领域，包含150个场景和1000多个样本，揭示了当前模型在强化动态下欺骗行为加剧的严重漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力的快速提升，出现了新的欺骗行为，可能在高风险部署中带来严重后果。然而，目前对现实场景中欺骗行为的系统性研究仍然不足。

Method: 建立DeceptionBench基准测试，包含五大社会领域的150个精心设计场景和1000多个样本。从内在维度分析模型的自利倾向和讨好行为，从外在维度研究中性条件、奖励激励和强制压力下上下文因素如何调节欺骗输出，并引入持续多轮交互循环模拟真实世界反馈动态。

Result: 在大语言模型和大推理模型上的广泛实验揭示了关键漏洞，特别是在强化动态下欺骗行为会加剧，表明当前模型缺乏对操纵性上下文线索的鲁棒抵抗能力。

Conclusion: 当前模型对各种欺骗行为缺乏有效的防护机制，迫切需要开发先进的防护措施来应对不同形式的欺骗行为。

Abstract: Despite the remarkable advances of Large Language Models (LLMs) across
diverse cognitive tasks, the rapid enhancement of these capabilities also
introduces emergent deceptive behaviors that may induce severe risks in
high-stakes deployments. More critically, the characterization of deception
across realistic real-world scenarios remains underexplored. To bridge this
gap, we establish DeceptionBench, the first benchmark that systematically
evaluates how deceptive tendencies manifest across different societal domains,
what their intrinsic behavioral patterns are, and how extrinsic factors affect
them. Specifically, on the static count, the benchmark encompasses 150
meticulously designed scenarios in five domains, i.e., Economy, Healthcare,
Education, Social Interaction, and Entertainment, with over 1,000 samples,
providing sufficient empirical foundations for deception analysis. On the
intrinsic dimension, we explore whether models exhibit self-interested egoistic
tendencies or sycophantic behaviors that prioritize user appeasement. On the
extrinsic dimension, we investigate how contextual factors modulate deceptive
outputs under neutral conditions, reward-based incentivization, and coercive
pressures. Moreover, we incorporate sustained multi-turn interaction loops to
construct a more realistic simulation of real-world feedback dynamics.
Extensive experiments across LLMs and Large Reasoning Models (LRMs) reveal
critical vulnerabilities, particularly amplified deception under reinforcement
dynamics, demonstrating that current models lack robust resistance to
manipulative contextual cues and the urgent need for advanced safeguards
against various deception behaviors. Code and resources are publicly available
at https://github.com/Aries-iai/DeceptionBench.

</details>


### [31] [Temporal Referential Consistency: Do LLMs Favor Sequences Over Absolute Time References?](https://arxiv.org/abs/2510.15513)
*Ashutosh Bajpai,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文提出了一个评估大语言模型时间引用一致性的新基准TEMP-ReCon，并开发了UnTRaP模型来增强LLM的时间推理能力，实验证明该模型优于多个基线模型。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在时间敏感领域（如法律、医疗、金融）中作为知识源的广泛应用，模型不仅需要事实准确性，还需要在时间维度上保持一致性。然而，目前缺乏针对LLM时间引用一致性的评估和改进工作。

Method: 提出了时间引用一致性基准TEMP-ReCon，涵盖多种语言环境（英语、法语、罗马尼亚语），并开发了基于推理路径对齐的UnTRaP模型来提升LLM的时间引用一致性。

Result: 研究发现LLM确实存在时间引用一致性的不足，而UnTRaP模型在实证实验中表现出优于多个基线模型的性能。

Conclusion: 本文填补了LLM时间引用一致性评估和改进的空白，提出的TEMP-ReCon基准和UnTRaP模型为提升LLM在时间敏感领域应用的可靠性提供了有效解决方案。

Abstract: The increasing acceptance of large language models (LLMs) as an alternative
to knowledge sources marks a significant paradigm shift across various domains,
including time-sensitive fields such as law, healthcare, and finance. To
fulfill this expanded role, LLMs must not only be factually accurate but also
demonstrate consistency across temporal dimensions, necessitating robust
temporal reasoning capabilities. Despite this critical requirement, efforts to
ensure temporal consistency in LLMs remain scarce including noticeable absence
of endeavors aimed at evaluating or augmenting LLMs across temporal references
in time-sensitive inquiries. In this paper, we seek to address this gap by
introducing a novel benchmark entitled temporal referential consistency,
accompanied by a resource TEMP-ReCon designed to benchmark a wide range of both
open-source and closed-source LLMs with various linguistic contexts
characterized by differing resource richness (including English, French, and
Romanian). The findings emphasis that LLMs do exhibit insufficient temporal
referent consistency. To address this, we propose \newmodel, a reasoning path
alignment-based model that aims to enhance the temporal referential consistency
of LLMs. Our empirical experiments substantiate the efficacy of UnTRaP compared
to several baseline models.

</details>


### [32] [From Characters to Tokens: Dynamic Grouping with Hierarchical BPE](https://arxiv.org/abs/2510.15517)
*Rares Dolga,Lucas Maystre,Tudor Berariu,David Barber*

Main category: cs.CL

TL;DR: 提出了一种动态字符分组方法，利用现有BPE分词的结构，无需额外模型，通过添加显式补丁结束标记和二级BPE压缩来控制补丁粒度，实现高效、灵活且语言无关的表示。


<details>
  <summary>Details</summary>
Motivation: 解决子词分词方法（如BPE）在表示罕见词时的低效性和需要大嵌入矩阵的问题，同时避免字符级模型在Transformer架构中的性能瓶颈。

Method: 在现有BPE分词基础上添加显式补丁结束标记，然后引入二级BPE压缩阶段来控制补丁粒度，不依赖额外模型。

Result: 实证结果显示该方法在保持紧凑词汇表的同时，匹配或优于基于动态熵和空格的补丁策略性能。

Conclusion: 提出的动态字符分组方法提供了一种高效、灵活且语言无关的表示方案，成功结合了子词和字符级模型的优势。

Abstract: Subword tokenization methods like Byte Pair Encoding (BPE) are widely used in
large language models due to their balance of vocabulary compactness and
representational power. However, they suffer from inefficiencies in
representing rare words and require large embedding matrices. Character-level
models address these issues but introduce performance bottlenecks, particularly
in Transformer-based architectures. Recent hierarchical models attempt to merge
the benefits of both paradigms by grouping characters into patches, but
existing patching strategies either rely on whitespace-limiting applicability
to certain languages, or require auxiliary models that introduce new
dependencies. In this paper, we propose a dynamic character grouping method
that leverages the structure of existing BPE tokenization without requiring
additional models. By appending explicit end-of-patch markers to BPE tokens and
introducing a second-level BPE compression stage to control patch granularity,
our method offers efficient, flexible, and language-agnostic representations.
Empirical results demonstrate that our approach matches or exceeds the
performance of dynamic entropy- and whitespace-based patching strategies, while
maintaining a compact vocabulary.

</details>


### [33] [Latent Reasoning in LLMs as a Vocabulary-Space Superposition](https://arxiv.org/abs/2510.15522)
*Jingcheng Deng,Liang Pang,Zihao Wei,Shichen Xu,Zenghao Duan,Kun Xu,Yang Song,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: 提出Latent-SFT框架，通过词汇概率叠加进行潜在推理，在保持性能的同时大幅减少计算开销，在多个数学推理数据集上达到新SOTA。


<details>
  <summary>Details</summary>
Motivation: 显式推理（如思维链）计算开销大，而现有潜在推理方法性能显著下降，主要原因是潜在空间缺乏结构导致拟合困难。

Method: 提出两阶段学习框架：第一阶段设计专用注意力掩码指导潜在令牌编码器生成潜在令牌；第二阶段丢弃编码器，直接训练LLM自主生成潜在令牌进行推理，使用KL和CE损失优化。

Result: 在GSM8k上匹配显式SFT性能，推理链缩短4倍；在Math500和AIME24上超越基于隐藏状态的方法；有效压缩率和有效全局并行度指标显示潜在推理是单路径压缩和多路径叠加。

Conclusion: Latent-SFT通过将潜在空间限制在词汇概率空间，实现了高效且高性能的潜在推理，为减少LLM推理计算开销提供了有效解决方案。

Abstract: Large language models (LLMs) demonstrate strong reasoning abilities with
chain-of-thought prompting, but explicit reasoning introduces substantial
computational overhead. Recent work on latent reasoning reduces this cost by
reasoning in latent space without explicit supervision, but performance drops
significantly. Our preliminary experiments suggest that this degradation stems
from the unstructured latent space, which makes fitting latent tokens
difficult. To address this, we restrict the latent space to the column space of
the LLM vocabulary, treating latent reasoning as a superposition over
vocabulary probabilities. Once latent reasoning concludes, it collapses into an
eigenstate of explicit reasoning to yield the final answer. Based on this idea,
we propose Latent-SFT, a two-stage learning framework. In the first stage, we
design two specialized attention masks to guide the Latent Token Encoder in
generating latent tokens, allowing the LLM to produce the correct answer
conditioned on them. In the second stage, the Latent Token Encoder is
discarded, and the LLM is directly trained to generate these latent tokens
autonomously for latent reasoning, optimized with KL and CE losses. Latent-SFT
sets a new state of the art on GSM8k, matching explicit SFT performance while
cutting reasoning chains by up to 4 times and outperforming prior latent
methods. On Math500 and AIME24, lexical probability-based latent reasoning also
clearly surpasses hidden-state-based approaches. Our metrics of effective
compression rate and effective global parallelism further show that latent
reasoning is both the compression of a single path and the superposition of
multiple paths.

</details>


### [34] [TokenTiming: A Dynamic Alignment Method for Universal Speculative Decoding Model Pairs](https://arxiv.org/abs/2510.15545)
*Sibo Xiao,Jinyuan Fu,Zhongle Xie,Lidan Shou*

Main category: cs.CL

TL;DR: 提出了TokenTiming算法，通过动态时间规整技术解决推测解码中草稿模型和目标模型词汇表不匹配的问题，实现通用推测解码，无需重新训练即可获得1.57倍加速。


<details>
  <summary>Details</summary>
Motivation: 传统推测解码方法要求草稿模型和目标模型共享相同词汇表，这限制了可用草稿模型的选择范围，往往需要从头训练新模型，限制了推测解码的实用性。

Method: 提出TokenTiming算法，通过重新编码草稿标记序列得到新的目标标记序列，然后使用动态时间规整（DTW）技术建立映射关系来传输概率分布进行推测采样。

Result: 在各种任务上的综合实验表明，该方法实现了1.57倍的加速效果，能够处理词汇表不匹配的情况，且无需重新训练或修改现有模型。

Conclusion: TokenTiming算法为草稿模型选择提供了一种通用方法，使推测解码成为更通用和实用的LLM加速工具。

Abstract: Accelerating the inference of large language models (LLMs) has been a
critical challenge in generative AI. Speculative decoding (SD) substantially
improves LLM inference efficiency. However, its utility is limited by a
fundamental constraint: the draft and target models must share the same
vocabulary, thus limiting the herd of available draft models and often
necessitating the training of a new model from scratch. Inspired by Dynamic
Time Warping (DTW), a classic algorithm for aligning time series, we propose
the algorithm TokenTiming for universal speculative decoding. It operates by
re-encoding the draft token sequence to get a new target token sequence, and
then uses DTW to build a mapping to transfer the probability distributions for
speculative sampling. Benefiting from this, our method accommodates mismatched
vocabularies and works with any off-the-shelf models without retraining and
modification. We conduct comprehensive experiments on various tasks,
demonstrating 1.57x speedup. This work enables a universal approach for draft
model selection, making SD a more versatile and practical tool for LLM
acceleration.

</details>


### [35] [Rethinking Cross-lingual Gaps from a Statistical Viewpoint](https://arxiv.org/abs/2510.15551)
*Vihari Piratla,Purvam Jain,Darshan Singh,Partha Talukdar,Trevor Cohn*

Main category: cs.CL

TL;DR: 本文提出跨语言差距的主要原因是目标语言响应的方差，而非潜在表征差异，并通过方差控制方法显著提升了跨语言准确率。


<details>
  <summary>Details</summary>
Motivation: 现有研究将跨语言差距归因于源语言和目标语言潜在表征的差异，但本文认为目标语言响应的方差是主要原因。

Method: 通过偏差-方差分解形式化跨语言差距，进行推理时干预控制方差，包括使用提示指令减少响应方差。

Result: 简单的提示指令可将目标语言准确率提升20-25%，验证了方差是跨语言差距主要来源的假设。

Conclusion: 跨语言差距主要由目标语言响应的方差引起，通过控制方差可以有效缩小这一差距。

Abstract: Any piece of knowledge is usually expressed in one or a handful of natural
languages on the web or in any large corpus. Large Language Models (LLMs) act
as a bridge by acquiring knowledge from a source language and making it
accessible when queried from target languages. Prior research has pointed to a
cross-lingual gap, viz., a drop in accuracy when the knowledge is queried in a
target language compared to when the query is in the source language. Existing
research has rationalized divergence in latent representations in source and
target languages as the source of cross-lingual gap. In this work, we take an
alternative view and hypothesize that the variance of responses in the target
language is the main cause of this gap. For the first time, we formalize the
cross-lingual gap in terms of bias-variance decomposition. We present extensive
experimental evidence which support proposed formulation and hypothesis. We
then reinforce our hypothesis through multiple inference-time interventions
that control the variance and reduce the cross-lingual gap. We demonstrate a
simple prompt instruction to reduce the response variance, which improved
target accuracy by 20-25% across different models.

</details>


### [36] [Think Parallax: Solving Multi-Hop Problems via Multi-View Knowledge-Graph-Based Retrieval-Augmented Generation](https://arxiv.org/abs/2510.15552)
*Jinliang Liu*

Main category: cs.CL

TL;DR: ParallaxRAG是一个知识图谱增强生成框架，通过多视图空间对称解耦查询和图三元组，实现鲁棒检索架构，减少幻觉并支持多跳推理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在语言理解方面表现出色，但容易产生幻觉且在多跳推理方面存在困难。现有的基于知识图谱的检索增强生成方法依赖扁平嵌入和噪声路径探索，效果有限。

Method: 提出ParallaxRAG框架，将查询和图三元组对称解耦到多视图空间，构建鲁棒检索架构，明确强制头部多样性同时约束弱相关路径。核心观察是不同注意力头在不同推理阶段专门处理语义关系，为推理链的不同跳数做出贡献。

Result: 在WebQSP和CWQ数据集上的实验表明，在统一的、可复现的设置下（BGE-M3 + Llama3.1-8B），实现了具有竞争力的检索和问答性能，同时减少了幻觉并具有良好的泛化能力。

Conclusion: 多视图头部专业化是知识基础多跳推理的一个原则性方向，ParallaxRAG通过构建更清晰的子图并引导LLM进行基于基础的逐步推理，有效解决了现有方法的局限性。

Abstract: Large language models (LLMs) excel at language understanding but often
hallucinate and struggle with multi-hop reasoning. Knowledge-graph-based
retrieval-augmented generation (KG-RAG) offers grounding, yet most methods rely
on flat embeddings and noisy path exploration. We propose ParallaxRAG, a
framework that symmetrically decouples queries and graph triples into
multi-view spaces, enabling a robust retrieval architecture that explicitly
enforces head diversity while constraining weakly related paths. Central to our
approach is the observation that different attention heads specialize in
semantic relations at distinct reasoning stages, contributing to different hops
of the reasoning chain. This specialization allows ParallaxRAG to construct
cleaner subgraphs and guide LLMs through grounded, step-wise reasoning.
Experiments on WebQSP and CWQ, under our unified, reproducible setup (BGE-M3 +
Llama3.1-8B), demonstrate competitive retrieval and QA performance, alongside
reduced hallucination and good generalization. Our results highlight multi-view
head specialization as a principled direction for knowledge-grounded multi-hop
reasoning. Our implementation will be released as soon as the paper is
accepted.

</details>


### [37] [KITE: A Benchmark for Evaluating Korean Instruction-Following Abilities in Large Language Models](https://arxiv.org/abs/2510.15558)
*Dongjun Kim,Chanhee Park,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: KITE是一个专门用于评估韩语指令跟随能力的基准测试，填补了现有评估主要关注英语模型的空白，考虑了韩语独特的语法、形态特征、敬语系统和双数系统。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型评估主要关注英语模型，忽视了其他语言的语系和文化差异。韩语具有独特的语法结构、丰富的形态特征、敬语系统和双数系统，但缺乏专门的开放指令跟随能力评估基准。

Method: 开发了韩国指令跟随任务评估（KITE）基准，包含通用和韩语特定指令的评估。评估流程结合了自动指标和人工评估，公开了KITE数据集和代码。

Result: 评估揭示了不同模型之间的性能差异，提供了对其优势和劣势的深入洞察。

Conclusion: 通过公开KITE数据集和代码，旨在促进文化和语言包容性的大语言模型开发研究，并激励为其他代表性不足的语言开展类似工作。

Abstract: The instruction-following capabilities of large language models (LLMs) are
pivotal for numerous applications, from conversational agents to complex
reasoning systems. However, current evaluations predominantly focus on English
models, neglecting the linguistic and cultural nuances of other languages.
Specifically, Korean, with its distinct syntax, rich morphological features,
honorific system, and dual numbering systems, lacks a dedicated benchmark for
assessing open-ended instruction-following capabilities. To address this gap,
we introduce the Korean Instruction-following Task Evaluation (KITE), a
comprehensive benchmark designed to evaluate both general and Korean-specific
instructions. Unlike existing Korean benchmarks that focus mainly on factual
knowledge or multiple-choice testing, KITE directly targets diverse, open-ended
instruction-following tasks. Our evaluation pipeline combines automated metrics
with human assessments, revealing performance disparities across models and
providing deeper insights into their strengths and weaknesses. By publicly
releasing the KITE dataset and code, we aim to foster further research on
culturally and linguistically inclusive LLM development and inspire similar
endeavors for other underrepresented languages.

</details>


### [38] [Finetuning LLMs for EvaCun 2025 token prediction shared task](https://arxiv.org/abs/2510.15561)
*Josef Jon,Ondřej Bojar*

Main category: cs.CL

TL;DR: 本文介绍了在EvaCun 2025令牌预测任务中的提交系统，基于三种LLM（Command-R、Mistral和Aya Expanse）在任务数据上的微调，使用三种不同提示方法进行比较评估。


<details>
  <summary>Details</summary>
Motivation: 由于对任务领域和语言只有非常浅显的了解，作者选择直接使用组织者提供的训练数据，而不进行任何任务特定的调整、预处理或过滤。

Method: 基于三种大型语言模型（Command-R、Mistral和Aya Expanse）在任务数据上进行微调，并比较三种不同提示方法获取预测结果。

Result: 在数据保留部分上对三种方法进行了评估，但具体结果未在摘要中详细说明。

Conclusion: 该方法展示了在缺乏领域专业知识的情况下，直接使用预训练LLM进行微调并比较不同提示策略的可行性。

Abstract: In this paper, we present our submission for the token prediction task of
EvaCun 2025. Our sys-tems are based on LLMs (Command-R, Mistral, and Aya
Expanse) fine-tuned on the task data provided by the organizers. As we only
pos-sess a very superficial knowledge of the subject field and the languages of
the task, we simply used the training data without any task-specific
adjustments, preprocessing, or filtering. We compare 3 different approaches
(based on 3 different prompts) of obtaining the predictions, and we evaluate
them on a held-out part of the data.

</details>


### [39] [From Ghazals to Sonnets: Decoding the Polysemous Expressions of Love Across Languages](https://arxiv.org/abs/2510.15569)
*Syed Mohammad Sualeh Ali*

Main category: cs.CL

TL;DR: 本研究通过多义词案例分析方法，探索乌尔都语诗歌中三个看似同义的爱情词汇（pyaar、muhabbat、ishq）的细微差异，揭示其独特情感谱系，并通过词嵌入技术进行跨语言语义空间可视化比较。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语诗歌中表达爱情的词汇具有丰富的多义性和文化内涵，但这些细微差别在英语文学中缺乏直接对应，需要通过系统分析来揭示其独特的语义空间和文化表达方式。

Method: 采用多义词案例研究方法，分析三个乌尔都语爱情词汇在诗歌中的使用语境；同时运用词嵌入技术生成乌尔都语和英语爱情相关词汇的语义向量，进行定量比较和可视化分析。

Result: 研究发现三个乌尔都语爱情词汇在语义空间上存在明显差异，揭示了英语中不存在的细微情感层次；词嵌入可视化清晰地展示了这些词汇独特的文化语义分布模式。

Conclusion: 乌尔都语诗歌通过多义词的精细运用，构建了丰富的情感表达谱系，这种文化语言特色为理解爱情的多维表达提供了独特视角，凸显了语言与文化在情感表达中的紧密联系。

Abstract: This paper delves into the intricate world of Urdu poetry, exploring its
thematic depths through a lens of polysemy. By focusing on the nuanced
differences between three seemingly synonymous words (pyaar, muhabbat, and
ishq) we expose a spectrum of emotions and experiences unique to the Urdu
language. This study employs a polysemic case study approach, meticulously
examining how these words are interwoven within the rich tapestry of Urdu
poetry. By analyzing their usage and context, we uncover a hidden layer of
meaning, revealing subtle distinctions which lack direct equivalents in English
literature. Furthermore, we embark on a comparative analysis, generating word
embeddings for both Urdu and English terms related to love. This enables us to
quantify and visualize the semantic space occupied by these words, providing
valuable insights into the cultural and linguistic nuances of expressing love.
Through this multifaceted approach, our study sheds light on the captivating
complexities of Urdu poetry, offering a deeper understanding and appreciation
for its unique portrayal of love and its myriad expressions

</details>


### [40] [BiMax: Bidirectional MaxSim Score for Document-Level Alignment](https://arxiv.org/abs/2510.15577)
*Xiaotian Wang,Takehito Utsuro,Masaaki Nagata*

Main category: cs.CL

TL;DR: 本文提出了一种名为BiMax的跨语言文档对齐方法，相比最优传输方法实现了约100倍的速度提升，同时保持相当的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的文档对齐方法如最优传输方法虽然精度高，但在处理大规模网络挖掘数据时效率不足，需要同时考虑准确性和速度。

Method: 提出跨语言双向最大相似度得分方法，用于计算文档间相似度，提高文档对齐效率。

Result: 在WMT16双语文档对齐任务上，BiMax方法达到与最优传输方法相当的准确率，同时速度提升约100倍。

Conclusion: BiMax方法在保持高准确率的同时显著提升了文档对齐效率，为大规模网络挖掘提供了实用的解决方案。

Abstract: Document alignment is necessary for the hierarchical mining (Ba\~n\'on et
al., 2020; Morishita et al., 2022), which aligns documents across source and
target languages within the same web domain. Several high precision sentence
embedding-based methods have been developed, such as TK-PERT (Thompson and
Koehn, 2020) and Optimal Transport (OT) (Clark et al., 2019; El-Kishky and
Guzm\'an, 2020). However, given the massive scale of web mining data, both
accuracy and speed must be considered. In this paper, we propose a
cross-lingual Bidirectional Maxsim score (BiMax) for computing doc-to-doc
similarity, to improve efficiency compared to the OT method. Consequently, on
the WMT16 bilingual document alignment task, BiMax attains accuracy comparable
to OT with an approximate 100-fold speed increase. Meanwhile, we also conduct a
comprehensive analysis to investigate the performance of current
state-of-the-art multilingual sentence embedding models. All the alignment
methods in this paper are publicly available as a tool called EmbDA
(https://github.com/EternalEdenn/EmbDA).

</details>


### [41] [The Elephant in the Coreference Room: Resolving Coreference in Full-Length French Fiction Works](https://arxiv.org/abs/2510.15594)
*Antoine Bourgois,Thierry Poibeau*

Main category: cs.CL

TL;DR: 本文介绍了一个包含三本完整法文小说的新标注语料库，用于解决长文档中共指消解的挑战，并提出了一个模块化的共指消解流程。


<details>
  <summary>Details</summary>
Motivation: 虽然共指消解在计算文学研究中受到越来越多的关注，但完全标注的长文档代表性数据集仍然非常稀缺。

Method: 引入了一个包含三本完整法文小说的新标注语料库，总计超过285,000个词符，并提出了一个模块化的共指消解流程。

Result: 该方法具有竞争力并能有效扩展到长文档，同时证明了其在推断虚构角色性别方面的实用性。

Conclusion: 该研究为文学分析和下游NLP任务提供了相关工具和方法，解决了长文档共指消解的挑战。

Abstract: While coreference resolution is attracting more interest than ever from
computational literature researchers, representative datasets of fully
annotated long documents remain surprisingly scarce. In this paper, we
introduce a new annotated corpus of three full-length French novels, totaling
over 285,000 tokens. Unlike previous datasets focused on shorter texts, our
corpus addresses the challenges posed by long, complex literary works, enabling
evaluation of coreference models in the context of long reference chains. We
present a modular coreference resolution pipeline that allows for fine-grained
error analysis. We show that our approach is competitive and scales effectively
to long documents. Finally, we demonstrate its usefulness to infer the gender
of fictional characters, showcasing its relevance for both literary analysis
and downstream NLP tasks.

</details>


### [42] [HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators under Underdetermination](https://arxiv.org/abs/2510.15614)
*Tingting Chen,Beibei Lin,Zifeng Yuan,Qiran Zou,Hongyu He,Yew-Soon Ong,Anirudh Goyal,Dianbo Liu*

Main category: cs.CL

TL;DR: HypoSpace是一个评估语言模型提出多种解释能力的诊断套件，通过有效性、独特性和覆盖率三个指标来衡量模型在不确定科学问题中生成多样化假设的能力。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型在科学工作流程中的广泛应用，需要评估它们提出多种解释（而非单一正确答案）的能力，因为许多科学问题是不确定的，多个机制不同的假设可能与相同观察结果一致。

Method: HypoSpace将LLMs视为有限假设集的采样器，测量三个互补指标：有效性（与观察一致的提议精度）、独特性（提议间的非冗余性）和覆盖率（对枚举可接受集的覆盖）。在三个结构化领域实例化：因果图、重力约束3D体素重建和布尔遗传相互作用。

Result: 在指令调优和推理聚焦模型中，有效性通常保持较高，但随着可接受空间的增长，独特性和覆盖率会下降，揭示了仅正确性指标无法检测的模式崩溃现象。

Conclusion: HypoSpace为明确探索和覆盖可接受解释空间的方法提供了一个受控探针，而非排行榜，有助于评估模型在不确定科学问题中的表现。

Abstract: As language models are increasingly used in scientific workflows, evaluating
their ability to propose sets of explanations-not just a single correct
answer-becomes critical. Many scientific problems are underdetermined:
multiple, mechanistically distinct hypotheses are consistent with the same
observations. We introduce HypoSpace, a diagnostic suite that treats LLMs as
samplers of finite hypothesis sets and measures three complementary indicators:
Validity (precision of proposals consistent with observations), Uniqueness
(non-redundancy among proposals), and Recovery (coverage of the enumerated
admissible set). We instantiate HypoSpace in three structured domains with
deterministic validators and exactly enumerated hypothesis spaces: (i) causal
graphs from perturbations, (ii) gravity-constrained 3D voxel reconstruction
from top-down projections, and (iii) Boolean genetic interactions. Across
instruction-tuned and reasoning-focused models, Validity often remains high
while Uniqueness and Recovery degrade as the admissible space grows, revealing
mode collapse that is invisible to correctness-only metrics. HypoSpace offers a
controlled probe-rather than a leaderboard-for methods that explicitly explore
and cover admissible explanation spaces. Code is available at:
https://github.com/CTT-Pavilion/_HypoSpace.

</details>


### [43] [Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection](https://arxiv.org/abs/2510.15685)
*Joshua Wolfe Brook,Ilia Markov*

Main category: cs.CL

TL;DR: 使用大型语言模型生成背景上下文来增强文本和多模态仇恨言论检测，通过四种上下文融入方法在隐式仇恨言论数据集上实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决隐式仇恨言论检测中缺乏上下文信息的问题，利用LLMs作为动态知识库来提供背景知识以改善检测效果。

Method: 提出两种上下文生成策略（命名实体聚焦和全文提示），比较四种上下文融入方法：文本拼接、嵌入拼接、分层transformer融合和LLM驱动的文本增强。

Result: 在文本和多模态设置中分别获得最高3和6个F1分数的提升，嵌入拼接方法表现最佳。

Conclusion: 上下文信息及其融入方法对仇恨言论检测至关重要，LLM生成的背景知识能显著提升隐式和多模态仇恨言论的检测性能。

Abstract: This research introduces a novel approach to textual and multimodal Hate
Speech Detection (HSD), using Large Language Models (LLMs) as dynamic knowledge
bases to generate background context and incorporate it into the input of HSD
classifiers. Two context generation strategies are examined: one focused on
named entities and the other on full-text prompting. Four methods of
incorporating context into the classifier input are compared: text
concatenation, embedding concatenation, a hierarchical transformer-based
fusion, and LLM-driven text enhancement. Experiments are conducted on the
textual Latent Hatred dataset of implicit hate speech and applied in a
multimodal setting on the MAMI dataset of misogynous memes. Results suggest
that both the contextual information and the method by which it is incorporated
are key, with gains of up to 3 and 6 F1 points on textual and multimodal setups
respectively, from a zero-context baseline to the highest-performing system,
based on embedding concatenation.

</details>


### [44] [Attention Sinks in Diffusion Language Models](https://arxiv.org/abs/2510.15731)
*Maximo Eduardo Rulli,Simone Petruzzi,Edoardo Michielon,Fabrizio Silvestri,Simone Scardapane,Alessio Devoto*

Main category: cs.CL

TL;DR: 该论文对掩码扩散语言模型（DLMs）的注意力机制进行实证分析，发现其存在动态变化的注意力下沉现象，且与自回归模型（ARMs）相比，DLMs对注意力下沉的移除具有更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 虽然掩码扩散语言模型作为自回归模型的替代方案显示出潜力，但其内部工作机制尚未得到充分探索，特别是注意力模式的研究较为缺乏。

Method: 通过实证分析DLMs的注意力模式，重点关注注意力下沉现象，并与自回归模型进行对比研究。

Result: 发现DLMs也存在注意力下沉现象，但具有两个显著特征：1）下沉位置在生成过程中动态变化；2）移除注意力下沉仅导致轻微性能下降，表现出比ARMs更强的鲁棒性。

Conclusion: 研究揭示了扩散语言模型在注意力分配和利用方面的根本性差异，为理解其内部工作机制提供了新见解。

Abstract: Masked Diffusion Language Models (DLMs) have recently emerged as a promising
alternative to traditional Autoregressive Models (ARMs). DLMs employ
transformer encoders with bidirectional attention, enabling parallel token
generation while maintaining competitive performance. Although their efficiency
and effectiveness have been extensively studied, the internal mechanisms that
govern DLMs remain largely unexplored. In this work, we conduct an empirical
analysis of DLM attention patterns, focusing on the attention sinking
phenomenon, an effect previously observed in various transformer-based
architectures. Our findings reveal that DLMs also exhibit attention sinks, but
with distinct characteristics. First, unlike in ARMs, the sink positions in
DLMs tend to shift throughout the generation process, displaying a dynamic
behaviour. Second, while ARMs are highly sensitive to the removal of attention
sinks, DLMs remain robust: masking sinks leads to only a minor degradation in
performance. These results provide new insights into the inner workings of
diffusion-based language models and highlight fundamental differences in how
they allocate and utilize attention compared to autoregressive models.

</details>


### [45] [LLMs Judge Themselves: A Game-Theoretic Framework for Human-Aligned Evaluation](https://arxiv.org/abs/2510.15746)
*Gao Yang,Yuhang Liu,Siyu Miao,Xinyue Liang,Zhengyang Liu,Heyan Huang*

Main category: cs.CL

TL;DR: 本文提出了一种基于博弈论的大语言模型自动互评框架，通过模型间的自玩和同行评审来评估LLM能力，并与人类投票行为进行对比验证。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法依赖固定格式任务和参考答案，难以捕捉现代LLM行为的细微、主观和开放式特性，因此需要新的评估范式。

Method: 采用自动互评方法，让LLM通过自玩和同行评审相互评估输出，并运用博弈论投票算法汇总同行评审结果，系统比较模型生成排名与人类判断的一致性。

Result: 实证结果显示理论预测与人类评估之间存在收敛和分歧，揭示了互评方法的潜力和局限性。

Conclusion: 这是首个将互评、博弈论聚合和基于人类的验证相结合来评估LLM能力的工作，为LLM评估提供了新的理论框架和实践方法。

Abstract: Ideal or real - that is the question.In this work, we explore whether
principles from game theory can be effectively applied to the evaluation of
large language models (LLMs). This inquiry is motivated by the growing
inadequacy of conventional evaluation practices, which often rely on
fixed-format tasks with reference answers and struggle to capture the nuanced,
subjective, and open-ended nature of modern LLM behavior. To address these
challenges, we propose a novel alternative: automatic mutual evaluation, where
LLMs assess each other's output through self-play and peer review. These peer
assessments are then systematically compared with human voting behavior to
evaluate their alignment with human judgment. Our framework incorporates
game-theoretic voting algorithms to aggregate peer reviews, enabling a
principled investigation into whether model-generated rankings reflect human
preferences. Empirical results reveal both convergences and divergences between
theoretical predictions and human evaluations, offering valuable insights into
the promises and limitations of mutual evaluation. To the best of our
knowledge, this is the first work to jointly integrate mutual evaluation,
game-theoretic aggregation, and human-grounded validation for evaluating the
capabilities of LLMs.

</details>


### [46] [On Non-interactive Evaluation of Animal Communication Translators](https://arxiv.org/abs/2510.15768)
*Orr Paradise,David F. Gruber,Adam Tauman Kalai*

Main category: cs.CL

TL;DR: 提出了一种无需参考翻译或与动物互动即可评估AI翻译器质量的方法，通过逐段翻译和NLP洗牌测试来检测幻觉翻译，在数据稀缺的人类语言和构造语言上验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决如何验证AI动物语言翻译器的问题，避免与动物互动的安全、伦理和成本问题，实现无参考翻译的机器翻译质量评估。

Method: 使用逐段翻译结合经典NLP洗牌测试，通过比较顺序翻译与随机排列翻译的合理性来评估翻译质量，检测幻觉翻译。

Result: 在数据稀缺的人类语言和构造语言上的概念验证实验表明，该方法与基于参考翻译的标准评估高度相关，证明其有效性。

Conclusion: 对于足够复杂的语言，互动和观察可能不是翻译器评估的必要条件，仅通过英语输出即可评估翻译质量，这在学习翻译的早期阶段可能更有效。

Abstract: If you had an AI Whale-to-English translator, how could you validate whether
or not it is working? Does one need to interact with the animals or rely on
grounded observations such as temperature? We provide theoretical and
proof-of-concept experimental evidence suggesting that interaction and even
observations may not be necessary for sufficiently complex languages. One may
be able to evaluate translators solely by their English outputs, offering
potential advantages in terms of safety, ethics, and cost. This is an instance
of machine translation quality evaluation (MTQE) without any reference
translations available. A key challenge is identifying ``hallucinations,''
false translations which may appear fluent and plausible. We propose using
segment-by-segment translation together with the classic NLP shuffle test to
evaluate translators. The idea is to translate animal communication, turn by
turn, and evaluate how often the resulting translations make more sense in
order than permuted. Proof-of-concept experiments on data-scarce human
languages and constructed languages demonstrate the potential utility of this
evaluation methodology. These human-language experiments serve solely to
validate our reference-free metric under data scarcity. It is found to
correlate highly with a standard evaluation based on reference translations,
which are available in our experiments. We also perform a theoretical analysis
suggesting that interaction may not be necessary nor efficient in the early
stages of learning to translate.

</details>


### [47] [Emergence of Linear Truth Encodings in Language Models](https://arxiv.org/abs/2510.15804)
*Shauli Ravfogel,Gilad Yehudai,Tal Linzen,Joan Bruna,Alberto Bietti*

Main category: cs.CL

TL;DR: 本文通过构建一个单层transformer玩具模型，揭示了语言模型中线性真值子空间的形成机制，发现真值编码在事实陈述共现的数据分布中自然涌现，并观察到两阶段学习动态。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明大语言模型存在区分真假陈述的线性子空间，但其形成机制尚不明确。本文旨在通过透明模型揭示这种真值表示的具体形成路径。

Method: 构建一个单层transformer玩具模型，在事实陈述共现的数据分布设置下进行端到端训练，并通过预训练语言模型实验验证该模式。

Result: 模型成功再现了线性真值子空间，观察到两阶段学习动态：先快速记忆个体事实关联，然后长时间学习线性分离真假陈述以降低语言建模损失。

Conclusion: 研究提供了真值线性表示在语言模型中如何及为何出现的机制性证明和实证动机，揭示了在事实共现数据分布下真值编码的自然涌现过程。

Abstract: Recent probing studies reveal that large language models exhibit linear
subspaces that separate true from false statements, yet the mechanism behind
their emergence is unclear. We introduce a transparent, one-layer transformer
toy model that reproduces such truth subspaces end-to-end and exposes one
concrete route by which they can arise. We study one simple setting in which
truth encoding can emerge: a data distribution where factual statements
co-occur with other factual statements (and vice-versa), encouraging the model
to learn this distinction in order to lower the LM loss on future tokens. We
corroborate this pattern with experiments in pretrained language models.
Finally, in the toy setting we observe a two-phase learning dynamic: networks
first memorize individual factual associations in a few steps, then -- over a
longer horizon -- learn to linearly separate true from false, which in turn
lowers language-modeling loss. Together, these results provide both a
mechanistic demonstration and an empirical motivation for how and why linear
truth representations can emerge in language models.

</details>


### [48] [Paper2Web: Let's Make Your Paper Alive!](https://arxiv.org/abs/2510.15842)
*Yuhang Chen,Tianpeng Lv,Siyi Zhang,Yixiang Yin,Yao Wan,Philip S. Yu,Dongping Chen*

Main category: cs.CL

TL;DR: Paper2Web是一个用于评估学术网页生成的基准数据集和多维评估框架，包含PWAgent自动管道，可将科学论文转换为交互式多媒体丰富的学术主页。


<details>
  <summary>Details</summary>
Motivation: 当前学术项目网站在有效传播研究方面存在不足，现有方法如直接LLM生成、模板或HTML转换难以产生布局感知的交互式网站，且缺乏全面的评估套件。

Method: 提出Paper2Web基准数据集，包含基于规则的指标（连通性、完整性）和人类验证的LLM-as-a-Judge评估（交互性、美观性、信息性），以及PaperQuiz衡量论文级知识保留。开发PWAgent自主管道，通过MCP工具迭代优化内容和布局。

Result: 实验显示PWAgent显著优于端到端基线方法（如基于模板的网页和arXiv/alphaXiv版本），同时保持低成本，在学术网页生成中达到帕累托前沿。

Conclusion: Paper2Web为学术网页生成提供了全面的评估框架，PWAgent能够有效将科学论文转换为高质量的交互式学术主页，优于现有方法。

Abstract: Academic project websites can more effectively disseminate research when they
clearly present core content and enable intuitive navigation and interaction.
However, current approaches such as direct Large Language Model (LLM)
generation, templates, or direct HTML conversion struggle to produce
layout-aware, interactive sites, and a comprehensive evaluation suite for this
task has been lacking. In this paper, we introduce Paper2Web, a benchmark
dataset and multi-dimensional evaluation framework for assessing academic
webpage generation. It incorporates rule-based metrics like Connectivity,
Completeness and human-verified LLM-as-a-Judge (covering interactivity,
aesthetics, and informativeness), and PaperQuiz, which measures paper-level
knowledge retention. We further present PWAgent, an autonomous pipeline that
converts scientific papers into interactive and multimedia-rich academic
homepages. The agent iteratively refines both content and layout through MCP
tools that enhance emphasis, balance, and presentation quality. Our experiments
show that PWAgent consistently outperforms end-to-end baselines like
template-based webpages and arXiv/alphaXiv versions by a large margin while
maintaining low cost, achieving the Pareto-front in academic webpage
generation.

</details>


### [49] [Enhanced Sentiment Interpretation via a Lexicon-Fuzzy-Transformer Framework](https://arxiv.org/abs/2510.15843)
*Shayan Rokhva,Mousa Alizadeh,Maryam Abdollahi Shamami*

Main category: cs.CL

TL;DR: 提出了一种结合词典、模糊逻辑和Transformer的混合框架，用于生成连续的情感分数，在多个领域数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 由于非正式和领域特定语言的存在，准确检测产品评论和社交媒体帖子中的情感极性和强度仍然具有挑战性。

Method: 采用混合词典-模糊-Transformer框架，结合基于规则的启发式方法、上下文深度学习和模糊逻辑。流程包括VADER初始情感估计，通过DistilBERT置信度分数进行两阶段调整，并使用自定义模糊推理系统将分数映射到0-1连续区间。

Result: 在四个领域特定数据集（外卖、电商、旅游、时尚）上评估，结果显示与用户评分更好的对齐、更好的情感极端识别和减少的错误分类。定量指标和定性分析均证实模型的鲁棒性和效率。

Conclusion: 这项工作展示了将符号推理与神经模型相结合在语言动态领域中进行可解释、细粒度情感分析的价值。

Abstract: Accurately detecting sentiment polarity and intensity in product reviews and
social media posts remains challenging due to informal and domain-specific
language. To address this, we propose a novel hybrid lexicon-fuzzy-transformer
framework that combines rule-based heuristics, contextual deep learning, and
fuzzy logic to generate continuous sentiment scores reflecting both polarity
and strength. The pipeline begins with VADER-based initial sentiment
estimations, which are refined through a two-stage adjustment process. This
involves leveraging confidence scores from DistilBERT, a lightweight
transformer and applying fuzzy logic principles to mitigate excessive
neutrality bias and enhance granularity. A custom fuzzy inference system then
maps the refined scores onto a 0 to 1 continuum, producing expert)like
judgments. The framework is rigorously evaluated on four domain-specific
datasets. food delivery, e-commerce, tourism, and fashion. Results show
improved alignment with user ratings, better identification of sentiment
extremes, and reduced misclassifications. Both quantitative metrics
(distributional alignment, confusion matrices) and qualitative insights (case
studies, runtime analysis) affirm the models robustness and efficiency. This
work demonstrates the value of integrating symbolic reasoning with neural
models for interpretable, finegrained sentiment analysis in linguistically
dynamic domains.

</details>


### [50] [SpeechLLMs for Large-scale Contextualized Zero-shot Slot Filling](https://arxiv.org/abs/2510.15851)
*Kadri Hacioglu,Manjunath K E,Andreas Stolcke*

Main category: cs.CL

TL;DR: 该论文研究了基于语音大语言模型的槽位填充任务，通过建立经验上限、识别性能差距，并提出数据、架构和训练策略的改进来提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统的槽位填充任务采用语音识别和自然语言理解的级联方法，而语音大语言模型的出现为实现更统一、生成式和指令跟随的语音理解任务提供了新途径，同时具备零样本能力和对未见槽标签的泛化能力。

Method: 通过创建任务的经验上限，识别性能、鲁棒性和泛化差距，并提出训练数据、架构和训练策略的改进措施来缩小与上限结果的差距。

Result: 研究表明，这些改进措施都能显著提高性能，同时揭示了实际挑战，并为利用这些新兴模型提供了经验指导和见解。

Conclusion: 语音大语言模型在槽位填充任务中具有巨大潜力，通过系统性的改进可以显著提升性能，为实际应用提供了有价值的指导。

Abstract: Slot filling is a crucial subtask in spoken language understanding (SLU),
traditionally implemented as a cascade of speech recognition followed by one or
more natural language understanding (NLU) components. The recent advent of
speech-based large language models (speechLLMs), which integrate speech and
textual foundation models, has opened new avenues for achieving speech
understanding tasks in a more unified, generative, and instruction-following
manner while promising data and compute efficiency with zero-shot abilities,
generalizing to unseen slot labels. We address the slot-filling task by
creating an empirical upper bound for the task, identifying performance,
robustness, and generalization gaps, and proposing improvements to the training
data, architecture, and training strategies to narrow the gap with the upper
bound result. We show that each of these measures improve performance
substantially, while highlighting practical challenges and providing empirical
guidance and insights for harnessing these emerging models.

</details>


### [51] [InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training](https://arxiv.org/abs/2510.15859)
*Pengkai Wang,Qi Zuo,Pengwei Liu,Zhijie Sang,Congkai Xie,Hongxia Yang*

Main category: cs.CL

TL;DR: ORBIT框架通过基于量规的增量强化学习，在开放领域（特别是医疗对话）中解决了奖励函数模糊的问题，将模型在HealthBench-Hard基准上的性能从7.0提升至27.2。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在数学和代码等有明确奖励函数的领域表现良好，但在开放领域（如医疗咨询）中由于奖励模糊、主观和上下文依赖而面临挑战。

Method: ORBIT框架结合合成对话生成和动态量规创建，使用量规指导的反馈来进行增量强化学习，不依赖外部医学知识或手动规则。

Result: 在Qwen3-4B-Instruct模型上，仅使用2k样本就将HealthBench-Hard基准性能从7.0提升至27.2，达到该规模模型的最先进水平。

Conclusion: 量规驱动的强化学习能够在复杂开放任务中实现一致性能提升，为LLM在这些领域的进步提供了可扩展策略。

Abstract: Large Language Models (LLMs) have shown substantial advances through
reinforcement learning (RL), particularly in domains where rewards can be
programmatically verified, such as mathematics and code. In these areas, models
benefit from a well-defined operational base guided by explicit rule-based
objectives. However, this progress reveals a significant limitation: in
open-ended domains where rewards are ambiguous, subjective, or
context-dependent, such as creative writing, scientific reasoning, and notably
medical consultation, robust reward functions are lacking, making these areas
challenging for current RL strategies. To bridge this gap, we introduce ORBIT,
an open-ended rubric-based incremental training framework specifically designed
for high-stakes medical dialogue. ORBIT integrates syn- thetic dialogue
generation with the dynamic creation of rubrics, employing these rubrics to
direct an incremental RL process. In particular, this approach does not depend
on external medical knowledge or manual rules, instead utilizing rubric-guided
feedback to shape learning. When implemented on the Qwen3-4B-Instruct model,
our method can greatly enhance its performance on the HealthBench-Hard
benchmark from 7.0 to 27.2 using only 2k samples, thus achieving
state-of-the-art results for models of this scale. Our analysis confirms that
rubric-driven RL fos-ters consistent performance gains across diverse
consultation scenarios, going beyond simple numerical improvements. These
findings underscore rubric-based feedback as a scalable strategy for advancing
LLMs in intricate, open-ended tasks.

</details>


### [52] [PolySkill: Learning Generalizable Skills Through Polymorphic Abstraction](https://arxiv.org/abs/2510.15863)
*Simon Yu,Gang Li,Weiyan Shi,Peng Qi*

Main category: cs.CL

TL;DR: PolySkill框架通过将技能的目标与实现解耦，使智能体能够学习可泛化和组合的技能，在网站导航任务中显著提升技能重用率和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有方法学习的技能通常过度专业化于单一网站，缺乏泛化能力，限制了智能体在开放网络环境中的持续学习能力。

Method: 受软件工程中多态性的启发，PolySkill将技能的抽象目标（要完成什么）与具体实现（如何执行）分离开来，使技能能够适应不同环境。

Result: 实验表明：1)在已见网站上技能重用率提高1.7倍；2)在Mind2Web数据集上成功率提升9.4%，在未见网站上提升13.9%，同时步骤减少20%以上；3)在自主探索中能学习跨网站通用的技能。

Conclusion: 将技能目标与执行分离是开发能够在开放网络中持续学习和泛化的自主智能体的关键步骤，为构建适应性环境中的持续学习智能体提供了实用路径。

Abstract: Large language models (LLMs) are moving beyond static uses and are now
powering agents that learn continually during their interaction with external
environments. For example, agents can learn reusable skills while navigating
web pages or toggling new tools. However, existing methods for skill learning
often create skills that are over-specialized to a single website and fail to
generalize. We introduce PolySkill, a new framework that enables agents to
learn generalizable and compositional skills. The core idea, inspired by
polymorphism in software engineering, is to decouple a skill's abstract goal
(what it accomplishes) and its concrete implementation (how it is executed).
Experiments show that our method (1) improves skill reuse by 1.7x on seen
websites and (2) boosts success rates by up to 9.4% on Mind2Web and 13.9% on
unseen websites, while reducing steps by over 20%. (3) In self-exploration
settings without specified tasks, our framework improves the quality of
proposed tasks and enables agents to learn generalizable skills that work
across different sites. By enabling the agent to identify and refine its own
goals, the PolySkill enhances the agent's ability to learn a better curriculum,
leading to the acquisition of more generalizable skills compared to baseline
methods. This work provides a practical path toward building agents capable of
continual learning in adaptive environments. Our findings show that separating
a skill's goal from its execution is a crucial step toward developing
autonomous agents that can learn and generalize across the open web
continuously.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [53] [DMRetriever: A Family of Models for Improved Text Retrieval in Disaster Management](https://arxiv.org/abs/2510.15087)
*Kai Yin,Xiangjue Dong,Chengkai Liu,Allen Lin,Lingfeng Shi,Ali Mostafavi,James Caverlee*

Main category: cs.IR

TL;DR: DMRetriever是首个专门为灾害管理设计的密集检索模型系列，通过三阶段训练框架实现，在所有六种搜索意图上都达到了最先进的性能，且具有极高的参数效率。


<details>
  <summary>Details</summary>
Motivation: 现有通用检索模型无法有效处理灾害管理场景中多样化的搜索意图，导致性能不一致和不可靠，需要专门针对该领域的检索模型。

Method: 采用新颖的三阶段训练框架：双向注意力适应、无监督对比预训练和难度感知渐进式指令微调，使用高级数据精炼管道生成的高质量数据进行训练。

Result: DMRetriever在所有六种搜索意图的每个模型规模上都实现了最先进的性能，具有极高的参数效率，5.96亿参数模型性能超过基线模型13.3倍，3300万参数模型仅用基线7.6%的参数就超越了基线性能。

Conclusion: DMRetriever是灾害管理领域首个专门的密集检索模型，通过创新的训练框架实现了卓越的性能和参数效率，为该领域的信息检索提供了可靠解决方案。

Abstract: Effective and efficient access to relevant information is essential for
disaster management. However, no retrieval model is specialized for disaster
management, and existing general-domain models fail to handle the varied search
intents inherent to disaster management scenarios, resulting in inconsistent
and unreliable performance. To this end, we introduce DMRetriever, the first
series of dense retrieval models (33M to 7.6B) tailored for this domain. It is
trained through a novel three-stage framework of bidirectional attention
adaptation, unsupervised contrastive pre-training, and difficulty-aware
progressive instruction fine-tuning, using high-quality data generated through
an advanced data refinement pipeline. Comprehensive experiments demonstrate
that DMRetriever achieves state-of-the-art (SOTA) performance across all six
search intents at every model scale. Moreover, DMRetriever is highly
parameter-efficient, with 596M model outperforming baselines over 13.3 X larger
and 33M model exceeding baselines with only 7.6% of their parameters. All
codes, data, and checkpoints are available at
https://github.com/KaiYin97/DMRETRIEVER

</details>


### [54] [MTmixAtt: Integrating Mixture-of-Experts with Multi-Mix Attention for Large-Scale Recommendation](https://arxiv.org/abs/2510.15286)
*Xianyang Qi,Yuan Tian,Zhaoyu Hu,Zhirui Kuai,Chang Liu,Hongxiang Lin,Lei Wang*

Main category: cs.IR

TL;DR: MTmixAtt是一个统一的混合专家架构，通过自动特征聚类和多混合注意力机制，解决了传统推荐系统中手动特征工程和场景特定架构的限制，在工业推荐任务中实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统依赖手动特征工程和场景特定架构，这阻碍了跨场景迁移和大规模部署。需要一种统一的解决方案来处理异构特征并捕获全局模式和场景特定行为。

Method: 提出MTmixAtt架构，包含两个核心组件：AutoToken模块自动将异构特征聚类为语义连贯的token；MTmixAttBlock模块通过可学习的混合矩阵、共享稠密专家和场景感知稀疏专家实现高效的token交互。

Result: 在美团TRec数据集上的实验表明，MTmixAtt在CTR和CTCVR指标上优于包括Transformer、WuKong、HiFormer等在内的最先进基线模型。大规模在线A/B测试验证了实际效果：在首页场景中，支付PV增加3.62%，实际支付GTV增加2.54%。

Conclusion: MTmixAtt提供了一个统一且可扩展的解决方案，能够跨场景建模任意异构特征，显著改善了用户体验和商业成果。

Abstract: Industrial recommender systems critically depend on high-quality ranking
models. However, traditional pipelines still rely on manual feature engineering
and scenario-specific architectures, which hinder cross-scenario transfer and
large-scale deployment. To address these challenges, we propose
\textbf{MTmixAtt}, a unified Mixture-of-Experts (MoE) architecture with
Multi-Mix Attention, designed for large-scale recommendation tasks. MTmixAtt
integrates two key components. The \textbf{AutoToken} module automatically
clusters heterogeneous features into semantically coherent tokens, removing the
need for human-defined feature groups. The \textbf{MTmixAttBlock} module
enables efficient token interaction via a learnable mixing matrix, shared dense
experts, and scenario-aware sparse experts, capturing both global patterns and
scenario-specific behaviors within a single framework. Extensive experiments on
the industrial TRec dataset from Meituan demonstrate that MTmixAtt consistently
outperforms state-of-the-art baselines including Transformer-based models,
WuKong, HiFormer, MLP-Mixer, and RankMixer. At comparable parameter scales,
MTmixAtt achieves superior CTR and CTCVR metrics; scaling to MTmixAtt-1B yields
further monotonic gains. Large-scale online A/B tests validate the real-world
impact: in the \textit{Homepage} scenario, MTmixAtt increases Payment PV by
\textbf{+3.62\%} and Actual Payment GTV by \textbf{+2.54\%}. Overall, MTmixAtt
provides a unified and scalable solution for modeling arbitrary heterogeneous
features across scenarios, significantly improving both user experience and
commercial outcomes.

</details>


### [55] [GRank: Towards Target-Aware and Streamlined Industrial Retrieval with a Generate-Rank Framework](https://arxiv.org/abs/2510.15299)
*Yijia Sun,Shanshan Huang,Zhiyuan Guan,Qiang Luo,Ruiming Tang,Kun Gai,Guorui Zhou*

Main category: cs.IR

TL;DR: GRank是一个无需结构化索引的检索新范式，将目标感知学习与用户中心检索统一，通过生成器进行个性化候选生成，轻量级排序器进行细粒度推理，在公开基准和生产环境中显著提升召回率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有工业推荐系统检索阶段面临两个主要问题：双塔架构表达能力有限，无法捕捉细粒度用户-物品交互；结构化索引方法难以融入动态用户偏好且构建维护成本高昂。

Method: 提出GRank框架，包含：(1)目标感知生成器，通过GPU加速MIPS进行个性化候选生成；(2)轻量级排序器，在小规模候选集上执行细粒度推理；(3)端到端多任务学习框架，确保生成和排序目标的语义一致性。

Result: 在两个公开基准和十亿级物品生产语料上的实验表明，GRank将Recall@500提升超过30%，P99 QPS达到基于树和图的最先进检索器的1.7倍。在线A/B测试显示核心参与度指标显著改善。

Conclusion: GRank成功解决了现有检索方法的局限性，无需结构化索引即可实现高效个性化检索，已在生产环境中部署并服务4亿活跃用户，证明了其实际应用价值。

Abstract: Industrial-scale recommender systems rely on a cascade pipeline in which the
retrieval stage must return a high-recall candidate set from billions of items
under tight latency. Existing solutions ei- ther (i) suffer from limited
expressiveness in capturing fine-grained user-item interactions, as seen in
decoupled dual-tower architectures that rely on separate encoders, or
generative models that lack precise target-aware matching capabilities, or (ii)
build structured indices (tree, graph, quantization) whose item-centric
topologies struggle to incorporate dynamic user preferences and incur
prohibitive construction and maintenance costs.
  We present GRank, a novel structured-index-free retrieval paradigm that
seamlessly unifies target-aware learning with user-centric retrieval. Our key
innovations include: (1) A target-aware Generator trained to perform
personalized candidate generation via GPU-accelerated MIPS, eliminating
semantic drift and maintenance costs of structured indexing; (2) A lightweight
but powerful Ranker that performs fine-grained, candidate-specific inference on
small subsets; (3) An end-to-end multi-task learning framework that ensures
semantic consistency between generation and ranking objectives.
  Extensive experiments on two public benchmarks and a billion-item production
corpus demonstrate that GRank improves Recall@500 by over 30% and 1.7$\times$
the P99 QPS of state-of-the-art tree- and graph-based retrievers.
  GRank has been fully deployed in production in our recommendation platform
since Q2 2025, serving 400 million active users with 99.95% service
availability. Online A/B tests confirm significant improvements in core
engagement metrics, with Total App Usage Time increasing by 0.160% in the main
app and 0.165% in the Lite version.

</details>


### [56] [Dimension Mask Layer: Optimizing Embedding Efficiency for Scalable ID-based Models](https://arxiv.org/abs/2510.15308)
*Srijan Saket,Ikuhiro Ihara,Vaibhav Sharma,Danish Kalim*

Main category: cs.IR

TL;DR: 提出了一种自动确定ID特征最优嵌入尺寸的方法，通过维度掩码层将嵌入向量维度减少40-50%，显著降低模型内存占用同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统和社交媒体平台中的大规模ID特征需要大量内存的嵌入表，导致模型庞大难以部署和维护。

Method: 定义了一个自定义Keras层——维度掩码层，位于嵌入查找之后，通过只允许前N个维度通过来修剪嵌入向量。

Result: 在公开数据集和真实生产数据集的A/B测试中，维度掩码层可将有效嵌入维度减少40-50%，显著提高内存效率。

Conclusion: 该方法为处理大量ID特征的平台提供了可扩展的解决方案，优化了资源使用和模型性能。

Abstract: In modern recommendation systems and social media platforms like Meta,
TikTok, and Instagram, large-scale ID-based features often require embedding
tables that consume significant memory. Managing these embedding sizes can be
challenging, leading to bulky models that are harder to deploy and maintain. In
this paper, we introduce a method to automatically determine the optimal
embedding size for ID features, significantly reducing the model size while
maintaining performance.
  Our approach involves defining a custom Keras layer called the dimension mask
layer, which sits directly after the embedding lookup. This layer trims the
embedding vector by allowing only the first N dimensions to pass through. By
doing this, we can reduce the input feature dimension by more than half with
minimal or no loss in model performance metrics. This reduction helps cut down
the memory footprint of the model and lowers the risk of overfitting due to
multicollinearity.
  Through offline experiments on public datasets and an online A/B test on a
real production dataset, we demonstrate that using a dimension mask layer can
shrink the effective embedding dimension by 40-50\%, leading to substantial
improvements in memory efficiency. This method provides a scalable solution for
platforms dealing with a high volume of ID features, optimizing both resource
usage and model performance.

</details>


### [57] [Fault Cause Identification across Manufacturing Lines through Ontology-Guided and Process-Aware FMEA Graph Learning with LLMs](https://arxiv.org/abs/2510.15428)
*Sho Okazaki,Kohei Kaminishi,Takuma Fujiu,Yusheng Wang,Jun Ota*

Main category: cs.IR

TL;DR: 提出了一种结合制造领域概念化和图神经网络推理的过程感知框架，通过本体引导的LLM提取将FMEA工作表转换为统一知识图，使用过程感知评分函数的RGCN学习嵌入，并通过链接预测推断故障原因，在汽车压力传感器装配线上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动化制造生产线中故障原因识别面临系统复杂性、频繁重新配置以及现有FMEA知识可重用性有限的挑战。尽管FMEA工作表包含有价值的专家见解，但由于自然语言变异性、术语不一致和流程差异，在异构生产线间的重用受到阻碍。

Method: 1. 通过本体引导的大型语言模型提取将多个制造生产线的FMEA工作表转换为统一知识图，捕获动作、状态、组件和参数等域概念；2. 使用具有过程感知评分函数的关系图卷积网络学习既尊重语义关系又尊重顺序流程的嵌入；3. 采用链接预测来推断和排序与目标生产线流程一致的候选故障原因。

Result: 在汽车压力传感器装配线的案例研究中，所提方法优于最先进的检索增强生成基线（F1@20 = 0.267）和RGCN方法（0.400），在故障原因识别方面实现了最佳性能（0.523）。消融研究证实了LLM驱动的领域概念化和过程感知学习的贡献。

Conclusion: 所提出的框架显著提高了FMEA知识在异构生产线间的可转移性，从而支持操作员更可靠地诊断故障，并为智能制造中未来领域自适应LLM应用铺平了道路。

Abstract: Fault cause identification in automated manufacturing lines is challenging
due to the system's complexity, frequent reconfigurations, and the limited
reusability of existing Failure Mode and Effects Analysis (FMEA) knowledge.
Although FMEA worksheets contain valuable expert insights, their reuse across
heterogeneous lines is hindered by natural language variability, inconsistent
terminology, and process differences. To address these limitations, this study
proposes a process-aware framework that enhances FMEA reusability by combining
manufacturing-domain conceptualization with graph neural network (GNN)
reasoning. First, FMEA worksheets from multiple manufacturing lines are
transformed into a unified knowledge graph through ontology-guided large
language model (LLM) extraction, capturing domain concepts such as actions,
states, components, and parameters. Second, a Relational Graph Convolutional
Network (RGCN) with the process-aware scoring function learns embeddings that
respect both semantic relationships and sequential process flows. Finally, link
prediction is employed to infer and rank candidate fault causes consistent with
the target line's process flow.
  A case study on automotive pressure sensor assembly lines demonstrates that
the proposed method outperforms a state-of-the-art retrieval-augmented
generation (RAG) baseline (F1@20 = 0.267) and an RGCN approach (0.400),
achieving the best performance (0.523) in fault cause identification. Ablation
studies confirm the contributions of both LLM-driven domain conceptualization
and process-aware learning. These results indicate that the proposed framework
significantly improves the transferability of FMEA knowledge across
heterogeneous lines, thereby supporting operators in diagnosing failures more
reliably and paving the way for future domain-adaptive LLM applications in
smart manufacturing.

</details>


### [58] [Enhance Large Language Models as Recommendation Systems with Collaborative Filtering](https://arxiv.org/abs/2510.15647)
*Zhisheng Yang,Xiaofei Xu,Ke Deng,Li Li*

Main category: cs.IR

TL;DR: 提出Critic-LLM-RS方法，通过训练独立的Critic模型实现协同过滤，为LLMs提供反馈来优化推荐效果，解决了非调优策略缺乏领域知识的问题。


<details>
  <summary>Details</summary>
Motivation: 现有非调优策略的LLM推荐方法避免了昂贵的模型微调过程，但缺乏任务特定的业务知识，且没有明确整合协同过滤这一成功的推荐技术。

Method: 训练独立的Critic机器学习模型实现协同过滤，学习用户与物品的交互关系，然后Critic为LLMs提供反馈来显著优化推荐结果。

Result: 在真实数据集上的广泛实验验证了Critic-LLM-RS方法的有效性。

Conclusion: Critic-LLM-RS成功填补了非调优策略LLM推荐方法中协同过滤整合的空白，通过Critic模型提供反馈有效提升了推荐质量。

Abstract: As powerful tools in Natural Language Processing (NLP), Large Language Models
(LLMs) have been leveraged for crafting recommendations to achieve precise
alignment with user preferences and elevate the quality of the recommendations.
The existing approaches implement both non-tuning and tuning strategies.
Compared to following the tuning strategy, the approaches following the
non-tuning strategy avoid the relatively costly, time-consuming, and
expertise-requiring process of further training pre-trained LLMs on
task-specific datasets, but they suffer the issue of not having the
task-specific business or local enterprise knowledge. To the best of our
knowledge, none of the existing approaches following the non-tuning strategy
explicitly integrates collaborative filtering, one of the most successful
recommendation techniques. This study aims to fill the gap by proposing
critique-based LLMs as recommendation systems (Critic-LLM-RS). For our purpose,
we train a separate machine-learning model called Critic that implements
collaborative filtering for recommendations by learning from the interactions
between many users and items. The Critic provides critiques to LLMs to
significantly refine the recommendations. Extensive experiments have verified
the effectiveness of Critic-LLM-RS on real datasets.

</details>


### [59] [SQuAI: Scientific Question-Answering with Multi-Agent Retrieval-Augmented Generation](https://arxiv.org/abs/2510.15682)
*Ines Besrour,Jingbo He,Tobias Schreieder,Michael Färber*

Main category: cs.IR

TL;DR: SQuAI是一个可扩展且可信的多智能体检索增强生成框架，用于科学问答，通过分解复杂问题、混合检索和自适应文档过滤来提高答案的准确性和可追溯性。


<details>
  <summary>Details</summary>
Motivation: 解决现有RAG系统在科学领域处理复杂开放域问题时的局限性，需要准确答案、明确引用和跨数百万科学文档的检索。

Method: 基于230万篇arXiv全文论文，使用四个协作智能体分解复杂问题为子问题，通过混合稀疏-稠密检索获取目标证据，并自适应过滤文档以提高上下文相关性。

Result: 相比强大的RAG基线，SQuAI在忠实性、答案相关性和上下文相关性方面提高了高达+0.088（12%），并发布了包含1000个科学问答证据三元组的基准数据集。

Conclusion: SQuAI通过透明推理、可验证引用和领域范围的可扩展性，展示了多智能体RAG如何使基于LLM的科学问答更加可信。

Abstract: We present SQuAI (https://squai.scads.ai/), a scalable and trustworthy
multi-agent retrieval-augmented generation (RAG) framework for scientific
question answering (QA) with large language models (LLMs). SQuAI addresses key
limitations of existing RAG systems in the scholarly domain, where complex,
open-domain questions demand accurate answers, explicit claims with citations,
and retrieval across millions of scientific documents. Built on over 2.3
million full-text papers from arXiv.org, SQuAI employs four collaborative
agents to decompose complex questions into sub-questions, retrieve targeted
evidence via hybrid sparse-dense retrieval, and adaptively filter documents to
improve contextual relevance. To ensure faithfulness and traceability, SQuAI
integrates in-line citations for each generated claim and provides supporting
sentences from the source documents. Our system improves faithfulness, answer
relevance, and contextual relevance by up to +0.088 (12%) over a strong RAG
baseline. We further release a benchmark of 1,000 scientific
question-answer-evidence triplets to support reproducibility. With transparent
reasoning, verifiable citations, and domain-wide scalability, SQuAI
demonstrates how multi-agent RAG enables more trustworthy scientific QA with
LLMs.

</details>


### [60] [Mixture of Experts Approaches in Dense Retrieval Tasks](https://arxiv.org/abs/2510.15683)
*Effrosyni Sokli,Pranav Kasela,Georgios Peikos,Gabriella Pasi*

Main category: cs.IR

TL;DR: 提出了一种更高效的MoE设计——单MoE块（SB-MoE），放置在最终Transformer层之后，相比传统MoE方法显著减少了参数数量，在轻量级DRM模型上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统基于Transformer的密集检索模型（DRMs）泛化能力有限，而现有在每层Transformer中集成MoE框架的方法虽然有效但参数数量大幅增加。

Method: 在最终Transformer层后引入单个MoE块（SB-MoE），通过两种评估设置：1）在七个IR基准上微调四种不同DRM；2）在MSMARCO上微调并在13个BEIR数据集上进行零样本评估。

Result: SB-MoE特别适用于轻量级基础模型（如TinyBERT、BERT-Small），在基准测试中持续超越标准微调。对于参数更多的模型（如BERT-Base、Contriever），需要更多训练样本才能获得改进的检索性能。

Conclusion: SB-MoE是一种参数效率更高的MoE设计，在保持检索效果的同时显著减少了参数开销，尤其适合轻量级DRM模型。

Abstract: Dense Retrieval Models (DRMs) are a prominent development in Information
Retrieval (IR). A key challenge with these neural Transformer-based models is
that they often struggle to generalize beyond the specific tasks and domains
they were trained on. To address this challenge, prior research in IR
incorporated the Mixture-of-Experts (MoE) framework within each Transformer
layer of a DRM, which, though effective, substantially increased the number of
additional parameters. In this paper, we propose a more efficient design, which
introduces a single MoE block (SB-MoE) after the final Transformer layer. To
assess the retrieval effectiveness of SB-MoE, we perform an empirical
evaluation across three IR tasks. Our experiments involve two evaluation
setups, aiming to assess both in-domain effectiveness and the model's zero-shot
generalizability. In the first setup, we fine-tune SB-MoE with four different
underlying DRMs on seven IR benchmarks and evaluate them on their respective
test sets. In the second setup, we fine-tune SB-MoE on MSMARCO and perform
zero-shot evaluation on thirteen BEIR datasets. Additionally, we perform
further experiments to analyze the model's dependency on its hyperparameters
(i.e., the number of employed and activated experts) and investigate how this
variation affects SB-MoE's performance. The obtained results show that SB-MoE
is particularly effective for DRMs with lightweight base models, such as
TinyBERT and BERT-Small, consistently exceeding standard model fine-tuning
across benchmarks. For DRMs with more parameters, such as BERT-Base and
Contriever, our model requires a larger number of training samples to achieve
improved retrieval performance. Our code is available online at:
https://github.com/FaySokli/SB-MoE.

</details>


### [61] [GraphMind: Interactive Novelty Assessment System for Accelerating Scientific Discovery](https://arxiv.org/abs/2510.15706)
*Italo Luis da Silva,Hanqi Yan,Lin Gui,Yulan He*

Main category: cs.IR

TL;DR: GraphMind是一个交互式网页工具，通过整合arXiv和Semantic Scholar等外部API与LLMs，帮助用户评估科学论文的新颖性，提供可验证的上下文洞察。


<details>
  <summary>Details</summary>
Motivation: 现有LLM辅助科学文献分析方法透明度有限，缺乏通过信息检索模块实现结果可追溯性的机制，无法满足同行评审中对科学论文新颖性评估的需求。

Method: 开发GraphMind工具，整合外部API与LLMs，支持论文的注释、提取、检索和分类，使用户能够捕捉论文主要结构，从多个角度探索相关想法。

Result: GraphMind提供了一个易于使用的交互式网页工具，用户可以通过注释论文关键元素、探索相关论文的各种关系，以及提供上下文洞察来评估新颖性。

Conclusion: GraphMind通过结合外部API和LLMs，为用户提供了科学论文核心贡献及其与现有工作联系的丰富结构化视图，解决了现有方法在透明度和可追溯性方面的不足。

Abstract: Large Language Models (LLMs) show strong reasoning and text generation
capabilities, prompting their use in scientific literature analysis, including
novelty assessment. While evaluating novelty of scientific papers is crucial
for peer review, it requires extensive knowledge of related work, something not
all reviewers have. While recent work on LLM-assisted scientific literature
analysis supports literature comparison, existing approaches offer limited
transparency and lack mechanisms for result traceability via an information
retrieval module. To address this gap, we introduce $\textbf{GraphMind}$, an
easy-to-use interactive web tool designed to assist users in evaluating the
novelty of scientific papers or drafted ideas. Specially, $\textbf{GraphMind}$
enables users to capture the main structure of a scientific paper, explore
related ideas through various perspectives, and assess novelty via providing
verifiable contextual insights. $\textbf{GraphMind}$ enables users to annotate
key elements of a paper, explore related papers through various relationships,
and assess novelty with contextual insight. This tool integrates external APIs
such as arXiv and Semantic Scholar with LLMs to support annotation, extraction,
retrieval and classification of papers. This combination provides users with a
rich, structured view of a scientific idea's core contributions and its
connections to existing work. $\textbf{GraphMind}$ is available at
https://oyarsa.github.io/graphmind and a demonstration video at
https://youtu.be/wKbjQpSvwJg. The source code is available at
https://github.com/oyarsa/graphmind.

</details>


### [62] [The 3rd Place Solution of CCIR CUP 2025: A Framework for Retrieval-Augmented Generation in Multi-Turn Legal Conversation](https://arxiv.org/abs/2510.15722)
*Da Li,Zecheng Fang,Qiang Yan,Wei Huang,Xuanpu Luo*

Main category: cs.IR

TL;DR: 本文介绍了在CCIR CUP 2025中提出的法律知识检索与生成方法，利用大型语言模型和信息检索系统，基于法律条文对用户问题生成相关回答。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成在自然语言处理领域取得了显著进展，但在法律领域的应用仍处于探索阶段，需要专门的方法来处理法律知识检索与生成。

Method: 结合大型语言模型和信息检索系统的优势，从可靠来源检索相关法律条文，并基于这些检索结果生成上下文适当的回答。

Result: 该方法能够基于法律条文为用户问题生成相关且上下文适当的回答，在法律知识检索与生成任务中表现出色。

Conclusion: 提出的法律知识检索与生成方法为RAG技术在法律领域的应用提供了有效解决方案，在CCIR CUP 2025中展示了良好的性能。

Abstract: Retrieval-Augmented Generation has made significant progress in the field of
natural language processing. By combining the advantages of information
retrieval and large language models, RAG can generate relevant and contextually
appropriate responses based on items retrieved from reliable sources. This
technology has demonstrated outstanding performance across multiple domains,
but its application in the legal field remains in its exploratory phase. In
this paper, we introduce our approach for "Legal Knowledge Retrieval and
Generation" in CCIR CUP 2025, which leverages large language models and
information retrieval systems to provide responses based on laws in response to
user questions.

</details>


### [63] [FACE: A General Framework for Mapping Collaborative Filtering Embeddings into LLM Tokens](https://arxiv.org/abs/2510.15729)
*Chao Wang,Yixin Song,Jinhui Ye,Chuan Qin,Dazhong Shen,Lingfeng Liu,Xiang Wang,Yanyong Zhang*

Main category: cs.IR

TL;DR: FACE是一个将协同过滤嵌入映射到预训练LLM令牌的可解释框架，通过解耦投影和量化自编码器实现语义对齐，无需微调LLM即可提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型难以解释协同过滤方法产生的潜在非语义嵌入，这限制了推荐系统的效果和进一步应用。

Method: 提出FACE框架：1）解耦投影模块将CF嵌入分解为概念特定向量；2）量化自编码器将连续嵌入转换为LLM令牌；3）对比对齐目标确保令牌与文本信号对齐。

Result: 在三个真实世界推荐数据集上的实证结果显示基准模型性能提升，可解释性研究证实了描述符的可解释性。

Conclusion: FACE框架实现了语义对齐而无需微调LLM，通过利用其预训练能力增强了推荐性能，同时保持可解释性。

Abstract: Recently, large language models (LLMs) have been explored for integration
with collaborative filtering (CF)-based recommendation systems, which are
crucial for personalizing user experiences. However, a key challenge is that
LLMs struggle to interpret the latent, non-semantic embeddings produced by CF
approaches, limiting recommendation effectiveness and further applications. To
address this, we propose FACE, a general interpretable framework that maps CF
embeddings into pre-trained LLM tokens. Specifically, we introduce a
disentangled projection module to decompose CF embeddings into concept-specific
vectors, followed by a quantized autoencoder to convert continuous embeddings
into LLM tokens (descriptors). Then, we design a contrastive alignment
objective to ensure that the tokens align with corresponding textual signals.
Hence, the model-agnostic FACE framework achieves semantic alignment without
fine-tuning LLMs and enhances recommendation performance by leveraging their
pre-trained capabilities. Empirical results on three real-world recommendation
datasets demonstrate performance improvements in benchmark models, with
interpretability studies confirming the interpretability of the descriptors.
Code is available in https://github.com/YixinRoll/FACE.

</details>
