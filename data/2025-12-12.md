<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 28]
- [cs.IR](#cs.IR) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [What Kind of Reasoning (if any) is an LLM actually doing? On the Stochastic Nature and Abductive Appearance of Large Language Models](https://arxiv.org/abs/2512.10080)
*Luciano Floridi,Jessica Morley,Claudio Novelli,David Watson*

Main category: cs.CL

TL;DR: 该文认为当前基于token-completion的LLMs并非真正进行溯因推理，而是通过统计模式模仿人类推理文本的表面特征，缺乏对真理、语义和验证的实质性理解。


<details>
  <summary>Details</summary>
Motivation: 探讨当前大型语言模型的推理本质，澄清它们是否真正具备溯因推理能力，揭示其基于统计模式而非真实推理的局限性，以指导对LLMs的正确评估和应用。

Method: 通过分析LLMs的token-completion机制和随机性本质，对比人类溯因推理的特征，使用具体案例展示LLMs如何产生看似合理但缺乏真实推理基础的回答。

Result: LLMs产生的"推理"输出只是训练数据中人类推理结构的统计模仿，它们不能识别真理、验证解释或理解语义，只能辅助人类思维但不能替代真正的推理。

Conclusion: LLMs具有随机性基础但表现类似溯因推理的双重特性，这要求在使用时必须批判性评估其输出，承认它们不能进行真实推理的局限性，同时探讨了五个反对观点并指出分析的限制。

Abstract: This article looks at how reasoning works in current Large Language Models (LLMs) that function using the token-completion method. It examines their stochastic nature and their similarity to human abductive reasoning. The argument is that these LLMs create text based on learned patterns rather than performing actual abductive reasoning. When their output seems abductive, this is largely because they are trained on human-generated texts that include reasoning structures. Examples are used to show how LLMs can produce plausible ideas, mimic commonsense reasoning, and give explanatory answers without being grounded in truth, semantics, verification, or understanding, and without performing any real abductive reasoning. This dual nature, where the models have a stochastic base but appear abductive in use, has important consequences for how LLMs are evaluated and applied. They can assist with generating ideas and supporting human thinking, but their outputs must be critically assessed because they cannot identify truth or verify their explanations. The article concludes by addressing five objections to these points, noting some limitations in the analysis, and offering an overall evaluation.

</details>


### [2] [Generate-Then-Validate: A Novel Question Generation Approach Using Small Language Models](https://arxiv.org/abs/2512.10110)
*Yumou Wei,John Stamper,Paulo F. Carvalho*

Main category: cs.CL

TL;DR: SLMs可以通过精心设计的生成-验证流程自动生成高质量问题，作为LLMs在生成式学习分析中的补充。


<details>
  <summary>Details</summary>
Motivation: 探索小型语言模型在自动问题生成中的应用，作为当前主要依赖大型语言模型的学习分析研究的补充方案。

Method: 提出基于SLMs的"生成-验证"流程：先进行扩展性生成大量候选问题，然后通过基于概率推理的选择性验证进行精炼。

Result: 通过人类专家和LLM评估，大多数评审者认为生成的问题答案清晰且与学习目标一致，表明SLMs能够生成高质量问题。

Conclusion: 精心设计的流程可以充分发挥SLMs的优势，使其能够有效生成高质量问题，为学习分析提供了一种资源效率更高的替代方案。

Abstract: We explore the use of small language models (SLMs) for automatic question generation as a complement to the prevalent use of their large counterparts in learning analytics research. We present a novel question generation pipeline that leverages both the text generation and the probabilistic reasoning abilities of SLMs to generate high-quality questions. Adopting a "generate-then-validate" strategy, our pipeline first performs expansive generation to create an abundance of candidate questions and refine them through selective validation based on novel probabilistic reasoning. We conducted two evaluation studies, one with seven human experts and the other with a large language model (LLM), to assess the quality of the generated questions. Most judges (humans or LLMs) agreed that the generated questions had clear answers and generally aligned well with the intended learning objectives. Our findings suggest that an SLM can effectively generate high-quality questions when guided by a well-designed pipeline that leverages its strengths.

</details>


### [3] [Workflow is All You Need: Escaping the "Statistical Smoothing Trap" via High-Entropy Information Foraging and Adversarial Pacing](https://arxiv.org/abs/2512.10121)
*Zhongjie Jiang*

Main category: cs.CL

TL;DR: DeepNews框架通过模拟金融记者的认知过程，采用三重粒度检索、模式引导战略规划和对抗约束提示，解决了长文本生成中的"不可能三角"问题，显著提升了生成内容的真实性、逻辑连贯性和个性化表达。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在垂直领域长文本生成中存在"不可能三角"瓶颈：难以同时实现低幻觉、深度逻辑连贯性和个性化表达。这源于现有生成范式陷入了"统计平滑陷阱"，忽视了专家写作所需的高熵信息获取和结构化认知过程。

Method: 提出DeepNews框架，模拟资深金融记者的隐含认知过程，包含三个核心模块：1)基于信息觅食理论的双重粒度检索机制，强制执行10:1的饱和信息输入比例；2)模式引导的战略规划，利用领域专家知识库（叙事模式）和原子块构建逻辑骨架；3)对抗约束提示，使用节奏中断和逻辑迷雾等技术打破模型生成文本的概率平滑性。

Result: 实验揭示了深度金融报道中的"知识悬崖"现象：当检索上下文低于15,000字符时，内容真实性崩溃；而超过30,000字符的高冗余输入可将幻觉免费率稳定在85%以上。在与中国顶级科技媒体的生态效度盲测中，基于旧一代模型的DeepNews系统获得了25%的投稿接受率，显著优于SOTA模型GPT-5的0%接受率。

Conclusion: DeepNews框架通过显式建模专家认知过程，成功解决了长文本生成中的"不可能三角"问题，证明了基于认知过程建模的方法在垂直领域专业内容生成中的有效性，为专业写作自动化提供了新范式。

Abstract: Central to long-form text generation in vertical domains is the "impossible trinity" confronting current large language models (LLMs): the simultaneous achievement of low hallucination, deep logical coherence, and personalized expression. This study establishes that this bottleneck arises from existing generative paradigms succumbing to the Statistical Smoothing Trap, a phenomenon that overlooks the high-entropy information acquisition and structured cognitive processes integral to expert-level writing. To address this limitation, we propose the DeepNews Framework, an agentic workflow that explicitly models the implicit cognitive processes of seasoned financial journalists. The framework integrates three core modules: first, a dual-granularity retrieval mechanism grounded in information foraging theory, which enforces a 10:1 saturated information input ratio to mitigate hallucinatory outputs; second, schema-guided strategic planning, a process leveraging domain expert knowledge bases (narrative schemas) and Atomic Blocks to forge a robust logical skeleton; third, adversarial constraint prompting, a technique deploying tactics including Rhythm Break and Logic Fog to disrupt the probabilistic smoothness inherent in model-generated text. Experiments delineate a salient Knowledge Cliff in deep financial reporting: content truthfulness collapses when retrieved context falls below 15,000 characters, while a high-redundancy input exceeding 30,000 characters stabilizes the Hallucination-Free Rate (HFR) above 85%. In an ecological validity blind test conducted with a top-tier Chinese technology media outlet, the DeepNews system--built on a previous-generation model (DeepSeek-V3-0324)-achieved a 25% submission acceptance rate, significantly outperforming the 0% acceptance rate of zero-shot generation by a state-of-the-art (SOTA) model (GPT-5).

</details>


### [4] [PARAN: Persona-Augmented Review ANswering system on Food Delivery Review Dataset](https://arxiv.org/abs/2512.10148)
*Moonsoo Park,Jeongseok Yun,Bohyung Kim*

Main category: cs.CL

TL;DR: 提出了一个两阶段提示框架，通过从简短评论中推断显式和隐式用户画像，提升个性化回复生成质量，无需微调模型。


<details>
  <summary>Details</summary>
Motivation: 在用户信息有限（如外卖平台）的场景下，个性化回复生成面临挑战。大语言模型缺乏上下文用户数据时往往产生通用回复，降低了参与度和有效性。

Method: 提出了一个两阶段提示框架：第一阶段从短评论文本中推断显式（如用户声明的偏好）和隐式（如人口统计或风格线索）用户画像；第二阶段将这些推断出的画像属性整合到回复生成提示中。在推理时调整解码温度以鼓励多样且忠实的生成。

Result: 在韩国外卖应用的真实数据集上评估了方法，结果显示该方法在精度、多样性和语义一致性方面表现良好，证明了画像增强提示在提升自动回复相关性和个性化方面的有效性。

Conclusion: 画像增强提示框架有效提升了自动化回复的相关性和个性化，且无需模型微调，为有限用户信息场景下的个性化回复生成提供了实用解决方案。

Abstract: Personalized review response generation presents a significant challenge in domains where user information is limited, such as food delivery platforms. While large language models (LLMs) offer powerful text generation capabilities, they often produce generic responses when lacking contextual user data, reducing engagement and effectiveness. In this work, we propose a two-stage prompting framework that infers both explicit (e.g., user-stated preferences) and implicit (e.g., demographic or stylistic cues) personas directly from short review texts. These inferred persona attributes are then incorporated into the response generation prompt to produce user-tailored replies. To encourage diverse yet faithful generations, we adjust decoding temperature during inference. We evaluate our method using a real-world dataset collected from a Korean food delivery app, and assess its impact on precision, diversity, and semantic consistency. Our findings highlight the effectiveness of persona-augmented prompting in enhancing the relevance and personalization of automated responses without requiring model fine-tuning.

</details>


### [5] [Unforgotten Safety: Preserving Safety Alignment of Large Language Models with Continual Learning](https://arxiv.org/abs/2512.10150)
*Lama Alssum,Hani Itani,Hasan Abed Al Kader Hammoud,Philip Torr,Adel Bibi,Bernard Ghanem*

Main category: cs.CL

TL;DR: 研究LLM微调过程中的安全退化问题，将安全保护视为持续学习问题，发现持续学习方法能有效降低攻击成功率，其中DER方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的普及，安全对齐变得越来越重要。研究发现LLM在适应新任务时会出现安全退化，这主要归因于灾难性遗忘。在微调即服务的场景下，用户上传数据到服务提供商以获得定制模型，需要解决微调过程中的安全问题。

Method: 将微调过程中的安全保护视为持续学习问题，采用三种持续学习方法：基于正则化的方法、基于记忆的方法和模型融合方法。在两种场景下进行评估：良性用户数据和中毒用户数据。实验覆盖三个下游任务（GSM8K、SST2、Code）和三个模型家族（LLaMA2-7B、Mistral-7B、Gemma-2B）。

Result: 持续学习方法相比标准微调始终能获得更低的攻击成功率。其中DER方法不仅优于其他持续学习方法，也超越了现有的安全保护基线方法，同时保持了任务效用。这些发现在不同任务和模型家族中都具有普遍性。

Conclusion: 持续学习是保护大语言模型微调过程中安全性的实用解决方案，能够有效缓解灾难性遗忘导致的安全退化问题。

Abstract: The safety alignment of large language models (LLMs) is becoming increasingly important with their democratization. In this paper, we study the safety degradation that comes with adapting LLMs to new tasks. We attribute this safety compromise to catastrophic forgetting and frame the problem of preserving safety when fine-tuning as a continual learning (CL) problem. We consider the fine-tuning-as-a-service setup where the user uploads their data to a service provider to get a customized model that excels on the user's selected task. We adapt several CL approaches from the literature and systematically evaluate their ability to mitigate safety degradation. These include regularization-based, memory-based, and model merging approaches. We consider two scenarios, (1) benign user data and (2) poisoned user data. Our results demonstrate that CL approaches consistently achieve lower attack success rates than standard fine-tuning. Among these, DER outperforms both other CL methods and existing safety-preserving baselines while maintaining task utility. These findings generalize across three downstream tasks (GSM8K, SST2, Code) and three model families (LLaMA2-7B, Mistral-7B, Gemma-2B), establishing CL as a practical solution to preserve safety.

</details>


### [6] [AutoMedic: An Automated Evaluation Framework for Clinical Conversational Agents with Medical Dataset Grounding](https://arxiv.org/abs/2512.10195)
*Gyutaek Oh,Sangjoon Park,Byung-Hoon Kim*

Main category: cs.CL

TL;DR: AutoMedic是一个多智能体模拟框架，用于自动评估大语言模型作为临床对话代理的性能，通过将静态QA数据集转换为虚拟患者档案，实现真实的多轮临床对话评估。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对LLMs在动态、交互式临床多轮对话情境中有效性的评估，以及超越简单准确性的多方面评估策略。动态临床情境的正式评估面临巨大组合空间挑战，难以标准化和量化测量。

Method: 开发AutoMedic多智能体模拟框架，将现成的静态QA数据集转换为虚拟患者档案，使LLM代理之间能够进行真实且临床基础的多轮临床对话。使用CARE指标（临床对话准确性、效率/策略、同理心和鲁棒性）评估临床对话代理性能。

Result: AutoMedic被证明是临床对话代理的有效自动化评估框架，经人类专家验证有效，为LLMs在对话式医疗应用中的有效开发提供实用指导。

Conclusion: AutoMedic解决了临床对话评估的标准化和量化难题，通过多智能体模拟和CARE指标实现了对LLMs在医疗对话中性能的全面评估，推动了LLMs在医疗领域的可信应用。

Abstract: Evaluating large language models (LLMs) has recently emerged as a critical issue for safe and trustworthy application of LLMs in the medical domain. Although a variety of static medical question-answering (QA) benchmarks have been proposed, many aspects remain underexplored, such as the effectiveness of LLMs in generating responses in dynamic, interactive clinical multi-turn conversation situations and the identification of multi-faceted evaluation strategies beyond simple accuracy. However, formally evaluating a dynamic, interactive clinical situation is hindered by its vast combinatorial space of possible patient states and interaction trajectories, making it difficult to standardize and quantitatively measure such scenarios. Here, we introduce AutoMedic, a multi-agent simulation framework that enables automated evaluation of LLMs as clinical conversational agents. AutoMedic transforms off-the-shelf static QA datasets into virtual patient profiles, enabling realistic and clinically grounded multi-turn clinical dialogues between LLM agents. The performance of various clinical conversational agents is then assessed based on our CARE metric, which provides a multi-faceted evaluation standard of clinical conversational accuracy, efficiency/strategy, empathy, and robustness. Our findings, validated by human experts, demonstrate the validity of AutoMedic as an automated evaluation framework for clinical conversational agents, offering practical guidelines for the effective development of LLMs in conversational medical applications.

</details>


### [7] [Multilingual VLM Training: Adapting an English-Trained VLM to French](https://arxiv.org/abs/2512.10336)
*Jules Lahmi,Alexis Roger*

Main category: cs.CL

TL;DR: 本文探讨了将英语训练的视觉语言模型适配到其他语言的方法，比较了翻译流水线、LoRA微调和两阶段微调策略，发现数据集翻译是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型主要在英语上取得进展，限制了非英语用户的可访问性，需要扩展这些能力到更广泛的语言。

Method: 比较了三种方法：基于翻译的流水线、LoRA微调、以及分离视觉适配和语言适配的两阶段微调策略。

Result: 数据集翻译是多语言VLM性能的主要瓶颈，数据质量限制了训练和评估的有效性。

Conclusion: 未来工作应专注于原生语言数据集的收集和改进翻译策略。

Abstract: Artificial intelligence has made great progress in recent years, particularly in the development of Vision--Language Models (VLMs) that understand both visual and textual data. However, these advancements remain largely limited to English, reducing their accessibility for non--English speakers. It is essential to extend these capabilities to a broader range of languages. This paper explores the challenges of adapting an English-trained VLM to different languages. To this end, we will explore and compare different methods for their performance and computational cost. We consider a translation-based pipeline, LoRA finetuning, and a two-stage finetuning strategy that separates vision adaptation from language adaptation. To evaluate these methods, we use a combination of standard multimodal benchmarks translated into the target language and manual assessments by native experts. The results reveal that dataset translation remains a major bottleneck in multilingual VLM performance, with data quality limiting the effectiveness of training and evaluation. These findings suggest that future efforts should focus on native-language dataset collection and improved translation strategies.

</details>


### [8] [Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale](https://arxiv.org/abs/2512.10398)
*Zhaodong Wang,Zhenting Qi,Sherman Wong,Nathan Hu,Samuel Lin,Jun Ge,Erwin Gao,Yining Yang,Ben Maurer,Wenlin Chen,David Recordon,Yilun Du,Minlan Yu,Ying Zhang*

Main category: cs.CL

TL;DR: Confucius Code Agent (CCA) 是一个开源AI软件工程师，基于Confucius SDK构建，具有分层工作记忆、持久笔记系统和模块化工具扩展，在SWE-Bench-Pro上达到54.3%的Resolve@1性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界的AI软件工程需要能够处理大规模代码库、保持持久记忆、协调复杂工具链的编码智能体。现有开源编码智能体在工业规模任务上表现不足，而专有智能体则缺乏可扩展性、可解释性和可控性。

Method: 基于Confucius SDK构建，包含：统一编排器与分层工作记忆支持长上下文推理；持久笔记系统实现跨会话持续学习；模块化扩展模块支持鲁棒工具使用；元智能体通过构建-测试-改进循环自动化配置合成、评估和优化。

Result: 在SWE-Bench-Pro上达到54.3%的Resolve@1性能，显著优于之前的编码智能体，创造了新的最先进水平。

Conclusion: Confucius SDK和CCA提供了一个透明、可扩展、可复现的AI智能体基础，弥合了研究原型与生产级系统之间的差距，支持工业规模的智能体开发和部署。

Abstract: Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.

</details>


### [9] [Sliding Window Attention Adaptation](https://arxiv.org/abs/2512.10411)
*Yijiong Yu,Jiale Liu,Qingyun Wu,Huazheng Wang,Ji Pei*

Main category: cs.CL

TL;DR: 提出SWAA方法，通过五种策略的组合适配，让全注意力预训练的LLM能够有效使用滑动窗口注意力，在保持长上下文性能的同时降低推理成本。


<details>
  <summary>Details</summary>
Motivation: Transformer自注意力机制在输入长度上的二次方复杂度使得长上下文推理成本高昂。滑动窗口注意力将复杂度降至线性，但将全注意力预训练的LLM直接用于滑动窗口注意力会导致严重的训练-推理不匹配和性能下降。需要研究能否在不重新预训练的情况下适配全注意力预训练模型到滑动窗口注意力。

Method: 提出滑动窗口注意力适配（SWAA）方法，包含五种策略的组合：(1)仅在预填充阶段使用SWA；(2)保留"sink"令牌；(3)交错FA/SWA层；(4)思维链；(5)微调。通过实验探索这些方法的协同组合效果。

Result: 实验表明SWA适配是可行但非平凡的：没有单一方法足够，但特定的协同组合能有效恢复原始长上下文性能。分析了不同SWAA配置的性能-效率权衡，并为不同场景提供了推荐方案。

Conclusion: SWAA方法成功实现了全注意力预训练LLM到滑动窗口注意力的有效适配，在保持性能的同时显著降低推理成本，为实际应用提供了实用的解决方案。

Abstract: The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe long-context performance degradation due to training-inference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), a set of practical recipes that combine five methods for better adaptation: (1) applying SWA only during prefilling; (2) preserving "sink" tokens; (3) interleaving FA/SWA layers; (4) chain-of-thought (CoT); and (5) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation

</details>


### [10] [Cooperative Retrieval-Augmented Generation for Question Answering: Mutual Information Exchange and Ranking by Contrasting Layers](https://arxiv.org/abs/2512.10422)
*Youmin Ko,Sungjong Seo,Hyunjoon Kim*

Main category: cs.CL

TL;DR: CoopRAG是一个新颖的检索增强生成框架，通过让检索器和LLM协同工作，以及检索器模型的早期和后期层相互协作，来提高多跳问答和简单问答的准确性和减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成方法在简单和多跳问答中仍然容易出现错误检索和幻觉问题，需要更有效的框架来缓解这些问题。

Method: 1) 将问题分解为子问题和推理链，并对不确定位置进行掩码；2) 使用增强后的查询检索相关文档；3) 通过对比检索器不同层来重新排序文档；4) 利用LLM填充掩码位置重建推理链。

Result: CoopRAG在三个多跳问答数据集和一个简单问答数据集上，在检索和问答性能方面均优于现有最先进的方法。

Conclusion: CoopRAG通过检索器和LLM之间的协同合作，以及检索器内部层的协同工作，有效提高了问答系统的准确性和可靠性。

Abstract: Since large language models (LLMs) have a tendency to generate factually inaccurate output, retrieval-augmented generation (RAG) has gained significant attention as a key means to mitigate this downside of harnessing only LLMs. However, existing RAG methods for simple and multi-hop question answering (QA) are still prone to incorrect retrievals and hallucinations. To address these limitations, we propose CoopRAG, a novel RAG framework for the question answering task in which a retriever and an LLM work cooperatively with each other by exchanging informative knowledge, and the earlier and later layers of the retriever model work cooperatively with each other to accurately rank the retrieved documents relevant to a given query. In this framework, we (i) unroll a question into sub-questions and a reasoning chain in which uncertain positions are masked, (ii) retrieve the documents relevant to the question augmented with the sub-questions and the reasoning chain, (iii) rerank the documents by contrasting layers of the retriever, and (iv) reconstruct the reasoning chain by filling the masked positions via the LLM. Our experiments demonstrate that CoopRAG consistently outperforms state-of-the-art QA methods on three multi-hop QA datasets as well as a simple QA dataset in terms of both the retrieval and QA performances. Our code is available.\footnote{https://github.com/meaningful96/CoopRAG}

</details>


### [11] [T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground](https://arxiv.org/abs/2512.10430)
*Dmitrii Stoianov,Danil Taranets,Olga Tsymboi,Ramil Latypov,Almaz Dautov,Vladislav Kruglikov,Nikita Surkov,German Abramov,Pavel Gein,Dmitry Abulkhanov,Mikhail Gashkov,Viktor Zelenkovskiy,Artem Batalov,Aleksandr Medvedev,Anatolii Potapov*

Main category: cs.CL

TL;DR: T-pro 2.0是一个开源的俄语大语言模型，支持混合推理和高效推理，提供模型权重、指令数据集、推理基准和推理优化工具。


<details>
  <summary>Details</summary>
Motivation: 为俄语语言推理研究提供可复现和可扩展的开源系统，支持高效实用的俄语LLM应用开发和评估。

Method: 使用西里尔密集分词器，采用适配的EAGLE推测解码流水线降低延迟，支持直接回答和推理轨迹生成。

Result: 发布了完整的开源资源：模型权重、T-Wix 50万指令语料库、T-Math推理基准和EAGLE权重，并提供了公开网页演示展示推理加速效果。

Conclusion: T-pro 2.0作为一个可访问的开源系统，为构建和评估高效实用的俄语LLM应用提供了基础。

Abstract: We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.

</details>


### [12] [Semantic Reconstruction of Adversarial Plagiarism: A Context-Aware Framework for Detecting and Restoring "Tortured Phrases" in Scientific Literature](https://arxiv.org/abs/2512.10435)
*Agniva Maiti,Prajwal Panth,Suresh Chandra Satapathy*

Main category: cs.CL

TL;DR: 提出SRAP框架，通过两阶段架构检测对抗性抄袭并数学重建原始术语，显著提升术语恢复准确率


<details>
  <summary>Details</summary>
Motivation: 科学文献完整性面临对抗性文本生成技术的严重威胁，特别是使用自动改写工具掩盖抄袭，现有检测方法依赖静态黑名单或通用领域语言模型，对新型混淆检测效果差且无法确定抄袭来源

Method: 采用两阶段架构：1) 使用领域特定的掩码语言模型(SciBERT)进行统计异常检测，基于词级伪困惑度；2) 使用密集向量检索(FAISS)和句子级对齐(SBERT)进行基于来源的语义重建

Result: 在对抗性科学文本平行语料库实验中，零样本基线完全失败(0.00%恢复准确率)，而检索增强方法达到23.67%恢复准确率，显著优于基线方法；静态决策边界在专业术语密集的科学文本中更稳健

Conclusion: SRAP框架不仅能检测对抗性抄袭，还能通过数学方法恢复原始术语，将混淆表达链接回最可能的源文档，实现法证分析

Abstract: The integrity and reliability of scientific literature is facing a serious threat by adversarial text generation techniques, specifically from the use of automated paraphrasing tools to mask plagiarism. These tools generate "tortured phrases", statistically improbable synonyms (e.g. "counterfeit consciousness" for "artificial intelligence"), that preserve the local grammar while obscuring the original source. Most existing detection methods depend heavily on static blocklists or general-domain language models, which suffer from high false-negative rates for novel obfuscations and cannot determine the source of the plagiarized content. In this paper, we propose Semantic Reconstruction of Adversarial Plagiarism (SRAP), a framework designed not only to detect these anomalies but to mathematically recover the original terminology. We use a two-stage architecture: (1) statistical anomaly detection with a domain-specific masked language model (SciBERT) using token-level pseudo-perplexity, and (2) source-based semantic reconstruction using dense vector retrieval (FAISS) and sentence-level alignment (SBERT). Experiments on a parallel corpus of adversarial scientific text show that while zero-shot baselines fail completely (0.00 percent restoration accuracy), our retrieval-augmented approach achieves 23.67 percent restoration accuracy, significantly outperforming baseline methods. We also show that static decision boundaries are necessary for robust detection in jargon-heavy scientific text, since dynamic thresholding fails under high variance. SRAP enables forensic analysis by linking obfuscated expressions back to their most probable source documents.

</details>


### [13] [Enhancing Next-Generation Language Models with Knowledge Graphs: Extending Claude, Mistral IA, and GPT-4 via KG-BERT](https://arxiv.org/abs/2512.10440)
*Nour El Houda Ben Chaabene,Hamza Hammami*

Main category: cs.CL

TL;DR: 通过集成知识图谱增强大语言模型的事实一致性和推理能力


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽然擅长自然语言处理，但缺乏结构化知识，导致事实不一致问题

Method: 使用KG-BERT将知识图谱集成到大语言模型中，增强模型的grounding和推理能力

Result: 实验显示在知识密集型任务（如问答和实体链接）上取得显著提升

Conclusion: 该方法提高了事实可靠性，使大语言模型更具上下文感知能力

Abstract: Large language models (LLMs) like Claude, Mistral IA, and GPT-4 excel in NLP but lack structured knowledge, leading to factual inconsistencies. We address this by integrating Knowledge Graphs (KGs) via KG-BERT to enhance grounding and reasoning. Experiments show significant gains in knowledge-intensive tasks such as question answering and entity linking. This approach improves factual reliability and enables more context-aware next-generation LLMs.

</details>


### [14] [Decoding Student Minds: Leveraging Conversational Agents for Psychological and Learning Analysis](https://arxiv.org/abs/2512.10441)
*Nour El Houda Ben Chaabene,Hamza Hammami,Laid Kahloul*

Main category: cs.CL

TL;DR: 本文提出了一种心理感知的对话代理，通过结合LLM、KG-BERT和双向LSTM+注意力机制，实时分类学生的认知和情感状态，提升学习表现和情感健康。


<details>
  <summary>Details</summary>
Motivation: 现有教育对话系统通常只专注于辅导或情感支持中的单一功能，缺乏对认知和情感状态的综合分析能力。教育环境中需要能同时关注学习表现和情感健康的智能系统。

Method: 系统结合了大型语言模型（LLMs）、知识图谱增强的BERT（KG-BERT）以及带注意力的双向长短期记忆网络（LSTM）。通过多模态数据（文本语义、语音韵律特征、时序行为趋势）实时推断学生的参与度、压力和概念理解水平。

Result: 针对大学生的试点研究表明，相比基线方法，该系统提高了学习动机、降低了压力水平，并取得了适度的学业成绩提升。

Conclusion: 结果表明，通过整合语义推理、多模态融合和时序建模，能够实现自适应、以学生为中心的教育干预，在教育环境中具有重要应用前景。

Abstract: This paper presents a psychologically-aware conversational agent designed to enhance both learning performance and emotional well-being in educational settings. The system combines Large Language Models (LLMs), a knowledge graph-enhanced BERT (KG-BERT), and a bidirectional Long Short-Term Memory (LSTM) with attention to classify students' cognitive and affective states in real time. Unlike prior chatbots limited to either tutoring or affective support, our approach leverages multimodal data-including textual semantics, prosodic speech features, and temporal behavioral trends-to infer engagement, stress, and conceptual understanding. A pilot study with university students demonstrated improved motivation, reduced stress, and moderate academic gains compared to baseline methods. These results underline the promise of integrating semantic reasoning, multimodal fusion, and temporal modeling to support adaptive, student-centered educational interventions.

</details>


### [15] [Grammaticality Judgments in Humans and Language Models: Revisiting Generative Grammar with LLMs](https://arxiv.org/abs/2512.10453)
*Lars G. B. Johnsen*

Main category: cs.CL

TL;DR: 大型语言模型仅通过表层形式训练，就能表现出对句法结构的敏感性，能够可靠地区分语法正确和不正确的句子变体。


<details>
  <summary>Details</summary>
Motivation: 传统生成语法将语法性系统对比作为内部层次语法的证据，本研究旨在测试仅通过表层形式训练的大型语言模型是否能重现这些对比，从而暗示其具有底层结构表征。

Method: 聚焦两个经典结构：主语-助动词倒装（测试主语边界识别）和寄生空位许可（测试抽象依赖结构）。使用GPT-4和LLaMA-3等模型，通过提示引出可接受性评分进行评估。

Result: 大型语言模型在两个结构中都能可靠地区分语法正确和不正确的变体，这表明它们对结构敏感，而不仅仅是线性顺序。结构概括从表层形式的预测训练中涌现，表明了对句法的功能敏感性，无需显式编码。

Conclusion: 仅通过表层形式训练的大型语言模型表现出对句法结构的敏感性，支持了结构概括从预测训练中涌现的观点，而非需要显式编码的认知知识。

Abstract: What counts as evidence for syntactic structure? In traditional generative grammar, systematic contrasts in grammaticality such as subject-auxiliary inversion and the licensing of parasitic gaps are taken as evidence for an internal, hierarchical grammar. In this paper, we test whether large language models (LLMs), trained only on surface forms, reproduce these contrasts in ways that imply an underlying structural representation.
  We focus on two classic constructions: subject-auxiliary inversion (testing recognition of the subject boundary) and parasitic gap licensing (testing abstract dependency structure). We evaluate models including GPT-4 and LLaMA-3 using prompts eliciting acceptability ratings. Results show that LLMs reliably distinguish between grammatical and ungrammatical variants in both constructions, and as such support that they are sensitive to structure and not just linear order. Structural generalizations, distinct from cognitive knowledge, emerge from predictive training on surface forms, suggesting functional sensitivity to syntax without explicit encoding.

</details>


### [16] [XDoGE: Multilingual Data Reweighting to Enhance Language Inclusivity in LLMs](https://arxiv.org/abs/2512.10545)
*Iñaki Lacunza,José Javier Saiz,Alexander Shvets,Aitor Gonzalez-Agirre,Marta Villegas*

Main category: cs.CL

TL;DR: 本文提出XDoGE算法优化多语言训练分布，通过重新平衡高低资源语言数据，训练出针对伊比利亚语言优化的IberianLLM-7B-Instruct模型。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型过度依赖英语等高资源语言，导致在中低资源语言上表现不佳。需要平衡多语言训练数据分布以提升模型在非主流语言上的性能。

Method: 1) 扩展DoGE算法为XDoGE，用于多语言设置，训练小型代理模型优化语言分布；2) 根据确定的语言权重重新调整数据规模，从头训练或持续预训练完整模型；3) 针对六种不同资源水平的伊比利亚语言进行实验。

Result: 实验表明对少数语言进行大量数据重复和对主流语言进行欠采样能改善多语言性能。最终发布了IberianLLM-7B-Instruct模型，该模型通过XDoGE权重从头预训练并持续优化。

Conclusion: XDoGE方法能有效平衡多语言训练分布，显著提升模型在中低资源语言上的性能。IberianLLM-7B-Instruct模型为伊比利亚语言社区提供了专门优化的解决方案。

Abstract: Current large language models (LLMs) are trained on massive amounts of text data, primarily from a few dominant languages. Studies suggest that this over-reliance on high-resource languages, such as English, hampers LLM performance in mid- and low-resource languages. To mitigate this problem, we propose to (i) optimize the language distribution by training a small proxy model within a domain-reweighing DoGE algorithm that we extend to XDoGE for a multilingual setup, and (ii) rescale the data and train a full-size model with the established language weights either from scratch or within a continual pre-training phase (CPT). We target six languages possessing a variety of geographic and intra- and inter-language-family relations, namely, English and Spanish (high-resource), Portuguese and Catalan (mid-resource), Galician and Basque (low-resource). We experiment with Salamandra-2b, which is a promising model for these languages. We investigate the effects of substantial data repetition on minor languages and under-sampling on dominant languages using the IberoBench framework for quantitative evaluation. Finally, we release a new promising IberianLLM-7B-Instruct model centering on Iberian languages and English that we pretrained from scratch and further improved using CPT with the XDoGE weights.

</details>


### [17] [Causal Reasoning Favors Encoders: On The Limits of Decoder-Only Models](https://arxiv.org/abs/2512.10561)
*Amartya Roy,Elamparithy M,Kripabandhu Ghosh,Ponnurangam Kumaraguru,Adrian de Wynter*

Main category: cs.CL

TL;DR: ICL在因果推理中表现不佳，特别是解码器模型对分布变化脆弱；编码器和编码器-解码器模型经过微调后能更稳健地进行多跳合取推理。


<details>
  <summary>Details</summary>
Motivation: 探究上下文学习（ICL）在因果推理中的作用和性能，因果推理需要多跳组合和严格合取控制，而ICL可能依赖输入中的虚假词汇关系导致误导结果。

Method: 比较微调后的编码器、编码器-解码器和仅解码器架构在零样本和少样本ICL设置下的表现，涵盖自然语言和非自然语言场景。

Result: ICL本身不足以进行可靠的因果推理，常过度关注无关输入特征。仅解码器模型对分布变化脆弱，而微调后的编码器和编码器-解码器模型能更稳健地泛化，包括非自然语言场景。仅在大规模时，仅解码器架构才能匹配或超越前两者。

Conclusion: 对于成本效益高、短期稳健的因果推理，经过针对性微调的编码器或编码器-解码器架构更优。

Abstract: In context learning (ICL) underpins recent advances in large language models (LLMs), although its role and performance in causal reasoning remains unclear. Causal reasoning demands multihop composition and strict conjunctive control, and reliance on spurious lexical relations of the input could provide misleading results. We hypothesize that, due to their ability to project the input into a latent space, encoder and encoder decoder architectures are better suited for said multihop conjunctive reasoning versus decoder only models. To do this, we compare fine-tuned versions of all the aforementioned architectures with zero and few shot ICL in both natural language and non natural language scenarios. We find that ICL alone is insufficient for reliable causal reasoning, often overfocusing on irrelevant input features. In particular, decoder only models are noticeably brittle to distributional shifts, while finetuned encoder and encoder decoder models can generalize more robustly across our tests, including the non natural language split. Both architectures are only matched or surpassed by decoder only architectures at large scales. We conclude by noting that for cost effective, short horizon robust causal reasoning, encoder or encoder decoder architectures with targeted finetuning are preferable.

</details>


### [18] [RoleRMBench & RoleRM: Towards Reward Modeling for Profile-Based Role Play in Dialogue Systems](https://arxiv.org/abs/2512.10575)
*Hang Ding,Qiming Feng,Dongqi Liu,Qi Zhao,Tao Yao,Shuo Wang,Dongsheng Chen,Jian Li,Zhenye Gan,Jiangning Zhang,Chengjie Wang,Yabiao Wang*

Main category: cs.CL

TL;DR: 论文提出RoleRMBench基准和RoleRM奖励模型，解决角色扮演对话中主观性奖励建模的挑战，通过连续隐式偏好方法显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型在处理主观开放领域（如角色扮演）时表现严重退化，难以捕捉基于角色的细微人类判断，需要专门的基准和模型来解决这一问题。

Method: 1. 创建RoleRMBench基准，涵盖角色扮演对话的七个细粒度能力；2. 提出RoleRM奖励模型，采用连续隐式偏好（CIP）方法，将主观评估重构为多结构策略下的连续一致成对监督。

Result: RoleRM在RoleRMBench上超越现有开源和闭源奖励模型平均24%以上，在叙事连贯性和风格保真度方面取得显著提升，验证了连续偏好表示和标注一致性的重要性。

Conclusion: 研究表明连续偏好表示和标注一致性对主观对齐至关重要，为以人为中心的对话系统建立了主观对齐的基础，解决了角色扮演领域奖励建模的核心挑战。

Abstract: Reward modeling has become a cornerstone of aligning large language models (LLMs) with human preferences. Yet, when extended to subjective and open-ended domains such as role play, existing reward models exhibit severe degradation, struggling to capture nuanced and persona-grounded human judgments. To address this gap, we introduce RoleRMBench, the first systematic benchmark for reward modeling in role-playing dialogue, covering seven fine-grained capabilities from narrative management to role consistency and engagement. Evaluation on RoleRMBench reveals large and consistent gaps between general-purpose reward models and human judgment, particularly in narrative and stylistic dimensions. We further propose RoleRM, a reward model trained with Continuous Implicit Preferences (CIP), which reformulates subjective evaluation as continuous consistent pairwise supervision under multiple structuring strategies. Comprehensive experiments show that RoleRM surpasses strong open- and closed-source reward models by over 24% on average, demonstrating substantial gains in narrative coherence and stylistic fidelity. Our findings highlight the importance of continuous preference representation and annotation consistency, establishing a foundation for subjective alignment in human-centered dialogue systems.

</details>


### [19] [AgriGPT-Omni: A Unified Speech-Vision-Text Framework for Multilingual Agricultural Intelligence](https://arxiv.org/abs/2512.10624)
*Bo Yang,Lanfei Feng,Yunkui Chen,Yu Zhang,Jianyu Zhang,Xiao Xu,Nueraili Aierken,Shijian Li*

Main category: cs.CL

TL;DR: 提出了首个农业多模态统一框架AgriGPT-Omni，集成了语音、视觉和文本，通过三阶段训练实现了跨语言和跨模态的统一推理，并构建了最大的农业语音数据集和首个三模态基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在农业应用中面临三大挑战：缺乏多语言语音数据、缺乏统一的多模态架构、缺乏全面的评估基准。这些问题限制了农业智能的发展，特别是在低资源地区。

Method: 1. 构建可扩展的数据合成和收集管道，将农业文本和图像转化为训练数据，创建了包含6种语言、49.2万合成和1.4万真实语音样本的最大农业语音数据集。
2. 采用三阶段训练范式：文本知识注入、渐进式多模态对齐、基于GRPO的强化学习，训练首个农业全模态模型。
3. 提出AgriBench-Omni-2K，首个农业三模态基准测试，涵盖多样化的语音-视觉-文本任务和多语言切片。

Result: 实验表明，AgriGPT-Omni在多语言和多模态推理以及真实世界语音理解方面显著优于通用基线模型。模型在农业特定任务上表现出色。

Conclusion: AgriGPT-Omni框架有效解决了农业多模态智能的关键瓶颈，通过开源模型、数据、基准和代码，将促进可重复研究、包容性农业智能和低资源地区的可持续AI发展。

Abstract: Despite rapid advances in multimodal large language models, agricultural applications remain constrained by the lack of multilingual speech data, unified multimodal architectures, and comprehensive evaluation benchmarks. To address these challenges, we present AgriGPT-Omni, an agricultural omni-framework that integrates speech, vision, and text in a unified framework. First, we construct a scalable data synthesis and collection pipeline that converts agricultural texts and images into training data, resulting in the largest agricultural speech dataset to date, including 492K synthetic and 1.4K real speech samples across six languages. Second, based on this, we train the first agricultural omni-model via a three-stage paradigm: textual knowledge injection, progressive multimodal alignment, and GRPO-based reinforcement learning, enabling unified reasoning across languages and modalities. Third, we propose AgriBench-Omni-2K, the first tri-modal benchmark for agriculture, covering diverse speech-vision-text tasks and multilingual slices, with standardized protocols and reproducible tools. Experiments show that AgriGPT-Omni significantly outperforms general-purpose baselines on multilingual and multimodal reasoning as well as real-world speech understanding. All models, data, benchmarks, and code will be released to promote reproducible research, inclusive agricultural intelligence, and sustainable AI development for low-resource regions.

</details>


### [20] [From Data Scarcity to Data Care: Reimagining Language Technologies for Serbian and other Low-Resource Languages](https://arxiv.org/abs/2512.10630)
*Smiljana Antonijevic Ubois*

Main category: cs.CL

TL;DR: 该研究以塞尔维亚语为例，探讨AI时代低资源语言技术发展的结构、历史和社会技术因素，提出基于CARE原则的Data Care框架，将偏见缓解从技术修复转变为语料设计、注释和治理的组成部分。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通常以英语等主导语言训练，对低资源语言的表示反映了源语言材料中的文化和语言偏见。研究以塞尔维亚语为案例，旨在理解AI时代低资源语言技术发展的影响因素。

Method: 采用半结构化访谈法，采访了10位学者和从业者，包括语言学家、数字人文主义者和AI开发者，分析塞尔维亚语技术发展面临的挑战。

Result: 研究发现挑战包括：历史性文本遗产破坏、当代问题驱动的还原性工程优先方法、肤浅的音译、依赖英语训练模型、数据偏见、缺乏文化特异性的数据集策展。提出了Data Care框架。

Conclusion: 研究提出基于CARE原则（集体利益、控制权、责任、伦理）的Data Care框架，将偏见缓解重构为语料设计、注释和治理的核心组成部分，为在传统LLM开发复制现有权力不平衡和文化盲点的背景下，构建包容、可持续和文化基础的语言技术提供了可复制模型。

Abstract: Large language models are commonly trained on dominant languages like English, and their representation of low resource languages typically reflects cultural and linguistic biases present in the source language materials. Using the Serbian language as a case, this study examines the structural, historical, and sociotechnical factors shaping language technology development for low resource languages in the AI age. Drawing on semi structured interviews with ten scholars and practitioners, including linguists, digital humanists, and AI developers, it traces challenges rooted in historical destruction of Serbian textual heritage, intensified by contemporary issues that drive reductive, engineering first approaches prioritizing functionality over linguistic nuance. These include superficial transliteration, reliance on English-trained models, data bias, and dataset curation lacking cultural specificity. To address these challenges, the study proposes Data Care, a framework grounded in CARE principles (Collective Benefit, Authority to Control, Responsibility, and Ethics), that reframes bias mitigation from a post hoc technical fix to an integral component of corpus design, annotation, and governance, and positions Data Care as a replicable model for building inclusive, sustainable, and culturally grounded language technologies in contexts where traditional LLM development reproduces existing power imbalances and cultural blind spots.

</details>


### [21] [Textual Data Bias Detection and Mitigation - An Extensible Pipeline with Experimental Evaluation](https://arxiv.org/abs/2512.10734)
*Rebekka Görge,Sujan Sai Gannamaneni,Tabea Naeven,Hammam Abdelwahab,Héctor Allende-Cid,Armin B. Cremers,Lennard Helmer,Michael Mock,Anna Schmitz,Songkai Xue,Elif Yildirir,Maximilian Poretschkin,Stefan Wrobel*

Main category: cs.CL

TL;DR: 提出一个包含四个组件的全面数据偏见检测与缓解流程，通过LLM生成词表、人口统计代表性评分、社会语言学过滤和反事实数据增强来减少文本数据集中的代表性偏见和刻板印象。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练数据存在多方面的偏见表现，包括有害语言和倾斜的人口统计分布。欧盟AI法案等法规要求识别和减轻针对受保护群体的数据偏见，但缺乏实际指导和可操作方案。

Method: 提出四组件管道：1) 基于质量标准的LLM生成相关群体标签词表；2) 使用人口统计代表性评分量化代表性偏见；3) 社会语言学信息过滤检测和缓解刻板印象；4) 语法和上下文感知的反事实数据增强补偿代表性偏见。

Result: 成功减少了文本数据集中的代表性偏见和显性刻板印象。然而，在偏见数据上微调的LLM在偏见基准测试中并未一致表现出改进，暴露了当前评估方法的不足，并凸显了需要更有针对性的数据操作来应对模型偏见。

Conclusion: 虽然提出的方法能有效减少数据偏见，但数据去偏见化并不能保证减少模型偏见，揭示了当前评估方法的局限性，并强调需要更精确的数据操作来应对具体的模型偏见表现。

Abstract: Textual data used to train large language models (LLMs) exhibits multifaceted bias manifestations encompassing harmful language and skewed demographic distributions. Regulations such as the European AI Act require identifying and mitigating biases against protected groups in data, with the ultimate goal of preventing unfair model outputs. However, practical guidance and operationalization are lacking. We propose a comprehensive data bias detection and mitigation pipeline comprising four components that address two data bias types, namely representation bias and (explicit) stereotypes for a configurable sensitive attribute. First, we leverage LLM-generated word lists created based on quality criteria to detect relevant group labels. Second, representation bias is quantified using the Demographic Representation Score. Third, we detect and mitigate stereotypes using sociolinguistically informed filtering. Finally, we compensate representation bias through Grammar- and Context-Aware Counterfactual Data Augmentation. We conduct a two-fold evaluation using the examples of gender, religion and age. First, the effectiveness of each individual component on data debiasing is evaluated through human validation and baseline comparison. The findings demonstrate that we successfully reduce representation bias and (explicit) stereotypes in a text dataset. Second, the effect of data debiasing on model bias reduction is evaluated by bias benchmarking of several models (0.6B-8B parameters), fine-tuned on the debiased text dataset. This evaluation reveals that LLMs fine-tuned on debiased data do not consistently show improved performance on bias benchmarks, exposing critical gaps in current evaluation methodologies and highlighting the need for targeted data manipulation to address manifested model bias.

</details>


### [22] [Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving](https://arxiv.org/abs/2512.10739)
*Songyang Gao,Yuzhe Gu,Zijian Wu,Lingkai Kong,Wenwei Zhang,Zhongrui Cai,Fan Zheng,Tianyou Ma,Junhao Shen,Haiteng Zhao,Duanyang Zhang,Huilun Zhang,Kuikun Liu,Chengqi Lyu,Yanhui Duan,Chiyu Chen,Ningsheng Ma,Jianfei Gao,Han Lyu,Dahua Lin,Kai Chen*

Main category: cs.CL

TL;DR: OPV是一种结合结果验证和过程验证的新方法，通过总结长推理链的结果来验证推理过程，实现了准确高效的大规模标注，在多个基准测试中取得了SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有结果验证器无法检查长推理链中的中间步骤，而过程验证器由于高质量标注成本高昂难以可靠检测复杂长推理链中的错误，需要一种既能准确验证又能大规模标注的解决方案。

Method: 提出基于结果的流程验证器(OPV)，通过总结长推理链的结果来验证推理过程；采用迭代主动学习框架，通过专家标注不确定案例，使用拒绝微调和RLVR训练新的OPV模型。

Result: OPV在held-out基准测试中F1得分83.1，优于Qwen3-Max-Preview的76.3；能有效检测合成数据集中的假阳性；与策略模型协作时显著提升性能，如将DeepSeek-R1-Distill-Qwen-32B在AIME2025上的准确率从55.2%提升到73.3%。

Conclusion: OPV验证器结合了结果验证和过程验证的优势，通过迭代主动学习实现了准确高效的大规模标注，在各种基准测试中表现出色，具有广泛的适用性。

Abstract: Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the \textbf{O}utcome-based \textbf{P}rocess \textbf{V}erifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \textsc{\thisbench}, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\% to 73.3\% on AIME2025 as the compute budget scales.

</details>


### [23] [TRIDENT: A Redundant Architecture for Caribbean-Accented Emergency Speech Triage](https://arxiv.org/abs/2512.10741)
*Elroy Galbraith,Chadwick Sutherland,Donahue Morgan*

Main category: cs.CL

TL;DR: TRIDENT是一个三层调度员支持架构，针对加勒比口音英语的紧急语音识别系统，通过加勒比口音调优的ASR、本地实体提取和生物声学痛苦检测，在ASR失败时仍能为调度员提供结构化的紧急呼叫输入，以应用标准分诊协议。


<details>
  <summary>Details</summary>
Motivation: 当前紧急语音识别系统在非标准英语变体（特别是加勒比口音）上存在系统性性能下降，导致加勒比人口在紧急服务中存在关键服务缺口，需要确保他们能够公平获得已建立的国家分诊协议。

Method: 采用三层架构：1) 加勒比口音调优的自动语音识别；2) 通过大语言模型进行本地实体提取；3) 生物声学痛苦检测。系统为调度员提供三个互补信号：转录置信度、结构化临床实体和声音压力指标。

Result: 提出了一种理论框架，关键洞察是：低ASR置信度不是系统失败，而是有价值的队列优先级信号，特别是与升高的声音痛苦标记结合时。另一个洞察是训练有素的响应者和冷静的旁观者可能报告危及生命的紧急情况而没有升高的声音压力，需要语义分析来捕捉副语言特征遗漏的临床指标。

Conclusion: 这项工作建立了一个口音弹性的紧急AI框架，确保加勒比声音能够公平获得已建立的国家分诊协议。系统架构考虑了灾难场景下的离线操作，但针对加勒比紧急呼叫的实证验证仍是未来工作。

Abstract: Emergency speech recognition systems exhibit systematic performance degradation on non-standard English varieties, creating a critical gap in services for Caribbean populations. We present TRIDENT (Transcription and Routing Intelligence for Dispatcher-Empowered National Triage), a three-layer dispatcher-support architecture designed to structure emergency call inputs for human application of established triage protocols (the ESI for routine operations and START for mass casualty events), even when automatic speech recognition fails.
  The system combines Caribbean-accent-tuned ASR, local entity extraction via large language models, and bio-acoustic distress detection to provide dispatchers with three complementary signals: transcription confidence, structured clinical entities, and vocal stress indicators. Our key insight is that low ASR confidence, rather than representing system failure, serves as a valuable queue prioritization signal -- particularly when combined with elevated vocal distress markers indicating a caller in crisis whose speech may have shifted toward basilectal registers. A complementary insight drives the entity extraction layer: trained responders and composed bystanders may report life-threatening emergencies without elevated vocal stress, requiring semantic analysis to capture clinical indicators that paralinguistic features miss.
  We describe the architectural design, theoretical grounding in psycholinguistic research on stress-induced code-switching, and deployment considerations for offline operation during disaster scenarios. This work establishes a framework for accent-resilient emergency AI that ensures Caribbean voices receive equitable access to established national triage protocols. Empirical validation on Caribbean emergency calls remains future work.

</details>


### [24] [OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification](https://arxiv.org/abs/2512.10756)
*Zijian Wu,Lingkai Kong,Wenwei Zhang,Songyang Gao,Yuzhe Gu,Zhongrui Cai,Tianyou Ma,Yuhong Liu,Zhi Wang,Runyuan Ma,Guangyu Wang,Wei Li,Conghui He,Dahua Lin,Kai Chen*

Main category: cs.CL

TL;DR: 提出了一种基于结果的流程验证器（OPV），通过验证长推理链中总结结果的推理过程，实现准确高效的验证和大规模标注。


<details>
  <summary>Details</summary>
Motivation: 现有基于结果的验证器无法检查长推理链中的不可靠中间步骤，而基于过程的验证器由于人工标注成本高昂导致高质量标注稀缺，难以可靠检测复杂长推理链中的错误。

Method: 提出OPV验证器，采用迭代主动学习框架，通过专家标注不确定案例，使用拒绝微调（RFT）和RLVR训练新的OPV，逐步提升验证能力。

Result: 在OPV-Bench上达到83.1的F1分数，优于Qwen3-Max-Preview的76.3；能有效检测合成数据集中的假阳性；与策略模型协作时能显著提升性能，如将DeepSeek-R1-Distill-Qwen-32B在AIME2025上的准确率从55.2%提升到73.3%。

Conclusion: OPV通过验证总结结果的推理过程，实现了准确高效的验证和大规模标注，在多项实验中表现出优越性能和广泛适用性。

Abstract: Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales.

</details>


### [25] [Grow Up and Merge: Scaling Strategies for Efficient Language Adaptation](https://arxiv.org/abs/2512.10772)
*Kevin Glocker,Kätriin Kukk,Romina Oji,Marcel Bollmann,Marco Kuhlmann,Jenny Kunz*

Main category: cs.CL

TL;DR: 研究表明，通过扩展英语基础模型来适应新语言比传统持续预训练更高效，能提升数据利用率和保持原语言能力，但语言模型合并仍不如联合多语言训练。


<details>
  <summary>Details</summary>
Motivation: 当前大规模多语言模型在中等和低资源语言上表现不佳，且语言特定适配模型在较小规模时仍表现更好。研究探索扩展作为适应新目标语言的高效策略。

Method: 通过全面的扩展消融实验，在近似FLOP匹配的模型上测试扩展英语基础模型与标准持续预训练的效果对比。同时探索扩展后的语言特定模型合并构建模块化多语言系统。

Result: 扩展模型在获得足够目标语言数据后，能够匹配或超越在更多数据上持续预训练的小模型，显示出扩展对数据效率的益处。扩展还有助于保持基础模型的英语能力，减少灾难性遗忘。模型合并虽然不如联合多语言训练有效，但扩展模型的合并表现优于小模型。

Conclusion: 扩展是适应新语言的高效策略，能提高数据效率并保持基础语言能力。语言特定模型的合并仍有改进空间，不同合并方法存在显著性能差异，需要专门针对语言级别整合的合并方法。

Abstract: Achieving high-performing language models which include medium- and lower-resource languages remains a challenge. Massively multilingual models still underperform compared to language-specific adaptations, especially at smaller model scales. In this work, we investigate scaling as an efficient strategy for adapting pretrained models to new target languages. Through comprehensive scaling ablations with approximately FLOP-matched models, we test whether upscaling an English base model enables more effective and resource-efficient adaptation than standard continued pretraining. We find that, once exposed to sufficient target-language data, larger upscaled models can match or surpass the performance of smaller models continually pretrained on much more data, demonstrating the benefits of scaling for data efficiency. Scaling also helps preserve the base model's capabilities in English, thus reducing catastrophic forgetting. Finally, we explore whether such scaled, language-specific models can be merged to construct modular and flexible multilingual systems. We find that while merging remains less effective than joint multilingual training, upscaled merges perform better than smaller ones. We observe large performance differences across merging methods, suggesting potential for improvement through merging approaches specialized for language-level integration.

</details>


### [26] [Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting](https://arxiv.org/abs/2512.10780)
*Manurag Khullar,Utkarsh Desai,Poorva Malviya,Aman Dalmia,Zheyuan Ryan Shi*

Main category: cs.CL

TL;DR: 研究发现，在印度母幼健康分诊任务中，LLMs对罗马化文本的处理性能显著低于原生文字，可能导致数百万额外错误，揭示了一个关键的安全盲点。


<details>
  <summary>Details</summary>
Motivation: 在印度等地区，人们经常使用罗马化文本而非原生文字进行交流，但现有研究很少用真实数据评估这种书写变体对LLMs在临床应用中可靠性的影响。

Method: 使用包含五种印度语言和尼泊尔语的真实用户查询数据集，对主流LLMs进行基准测试，评估罗马化文本对母幼健康分诊任务性能的影响。

Result: 罗马化文本导致LLMs性能持续下降，F1分数比原生文字低5-12个百分点。在合作医疗组织中，这种差距可能导致近200万次额外分诊错误。有趣的是，模型通常能正确理解罗马化查询的语义意图，但最终分类输出在罗马化输入的正交噪声面前仍然脆弱。

Conclusion: 研究揭示了基于LLM的健康系统中的一个关键安全盲点：看似能理解罗马化输入的模型可能仍无法可靠地基于其采取行动，即使它们具备正确的临床推理能力。

Abstract: Large Language Models (LLMs) are increasingly deployed in high-stakes clinical applications in India. In many such settings, speakers of Indian languages frequently communicate using romanized text rather than native scripts, yet existing research rarely evaluates this orthographic variation using real-world data. We investigate how romanization impacts the reliability of LLMs in a critical domain: maternal and newborn healthcare triage. We benchmark leading LLMs on a real-world dataset of user-generated queries spanning five Indian languages and Nepali. Our results reveal consistent degradation in performance for romanized messages, with F1 scores trailing those of native scripts by 5-12 points. At our partner maternal health organization in India, this gap could cause nearly 2 million excess errors in triage. Crucially, this performance gap by scripts is not due to a failure in clinical reasoning. We demonstrate that LLMs often correctly infer the semantic intent of romanized queries. Nevertheless, their final classification outputs remain brittle in the presence of orthographic noise in romanized inputs. Our findings highlight a critical safety blind spot in LLM-based health systems: models that appear to understand romanized input may still fail to act on it reliably.

</details>


### [27] [The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality](https://arxiv.org/abs/2512.10791)
*Aileen Cheng,Alon Jacovi,Amir Globerson,Ben Golan,Charles Kwong,Chris Alberti,Connie Tao,Eyal Ben-David,Gaurav Singh Tomar,Lukas Haas,Yonatan Bitton,Adam Bloniarz,Aijun Bai,Andrew Wang,Anfal Siddiqui,Arturo Bajuelos Castillo,Aviel Atias,Chang Liu,Corey Fry,Daniel Balle,Deepanway Ghosal,Doron Kukliansky,Dror Marcus,Elena Gribovskaya,Eran Ofek,Honglei Zhuang,Itay Laish,Jan Ackermann,Lily Wang,Meg Risdal,Megan Barnes,Michael Fink,Mohamed Amin,Moran Ambar,Natan Potikha,Nikita Gupta,Nitzan Katz,Noam Velan,Ofir Roval,Ori Ram,Polina Zablotskaia,Prathamesh Bang,Priyanka Agrawal,Rakesh Ghiya,Sanjay Ganapathy,Simon Baumgartner,Sofia Erell,Sushant Prakash,Thibault Sellam,Vikram Rao,Xuanhui Wang,Yaroslav Akulov,Yulong Yang,Zhen Yang,Zhixin Lai,Zhongru Wu,Anca Dragan,Avinatan Hassidim,Fernando Pereira,Slav Petrov,Srinivasan Venkatachary,Tulsee Doshi,Yossi Matias,Sasha Goldshtein,Dipanjan Das*

Main category: cs.CL

TL;DR: FACTS Leaderboard是一个综合评估语言模型事实准确性的在线基准套件，包含四个子基准测试，涵盖多模态、参数知识、搜索和文档基础等多个场景。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法往往只关注语言模型事实准确性的某个特定方面，缺乏对模型在不同场景下生成事实准确文本能力的全面评估。需要建立一个综合性基准来全面衡量模型的事实准确性。

Method: 构建了包含四个子基准的在线评测套件：FACTS多模态（图像问题回答）、FACTS参数（闭卷事实问答）、FACTS搜索（信息检索场景）和FACTS Grounding v2（文档基础长文本生成）。使用自动化评判模型对模型响应进行评分，最终得分是四个组成部分的平均值。

Result: 开发了一个全面评估语言模型事实准确性的基准套件，包含公开和私有数据集划分，支持外部参与同时保证基准完整性。该套件将在Kaggle平台上持续维护和更新。

Conclusion: FACTS Leaderboard提供了一个稳健、平衡的评估框架，能够全面衡量语言模型在不同场景下的事实准确性，填补了现有评估方法的空白，为模型开发和比较提供了有价值的基准。

Abstract: We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models. Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity. It can be found at https://www.kaggle.com/benchmarks/google/facts .

</details>


### [28] [LabelFusion: Learning to Fuse LLMs and Transformer Classifiers for Robust Text Classification](https://arxiv.org/abs/2512.10793)
*Michael Schlee,Christoph Weisser,Timo Kivimäki,Melchizedek Mashiku,Benjamin Saefken*

Main category: cs.CL

TL;DR: LabelFusion是一个融合集成框架，通过结合传统Transformer分类器和LLM的优势，实现准确且成本感知的文本分类。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer分类器与LLM各有优势，前者计算高效但可能缺乏推理能力，后者推理能力强但成本高。需要一种方法结合两者优势，实现性能与成本的平衡。

Method: 将传统Transformer的嵌入向量与LLM通过结构化提示工程生成的每类分数进行拼接，输入到多层感知机(FusionMLP)中学习融合，实现端到端训练。

Result: 在AG News上达到92.4%准确率，在10类Reuters 21578主题分类上达到92.3%准确率，实现了强大的跨领域性能。

Conclusion: LabelFusion成功融合了传统分类器和LLM的互补优势，在保持高性能的同时实现了准确性、延迟和成本的实用权衡。

Abstract: LabelFusion is a fusion ensemble for text classification that learns to combine a traditional transformer-based classifier (e.g., RoBERTa) with one or more Large Language Models (LLMs such as OpenAI GPT, Google Gemini, or DeepSeek) to deliver accurate and cost-aware predictions across multi-class and multi-label tasks. The package provides a simple high-level interface (AutoFusionClassifier) that trains the full pipeline end-to-end with minimal configuration, and a flexible API for advanced users. Under the hood, LabelFusion integrates vector signals from both sources by concatenating the ML backbone's embeddings with the LLM-derived per-class scores -- obtained through structured prompt-engineering strategies -- and feeds this joint representation into a compact multi-layer perceptron (FusionMLP) that produces the final prediction. This learned fusion approach captures complementary strengths of LLM reasoning and traditional transformer-based classifiers, yielding robust performance across domains -- achieving 92.4% accuracy on AG News and 92.3% on 10-class Reuters 21578 topic classification -- while enabling practical trade-offs between accuracy, latency, and cost.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [29] [STARS: Semantic Tokens with Augmented Representations for Recommendation at Scale](https://arxiv.org/abs/2512.10149)
*Han Chen,Steven Zhu,Yingrui Li*

Main category: cs.IR

TL;DR: STARS是一个为大规模、低延迟电商场景设计的基于Transformer的顺序推荐框架，通过双记忆用户嵌入、语义物品令牌、上下文感知评分和两阶段检索管道等创新，显著提升了推荐效果并满足毫秒级延迟要求。


<details>
  <summary>Details</summary>
Motivation: 现实电商推荐系统面临冷启动产品、快速变化的用户意图、季节性/节假日/促销等动态上下文挑战，同时必须在严格的数十毫秒延迟约束下提供相关推荐。

Method: STARS采用基于Transformer的顺序推荐框架，包含：1）分离长期偏好和短期会话意图的双记忆用户嵌入；2）融合预训练文本嵌入、可学习增量和LLM生成属性标签的语义物品令牌；3）学习日历和事件偏移的上下文感知评分；4）离线嵌入生成和在线最大内积搜索过滤的两阶段低延迟检索管道。

Result: 离线评估中，STARS相比现有LambdaMART系统提升Hit@5超过75%。大规模A/B测试（600万次访问）显示统计显著提升：总订单+0.8%，首页加购+2.0%，用户访问次数+0.5%。

Conclusion: STARS证明将语义增强、多意图建模和面向部署的设计相结合，可以在不牺牲服务效率的情况下，在现实环境中实现最先进的推荐质量。

Abstract: Real-world ecommerce recommender systems must deliver relevant items under strict tens-of-milliseconds latency constraints despite challenges such as cold-start products, rapidly shifting user intent, and dynamic context including seasonality, holidays, and promotions. We introduce STARS, a transformer-based sequential recommendation framework built for large-scale, low-latency ecommerce settings. STARS combines several innovations: dual-memory user embeddings that separate long-term preferences from short-term session intent; semantic item tokens that fuse pretrained text embeddings, learnable deltas, and LLM-derived attribute tags, strengthening content-based matching, long-tail coverage, and cold-start performance; context-aware scoring with learned calendar and event offsets; and a latency-conscious two-stage retrieval pipeline that performs offline embedding generation and online maximum inner-product search with filtering, enabling tens-of-milliseconds response times. In offline evaluations on production-scale data, STARS improves Hit@5 by more than 75 percent relative to our existing LambdaMART system. A large-scale A/B test on 6 million visits shows statistically significant lifts, including Total Orders +0.8%, Add-to-Cart on Home +2.0%, and Visits per User +0.5%. These results demonstrate that combining semantic enrichment, multi-intent modeling, and deployment-oriented design can yield state-of-the-art recommendation quality in real-world environments without sacrificing serving efficiency.

</details>


### [30] [The Best of the Two Worlds: Harmonizing Semantic and Hash IDs for Sequential Recommendation](https://arxiv.org/abs/2512.10388)
*Ziwei Liu,Yejing Wang,Qidong Liu,Zijian Zhang,Chong Chen,Wei Huang,Xiangyu Zhao*

Main category: cs.IR

TL;DR: H2Rec是一个新颖的推荐系统框架，通过协调语义ID和哈希ID来解决头部和尾部物品推荐性能不平衡的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的顺序推荐系统使用哈希ID构建物品嵌入，但面临长尾问题。现有的辅助信息方法存在噪声协作共享或语义同质化问题。语义ID虽然具有代码共享和多粒度语义建模能力，但协作压倒现象阻碍了其发展，量化机制会损害头部物品建模所需的标识符唯一性。

Method: 提出了H2Rec框架，采用双分支建模架构：一个分支捕获语义ID中的多粒度语义，另一个分支保留哈希ID的独特协作身份。同时引入双级对齐策略来桥接两种表示，促进知识转移和支持稳健的偏好建模。

Result: 在三个真实世界数据集上的广泛实验表明，H2Rec能有效平衡头部和尾部物品的推荐质量，并超越了现有基线方法。

Conclusion: 通过协调语义ID和哈希ID，H2Rec成功解决了推荐系统中头部和尾部物品性能不平衡的问题，为顺序推荐系统提供了一种有效的解决方案。

Abstract: Conventional Sequential Recommender Systems (SRS) typically assign unique Hash IDs (HID) to construct item embeddings. These HID embeddings effectively learn collaborative information from historical user-item interactions, making them vulnerable to situations where most items are rarely consumed (the long-tail problem). Recent methods that incorporate auxiliary information often suffer from noisy collaborative sharing caused by co-occurrence signals or semantic homogeneity caused by flat dense embeddings. Semantic IDs (SIDs), with their capability of code sharing and multi-granular semantic modeling, provide a promising alternative. However, the collaborative overwhelming phenomenon hinders the further development of SID-based methods. The quantization mechanisms commonly compromise the uniqueness of identifiers required for modeling head items, creating a performance seesaw between head and tail items. To address this dilemma, we propose \textbf{\name}, a novel framework that harmonizes the SID and HID. Specifically, we devise a dual-branch modeling architecture that enables the model to capture both the multi-granular semantics within SID while preserving the unique collaborative identity of HID. Furthermore, we introduce a dual-level alignment strategy that bridges the two representations, facilitating knowledge transfer and supporting robust preference modeling. Extensive experiments on three real-world datasets show that \name~ effectively balances recommendation quality for both head and tail items while surpassing the existing baselines. The implementation code can be found online\footnote{https://github.com/ziwliu8/H2Rec}.

</details>


### [31] [Rethinking Popularity Bias in Collaborative Filtering via Analytical Vector Decomposition](https://arxiv.org/abs/2512.10688)
*Lingfeng Liu,Yixin Song,Dazhong Shen,Bing Yin,Hao Li,Yanyong Zhang,Chao Wang*

Main category: cs.IR

TL;DR: 论文揭示了贝叶斯成对排序优化中流行度偏差的内在几何根源，提出了方向分解与校正框架，显著提升了推荐系统的个性化和公平性。


<details>
  <summary>Details</summary>
Motivation: 流行度偏差严重损害了协同过滤模型的个性化能力，导致过度推荐热门物品而忽视用户对小众内容的真实偏好。现有方法将这一问题视为外部混杂因素，但本文发现流行度偏差实际上是贝叶斯成对排序优化中的内在几何效应。

Method: 通过严格的数学分析证明BPR优化系统性地将物品嵌入沿"流行度方向"组织，使得嵌入大小与交互频率直接相关。提出了方向分解与校正框架，通过非对称方向更新手术式修正嵌入几何，引导正交互沿个性化偏好方向，负交互远离全局流行度方向。

Result: 在多个基于BPR的架构上进行广泛实验，DDC显著优于现有最先进的去偏方法，将训练损失减少到经过大量调优基线的不到5%，同时实现了更优的推荐质量和公平性。

Conclusion: 流行度偏差是BPR优化的内在几何产物，而非外部混杂因素。DDC框架通过几何层面的干预，在源头分离偏好与流行度，为协同过滤模型的去偏提供了通用且有效的解决方案。

Abstract: Popularity bias fundamentally undermines the personalization capabilities of collaborative filtering (CF) models, causing them to disproportionately recommend popular items while neglecting users' genuine preferences for niche content. While existing approaches treat this as an external confounding factor, we reveal that popularity bias is an intrinsic geometric artifact of Bayesian Pairwise Ranking (BPR) optimization in CF models. Through rigorous mathematical analysis, we prove that BPR systematically organizes item embeddings along a dominant "popularity direction" where embedding magnitudes directly correlate with interaction frequency. This geometric distortion forces user embeddings to simultaneously handle two conflicting tasks-expressing genuine preference and calibrating against global popularity-trapping them in suboptimal configurations that favor popular items regardless of individual tastes. We propose Directional Decomposition and Correction (DDC), a universally applicable framework that surgically corrects this embedding geometry through asymmetric directional updates. DDC guides positive interactions along personalized preference directions while steering negative interactions away from the global popularity direction, disentangling preference from popularity at the geometric source. Extensive experiments across multiple BPR-based architectures demonstrate that DDC significantly outperforms state-of-the-art debiasing methods, reducing training loss to less than 5% of heavily-tuned baselines while achieving superior recommendation quality and fairness. Code is available in https://github.com/LingFeng-Liu-AI/DDC.

</details>
