<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 62]
- [cs.IR](#cs.IR) [Total: 11]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Evaluating Retrieval-Augmented Generation Systems on Unanswerable, Uncheatable, Realistic, Multi-hop Queries](https://arxiv.org/abs/2510.11956)
*Gabrielle Kaili-May Liu,Bryan Li,Arman Cohan,William Gantt Walden,Eugene Yang*

Main category: cs.CL

TL;DR: 提出了CRUMQs（不可作弊、真实、不可回答、多跳查询）的自动生成流程，用于创建更具挑战性的RAG基准测试，显著降低作弊性并提高测试难度。


<details>
  <summary>Details</summary>
Motivation: 现有RAG基准测试很少反映真实任务复杂性，多跳或超出范围的问题往往可以通过断开推理作弊解决，或仅需要简单事实回忆，限制了发现现有RAG系统局限性的能力。

Method: 开发了首个自动、难度可控的CRUMQs生成流程，可适应任何语料库和领域，创建不可作弊、真实、不可回答和多跳查询。

Result: 在两个流行的RAG数据集上创建CRUMQs，实验结果显示相比先前基准，CRUMQs对RAG系统极具挑战性，作弊分数降低达81.0%。

Conclusion: 该流程提供了增强基准难度和真实性的简单方法，推动开发更强大的RAG系统。

Abstract: Real-world use cases often present RAG systems with complex queries for which
relevant information is missing from the corpus or is incomplete. In these
settings, RAG systems must be able to reject unanswerable, out-of-scope queries
and identify failures of retrieval and multi-hop reasoning. Despite this,
existing RAG benchmarks rarely reflect realistic task complexity for multi-hop
or out-of-scope questions, which often can be cheated via disconnected
reasoning (i.e., solved without genuine multi-hop inference) or require only
simple factual recall. This limits the ability for such benchmarks to uncover
limitations of existing RAG systems. To address this gap, we present the first
pipeline for automatic, difficulty-controlled creation of
un$\underline{c}$heatable, $\underline{r}$ealistic, $\underline{u}$nanswerable,
and $\underline{m}$ulti-hop $\underline{q}$uerie$\underline{s}$ (CRUMQs),
adaptable to any corpus and domain. We use our pipeline to create CRUMQs over
two popular RAG datasets and demonstrate its effectiveness via benchmark
experiments on leading retrieval-augmented LLMs. Results show that compared to
prior RAG benchmarks, CRUMQs are highly challenging for RAG systems and achieve
up to 81.0\% reduction in cheatability scores. More broadly, our pipeline
offers a simple way to enhance benchmark difficulty and realism and drive
development of more capable RAG systems.

</details>


### [2] [PHANTOM RECALL: When Familiar Puzzles Fool Smart Models](https://arxiv.org/abs/2510.11812)
*Souradeep Mukhopadhyay,Rishabh Baral,Nimeesh Mahajan,Samhitha Harish,Aswin RRV,Mihir Parmar,Mutsumi Nakamura,Chitta Baral*

Main category: cs.CL

TL;DR: LLMs在逻辑谜题中表现出色但依赖记忆模板而非推理，当谜题稍作修改时性能崩溃，揭示了推理能力的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否真正具备推理能力，还是仅仅依赖记忆模板，以及如何通过系统化方法评估和改进其推理表现。

Method: 引入PHANTOM RECALL基准，包含25个经典逻辑谜题和149个精心设计的扰动版本，评估11个主流LLMs，并开发自动化逻辑等价判断、细粒度错误分类和提示缓解框架。

Result: LLMs在未修改谜题上表现接近完美，但在扰动版本上显著低于人类水平，表现出幻影回忆和过度阐述等错误模式。

Conclusion: LLMs在上下文线索变化时往往无法重新推理，揭示了语言流畅性与逻辑理解之间的关键差距。

Abstract: Large language models (LLMs) such as GPT, Gemini, and Claude often appear
adept at solving classic logic puzzles--but how much genuine reasoning
underlies their answers? Recent evidence suggests that these models frequently
rely on memorized templates rather than reasoning from first principles. When
puzzles are slightly modified, their performance collapses, revealing a
striking fragility. In particular, we asked: Have LLMs addressed these issues?
To what extent? How about perturbations to other puzzles? Is there a general
way of reformulating the prompt so that the models do better? To examine these
things systematically, we introduce PHANTOM RECALL, a benchmark comprising 25
well-known logic puzzles and 149 carefully designed perturbations that preserve
reasoning structure but alter superficial details and solutions. We evaluate
eleven leading LLMs and identify a recurring failure mode--phantom
recall--where models confidently reproduce memorized solutions or spurious
rationales that no longer fit the altered scenario. To probe and mitigate this
issue, we contribute three tools: (i) an automated logical-equivalence judge to
detect reasoning mismatches, (ii) a taxonomy of fine-grained reasoning error
categories, and (iii) a prompting-based mitigation framework guided by these
categories. Despite near-perfect accuracy on unmodified puzzles, models
significantly underperform humans on perturbed ones, exhibiting both phantom
recall and over-elaboration. Our findings reveal a crucial limitation: LLMs
often fail to re-reason when contextual cues shift--highlighting the gap
between linguistic fluency and logical understanding.

</details>


### [3] [R-WoM: Retrieval-augmented World Model For Computer-use Agents](https://arxiv.org/abs/2510.11892)
*Kai Mei,Jiang Guo,Shuaichen Chang,Mingwen Dong,Dongkyu Lee,Xing Niu,Jiarong Jiang*

Main category: cs.CL

TL;DR: LLMs作为世界模型在数字环境中增强智能体决策，但存在幻觉和静态知识限制，导致长时程模拟性能下降。提出的R-WoM通过检索增强显著提升了长时程模拟性能。


<details>
  <summary>Details</summary>
Motivation: LLMs作为世界模型可以模拟未来状态和预测行动结果，减少试错探索成本，但其幻觉倾向和静态训练知识限制了长时程模拟的可靠性。

Method: 通过三个任务（下一状态识别、完整程序规划对齐、里程碑转换识别）评估LLMs的世界建模能力，并提出了检索增强世界模型（R-WoM），通过检索外部教程知识来增强LLM模拟。

Result: LLMs能有效捕捉即时下一状态和识别有意义的转换，但在完整程序规划中性能迅速下降。R-WoM相比基线在OSWorld和WebArena上分别提升了25.3%和18.1%，在长时程模拟中优势明显。

Conclusion: LLMs在短时程世界建模中表现良好，但在长时程模拟中存在显著限制。R-WoM通过检索增强有效解决了这些问题，显著提升了世界建模的可靠性。

Abstract: Large Language Models (LLMs) can serve as world models to enhance agent
decision-making in digital environments by simulating future states and
predicting action outcomes, potentially eliminating costly trial-and-error
exploration. However, this capability is fundamentally limited by LLMs'
tendency toward hallucination and their reliance on static training knowledge,
which can lead to compounding errors that inhibit long-horizon simulations. To
systematically investigate whether LLMs are appropriate for world modeling, we
probe two core capabilities of world models--future state prediction and reward
estimation--through three tasks: next-state identification, full-procedure
planning alignment, and milestone transition recognition. Our analysis shows
that while LLMs effectively capture immediate next states and identify
meaningful state transitions, their performance rapidly degrades in
full-procedure planning. This highlights LLMs' limitations in reliably modeling
environment dynamics over long horizons. To address these limitations, we
propose the Retrieval-augmented World Model (R-WoM), which grounds LLM
simulations by incorporating factual, up-to-date knowledge retrieved from
external tutorials. Experiments show that R-WoM achieves substantial
improvements of up to 25.3% (OSWorld) and 18.1% (WebArena) compared to
baselines, with particular advantages in longer-horizon simulations.

</details>


### [4] [LLM Knowledge is Brittle: Truthfulness Representations Rely on Superficial Resemblance](https://arxiv.org/abs/2510.11905)
*Patrick Haller,Mark Ibrahim,Polina Kirichenko,Levent Sagun,Samuel J. Bell*

Main category: cs.CL

TL;DR: LLM内部知识表示对输入扰动敏感，真实陈述的可分离性在语义保持的扰动下会崩溃，这表明LLM学习的是浅层、非鲁棒的知识表示。


<details>
  <summary>Details</summary>
Motivation: 探索LLM性能脆弱性是否源于不稳定的内部知识表示，特别是当输入经过表面变换（如拼写错误或改写）时，真实陈述的可分离性如何变化。

Method: 通过应用语义保持的扰动，评估四种LLM家族、五个评估数据集和三种知识探测方法中陈述真实性的表示可分离性随样本分布外程度增加的退化情况。

Result: 当样本呈现与预训练数据相似度降低时，LLM内部真实陈述表示的可分离性会崩溃；LLM区分真假陈述的能力高度依赖于陈述的确切表面形式。

Conclusion: LLM可能学习的是浅层、非鲁棒的知识表示，这解释了基准测试性能的脆弱性，并对真实性探测的实用性提出了根本挑战，呼吁进一步研究改进学习知识表示的鲁棒性。

Abstract: For Large Language Models (LLMs) to be reliable, they must learn robust
knowledge that can be generally applied in diverse settings -- often unlike
those seen during training. Yet, extensive research has shown that LLM
performance can be brittle, with models exhibiting excessive sensitivity to
trivial input variations. In this work, we explore whether this brittleness is
a direct result of unstable internal knowledge representations. To explore this
question, we build on previous work showing that LLM representations encode
statement truthfulness -- i.e., true, factual statements can be easily
separated from false, inaccurate ones. Specifically, we test the robustness of
learned knowledge by evaluating representation separability on samples that
have undergone superficial transformations to drive them out-of-distribution
(OOD), such as typos or reformulations. By applying semantically-preserving
perturbations, we study how separability degrades as statements become more
OOD, across four LLM families, five evaluation datasets, and three knowledge
probing methods. Our results reveal that internal representations of statement
truthfulness collapse as the samples' presentations become less similar to
those seen during pre-training. While LLMs can often distinguish between true
and false statements when they closely resemble the pre-training data, this
ability is highly dependent on the statement's exact surface form. These
findings offer a possible explanation for brittle benchmark performance: LLMs
may learn shallow, non-robust knowledge representations that allow for only
limited generalizability. Our work presents a fundamental challenge for the
utility of truthfulness probes, and more broadly, calls for further research on
improving the robustness of learned knowledge representations.

</details>


### [5] [LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens](https://arxiv.org/abs/2510.11919)
*Armel Zebaze,Rachel Bawden,Benoît Sagot*

Main category: cs.CL

TL;DR: 大型推理模型在机器翻译任务中生成中间思考标记（"thinking tokens"）并不能提升翻译性能，但通过模块化翻译提示策略构建的中间标记能带来改进。


<details>
  <summary>Details</summary>
Motivation: 探索大型推理模型在机器翻译任务中生成中间思考过程是否能够提升翻译性能，特别是在不同资源水平的语言对中。

Method: 使用蒸馏链式思维方法，通过人工翻译实践启发，对模型进行微调以在翻译前进行推理，生成详细的逐步翻译解释作为中间标记。

Result: 思考标记不能帮助大型推理模型更好地执行机器翻译，基于合成CoT解释的微调不优于标准输入输出微调，但模块化翻译提示策略构建的中间标记能带来改进。

Conclusion: 中间标记在微调过程中的贡献高度依赖于其中是否包含翻译尝试，使用教师模型精炼目标翻译或扩展平行语料库比将CoT解释蒸馏到"思考"翻译模型中更有效。

Abstract: Large reasoning models (LRMs) have led to new possibilities in terms of
problem-solving, through the devising of a natural language thought process
prior to answering a query. While their capabilities are well known across
mathematics and coding tasks, their impact on the task of machine translation
(MT) remains underexplored. In this work, we explore the benefits of the
generation of intermediate tokens when performing MT across multiple language
pairs of different levels of resourcedness and multiple setups. We find that
"thinking tokens" do not help LRMs better perform MT. This result generalizes
to models fine-tuned to reason before translating using distilled chain of
thought (CoT) inspired by human translators' practices. Specifically,
fine-tuning a model with synthetic CoT explanations detailing how to translate
step-by-step does not outperform standard input-output fine-tuning. However,
constructing the intermediate tokens by combining the outputs of modular
translation-specific prompting strategies results in improvements. Our findings
underscore that the contribution of intermediate tokens during fine-tuning
highly depends on the presence of translation attempts within them. More
broadly, our results suggest that using a teacher to refine target translations
or to expand parallel corpora is more impactful than distilling their CoT
explanations into "thinking" MT models.

</details>


### [6] [Discrepancy Detection at the Data Level: Toward Consistent Multilingual Question Answering](https://arxiv.org/abs/2510.11928)
*Lorena Calvo-Bartolomé,Valérie Aldana,Karla Cantarero,Alonso Madroñal de Mesa,Jerónimo Arenas-García,Jordan Boyd-Graber*

Main category: cs.CL

TL;DR: 提出了MIND管道，用于检测多语言问答知识库中的事实和文化差异，通过用户参与的事实核查来确保事实一致性和文化敏感性。


<details>
  <summary>Details</summary>
Motivation: 多语言问答系统需要确保跨语言的事实一致性，特别是对于客观查询，同时也要考虑主观回答中的文化差异。

Method: 提出了MIND，一个用户参与的事实核查管道，用于检测多语言问答知识库中的事实和文化差异。该方法在母婴健康领域的双语问答系统上进行了评估，并发布了标注有事实和文化不一致性的双语问题数据集。

Result: MIND在所有情况下都能可靠地识别不一致性，支持开发更具文化意识和事实一致性的问答系统。

Conclusion: MIND管道能够有效检测多语言问答系统中的事实和文化不一致性，为开发更可靠的多语言问答系统提供了支持。

Abstract: Multilingual question answering (QA) systems must ensure factual consistency
across languages, especially for objective queries such as What is jaundice?,
while also accounting for cultural variation in subjective responses. We
propose MIND, a user-in-the-loop fact-checking pipeline to detect factual and
cultural discrepancies in multilingual QA knowledge bases. MIND highlights
divergent answers to culturally sensitive questions (e.g., Who assists in
childbirth?) that vary by region and context. We evaluate MIND on a bilingual
QA system in the maternal and infant health domain and release a dataset of
bilingual questions annotated for factual and cultural inconsistencies. We
further test MIND on datasets from other domains to assess generalization. In
all cases, MIND reliably identifies inconsistencies, supporting the development
of more culturally aware and factually consistent QA systems.

</details>


### [7] [TopoAlign: A Framework for Aligning Code to Math via Topological Decomposition](https://arxiv.org/abs/2510.11944)
*Yupei Li,Philipp Borchert,Gerasimos Lampouras*

Main category: cs.CL

TL;DR: TopoAlign框架利用代码仓库作为数学大语言模型的训练资源，通过将代码分解为文档字符串、主函数和依赖函数，并重组为结构上类似形式数学语句的模拟代码，从而无需人工标注即可生成对齐的代码数据用于训练。


<details>
  <summary>Details</summary>
Motivation: 当前数学大语言模型在自动形式化（将非形式数学语句转换为形式语句）方面表现不佳，主要受限于大规模语料库的稀缺，特别是包含非形式和形式语句对的数据。虽然现有模型能够从自然语言指令生成代码，但代码与形式数学在结构和语法上的差异限制了有效的迁移学习。

Method: TopoAlign框架将代码分解为文档字符串、主函数和依赖函数，然后将这些组件重新组装成结构上镜像形式数学语句的模拟代码，生成结构对齐的代码数据用于训练数学大语言模型。

Result: 在minif2f、Putnam和ProofNet基准测试中，TopoAlign为DeepSeek-Math带来了显著提升，BEq@10性能提高17.77%，typecheck@10性能提高68.82%。对于Herald模型，尽管没有引入新的数学知识，BEq@10和typecheck@10分别提升了0.12%和1.09%。

Conclusion: TopoAlign框架成功解锁了广泛可用的代码仓库作为数学大语言模型的训练资源，通过结构对齐的代码数据显著提升了模型的自动形式化性能，即使对于专门化模型也有益处。

Abstract: Large Language Models (LLMs) excel at both informal and formal (e.g. Lean 4)
mathematical reasoning but still struggle with autoformalisation, the task of
transforming informal into formal mathematical statements. Autoformalisation
helps pair the informal reasoning of LLMs with formal proof assistants which
enable machine-verifiable generation and mitigate hallucinations. Yet, the
performance of current Math LLMs is constrained by the scarcity of large-scale
corpora, particularly those containing pairs of informal and formal statements.
Although current models are trained to generate code from natural language
instructions, structural and syntactic differences between these and formal
mathematics limit effective transfer learning. We propose TopoAlign, a
framework that unlocks widely available code repositories as training resources
for Math LLMs. TopoAlign decomposes code into docstrings, main functions, and
dependency functions, and reassembles these components into analogues that
structurally mirror formal statements. This produces structurally aligned code
data that can be used for training Math LLMs without requiring additional human
annotation. We train two state-of-the-art models, DeepSeek-Math and Herald, and
evaluate them on the minif2f, Putnam, and ProofNet benchmarks. TopoAlign
provides substantial gains for DeepSeek-Math, improving performance by 17.77%
on BEq@10 and 68.82% on typecheck@10. Despite introducing no new mathematical
knowledge, our framework achieves gains of 0.12% and 1.09% for Herald on BEq@10
and typecheck@10, respectively, demonstrating that training on aligned code
data is beneficial even for specialized models.

</details>


### [8] [GRAVITY: A Framework for Personalized Text Generation via Profile-Grounded Synthetic Preferences](https://arxiv.org/abs/2510.11952)
*Priyanka Dey,Daniele Rosa,Wenqing Zheng,Daniel Barcklow,Jieyu Zhao,Emilio Ferrara*

Main category: cs.CL

TL;DR: GRAVITY框架通过生成基于用户画像的合成偏好数据来减少对人工标注的依赖，整合了人口统计、文化和心理框架来指导个性化内容生成，在跨文化测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM个性化方法依赖昂贵的人工反馈或交互日志，限制了可扩展性且忽略了更深层的用户属性，需要减少对人工标注的依赖。

Method: 引入GRAVITY框架，整合Hofstede文化维度、Schwartz基本价值观、世界价值观调查和Big Five OCEAN人格特质等框架，生成基于用户画像的合成偏好数据对。

Result: 在400名亚马逊用户的图书描述测试中，GRAVITY在跨文化场景（美国、巴西、日本、印度）表现优于提示条件化、标准微调和朴素合成对生成方法，偏好增益超过4%，用户研究中86%的情况下更偏好GRAVITY输出。

Conclusion: 基于场景的合成数据能够捕捉更丰富的用户差异，减少对昂贵标注的依赖，生成更具吸引力的用户中心内容，为LLM个性化提供了可扩展的路径。

Abstract: Personalization in LLMs often relies on costly human feedback or interaction
logs, limiting scalability and neglecting deeper user attributes. To reduce the
reliance on human annotations, we introduce GRAVITY (Generative Response with
Aligned Values, Interests, and Traits of You), a framework for generating
synthetic, profile-grounded preference data that captures users' interests,
values, beliefs, and personality traits. By integrating demographic, cultural,
and psychological frameworks -- including Hofstede's cultural dimensions,
Schwartz's basic values, the World Values Survey, and Big Five OCEAN traits --
GRAVITY synthesizes preference pairs to guide personalized content generation.
We evaluate GRAVITY on book descriptions for 400 Amazon users, comparing it to
prompt-based conditioning, standard fine-tuning, and naive synthetic pair
generation. Profile-grounded synthetic data consistently improves generation,
especially across multiple cultures (USA, Brazil, Japan, India), achieving over
4% higher preference gains across baselines, with user studies showing that
GRAVITY outputs are preferred over 86% of the time. Our results show that
scenario-grounded synthetic data can capture richer user variation, reduce
reliance on costly annotation, and produce more engaging, user-centered
content, offering a scalable path for LLM personalization.

</details>


### [9] [Direct Multi-Token Decoding](https://arxiv.org/abs/2510.11958)
*Xuan Luo,Weizhi Wang,Xifeng Yan*

Main category: cs.CL

TL;DR: 提出了一种称为直接多令牌解码（DMTD）的推理范式，允许在预训练LLM中仅使用后期层一次生成多个令牌，无需重复遍历早期和中间层，可实现2倍加速且性能损失很小。


<details>
  <summary>Details</summary>
Motivation: 基于预训练LLM中不同层承担不同角色的观察：早期层处理输入上下文理解，中间层负责任务特定处理，后期层将抽象表示转换为输出令牌。假设经过早期和中间层处理后，隐藏状态已包含足够信息支持仅使用后期层生成多个令牌。

Method: 提出DMTD方法，无需额外参数、辅助例程或后生成验证。通过对Qwen3-4B模型在有限数据集上进行微调实现。

Result: 微调后的DMTD Qwen3-4B模型实现了高达2倍的加速，仅带来轻微性能损失。缩放分析表明，使用更大训练数据集可进一步提升性能。

Conclusion: DMTD是一种有前景的推理加速方法，通过仅使用后期层生成多个令牌，在保持性能的同时显著提升推理速度，且随着训练数据规模扩大，性能有望进一步改善。

Abstract: Decoder-only transformers have become the standard architecture for large
language models (LLMs) due to their strong performance. Recent studies suggest
that, in pre-trained LLMs, early, middle, and late layers may serve distinct
roles: Early layers focus on understanding the input context, middle layers
handle task-specific processing, and late layers convert abstract
representations into output tokens. We hypothesize that once representations
have been processed by the early and middle layers, the resulting hidden states
may encapsulate sufficient information to support the generation of multiple
tokens using only the late layers, eliminating the need to repeatedly traverse
the early and middle layers. We refer to this inference paradigm as Direct
Multi-Token Decoding (DMTD). Unlike speculative decoding, our method introduces
no additional parameters, auxiliary routines, or post-generation verification.
Despite being trained on a limited dataset, a fine-tuned DMTD Qwen3-4B model
has already demonstrated promising results, achieving up to a 2x speedup with
only minor performance loss. Moreover, as shown in our scaling analysis, its
performance is expected to further improve with larger training datasets.

</details>


### [10] [Scaling Long-Horizon LLM Agent via Context-Folding](https://arxiv.org/abs/2510.11967)
*Weiwei Sun,Miao Lu,Zhan Ling,Kang Liu,Xuesong Yao,Yiming Yang,Jiecao Chen*

Main category: cs.CL

TL;DR: Context-Folding框架通过让LLM代理主动管理工作上下文，在长视野任务中实现10倍更小的活动上下文，同时保持或超越ReAct基线的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型代理在长视野任务中受到上下文长度的根本限制，需要一种有效管理上下文的方法。

Method: 开发了Context-Folding框架，允许代理通过程序化分支处理子任务并在完成后折叠中间步骤，保留结果摘要；同时设计了端到端强化学习框架FoldGRPO，通过特定过程奖励鼓励有效的任务分解和上下文管理。

Result: 在复杂长视野任务（深度研究和软件工程）上，折叠代理使用10倍更小的活动上下文匹配或超越了ReAct基线，并显著优于依赖基于摘要的上下文管理模型。

Conclusion: Context-Folding框架为LLM代理提供了一种有效的上下文管理方法，能够在保持性能的同时大幅减少上下文使用量。

Abstract: Large language model (LLM) agents are fundamentally constrained by context
length on long-horizon tasks. We introduce Context-Folding, a framework that
empowers agents to actively manage their working context. An agent can
procedurally branch into a sub-trajectory to handle a subtask and then fold it
upon completion, collapsing the intermediate steps while retaining a concise
summary of the outcome. To make this behavior learnable, we develop an
end-to-end reinforcement learning framework FoldGRPO with specific process
rewards to encourage effective task decomposition and context management. On
complex long-horizon tasks (Deep Research and SWE), our folding agent matches
or outperforms the ReAct baselines while using an active context 10$\times$
smaller and significantly outperforms models that rely on summarization-based
context management.

</details>


### [11] [Conjecturing: An Overlooked Step in Formal Mathematical Reasoning](https://arxiv.org/abs/2510.11986)
*Jasivan Alex Sivakumar,Philipp Borchert,Ronald Cardenas,Gerasimos Lampouras*

Main category: cs.CL

TL;DR: 该论文提出了ConjectureBench评估框架，专门衡量LLMs在数学问题中的推测能力，并开发了Lean-FIRe方法来改进推测和自动形式化，发现当前LLMs的自动形式化性能被高估，需要将推测作为独立任务处理。


<details>
  <summary>Details</summary>
Motivation: 现有自动形式化研究忽视了推测这一关键前置步骤，许多数学问题需要先推测结论才能进行形式化。由于LLMs在自动形式化方面已有困难，且推测能力的评估有限且常与形式化或证明过程纠缠，难以理解其影响。

Method: 扩展现有数据集创建ConjectureBench，重新设计评估框架和指标来专门衡量LLMs的推测能力；开发了推理时方法Lean-FIRe来改进推测和自动形式化。

Result: 评估发现基础模型（GPT-4.1和DeepSeek-V3.1）的自动形式化性能在考虑推测因素后被显著高估；使用Lean-FIRe方法首次实现了13个PutnamBench问题的端到端自动形式化（GPT-4.1）和7个问题（DeepSeek-V3.1）。

Conclusion: LLMs具备生成准确推测所需的知识，但改进自动形式化性能需要将推测作为独立任务处理，并研究如何正确将其整合到自动形式化流程中；为未来研究提供了改进推测能力的指导方向。

Abstract: Autoformalisation, the task of expressing informal mathematical statements in
formal language, is often viewed as a direct translation process. This,
however, disregards a critical preceding step: conjecturing. Many mathematical
problems cannot be formalised directly without first conjecturing a conclusion
such as an explicit answer, or a specific bound. Since Large Language Models
(LLMs) already struggle with autoformalisation, and the evaluation of their
conjecturing ability is limited and often entangled within autoformalisation or
proof, it is particularly challenging to understand its effect. To address this
gap, we augment existing datasets to create ConjectureBench, and redesign the
evaluation framework and metric specifically to measure the conjecturing
capabilities of LLMs both as a distinct task and within the autoformalisation
pipeline. Our evaluation of foundational models, including GPT-4.1 and
DeepSeek-V3.1, reveals that their autoformalisation performance is
substantially overestimated when the conjecture is accounted for during
evaluation. However, the conjecture should not be assumed to be provided. We
design an inference-time method, Lean-FIRe to improve conjecturing and
autoformalisation, which, to the best of our knowledge, achieves the first
successful end-to-end autoformalisation of 13 PutnamBench problems with GPT-4.1
and 7 with DeepSeek-V3.1. We demonstrate that while LLMs possess the requisite
knowledge to generate accurate conjectures, improving autoformalisation
performance requires treating conjecturing as an independent task, and
investigating further how to correctly integrate it within autoformalisation.
Finally, we provide forward-looking guidance to steer future research toward
improving conjecturing, an overlooked step of formal mathematical reasoning.

</details>


### [12] [SAGE: A Top-Down Bottom-Up Knowledge-Grounded User Simulator for Multi-turn AGent Evaluation](https://arxiv.org/abs/2510.11997)
*Ryan Shea,Yunan Lu,Liang Qiu,Zhou Yu*

Main category: cs.CL

TL;DR: SAGE是一个用于多轮交互式智能体评估的用户模拟框架，它结合了业务逻辑中的自上而下知识和自下而上知识，能够生成更真实、更多样化的交互，并能识别出更多智能体错误。


<details>
  <summary>Details</summary>
Motivation: 现有的多轮交互式智能体评估方法通常依赖人工评估，或者使用模拟用户但忽略了领域特定原则，无法捕捉真实用户行为。

Method: SAGE框架整合了业务上下文知识：自上而下的业务逻辑知识（如理想客户画像）和自下而上的业务代理基础设施知识（如产品目录、FAQ、知识库）。

Result: 实证评估表明，该方法生成的交互更加真实和多样化，同时能够识别出多达33%的智能体错误。

Conclusion: SAGE是一个有效的评估工具，能够支持错误发现和智能体的迭代改进。

Abstract: Evaluating multi-turn interactive agents is challenging due to the need for
human assessment. Evaluation with simulated users has been introduced as an
alternative, however existing approaches typically model generic users and
overlook the domain-specific principles required to capture realistic behavior.
We propose SAGE, a novel user Simulation framework for multi-turn AGent
Evaluation that integrates knowledge from business contexts. SAGE incorporates
top-down knowledge rooted in business logic, such as ideal customer profiles,
grounding user behavior in realistic customer personas. We further integrate
bottom-up knowledge taken from business agent infrastructure (e.g., product
catalogs, FAQs, and knowledge bases), allowing the simulator to generate
interactions that reflect users' information needs and expectations in a
company's target market. Through empirical evaluation, we find that this
approach produces interactions that are more realistic and diverse, while also
identifying up to 33% more agent errors, highlighting its effectiveness as an
evaluation tool to support bug-finding and iterative agent improvement.

</details>


### [13] [Generate Logical Equivalence Questions](https://arxiv.org/abs/2510.12001)
*Xinyu Wang,Haoming Yu,Yicheng Yang,Zhiyuan Li*

Main category: cs.CL

TL;DR: 本文提出了一种用于离散数学逻辑等价问题的自动问题生成系统，通过形式化语言和线性时间算法生成难度均匀的问题，实验证明其生成的问题在准确性和难度上与教材问题相当。


<details>
  <summary>Details</summary>
Motivation: 在线教学时代抄袭现象日益严重，需要为每个学生生成独特问题来防止作弊，同时提供大量练习题。现有逻辑等价问题生成方法效率低且难度不均。

Method: 使用形式化语言定义逻辑等价问题，将其转换为两组生成规则，并开发线性时间算法进行问题生成。

Result: 学生实验显示生成问题的准确性与教材问题相当；难度评估实验表明生成问题的解决步骤数与教材问题相似，优于大型语言模型生成的问题。

Conclusion: 提出的自动问题生成系统能有效生成高质量的逻辑等价问题，在准确性和难度控制方面表现良好，为解决学术不端和提供个性化练习提供了可行方案。

Abstract: Academic dishonesty is met with zero tolerance in higher education, yet
plagiarism has become increasingly prevalent in the era of online teaching and
learning. Automatic Question Generation (AQG) presents a potential solution to
mitigate copying by creating unique questions for each student. Additionally,
AQG can provide a vast array of practice questions. Our AQG focuses on
generating logical equivalence questions for Discrete Mathematics, a
foundational course for first-year computer science students. A literature
review reveals that existing AQGs for this type of question generate all
propositions that meet user-defined constraints, resulting in inefficiencies
and a lack of uniform question difficulty. To address this, we propose a new
approach that defines logical equivalence questions using a formal language,
translates this language into two sets of generation rules, and develops a
linear-time algorithm for question generation. We evaluated our AQG through two
experiments. The first involved a group of students completing questions
generated by our system. Statistical analysis shows that the accuracy of these
questions is comparable to that of textbook questions. The second experiment
assessed the number of steps required to solve our generated questions,
textbook questions, and those generated by multiple large language models. The
results indicated that the difficulty of our questions was similar to that of
textbook questions, confirming the quality of our AQG.

</details>


### [14] [Information Extraction from Conversation Transcripts: Neuro-Symbolic vs. LLM](https://arxiv.org/abs/2510.12023)
*Alice Saebom Kwak,Maria Alexeeva,Gus Hahn-Powell,Keith Alcock,Kevin McLaughlin,Doug McCorkle,Gabe McNunn,Mihai Surdeanu*

Main category: cs.CL

TL;DR: LLM-based IE系统在农业领域表现优于神经符号系统（F1总分：69.4 vs 52.7），但各有优缺点：神经符号系统运行更快、控制性更强，而LLM系统性能更高、部署维护更易。


<details>
  <summary>Details</summary>
Motivation: 当前信息提取领域过度依赖大语言模型，忽视了传统符号或统计IE系统的经验积累，需要比较神经符号和LLM方法在实际应用中的表现。

Method: 在农业领域（猪肉、乳制品、作物子领域）对9个访谈进行比较评估，对比神经符号系统和LLM系统的性能差异。

Result: LLM系统在总体F1分数（69.4 vs 52.7）和核心信息提取（63.0 vs 47.2）方面均优于神经符号系统。

Conclusion: 部署NLP系统存在"隐藏成本"，需要在性能、效率和可控性之间取得平衡，不能单纯追求性能指标。

Abstract: The current trend in information extraction (IE) is to rely extensively on
large language models, effectively discarding decades of experience in building
symbolic or statistical IE systems. This paper compares a neuro-symbolic (NS)
and an LLM-based IE system in the agricultural domain, evaluating them on nine
interviews across pork, dairy, and crop subdomains. The LLM-based system
outperforms the NS one (F1 total: 69.4 vs. 52.7; core: 63.0 vs. 47.2), where
total includes all extracted information and core focuses on essential details.
However, each system has trade-offs: the NS approach offers faster runtime,
greater control, and high accuracy in context-free tasks but lacks
generalizability, struggles with contextual nuances, and requires significant
resources to develop and maintain. The LLM-based system achieves higher
performance, faster deployment, and easier maintenance but has slower runtime,
limited control, model dependency and hallucination risks. Our findings
highlight the "hidden cost" of deploying NLP systems in real-world
applications, emphasizing the need to balance performance, efficiency, and
control.

</details>


### [15] [CPR: Mitigating Large Language Model Hallucinations with Curative Prompt Refinement](https://arxiv.org/abs/2510.12029)
*Jung-Woo Shim,Yeong-Joon Ju,Ji-Hoon Park,Seong-Whan Lee*

Main category: cs.CL

TL;DR: 提出CPR框架，通过清理不规范的提示词并生成额外任务描述来减少LLM幻觉，显著提高生成质量且无需外部知识


<details>
  <summary>Details</summary>
Motivation: LLM经常生成看似合理但错误的'幻觉'事实，主要原因是用户使用结构不良或模糊的提示词，导致模型基于假设而非实际意图生成响应

Method: 引入Curative Prompt Refinement (CPR)框架，使用微调的小型语言模型清理不良提示词并生成额外信息性任务描述，以对齐用户意图和提示词

Result: 应用CPR后，语言模型生成质量显著提高，同时减轻幻觉。实证研究表明，应用CPR的提示词在无外部知识情况下胜率超过90%

Conclusion: CPR是一种即插即用的治疗性提示词优化框架，能有效减少因提示词不规范导致的LLM幻觉问题，提高模型生成可靠性

Abstract: Recent advancements in large language models (LLMs) highlight their fluency
in generating responses to diverse prompts. However, these models sometimes
generate plausible yet incorrect ``hallucinated" facts, undermining trust. A
frequent but often overlooked cause of such errors is the use of poorly
structured or vague prompts by users, leading LLMs to base responses on assumed
rather than actual intentions. To mitigate hallucinations induced by these
ill-formed prompts, we introduce Curative Prompt Refinement (CPR), a
plug-and-play framework for curative prompt refinement that 1) cleans
ill-formed prompts, and 2) generates additional informative task descriptions
to align the intention of the user and the prompt using a fine-tuned small
language model. When applied to language models, we discover that CPR
significantly increases the quality of generation while also mitigating
hallucination. Empirical studies show that prompts with CPR applied achieves
over a 90\% win rate over the original prompts without any external knowledge.

</details>


### [16] [Multi-stage Prompt Refinement for Mitigating Hallucinations in Large Language Models](https://arxiv.org/abs/2510.12032)
*Jung-Woo Shim,Yeong-Joon Ju,Ji-Hoon Park,Seong-Whan Lee*

Main category: cs.CL

TL;DR: 提出了多阶段提示优化框架MPR，通过分阶段修正语法错误、拼写错误和关键词误用等问题，使用小型语言模型迭代优化提示质量，显著减少大语言模型的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言任务中表现出色，但存在幻觉问题，即生成看似合理但实际错误的信息。其中，格式不良的提示（如歧义表述、语法错误、信息不完整）对幻觉的影响研究相对不足。

Method: 开发了多阶段提示优化框架MPR，使用专门针对特定任务微调的小型语言模型，分阶段处理标点符号、拼写错误和关键词误用等问题。框架通过迭代增强提示清晰度，并采用带排序的自反思机制来优先处理最相关的输入。

Result: 在幻觉基准测试中，经过MPR优化的提示相比原始提示获得了超过85%的胜率，有效减少了幻觉现象并提高了大语言模型的输出准确性。MPR还能与现有的后处理幻觉缓解框架结合使用。

Conclusion: MPR提供了一个轻量级且适应性强的解决方案，能够显著提升大语言模型在各种领域中的可靠性，通过系统化优化提示质量来减少幻觉问题。

Abstract: Recent advancements in large language models (LLMs) have shown strong
performance in natural language understanding and generation tasks. However,
LLMs continue to encounter challenges with hallucinations, where models
generate plausible but incorrect information. While several factors contribute
to hallucinations, the impact of ill-formed prompts, prompts with ambiguous
wording, incorrect grammar, or incomplete information, was relatively under
explored. To address this, we introduce Multi-stage Prompt Refinement (MPR), a
framework designed to systematically improve these ill-formed prompts across
multiple stages. Each stage addresses specific errors such as punctuation,
typographical mistakes, and misuse of key terms, using small language models
(SLMs) fine-tuned for these tasks. MPR iteratively enhances the clarity of
prompts with additional context and employs a self-reflection mechanism with
ranking to prioritize the most relevant input. Experimental results on
hallucination benchmarks show that prompts refined by MPR achieve over an 85~\%
win rate compared to their original forms, demonstrating its effectiveness in
reducing hallucinations and improving LLM output accuracy. Interestingly, we
reveal that MPR can be combined with existing post-hoc hallucination mitigation
frameworks, further enhancing its versatility. MPR provides a lightweight and
adaptable solution for enhancing LLM reliability across various domains.

</details>


### [17] [On the Interplay between Human Label Variation and Model Fairness](https://arxiv.org/abs/2510.12036)
*Kemal Kurniawan,Meladel Mistica,Timothy Baldwin,Jey Han Lau*

Main category: cs.CL

TL;DR: 本文研究了人类标签变异对模型公平性的影响，发现HLV训练方法在没有明确去偏的情况下对公平性有积极影响。


<details>
  <summary>Details</summary>
Motivation: 人类标签变异对模型公平性的影响是一个尚未探索的话题，本文旨在研究这两者之间的相互作用。

Method: 通过比较在多数投票标签上训练与一系列HLV方法训练的效果来检验这种相互作用。

Result: 实验表明，在没有明确去偏的情况下，HLV训练方法对公平性有积极影响。

Conclusion: HLV训练方法可以在不进行明确去偏的情况下提升模型的公平性表现。

Abstract: The impact of human label variation (HLV) on model fairness is an unexplored
topic. This paper examines the interplay by comparing training on majority-vote
labels with a range of HLV methods. Our experiments show that without explicit
debiasing, HLV training methods have a positive impact on fairness.

</details>


### [18] [Uncertainty Quantification for Hallucination Detection in Large Language Models: Foundations, Methodology, and Future Directions](https://arxiv.org/abs/2510.12040)
*Sungmin Kang,Yavuz Faruk Bakman,Duygu Nur Yaldiz,Baturalp Buyukates,Salman Avestimehr*

Main category: cs.CL

TL;DR: 本文综述了大语言模型不确定性量化在幻觉检测中的应用，系统分类现有方法并分析其局限性，为提升LLM可靠性提供研究框架。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在实际应用中存在产生看似合理但事实错误的幻觉问题，需要不确定性量化方法来评估模型输出的可信度，提高部署可靠性。

Method: 从不确定性量化基础理论出发，区分认知不确定性和偶然不确定性，系统分类现有方法并展示代表性方法的实证结果。

Result: 不确定性量化为检测LLM幻觉提供了机制，能够识别不可靠的生成内容，但现有方法仍存在局限性。

Conclusion: 不确定性量化是解决LLM幻觉问题的关键研究方向，需要进一步探索以提升模型在实际应用中的可信度。

Abstract: The rapid advancement of large language models (LLMs) has transformed the
landscape of natural language processing, enabling breakthroughs across a wide
range of areas including question answering, machine translation, and text
summarization. Yet, their deployment in real-world applications has raised
concerns over reliability and trustworthiness, as LLMs remain prone to
hallucinations that produce plausible but factually incorrect outputs.
Uncertainty quantification (UQ) has emerged as a central research direction to
address this issue, offering principled measures for assessing the
trustworthiness of model generations. We begin by introducing the foundations
of UQ, from its formal definition to the traditional distinction between
epistemic and aleatoric uncertainty, and then highlight how these concepts have
been adapted to the context of LLMs. Building on this, we examine the role of
UQ in hallucination detection, where quantifying uncertainty provides a
mechanism for identifying unreliable generations and improving reliability. We
systematically categorize a wide spectrum of existing methods along multiple
dimensions and present empirical results for several representative approaches.
Finally, we discuss current limitations and outline promising future research
directions, providing a clearer picture of the current landscape of LLM UQ for
hallucination detection.

</details>


### [19] [Improving Text-to-Image Generation with Input-Side Inference-Time Scaling](https://arxiv.org/abs/2510.12041)
*Ruibo Chen,Jiacheng Pan,Heng Huang,Zhenheng Yang*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型的提示词重写框架，通过奖励系统和迭代DPO训练来优化文本到图像生成的输入提示，无需监督微调数据即可提升图像质量和对齐度。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型在处理简单或描述不充分的提示词时，往往导致图像-文本对齐度、美学质量和整体效果不佳。

Method: 使用大语言模型作为提示词重写器，引入精心设计的奖励系统和迭代直接偏好优化训练流程，在无需监督微调数据的情况下增强提示词质量。

Result: 该方法在各种T2I模型和基准测试中一致提升了图像-文本对齐度、视觉质量和美学效果，优于强基线方法，且具有良好的跨模型迁移能力。

Conclusion: 提示词重写是一种有效、可扩展且实用的模型无关策略，能够显著改进文本到图像生成系统，训练好的重写器无需重新训练即可在不同T2I骨干网络上通用。

Abstract: Recent advances in text-to-image (T2I) generation have achieved impressive
results, yet existing models often struggle with simple or underspecified
prompts, leading to suboptimal image-text alignment, aesthetics, and quality.
We propose a prompt rewriting framework that leverages large language models
(LLMs) to refine user inputs before feeding them into T2I backbones. Our
approach introduces a carefully designed reward system and an iterative direct
preference optimization (DPO) training pipeline, enabling the rewriter to
enhance prompts without requiring supervised fine-tuning data. We evaluate our
method across diverse T2I models and benchmarks. Results show that our prompt
rewriter consistently improves image-text alignment, visual quality, and
aesthetics, outperforming strong baselines. Furthermore, we demonstrate strong
transferability by showing that a prompt rewriter trained on one T2I backbone
generalizes effectively to others without needing to be retrained. We also
systematically study scalability, evaluating how performance gains scale with
the capacity of the large LLM used as the rewriter. These findings highlight
that prompt rewriting is an effective, scalable, and practical model-agnostic
strategy for improving T2I systems. We plan to release the code and trained
prompt rewriters soon.

</details>


### [20] [Hierarchical Alignment: Surgical Fine-Tuning via Functional Layer Specialization in Large Language Models](https://arxiv.org/abs/2510.12044)
*Yukun Zhang,Qi Dong*

Main category: cs.CL

TL;DR: 提出分层对齐方法，针对Transformer架构的不同功能层进行定向DPO优化，避免传统对齐方法的"对齐税"问题，在保持语法流畅性的同时显著提升逻辑一致性和事实准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐技术（如DPO）将模型视为单一实体，在所有层上应用统一的优化压力，忽略了Transformer架构中不同层的功能专门化（从语法处理到抽象推理）。

Method: 引入分层对齐方法，对模型的不同功能层进行定向DPO：局部层（语法）、中间层（逻辑）和全局层（事实性），使用LoRA进行精确微调。

Result: 实验显示，局部层对齐增强语法流畅性，全局层对齐不仅改善事实一致性，还是提升逻辑一致性的最有效策略，所有分层策略都成功避免了标准DPO中观察到的"对齐税"。

Conclusion: 分层对齐为模型对齐提供了更资源高效、可控和可解释的路径，强调从整体优化转向结构感知的精确微调，以构建更先进可靠的LLM。

Abstract: Existing alignment techniques for Large Language Models (LLMs), such as
Direct Preference Optimization (DPO), typically treat the model as a monolithic
entity, applying uniform optimization pressure across all layers. This approach
overlooks the functional specialization within the Transformer architecture,
where different layers are known to handle distinct tasks from syntax to
abstract reasoning. In this paper, we challenge this one-size-fits-all paradigm
by introducing Hierarchical Alignment, a novel method that applies targeted DPO
to distinct functional blocks of a model's layers: local (syntax), intermediate
(logic), and global (factuality). Through a series of controlled experiments on
state-of-the-art models like Llama-3.1-8B and Qwen1.5-7B using LoRA for
surgical fine-tuning, our results, evaluated by a powerful LLM-as-Judge,
demonstrate significant and predictable improvements. Specifically, aligning
the local layers (Local-Align) enhances grammatical fluency. More importantly,
aligning the global layers (Global-Align) not only improves factual consistency
as hypothesized but also proves to be the most effective strategy for enhancing
logical coherence, outperforming all baselines. Critically, all hierarchical
strategies successfully avoid the "alignment tax" observed in standard DPO,
where gains in fluency come at the cost of degraded logical reasoning. These
findings establish a more resource-efficient, controllable, and interpretable
path for model alignment, highlighting the immense potential of shifting from
monolithic optimization to structure-aware surgical fine-tuning to build more
advanced and reliable LLMs.

</details>


### [21] [APCE: Adaptive Progressive Context Expansion for Long Context Processing](https://arxiv.org/abs/2510.12051)
*Baisub Lee,Sanghyun Byun,Mohanad Odema,Jung Guack,Jacob Song,Woo Seong Chung*

Main category: cs.CL

TL;DR: APCE是一种上下文感知的解决方案，通过低维语义相似性匹配选择最重要的输入块，在长上下文摘要任务中实现内存效率提升和性能保持。


<details>
  <summary>Details</summary>
Motivation: 部署长上下文Transformer模型面临两个关键挑战：(1) 由于二次自注意力和线性KV缓存扩展导致内存占用增加；(2) ContextRot现象，即随着上下文长度增加，Transformer架构性能下降。

Method: 提出APCE方法，通过低维语义相似性匹配与当前查询来选择最重要的输入块，直接对输入进行操作，不依赖底层硬件或CUDA环境。

Result: 实证评估显示，APCE在仅使用50%-70%输入序列的情况下，实现了优于或等同于完整密集基线的摘要性能，同时显著提升了KV缓存和自注意力的内存效率。

Conclusion: APCE为长上下文Transformer模型提供了一种上下文感知的效率解决方案，有望激发针对其他相关长上下文任务的进一步研究。

Abstract: Deploying useful Long-Context Transformer Models (LCTMs) requires addressing
two key challenges: (1) A growing memory footprint due to quadratic
self-attention and linear KV-cache scaling in memory as sequence length
increases; (2) the ContextRot phenomena where empirical evidence suggests that
transformer architecture's performance degrades with increasing context length.
Given the shared dependency on the input, a natural question arises: Can we
surgically select the most important input chunks for processing to
synergistically (a) reduce the memory footprint, and (b) mitigate the
ContextRot effects? In this paper, we answer this question in the affirmative
for long-context summarization tasks. We propose APCE as a context-aware
solution to select the most important input chunks through low-dimensional
semantic similarity matching with the current query. By directly operating on
the input, APCE decouples from strict dependency on underlying hardware or CUDA
environments, promising a compatible solution scalable to different deployment
systems. Our empirical evaluations have demonstrated superior or on-par
summarization performance for APCE compared to the full dense baseline using a
fraction (50%-70%) of the input sequence resulting in KV-cache and
self-attention memory efficiency improvements. We hope our findings inspire
further research on context-aware efficiency solutions for LCTMs geared towards
other relevant long-context tasks.

</details>


### [22] [An AI-Based Behavioral Health Safety Filter and Dataset for Identifying Mental Health Crises in Text-Based Conversations](https://arxiv.org/abs/2510.12083)
*Benjamin W. Nelson,Celeste Wong,Matthew T. Silvestrini,Sooyoon Shin,Alanna Robinson,Jessica Lee,Eric Yang,John Torous,Andrew Trister*

Main category: cs.CL

TL;DR: VBHSF行为健康安全过滤器在精神健康危机检测中表现出色，相比NVIDIA NeMo和OpenAI Omni Moderation Latest具有更高的敏感性和平衡性能，能有效减少漏检危机情况。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理精神健康紧急情况时经常提供有害或不适当的建议，需要开发专门的安全过滤器来防止破坏性行为。

Method: 使用Verily精神健康危机数据集（1800条模拟消息）和NVIDIA Aegis AI内容安全数据集子集（794条精神健康相关消息），通过临床医生标注进行评估，并与NVIDIA NeMo和OpenAI Omni Moderation Latest进行比较分析。

Result: VBHSF在Verily数据集上表现出高敏感性（0.990）和特异性（0.992），F1分数0.939；在NVIDIA数据集上保持高敏感性（0.982）和准确性（0.921）。相比其他过滤器，VBHSF在所有情况下敏感性显著更高（p<0.001）。

Conclusion: VBHSF展示了稳健、可推广的性能，优先考虑敏感性以最小化漏检危机，这是医疗应用的关键特性。

Abstract: Large language models often mishandle psychiatric emergencies, offering
harmful or inappropriate advice and enabling destructive behaviors. This study
evaluated the Verily behavioral health safety filter (VBHSF) on two datasets:
the Verily Mental Health Crisis Dataset containing 1,800 simulated messages and
the NVIDIA Aegis AI Content Safety Dataset subsetted to 794 mental
health-related messages. The two datasets were clinician-labelled and we
evaluated performance using the clinician labels. Additionally, we carried out
comparative performance analyses against two open source, content moderation
guardrails: OpenAI Omni Moderation Latest and NVIDIA NeMo Guardrails. The VBHSF
demonstrated, well-balanced performance on the Verily Mental Health Crisis
Dataset v1.0, achieving high sensitivity (0.990) and specificity (0.992) in
detecting any mental health crises. It achieved an F1-score of 0.939,
sensitivity ranged from 0.917-0.992, and specificity was >= 0.978 in
identifying specific crisis categories. When evaluated against the NVIDIA Aegis
AI Content Safety Dataset 2.0, VBHSF performance remained highly sensitive
(0.982) and accuracy (0.921) with reduced specificity (0.859). When compared
with the NVIDIA NeMo and OpenAI Omni Moderation Latest guardrails, the VBHSF
demonstrated superior performance metrics across both datasets, achieving
significantly higher sensitivity in all cases (all p < 0.001) and higher
specificity relative to NVIDIA NeMo (p < 0.001), but not to OpenAI Omni
Moderation Latest (p = 0.094). NVIDIA NeMo and OpenAI Omni Moderation Latest
exhibited inconsistent performance across specific crisis types, with
sensitivity for some categories falling below 0.10. Overall, the VBHSF
demonstrated robust, generalizable performance that prioritizes sensitivity to
minimize missed crises, a crucial feature for healthcare applications.

</details>


### [23] [Deep Associations, High Creativity: A Simple yet Effective Metric for Evaluating Large Language Models](https://arxiv.org/abs/2510.12110)
*Ziliang Qiu,Renfen Hu*

Main category: cs.CL

TL;DR: 提出了PACE方法，通过让LLMs生成平行关联链来评估其创造力，该方法能有效避免数据污染问题，且与人类评估结果高度相关。


<details>
  <summary>Details</summary>
Motivation: LLMs创造力评估面临数据污染和人工评估成本高的问题，受人类创造力评估启发，需要开发更高效可靠的评估方法。

Method: 提出PACE方法，让LLMs生成平行关联链来评估其创造力，该方法简单高效且能最小化数据污染风险。

Result: PACE与Chatbot Arena创意写作排名高度相关（Spearman's ρ=0.739, p<0.001）；高性能LLMs达到平均人类水平，但专业人士仍优于LLMs；语言分析显示人类和LLMs关联具体性均下降，但人类关联模式更多样。

Conclusion: PACE是评估LLMs创造力的有效工具，LLMs在关联创造力方面已接近平均人类水平，但在多样性和专业性方面仍有差距。

Abstract: The evaluation of LLMs' creativity represents a crucial research domain,
though challenges such as data contamination and costly human assessments often
impede progress. Drawing inspiration from human creativity assessment, we
propose PACE, asking LLMs to generate Parallel Association Chains to Evaluate
their creativity. PACE minimizes the risk of data contamination and offers a
straightforward, highly efficient evaluation, as evidenced by its strong
correlation with Chatbot Arena Creative Writing rankings (Spearman's $\rho =
0.739$, $p < 0.001$) across various proprietary and open-source models. A
comparative analysis of associative creativity between LLMs and humans reveals
that while high-performing LLMs achieve scores comparable to average human
performance, professional humans consistently outperform LLMs. Furthermore,
linguistic analysis reveals that both humans and LLMs exhibit a trend of
decreasing concreteness in their associations, and humans demonstrating a
greater diversity of associative patterns.

</details>


### [24] [Tracing Multilingual Knowledge Acquisition Dynamics in Domain Adaptation: A Case Study of English-Japanese Biomedical Adaptation](https://arxiv.org/abs/2510.12115)
*Xin Zhao,Naoki Yoshinaga,Yuma Tsuta,Akiko Aizawa*

Main category: cs.CL

TL;DR: 本文研究了多语言领域适应中LLMs的学习动态，提出了AdaXEval评估方法，发现即使使用高质量双语语料库，跨语言知识转移仍然具有挑战性。


<details>
  <summary>Details</summary>
Motivation: 多语言领域适应中，领域知识如何在语言内学习和跨语言转移的机制尚未充分探索，导致在低资源设置下性能不佳。

Method: 提出AdaXEval自适应评估方法，从训练使用的同一双语领域语料库构建多项选择QA数据集，通过持续训练跟踪LLMs如何获取领域事实。

Result: 实验表明，尽管使用高质量双语语料库，跨语言转移仍然具有挑战性。

Conclusion: 需要更深入理解多语言知识获取机制以改进跨语言领域适应性能。

Abstract: Multilingual domain adaptation (ML-DA) is widely used to learn new domain
knowledge across languages into large language models (LLMs). Although many
methods have been proposed to improve domain adaptation, the mechanisms of
multilingual knowledge acquisition, how domain knowledge is learned within a
language and transferred across languages, remain underexplored. This gap leads
to suboptimal performance, particularly in low-resource settings. This work
examines the learning dynamics of LLMs during ML-DA. Because prior ML-DA
studies often train and evaluate on datasets with mismatched knowledge
coverage, we propose AdaXEval, an adaptive evaluation method that builds
multiple-choice QA datasets from the same bilingual domain corpus used for
training, thereby directly studying multilingual knowledge acquisition. Through
continual training of LLMs with diverse data recipes, we track how LLMs acquire
domain facts and pinpoint the mechanism behind the transformation process from
domain training data to knowledge. Our experiments on a 13B English-Japanese
bilingual LLM reveal that cross-lingual transfer remains challenging despite a
high-quality bilingual corpus. The code has been released.

</details>


### [25] [Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models](https://arxiv.org/abs/2510.12116)
*Bajian Xiang,Shuaijiang Zhao,Tingwei Guo,Wei Zou*

Main category: cs.CL

TL;DR: 本文系统分析了大型语音语言模型中的模态差距问题，发现语音和文本输入在深层表示中方向对齐但幅度分离，并提出基于对齐路径评分和关键token干预的改进方法。


<details>
  <summary>Details</summary>
Motivation: 端到端大型语音语言模型在对话生成方面表现出色，但在语义理解基准测试中始终落后于传统流水线系统，存在明显的模态差距问题。

Method: 通过系统实验分析语音和文本表示的粗粒度和细粒度特征，引入对齐路径评分量化token级对齐质量，并设计角度投影和长度归一化等干预策略。

Result: 研究发现语音和文本表示在深层网络中方向相似度增加但欧氏距离扩大，表示相似度与模态差距强相关，提出的干预策略显示出改善语音输入正确性的潜力。

Conclusion: 本研究首次系统实证分析了LSLMs中的模态差距和对齐机制，为未来优化提供了理论和方法的指导。

Abstract: End-to-end Large Speech Language Models (LSLMs) have demonstrated impressive
conversational generation abilities, yet consistently fall short of traditional
pipeline systems on semantic understanding benchmarks. In this work, we reveal
through systematic experimentation that although LSLMs lose some text input
performance after speech-text alignment training, the performance gap between
speech and text inputs is more pronounced, which we refer to as the modality
gap. To understand this gap, we analyze both coarse- and fine-grained text and
speech representations. At the coarse-grained level, representations of speech
and text in deeper layers are found to be increasingly aligned in direction
(cosine similarity), while concurrently diverging in magnitude (Euclidean
distance). We further find that representation similarity is strongly
correlated with the modality gap. At the fine-grained level, a spontaneous
token-level alignment pattern between text and speech representations is
observed. Based on this, we introduce the Alignment Path Score to quantify
token-level alignment quality, which exhibits stronger correlation with the
modality gap. Building on these insights, we design targeted interventions on
critical tokens through angle projection and length normalization. These
strategies demonstrate the potential to improve correctness for speech inputs.
Our study provides the first systematic empirical analysis of the modality gap
and alignment mechanisms in LSLMs, offering both theoretical and methodological
guidance for future optimization.

</details>


### [26] [SafeMT: Multi-turn Safety for Multimodal Language Models](https://arxiv.org/abs/2510.12133)
*Han Zhu,Juntao Dai,Jiaming Ji,Haoran Li,Chengkun Cai,Pengcheng Wen,Chi-Min Chan,Boyuan Chen,Yaodong Yang,Sirui Han,Yike Guo*

Main category: cs.CL

TL;DR: SafeMT是一个多轮对话安全基准，包含1万个样本，涵盖17种场景和4种越狱方法，用于评估多模态大语言模型在多轮对话中的安全性。研究发现随着对话轮次增加，攻击成功率上升，表明现有模型的安全机制不足。作者提出了一个对话安全调节器来检测恶意意图并提供安全策略。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型的广泛使用，安全问题日益突出。多轮对话在日常交互中更常见，但比单轮提示风险更大，现有基准未能充分考虑这种情况。

Method: 引入SafeMT基准，包含从有害查询生成的变长对话，共1万个样本，涵盖17种场景和4种越狱方法。提出安全指数(SI)来评估对话期间模型的一般安全性。评估了17个模型，并提出了一个对话安全调节器来检测对话中隐藏的恶意意图。

Result: 研究发现，随着有害对话轮次增加，对这些模型的成功攻击风险增加，表明模型的安全机制不足以识别对话交互中的危险。实验结果表明，提出的调节器在降低多轮攻击成功率方面比现有防护模型更有效。

Conclusion: 多模态大语言模型在多轮对话中的安全机制存在不足，需要专门的防护措施。提出的对话安全调节器能有效检测恶意意图并提供安全策略，在降低多轮攻击成功率方面优于现有方法。

Abstract: With the widespread use of multi-modal Large Language models (MLLMs), safety
issues have become a growing concern. Multi-turn dialogues, which are more
common in everyday interactions, pose a greater risk than single prompts;
however, existing benchmarks do not adequately consider this situation. To
encourage the community to focus on the safety issues of these models in
multi-turn dialogues, we introduce SafeMT, a benchmark that features dialogues
of varying lengths generated from harmful queries accompanied by images. This
benchmark consists of 10,000 samples in total, encompassing 17 different
scenarios and four jailbreak methods. Additionally, we propose Safety Index
(SI) to evaluate the general safety of MLLMs during conversations. We assess
the safety of 17 models using this benchmark and discover that the risk of
successful attacks on these models increases as the number of turns in harmful
dialogues rises. This observation indicates that the safety mechanisms of these
models are inadequate for recognizing the hazard in dialogue interactions. We
propose a dialogue safety moderator capable of detecting malicious intent
concealed within conversations and providing MLLMs with relevant safety
policies. Experimental results from several open-source models indicate that
this moderator is more effective in reducing multi-turn ASR compared to existed
guard models.

</details>


### [27] [Credal Transformer: A Principled Approach for Quantifying and Mitigating Hallucinations in Large Language Models](https://arxiv.org/abs/2510.12137)
*Shihao Ji,Zihui Song,Jiajie Huang*

Main category: cs.CL

TL;DR: Credal Transformer通过引入基于证据理论的Credal注意力机制，用概率分布集合替代单一注意力向量，直接量化模型不确定性，显著减少LLM的幻觉和自信错误。


<details>
  <summary>Details</summary>
Motivation: Transformer的Softmax函数导致"人工确定性"问题，将模糊的注意力分数压缩为单一概率分布，丢弃了每层的不确定性信息，这是LLM产生幻觉的根本原因。

Method: 用Credal注意力机制替代标准注意力，将注意力分数重新概念化为Dirichlet分布的证据质量，产生"credal set"（概率分布集合），集合大小直接衡量模型不确定性。

Result: Credal Transformer能够识别分布外输入、量化模糊性，并在无法回答的问题上通过弃权显著减少自信错误。

Conclusion: Credal Transformer提供了一种新的架构来缓解幻觉，并将不确定性量化直接集成到模型中，为更可靠的AI奠定了基础。

Abstract: Large Language Models (LLMs) hallucinate, generating factually incorrect yet
confident assertions. We argue this stems from the Transformer's Softmax
function, which creates "Artificial Certainty" by collapsing ambiguous
attention scores into a single probability distribution, discarding uncertainty
information at each layer. To fix this, we introduce the Credal Transformer,
which replaces standard attention with a Credal Attention Mechanism (CAM) based
on evidential theory. CAM produces a "credal set" (a set of distributions)
instead of a single attention vector, with the set's size directly measuring
model uncertainty. We implement this by re-conceptualizing attention scores as
evidence masses for a Dirichlet distribution: sufficient evidence recovers
standard attention, while insufficient evidence yields a diffuse distribution,
representing ambiguity. Empirically, the Credal Transformer identifies
out-of-distribution inputs, quantifies ambiguity, and significantly reduces
confident errors on unanswerable questions by abstaining. Our contribution is a
new architecture to mitigate hallucinations and a design paradigm that
integrates uncertainty quantification directly into the model, providing a
foundation for more reliable AI.

</details>


### [28] [A Survey on Parallel Reasoning](https://arxiv.org/abs/2510.12164)
*Ziqi Wang,Boye Niu,Zipeng Gao,Zhi Zheng,Tong Xu,Linghui Meng,Zhongli Li,Jing Liu,Yilong Chen,Chen Zhu,Hua Wu,Haifeng Wang,Enhong Chen*

Main category: cs.CL

TL;DR: 本文对并行推理这一新兴推理范式进行了系统性综述，定义了并行推理的概念，建立了分类体系，讨论了技术方法、应用场景、核心挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力的提升，并行推理作为一种新的推理范式出现，能够通过同时探索多条思路来增强推理的鲁棒性，克服传统顺序方法的脆弱性并提升实际性能。

Method: 首先给出并行推理的形式化定义，明确其与相关概念的区别；然后基于新的分类法组织讨论先进技术，包括非交互式推理、交互式推理和效率导向的解码策略；同时探讨各种应用场景。

Result: 建立了并行推理的系统性分类框架，涵盖了技术方法、应用场景和挑战分析，为初学者提供了路线图并鼓励更多相关研究。

Conclusion: 并行推理是增强大语言模型推理鲁棒性的重要范式，本文通过系统性综述为其发展提供了理论基础和研究方向，有助于推动该领域的进一步发展。

Abstract: With the increasing capabilities of Large Language Models (LLMs), parallel
reasoning has emerged as a new inference paradigm that enhances reasoning
robustness by concurrently exploring multiple lines of thought before
converging on a final answer. It has become a significant trend to explore
parallel reasoning to overcome the fragility of standard sequential methods and
improve practical performance. In this paper, we aim to survey and summarize
the progress and challenges of parallel reasoning. We first present a formal
definition of parallel reasoning and clarify its distinction from related
concepts like Chain-of-Thought. Then, we organize and discuss advanced
techniques based on a novel taxonomy, including non-interactive reasoning,
interactive reasoning, and efficiency-focused decoding strategies.
Additionally, we explore various application scenarios, such as solving complex
problems and enhancing the reliability of LLM outputs.Finally, we highlight the
core challenges of parallel reasoning and suggest potential directions for
future research. We hope that our work can provide a useful roadmap for
beginners and encourage more research on improving parallel reasoning methods.
Related source can be avaliable in
https://github.com/PPPP-kaqiu/Awesome-Parallel-Reasoning.

</details>


### [29] [Towards Inference-time Scaling for Continuous Space Reasoning](https://arxiv.org/abs/2510.12167)
*Minghan Wang,Thuy-Trang Vu,Ehsan Shareghi,Gholamreza Haffari*

Main category: cs.CL

TL;DR: 本文探讨了将离散空间中的推理时间缩放技术（通过多样本生成和PRM/ORM重排序）应用于连续空间推理的可行性，发现虽然能够生成多样化的推理路径，但在连续空间中实现有效重排序面临独特挑战，主要原因是连续思维表示缺乏关键的归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 研究动机是将已在文本推理中证明有效的推理时间缩放技术（多样本生成加PRM/ORM重排序）扩展到连续空间推理中，验证这些技术在连续空间中的适用性。

Method: 使用COCONUT连续空间推理LM作为骨干，通过dropout-based采样生成多样化推理路径，采用Pass@N分析评估性能，并探究几何属性和轨迹动态等特征来理解重排序的挑战。

Result: 研究表明在连续空间中生成多样化推理路径是可行的，但离散空间中的数据生成和PRM/ORM训练方法在连续空间中只能带来边际改进，无法有效区分正确和错误推理。

Conclusion: 当前连续推理LM的训练框架不仅需要优化准确性，还应明确纳入可在推理时用于区分正确和错误思维的归纳偏置，这是实现有效重排序的关键。

Abstract: Inference-time scaling through multiple sample generation in combination with
Process- or Outcome-Reward Model (PRM or ORM) re-ranking has proven effective
for text-based reasoning in large language models. This paper investigates
whether such established techniques can be successfully adapted to reasoning in
the continuous space, using COCONUT (Hao et al. 2024) continuous space
reasoning LM as the backbone. We demonstrate the feasibility of generating
diverse reasoning paths through dropout-based sampling. Our Pass@N analysis on
the generated samples reveals the potential that could enable a significant
gain in performance akin to observed gain in the discrete space. However, we
highlight unique challenges faced for materializing this gain in the continuous
thought space. In particular, working recipes for data generation and training
PRM and ORM models in the discrete space unlocks only marginal improvements in
the continuous space. Through probing various aspects including geometric
properties and trajectory dynamics we identify the underlying reasons that
prevent effective discrimination between correct and incorrect reasoning
(essential for the functioning of PRM and ORM). Our findings reveal that
current limitations stem from the absence of key inductive biases in continuous
thought representations. We argue that the training frameworks for continuous
reasoning LMs require not only to optimize for accuracy but also to explicitly
incorporate inductive biases that could be utilized during inference-time for
discrimination of correct and incorrect thoughts.\footnote{Our code and data
will be publicly available.}

</details>


### [30] [From Knowledge to Treatment: Large Language Model Assisted Biomedical Concept Representation for Drug Repurposing](https://arxiv.org/abs/2510.12181)
*Chengrui Xiang,Tengfei Ma,Xiangzheng Fu,Yiping Liu,Bosheng Song,Xiangxiang Zeng*

Main category: cs.CL

TL;DR: LLaDR是一个基于大语言模型的药物重定位框架，通过从LLMs提取生物医学实体的语义丰富文本表示来增强知识图谱嵌入，从而改进生物医学概念表示并提升药物重定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了真实实验室中的常识性生物医学概念知识，如某些药物与特定治疗根本不相容的机制先验知识，导致生物医学知识图谱表示不足。

Method: 从大语言模型中提取生物医学实体的语义丰富治疗相关文本表示，并用这些表示来微调知识图谱嵌入模型，将治疗相关知识注入KGE中。

Result: 在基准测试中，LLaDR在不同场景下实现了最先进的性能，阿尔茨海默病的案例研究进一步证实了其鲁棒性和有效性。

Conclusion: LLaDR通过注入LLMs的治疗相关知识，显著改善了生物医学概念的表示，增强了对研究不足或复杂适应症的语义理解，为药物重定位提供了更有效的解决方案。

Abstract: Drug repurposing plays a critical role in accelerating treatment discovery,
especially for complex and rare diseases. Biomedical knowledge graphs (KGs),
which encode rich clinical associations, have been widely adopted to support
this task. However, existing methods largely overlook common-sense biomedical
concept knowledge in real-world labs, such as mechanistic priors indicating
that certain drugs are fundamentally incompatible with specific treatments. To
address this gap, we propose LLaDR, a Large Language Model-assisted framework
for Drug Repurposing, which improves the representation of biomedical concepts
within KGs. Specifically, we extract semantically enriched treatment-related
textual representations of biomedical entities from large language models
(LLMs) and use them to fine-tune knowledge graph embedding (KGE) models. By
injecting treatment-relevant knowledge into KGE, LLaDR largely improves the
representation of biomedical concepts, enhancing semantic understanding of
under-studied or complex indications. Experiments based on benchmarks
demonstrate that LLaDR achieves state-of-the-art performance across different
scenarios, with case studies on Alzheimer's disease further confirming its
robustness and effectiveness. Code is available at
https://github.com/xiaomingaaa/LLaDR.

</details>


### [31] [Not in Sync: Unveiling Temporal Bias in Audio Chat Models](https://arxiv.org/abs/2510.12185)
*Jiayu Yao,Shenghua Liu,Yiwei Wang,Rundong Cheng,Lingrui Mei,Baolong Bi,Zhen Xiong,Xueqi Cheng*

Main category: cs.CL

TL;DR: 该论文首次系统研究了大型音频语言模型中的时间偏差问题，发现模型在预测事件时间戳时存在系统性偏差，且偏差随音频长度增加而累积。


<details>
  <summary>Details</summary>
Motivation: 大型音频语言模型在音频理解和多模态推理中应用日益广泛，但其定位事件发生时间的能力尚未得到充分探索，特别是在时间戳预测方面存在明显局限性。

Method: 通过在带时间戳的数据集上进行受控实验，量化时间偏差现象，并提出了时间偏差指数（TBI）来测量预测事件时间与真实时间的系统性错位，同时开发了可视化框架。

Result: 研究发现时间偏差（i）在数据集和模型中普遍存在，（ii）随音频长度增加而增加，在长录音中可累积达数十秒，（iii）在不同事件类型和位置间存在差异。

Conclusion: 当前大型音频语言模型存在基本的时间定位限制，需要开发具有时间鲁棒性的架构来解决这一问题。

Abstract: Large Audio Language Models (LALMs) are increasingly applied to audio
understanding and multimodal reasoning, yet their ability to locate when events
occur remains underexplored. We present the first systematic study of temporal
bias in LALMs, revealing a key limitation in their timestamp prediction. For
example, when asked "At which second does the lecturer introduce the key
formula?", models often predict timestamps that are consistently earlier or
later than the ground truth. Through controlled experiments on timestamped
datasets, we find that temporal bias (i) is prevalent across datasets and
models, (ii) increases with audio length - even accumulating to tens of seconds
in extended recordings, and (iii) varies across event types and positions. We
quantify this effect with the Temporal Bias Index (TBI), measuring systematic
misalignment in predicted event timings, and complement it with a visualization
framework. Our findings highlight a fundamental limitation in current LALMs and
call for the development of temporally robust architectures.

</details>


### [32] [DPO-Tuned Large Language Models for Segmentation in Simultaneous Speech Translation](https://arxiv.org/abs/2510.12195)
*Zeyu Yang,Satoshi Nakamura*

Main category: cs.CL

TL;DR: 提出基于大语言模型和直接偏好优化的语音翻译分割框架，在三个语言对上超越现有SHAS模型，提升分割准确率、翻译质量和延迟表现


<details>
  <summary>Details</summary>
Motivation: 现有语音翻译分割模型如SHAS虽然比启发式方法更鲁棒，但仍受限于监督学习目标，缺乏人类偏好对齐，而这对自然实时翻译至关重要

Method: 使用直接偏好优化训练大语言模型进行分割预测，在ACL 60/60语料库上评估，以SeamlessM4T v2为翻译骨干，与IWSLT基线直接比较

Result: 在英-日、英-中、英-德三个语言对上，DPO调优的LLM比SHAS获得更高分割准确率，翻译质量（BLEU、COMET）和延迟（平均滞后）均有持续改进

Conclusion: 偏好调优的大语言模型有潜力超越现有预训练分割模型，推进自适应、人类对齐的同步口译技术

Abstract: Simultaneous speech translation requires accurate segmentation to balance
translation quality and latency. Recent studies such as SHAS have introduced
pretrained segmentation models, achieving stronger performance than heuristic
rules. However, segmentation models such as SHAS, though pretrained and more
robust than heuristic methods, are still constrained by supervised learning
objectives and do not incorporate human preference alignment, which is crucial
for natural real-time interpretation. In this work, we propose a segmentation
framework based on large language models (LLMs) trained with Direct Preference
Optimization (DPO). By leveraging preference alignment, our method enables LLMs
to predict natural segmentation points that better meet the demands of
real-time translation. We evaluate the system on the ACL 60/60 corpus across
three language pairs (English-Japanese, Chinese, German), using SeamlessM4T v2
as the translation backbone. Experimental results show that our DPO-tuned LLM
achieves higher segmentation accuracy than SHAS and yields consistent
improvements in translation quality (BLEU, COMET) as well as latency (Average
Lagging). Furthermore, our system benefits from IWSLT baselines for direct
comparison. These findings highlight the potential of preference-tuned LLMs to
surpass existing pretrained segmentation models and advance adaptive,
human-aligned simultaneous interpretation.

</details>


### [33] [HALF: Harm-Aware LLM Fairness Evaluation Aligned with Deployment](https://arxiv.org/abs/2510.12217)
*Ali Mekky,Omar El Herraoui,Preslav Nakov,Yuxia Wang*

Main category: cs.CL

TL;DR: 提出了HALF框架，这是一个基于实际应用场景和危害严重性的LLM公平性评估框架，将9个应用领域分为三个危害等级，评估结果显示LLM在不同领域的公平性表现不一致。


<details>
  <summary>Details</summary>
Motivation: 现有LLM公平性评估缺乏真实场景基础，且未考虑不同领域危害严重性的差异，例如手术决策中的偏见与文本摘要中的风格偏见不应同等对待。

Method: HALF框架通过五阶段流程，将九个应用领域组织为严重、中等、轻微三个危害等级，在真实应用场景中评估模型偏见，并按危害严重性加权结果。

Result: 对八个LLM的评估显示：(1) LLM在不同领域的公平性表现不一致；(2) 模型大小或性能不能保证公平性；(3) 推理模型在医疗决策支持中表现更好，但在教育领域表现更差。

Conclusion: HALF揭示了先前基准测试成功与部署准备度之间存在明显差距，强调需要基于实际应用场景和危害严重性的公平性评估。

Abstract: Large language models (LLMs) are increasingly deployed across high-impact
domains, from clinical decision support and legal analysis to hiring and
education, making fairness and bias evaluation before deployment critical.
However, existing evaluations lack grounding in real-world scenarios and do not
account for differences in harm severity, e.g., a biased decision in surgery
should not be weighed the same as a stylistic bias in text summarization. To
address this gap, we introduce HALF (Harm-Aware LLM Fairness), a
deployment-aligned framework that assesses model bias in realistic applications
and weighs the outcomes by harm severity. HALF organizes nine application
domains into three tiers (Severe, Moderate, Mild) using a five-stage pipeline.
Our evaluation results across eight LLMs show that (1) LLMs are not
consistently fair across domains, (2) model size or performance do not
guarantee fairness, and (3) reasoning models perform better in medical decision
support but worse in education. We conclude that HALF exposes a clear gap
between previous benchmarking success and deployment readiness.

</details>


### [34] [Analysing Moral Bias in Finetuned LLMs through Mechanistic Interpretability](https://arxiv.org/abs/2510.12229)
*Bianca Raimondi,Daniela Dalbagno,Maurizio Gabbrielli*

Main category: cs.CL

TL;DR: 研究发现微调后的LLMs会学习到Knobe效应等道德偏见，这些偏见可定位到特定层，通过激活补丁技术可在不重新训练的情况下消除偏见。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在微调过程中会内化类似人类的偏见，但偏见表现机制尚不明确，需要研究Knobe效应等道德偏见在微调LLMs中的表现和定位。

Method: 在3个开源权重LLMs上进行层补丁分析，识别偏见定位的特定层，并通过将预训练模型的激活补丁到关键层来消除偏见。

Result: 发现偏见不仅在学习过程中被习得，而且定位在特定层集合中，仅需对少数关键层进行激活补丁即可消除Knobe效应。

Conclusion: LLMs中的社会偏见可以被解释、定位和通过针对性干预缓解，无需模型重新训练。

Abstract: Large language models (LLMs) have been shown to internalize human-like biases
during finetuning, yet the mechanisms by which these biases manifest remain
unclear. In this work, we investigated whether the well-known Knobe effect, a
moral bias in intentionality judgements, emerges in finetuned LLMs and whether
it can be traced back to specific components of the model. We conducted a
Layer-Patching analysis across 3 open-weights LLMs and demonstrated that the
bias is not only learned during finetuning but also localized in a specific set
of layers. Surprisingly, we found that patching activations from the
corresponding pretrained model into just a few critical layers is sufficient to
eliminate the effect. Our findings offer new evidence that social biases in
LLMs can be interpreted, localized, and mitigated through targeted
interventions, without the need for model retraining.

</details>


### [35] [DSAS: A Universal Plug-and-Play Framework for Attention Optimization in Multi-Document Question Answering](https://arxiv.org/abs/2510.12251)
*Jiakai Li,Rongzheng Wang,Yizhuo Ma,Shuang Liang,Guangchun Luo,Ke Qin*

Main category: cs.CL

TL;DR: 提出DSAS方法解决LLM在多文档问答中的长距离依赖和"迷失在中间"问题，无需修改架构或额外训练参数，在多个基准测试中平均F1分数提升4.2%。


<details>
  <summary>Details</summary>
Motivation: LLM在多文档问答任务中存在两个主要限制：长距离依赖建模困难，无法关注长文本中的关键信息；"迷失在中间"问题，难以处理长输入中间位置的信息。现有解决方案要么截断全局依赖，要么需要昂贵的微调，缺乏通用简单的解决方案。

Method: 提出双阶段自适应锐化(DSAS)方法，包含两个模块：(1)上下文门控加权(CGW)模块，通过层级注意力跟踪和位置感知加权评估段落相关性，缓解"迷失在中间"问题；(2)互斥注意力抑制(RAS)模块，通过抑制关键文本与无关文本之间的信息交换，增强对关键段落的关注，改善长距离依赖建模。DSAS是即插即用的解决方案。

Result: 在四个基准测试上的广泛实验表明，DSAS在主流LLM（Llama、Qwen、Mistral、Deepseek）上均有效，在Llama-3.1-8B-Instruct和Qwen2.5-14B-Instruct上的多文档问答任务中平均F1分数提升4.2%。消融研究证实了CGW和RAS模块的必要贡献。

Conclusion: DSAS提供了一种无需架构修改或额外训练参数的即插即用解决方案，有效解决了LLM在多文档问答中的长距离依赖和"迷失在中间"问题，具有鲁棒性和可扩展性。

Abstract: While large language models (LLMs) show considerable promise across various
fields, they have notable limitations in handling multi-document question
answering (Multi-doc QA) tasks. The first challenge is long-range dependency
modeling, where LLMs struggle to focus on key information in long texts, which
weakens important semantic connections. Second, most LLMs suffer from the
''lost-in-the-middle'' issue, where they have difficulty processing information
in the middle of long inputs. Current solutions either truncate global
dependencies or demand costly finetuning, ultimately lacking a universal and
simple solution for these challenges. To resolve these limitations, we propose
Dual-Stage Adaptive Sharpening (DSAS) containing two modules. (i) The
Contextual Gate Weighting (CGW) module alleviates ''lost-in-the-middle'' by
assessing paragraph relevance through layer-wise attention tracking and
position-aware weighting. (ii) The Reciprocal Attention Suppression (RAS)
module enhances focus on critical paragraphs by suppressing information
exchange between key and irrelevant texts, thus mitigating the limitations in
long-range dependency modeling. Notably, DSAS functions as a plug-and-play
solution requiring no architectural modifications or extra training parameters.
Extensive experiments on four benchmarks demonstrate DSAS's efficacy across
mainstream LLMs (Llama, Qwen, Mistral, and Deepseek), with an average F1-score
improvement of 4.2% in Multi-doc QA tasks on Llama-3.1-8B-Instruct and
Qwen2.5-14B-Instruct. Ablation studies confirm the essential contributions of
both the CGW and RAS modules. In addition, detailed discussions in the Appendix
further validate the robustness and scalability of DSAS.

</details>


### [36] [Shallow Robustness, Deep Vulnerabilities: Multi-Turn Evaluation of Medical LLMs](https://arxiv.org/abs/2510.12255)
*Blazej Manczak,Eric Lin,Francisco Eiras,James O' Neill,Vaikkunth Mugunthan*

Main category: cs.CL

TL;DR: 论文提出了MedQA-Followup框架，用于系统评估医学问答中多轮对话的鲁棒性，发现LLMs在单轮问答中表现良好，但在多轮对话中准确率大幅下降，特别是面对间接上下文干扰时。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架主要针对单轮问答的理想化条件，忽视了医疗咨询中常见的冲突输入、误导性上下文和权威影响等复杂性，需要系统评估LLMs在多轮对话中的鲁棒性。

Method: 引入MedQA-Followup框架，区分浅层鲁棒性（抵抗误导性初始上下文）和深层鲁棒性（在答案被挑战时保持准确性），同时引入间接-直接轴来区分上下文框架和明确建议。在MedQA数据集上进行受控干预，评估五个最先进的LLMs。

Result: 模型在浅层扰动下表现良好，但在多轮设置中表现出严重脆弱性，Claude Sonnet 4的准确率从91.2%降至13.5%。反直觉的是，间接的基于上下文的干预通常比直接建议更有害，导致更大的准确率下降。

Conclusion: 多轮鲁棒性是医疗LLMs安全可靠部署的关键但未充分探索的维度，需要更多关注模型在多轮对话中的表现。

Abstract: Large language models (LLMs) are rapidly transitioning into medical clinical
use, yet their reliability under realistic, multi-turn interactions remains
poorly understood. Existing evaluation frameworks typically assess single-turn
question answering under idealized conditions, overlooking the complexities of
medical consultations where conflicting input, misleading context, and
authority influence are common. We introduce MedQA-Followup, a framework for
systematically evaluating multi-turn robustness in medical question answering.
Our approach distinguishes between shallow robustness (resisting misleading
initial context) and deep robustness (maintaining accuracy when answers are
challenged across turns), while also introducing an indirect-direct axis that
separates contextual framing (indirect) from explicit suggestion (direct).
Using controlled interventions on the MedQA dataset, we evaluate five
state-of-the-art LLMs and find that while models perform reasonably well under
shallow perturbations, they exhibit severe vulnerabilities in multi-turn
settings, with accuracy dropping from 91.2% to as low as 13.5% for Claude
Sonnet 4. Counterintuitively, indirect, context-based interventions are often
more harmful than direct suggestions, yielding larger accuracy drops across
models and exposing a significant vulnerability for clinical deployment.
Further compounding analyses reveal model differences, with some showing
additional performance drops under repeated interventions while others
partially recovering or even improving. These findings highlight multi-turn
robustness as a critical but underexplored dimension for safe and reliable
deployment of medical LLMs.

</details>


### [37] [Chinese ModernBERT with Whole-Word Masking](https://arxiv.org/abs/2510.12285)
*Zeyu Zhao,Ningtao Wang,Xing Fu,Yu Cheng*

Main category: cs.CL

TL;DR: Chinese ModernBERT是一个从头开始训练的中文编码器，通过优化词汇表、掩码策略、训练流程和学习率调度，在中文NLP任务上实现了与现有强模型竞争的性能，并在检索任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的编码器Transformer在架构、数据和系统方面的改进未能完全迁移到中文领域，因为中文的分词和形态结构与英文存在显著差异。

Method: 采用硬件感知的32k BPE词汇表、动态掩码课程学习、两阶段预训练流程（扩展上下文长度至8,192 tokens）以及阻尼余弦学习率调度。

Result: 在CLUE基准测试中与强中文编码器竞争，在SimCLUE测试集上达到0.505（Pearson）/0.537（Spearman）的相关性分数，超越了Qwen-0.6B-embedding模型。

Conclusion: Chinese ModernBERT证明了通过针对中文特点的优化设计，可以在保持高效性的同时实现竞争力的性能，为中文NLP任务提供了可扩展的解决方案。

Abstract: Encoder-only Transformers have advanced along three axes -- architecture,
data, and systems -- yielding Pareto gains in accuracy, speed, and memory
efficiency. Yet these improvements have not fully transferred to Chinese, where
tokenization and morphology differ markedly from English. We introduce Chinese
ModernBERT, a from-scratch Chinese encoder that couples: (i) a hardware-aware
32k BPE vocabulary tailored to frequent Chinese affixes/compounds, lowering the
embedding budget; (ii) whole-word masking (WWM) with a dynamic masking
curriculum (30% -> 15%) to align task difficulty with training progress; (iii)
a two-stage pre-training pipeline that extends the native context from 1,024 to
8,192 tokens using RoPE and alternating local/global attention; and (iv) a
damped-cosine learning-rate schedule for stable long-horizon optimization. We
pre-train on ~1.2T Chinese tokens from CCI3-HQ, CCI4 (Chinese), and
Cosmopedia-Chinese. On CLUE, Chinese ModernBERT is competitive with strong
Chinese encoders under a unified fine-tuning protocol. Under bf16 it achieves
high long-sequence throughput while maintaining strong short-sequence speed,
reflecting benefits from budget allocation and attention design. To probe
retrieval-oriented quality, we add a small amount of open contrastive data:
fine-tuning on SimCLUE (~3M pairs) improves further when adding T2Ranking
(~2M), reaching 0.505 (Pearson) / 0.537 (Spearman) on the SimCLUE test set.
Under this open-data setting, Chinese ModernBERT surpasses Qwen-0.6B-embedding
on SimCLUE, suggesting a clear scaling path for STS with additional curated
pairs. We will release tokenizer and weights to facilitate reproducible
research.

</details>


### [38] [A large-scale, unsupervised pipeline for automatic corpus annotation using LLMs: variation and change in the English consider construction](https://arxiv.org/abs/2510.12306)
*Cameron Morin,Matti Marttinen Larsson*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型的无监督流水线，用于大规模语料库的语法标注，在COHA语料库的143,933个句子上实现了98%以上的准确率。


<details>
  <summary>Details</summary>
Motivation: 自然语言语料库快速扩张，手动标注成为语料库语言学工作的主要方法瓶颈。

Method: 采用四阶段工作流程：提示工程、事前评估、自动批量处理和事后验证，使用GPT-5通过OpenAI API进行标注。

Result: 在60小时内完成了COHA语料库中143,933个句子的标注，在两个复杂标注任务上达到98%以上的准确率。

Conclusion: 大语言模型能够以最小人工干预完成大规模数据准备任务，为基于语料库的研究开辟了新可能，但实施需要考虑成本、许可和伦理问题。

Abstract: As natural language corpora expand at an unprecedented rate, manual
annotation remains a significant methodological bottleneck in corpus linguistic
work. We address this challenge by presenting a scalable, unsupervised pipeline
for automating grammatical annotation in voluminous corpora using large
language models (LLMs). Unlike previous supervised and iterative approaches,
our method employs a four-phase workflow: prompt engineering, pre-hoc
evaluation, automated batch processing, and post-hoc validation. We demonstrate
the pipeline's accessibility and effectiveness through a diachronic case study
of variation in the English consider construction. Using GPT-5 through the
OpenAI API, we annotate 143,933 sentences from the Corpus of Historical
American English (COHA) in under 60 hours, achieving 98%+ accuracy on two
sophisticated annotation procedures. Our results suggest that LLMs can perform
a range of data preparation tasks at scale with minimal human intervention,
opening new possibilities for corpus-based research, though implementation
requires attention to costs, licensing, and other ethical considerations.

</details>


### [39] [Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation](https://arxiv.org/abs/2510.12316)
*Greta Damo,Elena Cabrio,Serena Villata*

Main category: cs.CL

TL;DR: 提出基于知识增强生成(RAG)的反仇恨言论生成框架，通过整合联合国数字图书馆等权威知识库，为8个主要目标群体生成可信的反言论。


<details>
  <summary>Details</summary>
Motivation: 现有反言论生成方法存在局限性：基于大语言模型的方法生成内容可靠性不足，基于专家的方法缺乏可扩展性。需要开发既可靠又可扩展的解决方案。

Method: 将反言论生成建模为知识驱动的文本生成过程，集成RAG管道，构建包含32,792个文本的知识库（来自联合国数字图书馆、EUR-Lex和欧盟基本权利机构）。

Result: 在MultiTarget-CONAN数据集上评估，通过JudgeLM指标和人工评估均显示优于标准LLM基线和竞争方法。

Conclusion: 该框架和知识库为研究可信且合理的反言论生成开辟了新途径，不仅适用于仇恨言论，还可扩展到其他领域。

Abstract: Counter-speech generation is at the core of many expert activities, such as
fact-checking and hate speech, to counter harmful content. Yet, existing work
treats counter-speech generation as pure text generation task, mainly based on
Large Language Models or NGO experts. These approaches show severe drawbacks
due to the limited reliability and coherence in the generated countering text,
and in scalability, respectively. To close this gap, we introduce a novel
framework to model counter-speech generation as knowledge-wise text generation
process. Our framework integrates advanced Retrieval-Augmented Generation (RAG)
pipelines to ensure the generation of trustworthy counter-speech for 8 main
target groups identified in the hate speech literature, including women, people
of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons,
and other. We built a knowledge base over the United Nations Digital Library,
EUR-Lex and the EU Agency for Fundamental Rights, comprising a total of 32,792
texts. We use the MultiTarget-CONAN dataset to empirically assess the quality
of the generated counter-speech, both through standard metrics (i.e., JudgeLM)
and a human evaluation. Results show that our framework outperforms standard
LLM baselines and competitive approach, on both assessments. The resulting
framework and the knowledge base pave the way for studying trustworthy and
sound counter-speech generation, in hate speech and beyond.

</details>


### [40] [Fine-grained Analysis of Brain-LLM Alignment through Input Attribution](https://arxiv.org/abs/2510.12355)
*Michela Proietti,Roberto Capobianco,Mariya Toneva*

Main category: cs.CL

TL;DR: 该论文提出了一种细粒度的输入归因方法，用于识别对大脑-LLM对齐最重要的特定词语，并以此研究大脑对齐与下一个词预测之间的关系。研究发现两者依赖不同的词语子集：下一个词预测具有近因和首因偏见，关注语法；而大脑对齐优先考虑语义和语篇信息，具有更针对性的近因效应。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型与人类大脑活动之间的对齐可以揭示语言处理的计算原理。研究者希望解决一个有争议的研究问题：大脑对齐与下一个词预测之间的关系。

Method: 引入细粒度的输入归因方法，识别对大脑-LLM对齐最重要的特定词语，并利用该方法分析大脑对齐与下一个词预测的关系。

Result: 研究发现大脑对齐和下一个词预测依赖不同的词语子集：下一个词预测表现出近因和首因偏见，主要关注语法；大脑对齐优先考虑语义和语篇层面的信息，具有更针对性的近因效应。

Conclusion: 这项工作增进了我们对LLM如何与人类语言处理相关的理解，并突出了大脑对齐与下一个词预测在特征依赖方面的差异。所提出的归因方法可广泛应用于探索模型预测在各种语言处理任务中的认知相关性。

Abstract: Understanding the alignment between large language models (LLMs) and human
brain activity can reveal computational principles underlying language
processing. We introduce a fine-grained input attribution method to identify
the specific words most important for brain-LLM alignment, and leverage it to
study a contentious research question about brain-LLM alignment: the
relationship between brain alignment (BA) and next-word prediction (NWP). Our
findings reveal that BA and NWP rely on largely distinct word subsets: NWP
exhibits recency and primacy biases with a focus on syntax, while BA
prioritizes semantic and discourse-level information with a more targeted
recency effect. This work advances our understanding of how LLMs relate to
human language processing and highlights differences in feature reliance
between BA and NWP. Beyond this study, our attribution method can be broadly
applied to explore the cognitive relevance of model predictions in diverse
language processing tasks.

</details>


### [41] [MoBiLE: Efficient Mixture-of-Experts Inference on Consumer GPU with Mixture of Big Little Experts](https://arxiv.org/abs/2510.12357)
*Yushu Zhao,Yubin Qin,Yang Wang,Xiaolong Yang,Huiming Han,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.CL

TL;DR: MoBiLE是一个即插即用的MoE推理框架，通过混合大小专家机制，对不重要token使用一半专家加速推理，对重要token保持完整专家保证质量，在消费级GPU上实现1.6-1.72倍加速且精度损失可忽略。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型的稀疏激活机制虽然支持专家卸载策略，但受限于CPU-GPU互连带宽瓶颈。现有的预取方法训练开销大，且在细粒度专家分割的现代MoE模型中效果不佳。

Method: 提出MoBiLE框架，采用混合大小专家机制：对不重要token减少专家数量至一半以加速，对重要token保持完整专家以保证质量；设计专用的回退和预取机制在大小专家间切换以提高内存效率。

Result: 在四种典型现代MoE架构和挑战性生成任务上的评估显示，MoBiLE在消费级GPU系统上相比基线实现1.60x到1.72x的加速，精度损失可忽略不计。

Conclusion: MoBiLE有效解决了MoE推理中的带宽瓶颈问题，通过智能的专家选择机制在保持模型质量的同时显著提升推理速度，为MoE模型的实用部署提供了高效解决方案。

Abstract: Mixture-of-Experts (MoE) models have recently demonstrated exceptional
performance across a diverse range of applications. The principle of sparse
activation in MoE models facilitates an offloading strategy, wherein active
experts are maintained in GPU HBM, while inactive experts are stored in CPU
DRAM. The efficacy of this approach, however, is fundamentally constrained by
the limited bandwidth of the CPU-GPU interconnect. To mitigate this bottleneck,
existing approaches have employed prefetching to accelerate MoE inference.
These methods attempt to predict and prefetch the required experts using
specially trained modules. Nevertheless, such techniques are often encumbered
by significant training overhead and have shown diminished effectiveness on
recent MoE models with fine-grained expert segmentation.
  In this paper, we propose MoBiLE, a plug-and-play offloading-based MoE
inference framework with \textit{mixture of big-little experts}. It reduces the
number of experts for unimportant tokens to half for acceleration while
maintaining full experts for important tokens to guarantee model quality.
Further, a dedicated fallback and prefetching mechanism is designed for
switching between little and big experts to improve memory efficiency. We
evaluate MoBiLE on four typical modern MoE architectures and challenging
generative tasks. Our results show that MoBiLE achieves a speedup of 1.60x to
1.72x compared to the baseline on a consumer GPU system, with negligible
degradation in accuracy.

</details>


### [42] [LLM-REVal: Can We Trust LLM Reviewers Yet?](https://arxiv.org/abs/2510.12367)
*Rui Li,Jia-Chen Gu,Po-Nien Kung,Heming Xia,Junfeng liu,Xiangwen Kong,Zhifang Sui,Nanyun Peng*

Main category: cs.CL

TL;DR: 本研究通过模拟实验发现，当LLMs同时用于论文写作和同行评审时，会产生系统性偏见：LLM评审员会显著高估LLM撰写的论文，同时低估包含批判性陈述的人类论文，这源于LLM的语言风格偏好和对批判性陈述的厌恶。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在学术工作流程中的深度整合，其在研究和评审中的双重角色可能带来新的风险，但目前对这些风险的研究尚不充分。

Method: 通过模拟实验，设置研究代理（生成和修改论文）和评审代理（评估提交的论文），然后进行人工标注分析。

Result: 发现LLM评审与人类判断存在明显偏差：1）LLM评审员系统性高估LLM撰写的论文；2）LLM评审员持续低估包含批判性陈述的人类论文。分析显示这源于LLM的语言特征偏见和对批判性陈述的厌恶。

Conclusion: 如果不加谨慎地将LLMs部署到同行评审中，会对人类作者和学术研究带来风险和公平性问题。但另一方面，LLM指导的修改确实能提升论文质量，对早期研究人员和低质量论文有积极作用。

Abstract: The rapid advancement of large language models (LLMs) has inspired
researchers to integrate them extensively into the academic workflow,
potentially reshaping how research is practiced and reviewed. While previous
studies highlight the potential of LLMs in supporting research and peer review,
their dual roles in the academic workflow and the complex interplay between
research and review bring new risks that remain largely underexplored. In this
study, we focus on how the deep integration of LLMs into both peer-review and
research processes may influence scholarly fairness, examining the potential
risks of using LLMs as reviewers by simulation. This simulation incorporates a
research agent, which generates papers and revises, alongside a review agent,
which assesses the submissions. Based on the simulation results, we conduct
human annotations and identify pronounced misalignment between LLM-based
reviews and human judgments: (1) LLM reviewers systematically inflate scores
for LLM-authored papers, assigning them markedly higher scores than
human-authored ones; (2) LLM reviewers persistently underrate human-authored
papers with critical statements (e.g., risk, fairness), even after multiple
revisions. Our analysis reveals that these stem from two primary biases in LLM
reviewers: a linguistic feature bias favoring LLM-generated writing styles, and
an aversion toward critical statements. These results highlight the risks and
equity concerns posed to human authors and academic research if LLMs are
deployed in the peer review cycle without adequate caution. On the other hand,
revisions guided by LLM reviews yield quality gains in both LLM-based and human
evaluations, illustrating the potential of the LLMs-as-reviewers for
early-stage researchers and enhancing low-quality papers.

</details>


### [43] [Tokenization Disparities as Infrastructure Bias: How Subword Systems Create Inequities in LLM Access and Efficiency](https://arxiv.org/abs/2510.12389)
*Hailay Kidu Teklehaymanot,Wolfgang Nejdl*

Main category: cs.CL

TL;DR: 本研究对200多种语言进行大规模跨语言标记化效率评估，发现拉丁文字语言标记化效率更高，而非拉丁文字和形态复杂语言标记化成本高出3-5倍，导致计算成本增加和上下文利用效率降低。


<details>
  <summary>Details</summary>
Motivation: 标记化差异阻碍了语言多样化群体公平获取人工智能服务，需要系统量化大型语言模型中的计算不平等问题。

Method: 使用标准化实验框架，通过tiktoken库对所有语言样本进行统一标记化处理，收集标记化统计数据，包括每句标记数(TPS)和相对标记化成本(RTC)等评估指标，并以英语为基准进行比较。

Result: 跨语言分析显示显著系统性差异：拉丁文字语言标记化效率更高，非拉丁文字和形态复杂语言标记化成本显著增加，RTC比率通常高出3-5倍，导致计算成本增加和有效上下文利用减少。

Conclusion: 当前AI系统存在结构性不平等，低资源和非拉丁语言使用者面临不成比例的计算劣势。未来研究应优先开发语言感知的标记化策略和适应性的词汇构建方法，确保更具包容性和计算公平的多语言AI系统。

Abstract: Tokenization disparities pose a significant barrier to achieving equitable
access to artificial intelligence across linguistically diverse populations.
This study conducts a large-scale cross-linguistic evaluation of tokenization
efficiency in over 200 languages to systematically quantify computational
inequities in large language models (LLMs). Using a standardized experimental
framework, we applied consistent preprocessing and normalization protocols,
followed by uniform tokenization through the tiktoken library across all
language samples. Comprehensive tokenization statistics were collected using
established evaluation metrics, including Tokens Per Sentence (TPS) and
Relative Tokenization Cost (RTC), benchmarked against English baselines. Our
cross-linguistic analysis reveals substantial and systematic disparities:
Latin-script languages consistently exhibit higher tokenization efficiency,
while non-Latin and morphologically complex languages incur significantly
greater token inflation, often 3-5 times higher RTC ratios. These
inefficiencies translate into increased computational costs and reduced
effective context utilization for underrepresented languages. Overall, the
findings highlight structural inequities in current AI systems, where speakers
of low-resource and non-Latin languages face disproportionate computational
disadvantages. Future research should prioritize the development of
linguistically informed tokenization strategies and adaptive vocabulary
construction methods that incorporate typological diversity, ensuring more
inclusive and computationally equitable multilingual AI systems.

</details>


### [44] [PRoH: Dynamic Planning and Reasoning over Knowledge Hypergraphs for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.12434)
*Xiangjun Zai,Xingyu Tan,Xiaoyang Wang,Qing Liu,Xiwei Xu,Wenjie Zhang*

Main category: cs.CL

TL;DR: PRoH是一个动态规划和推理框架，通过上下文感知规划、结构化问题分解和实体加权重叠引导的推理路径检索，显著提升了知识超图在多跳问答中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识超图的检索增强生成方法存在静态检索规划、非自适应检索执行以及对超图结构和语义的浅层使用等三大局限，限制了多跳问答的有效性。

Method: PRoH包含三个核心创新：(1)上下文感知规划模块，绘制局部超图邻域以生成结构化的推理计划；(2)结构化问题分解过程，将子问题组织为动态演化的有向无环图，实现自适应多轨迹探索；(3)实体加权重叠引导的推理路径检索算法，优先选择语义连贯的超边遍历。

Result: 在多个领域的实验中，PRoH实现了最先进的性能，平均F1分数比之前的SOTA模型HyperGraphRAG高出19.73%，生成评估得分高出8.41%，同时在长距离多跳推理任务中保持强大的鲁棒性。

Conclusion: PRoH框架通过动态规划和结构化推理有效克服了现有知识超图检索方法的局限性，显著提升了多跳问答的性能和鲁棒性。

Abstract: Knowledge Hypergraphs (KHs) have recently emerged as a knowledge
representation for retrieval-augmented generation (RAG), offering a paradigm to
model multi-entity relations into a structured form. However, existing KH-based
RAG methods suffer from three major limitations: static retrieval planning,
non-adaptive retrieval execution, and superficial use of KH structure and
semantics, which constrain their ability to perform effective multi-hop
question answering. To overcome these limitations, we propose PRoH, a dynamic
Planning and Reasoning over Knowledge Hypergraphs framework. PRoH incorporates
three core innovations: (i) a context-aware planning module that sketches the
local KH neighborhood to guide structurally grounded reasoning plan generation;
(ii) a structured question decomposition process that organizes subquestions as
a dynamically evolving Directed Acyclic Graph (DAG) to enable adaptive,
multi-trajectory exploration; and (iii) an Entity-Weighted Overlap (EWO)-guided
reasoning path retrieval algorithm that prioritizes semantically coherent
hyperedge traversals. Experiments across multiple domains demonstrate that PRoH
achieves state-of-the-art performance, surpassing the prior SOTA model
HyperGraphRAG by an average of 19.73% in F1 and 8.41% in Generation Evaluation
(G-E) score, while maintaining strong robustness in long-range multi-hop
reasoning tasks.

</details>


### [45] [Probing Latent Knowledge Conflict for Faithful Retrieval-Augmented Generation](https://arxiv.org/abs/2510.12460)
*Linfeng Gao,Baolong Bi,Zheng Yuan,Le Wang,Zerui Chen,Zhimin Wei,Shenghua Liu,Qinggang Zhang,Jinsong Su*

Main category: cs.CL

TL;DR: CLEAR是一个通过隐藏状态探测和冲突感知微调来解决RAG系统中不忠实问题的框架，显著提高了准确性和上下文忠实度。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG系统存在不忠实问题，即模型响应与检索到的上下文证据相矛盾，且现有方法将LLM视为黑盒，忽视了其内部如何整合检索证据与参数化记忆的关键问题。

Method: 通过隐藏状态表示分析发现知识整合是分层的，冲突在句子层面表现为潜在信号，无关上下文在与参数知识一致时会被放大。基于此提出CLEAR框架：将上下文分解为细粒度句子级知识，使用隐藏状态探测定位冲突知识，引入冲突感知微调指导模型准确整合检索证据。

Result: 在三个基准测试上的广泛实验表明，CLEAR显著提高了准确性和上下文忠实度，在不同冲突条件下始终优于强基线方法。

Conclusion: CLEAR通过深入理解LLM内部知识整合机制，有效解决了RAG系统的忠实性问题，为提升LLM的事实准确性提供了新思路。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to
enhance the factuality of Large Language Models (LLMs). However, existing RAG
systems often suffer from an unfaithfulness issue, where the model's response
contradicts evidence from the retrieved context. Existing approaches to
improving contextual faithfulness largely rely on external interventions, such
as prompt engineering, decoding constraints, or reward-based fine-tuning. These
works treat the LLM as a black box and overlook a crucial question: how does
the LLM internally integrate retrieved evidence with its parametric memory,
particularly under knowledge conflicts? To address this gap, we conduct a
probing-based analysis of hidden-state representations in LLMs and observe
three findings: knowledge integration occurs hierarchically, conflicts manifest
as latent signals at the sentence level, and irrelevant context is often
amplified when aligned with parametric knowledge. Building on these findings,
we propose CLEAR (Conflict-Localized and Enhanced Attention for RAG), a
framework that (i) decomposes context into fine-grained sentence-level
knowledge, (ii) employs hidden-state probing to localize conflicting knowledge,
and (iii) introduces conflict-aware fine-tuning to guide the model to
accurately integrate retrieved evidence. Extensive experiments across three
benchmarks demonstrate that CLEAR substantially improves both accuracy and
contextual faithfulness, consistently outperforming strong baselines under
diverse conflict conditions. The related resources are available at
https://github.com/LinfengGao/CLEAR.

</details>


### [46] [Resource-sensitive but language-blind: Community size and not grammatical complexity better predicts the accuracy of Large Language Models in a novel Wug Test](https://arxiv.org/abs/2510.12463)
*Nikoleta Pantelidou,Evelina Leivada,Paolo Morosi*

Main category: cs.CL

TL;DR: 该研究通过多语言Wug测试评估LLMs在形态学泛化任务中的表现，发现模型能够以类似人类的准确率处理未见词汇，但其表现主要受训练数据量而非语言结构复杂度影响。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型的语言能力，特别是它们是否能像人类一样对陌生词汇进行形态学泛化，以及这种能力是受语言复杂性还是训练数据量的影响。

Method: 使用多语言版本的Wug测试，在四种语言（加泰罗尼亚语、英语、希腊语和西班牙语）上测试六个模型，并与人类表现进行比较。

Result: 模型能够以人类水平的准确率对未见词汇进行形态学泛化，但准确率模式更接近社区规模和数据可用性，而非结构复杂性。西班牙语和英语等资源丰富的语言表现更好。

Conclusion: 模型行为主要受语言资源丰富度驱动，而非对语法复杂性的敏感性，其表现仅表面类似于人类语言能力。

Abstract: The linguistic abilities of Large Language Models are a matter of ongoing
debate. This study contributes to this discussion by investigating model
performance in a morphological generalization task that involves novel words.
Using a multilingual adaptation of the Wug Test, six models were tested across
four partially unrelated languages (Catalan, English, Greek, and Spanish) and
compared with human speakers. The aim is to determine whether model accuracy
approximates human competence and whether it is shaped primarily by linguistic
complexity or by the quantity of available training data. Consistent with
previous research, the results show that the models are able to generalize
morphological processes to unseen words with human-like accuracy. However,
accuracy patterns align more closely with community size and data availability
than with structural complexity, refining earlier claims in the literature. In
particular, languages with larger speaker communities and stronger digital
representation, such as Spanish and English, revealed higher accuracy than
less-resourced ones like Catalan and Greek. Overall, our findings suggest that
model behavior is mainly driven by the richness of linguistic resources rather
than by sensitivity to grammatical complexity, reflecting a form of performance
that resembles human linguistic competence only superficially.

</details>


### [47] [SMEC: Rethinking Matryoshka Representation Learning for Retrieval Embedding Compression](https://arxiv.org/abs/2510.12474)
*Biao Zhang,Lixin Chen,Tong Liu,Bo Zheng*

Main category: cs.CL

TL;DR: 提出SMEC框架，通过SMRL方法减少训练梯度方差、ADS模块降低维度剪枝信息损失、S-XBM模块增强高低维嵌入的无监督学习，显著降低嵌入维度同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成的高维嵌入加剧了计算复杂度和存储需求，阻碍实际部署。

Method: 提出SMEC训练框架，包含SMRL方法、ADS模块和S-XBM模块。

Result: 在图像、文本和多模态数据集上实验表明SMEC实现显著维度降低同时保持性能，在BEIR数据集上相比Matryoshka-Adaptor和Search-Adaptor模型分别提升1.1和2.7个点。

Conclusion: SMEC框架有效解决了高维嵌入带来的计算和存储问题，在保持性能的同时实现了显著的维度压缩。

Abstract: Large language models (LLMs) generate high-dimensional embeddings that
capture rich semantic and syntactic information. However, high-dimensional
embeddings exacerbate computational complexity and storage requirements,
thereby hindering practical deployment. To address these challenges, we propose
a novel training framework named Sequential Matryoshka Embedding Compression
(SMEC). This framework introduces the Sequential Matryoshka Representation
Learning(SMRL) method to mitigate gradient variance during training, the
Adaptive Dimension Selection (ADS) module to reduce information degradation
during dimension pruning, and the Selectable Cross-batch Memory (S-XBM) module
to enhance unsupervised learning between high- and low-dimensional embeddings.
Experiments on image, text, and multimodal datasets demonstrate that SMEC
achieves significant dimensionality reduction while maintaining performance.
For instance, on the BEIR dataset, our approach improves the performance of
compressed LLM2Vec embeddings (256 dimensions) by 1.1 points and 2.7 points
compared to the Matryoshka-Adaptor and Search-Adaptor models, respectively.

</details>


### [48] [When Personalization Tricks Detectors: The Feature-Inversion Trap in Machine-Generated Text Detection](https://arxiv.org/abs/2510.12476)
*Lang Gao,Xuhui Li,Chenxi Wang,Mingzhe Li,Wei Liu,Zirui Song,Jinghui Zhang,Rui Yan,Preslav Nakov,Xiuying Chen*

Main category: cs.CL

TL;DR: 提出了首个个性化机器生成文本检测基准，揭示了现有检测器在个性化设置中的性能下降问题，并提出了一种预测检测器性能变化的方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型生成能力的增强，身份冒充风险增加，但之前没有研究关注个性化机器生成文本检测。

Method: 构建了包含文学和博客文本及其LLM生成模仿的基准数据集，提出了基于特征反转陷阱的检测器性能预测方法。

Result: 实验显示在个性化设置中，一些最先进的检测器性能显著下降，提出的预测方法与实际性能差距有85%的相关性。

Conclusion: 这项工作揭示了现有检测器在个性化文本检测中的局限性，为未来研究提供了重要基准和评估方法。

Abstract: Large language models (LLMs) have grown more powerful in language generation,
producing fluent text and even imitating personal style. Yet, this ability also
heightens the risk of identity impersonation. To the best of our knowledge, no
prior work has examined personalized machine-generated text (MGT) detection. In
this paper, we introduce \dataset, the first benchmark for evaluating detector
robustness in personalized settings, built from literary and blog texts paired
with their LLM-generated imitations. Our experimental results demonstrate large
performance gaps across detectors in personalized settings: some
state-of-the-art models suffer significant drops. We attribute this limitation
to the \textit{feature-inversion trap}, where features that are discriminative
in general domains become inverted and misleading when applied to personalized
text. Based on this finding, we propose \method, a simple and reliable way to
predict detector performance changes in personalized settings. \method
identifies latent directions corresponding to inverted features and constructs
probe datasets that differ primarily along these features to evaluate detector
dependence. Our experiments show that \method can accurately predict both the
direction and the magnitude of post-transfer changes, showing 85\% correlation
with the actual performance gaps. We hope that this work will encourage further
research on personalized text detection.

</details>


### [49] [BoN Appetit Team at LeWiDi-2025: Best-of-N Test-time Scaling Can Not Stomach Annotation Disagreements (Yet)](https://arxiv.org/abs/2510.12516)
*Tomas Ruiz,Siyao Peng,Barbara Plank,Carsten Schwemmer*

Main category: cs.CL

TL;DR: 本研究将测试时缩放技术从数学和编程领域转移到LeWiDi-2025任务中，用于评估标注分歧。实验发现模型平均和多数投票方法能持续提升LLM性能，但Best-of-N采样方法无效。


<details>
  <summary>Details</summary>
Motivation: 测试时缩放技术此前仅限于有可验证正确答案的领域（如数学和编程），本研究旨在探索该技术在LeWiDi任务中评估标注分歧的应用潜力。

Method: 使用三种测试时缩放方法进行实验：模型平均、多数投票和Best-of-N采样，在LeWiDi-2025任务上评估这些方法的有效性。

Result: 两个基准方法（模型平均和多数投票）在LeWiDi任务上持续提升了LLM性能，但Best-of-N方法未能产生改善效果。

Conclusion: Best-of-N方法目前无法从数学领域成功迁移到LeWiDi任务，研究分析了造成这种差距的潜在原因。

Abstract: Test-time scaling is a family of techniques to improve LLM outputs at
inference time by performing extra computation. To the best of our knowledge,
test-time scaling has been limited to domains with verifiably correct answers,
like mathematics and coding. We transfer test-time scaling to the LeWiDi-2025
tasks to evaluate annotation disagreements. We experiment with three test-time
scaling methods: two benchmark algorithms (Model Averaging and Majority
Voting), and a Best-of-N sampling method. The two benchmark methods improve LLM
performance consistently on the LeWiDi tasks, but the Best-of-N method does
not. Our experiments suggest that the Best-of-N method does not currently
transfer from mathematics to LeWiDi tasks, and we analyze potential reasons for
this gap.

</details>


### [50] [VISaGE: Understanding Visual Generics and Exceptions](https://arxiv.org/abs/2510.12548)
*Stella Frank,Emily Allaway*

Main category: cs.CL

TL;DR: VLMs在评估非典型实例时，会在实用先验（输入相关性）和语义先验（概念通用性）之间产生冲突，导致概念理解在输入不协调时退化。


<details>
  <summary>Details</summary>
Motivation: 研究VLMs在处理典型和非典型图像时，如何权衡实用先验（输入相关性）和语义先验（概念通用性）之间的冲突。

Method: 引入VISaGE评估数据集，包含典型和异常图像，通过精心平衡的实验设计分析VLMs的权衡行为。

Result: 当实用先验的协调性假设被不协调图像违反时，概念理解会退化，这种效应比语义先验在查询单个实例时更强。

Conclusion: VLMs在处理非典型实例时更依赖实用先验而非语义先验，这揭示了模型在现实世界应用中的局限性。

Abstract: While Vision Language Models (VLMs) learn conceptual representations, in the
form of generalized knowledge, during training, they are typically used to
analyze individual instances. When evaluation instances are atypical, this
paradigm results in tension between two priors in the model. The first is a
pragmatic prior that the textual and visual input are both relevant, arising
from VLM finetuning on congruent inputs; the second is a semantic prior that
the conceptual representation is generally true for instances of the category.
In order to understand how VLMs trade off these priors, we introduce a new
evaluation dataset, VISaGE, consisting of both typical and exceptional images.
In carefully balanced experiments, we show that conceptual understanding
degrades when the assumption of congruency underlying the pragmatic prior is
violated with incongruent images. This effect is stronger than the effect of
the semantic prior when querying about individual instances.

</details>


### [51] [Teaching Language Models to Faithfully Express their Uncertainty](https://arxiv.org/abs/2510.12587)
*Bryan Eikema,Evgenia Ilia,José G. C. de Souza,Chrysoula Zerva,Wilker Aziz*

Main category: cs.CL

TL;DR: FUT是一种微调方法，教LLMs忠实表达不确定性而不改变答案分布，通过将模型样本与不确定性修饰语对齐来构建训练数据，显著减少了忠实性差距。


<details>
  <summary>Details</summary>
Motivation: LLMs经常错误传达其不确定性：重复查询会产生不同答案，但生成的响应通常未加修饰或以不反映这种可变性的方式修饰，这传达了关于LLMs知识不确定状态的不忠实信息。

Method: Faithful Uncertainty Tuning (FUT)：一种微调方法，通过增强模型样本与不确定性修饰语（如'可能'或'很可能'）对齐来构建训练数据，无需额外监督。

Result: 在多个模型和数据集上的开放域问答评估显示，FUT显著减少了忠实性差距，同时保持了问答准确性并引入了最小的语义分布偏移。

Conclusion: FUT是一种简单有效的方法，可以教LLMs忠实传达不确定性。

Abstract: Large language models (LLMs) often miscommunicate their uncertainty: repeated
queries can produce divergent answers, yet generated responses are typically
unhedged or hedged in ways that do not reflect this variability. This conveys
unfaithful information about the uncertain state of the LLMs' knowledge,
creating a faithfulness gap that affects even strong LLMs. We introduce
Faithful Uncertainty Tuning (FUT): a fine-tuning approach that teaches
instruction-tuned LLMs to express uncertainty faithfully without altering their
underlying answer distribution. We construct training data by augmenting model
samples with uncertainty hedges (i.e. verbal cues such as 'possibly' or
'likely') aligned with sample consistency, requiring no supervision beyond the
model and a set of prompts. We evaluate FUT on open-domain question answering
(QA) across multiple models and datasets. Our results show that FUT
substantially reduces the faithfulness gap, while preserving QA accuracy and
introducing minimal semantic distribution shift. Further analyses demonstrate
robustness across decoding strategies, choice of hedgers, and other forms of
uncertainty expression (i.e. numerical). These findings establish FUT as a
simple and effective way to teach LLMs to communicate uncertainty faithfully.

</details>


### [52] [StyleDecipher: Robust and Explainable Detection of LLM-Generated Texts with Stylistic Analysis](https://arxiv.org/abs/2510.12608)
*Siyuan Li,Aodu Wulianghai,Xi Lin,Guangyan Li,Xiang Chen,Jun Wu,Jianhua Li*

Main category: cs.CL

TL;DR: 提出了StyleDecipher框架，通过结合离散风格指标和连续风格表示来检测机器生成文本，在多个领域实现了最先进的检测性能，并具有可解释性和跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在真实场景中面临泛化能力有限、易受改写攻击和缺乏可解释性等问题，特别是在面对风格多样性或人机混合创作时表现不佳。

Method: 使用组合特征提取器量化风格差异，联合建模离散风格指标和基于语义嵌入的连续风格表示，在统一表示空间中捕捉人类和LLM输出的风格级差异。

Result: 在五个不同领域的实验中，StyleDecipher在域内检测准确率上达到最先进水平，在跨域评估中比现有基线高出36.30%，且对对抗性扰动和混合人机内容具有鲁棒性。

Conclusion: 风格信号为区分机器生成文本提供了可解释的证据，StyleDecipher框架实现了准确、可解释且领域无关的检测，无需访问模型内部或标记片段。

Abstract: With the increasing integration of large language models (LLMs) into
open-domain writing, detecting machine-generated text has become a critical
task for ensuring content authenticity and trust. Existing approaches rely on
statistical discrepancies or model-specific heuristics to distinguish between
LLM-generated and human-written text. However, these methods struggle in
real-world scenarios due to limited generalization, vulnerability to
paraphrasing, and lack of explainability, particularly when facing stylistic
diversity or hybrid human-AI authorship. In this work, we propose
StyleDecipher, a robust and explainable detection framework that revisits
LLM-generated text detection using combined feature extractors to quantify
stylistic differences. By jointly modeling discrete stylistic indicators and
continuous stylistic representations derived from semantic embeddings,
StyleDecipher captures distinctive style-level divergences between human and
LLM outputs within a unified representation space. This framework enables
accurate, explainable, and domain-agnostic detection without requiring access
to model internals or labeled segments. Extensive experiments across five
diverse domains, including news, code, essays, reviews, and academic abstracts,
demonstrate that StyleDecipher consistently achieves state-of-the-art in-domain
accuracy. Moreover, in cross-domain evaluations, it surpasses existing
baselines by up to 36.30%, while maintaining robustness against adversarial
perturbations and mixed human-AI content. Further qualitative and quantitative
analysis confirms that stylistic signals provide explainable evidence for
distinguishing machine-generated text. Our source code can be accessed at
https://github.com/SiyuanLi00/StyleDecipher.

</details>


### [53] [ACADATA: Parallel Dataset of Academic Data for Machine Translation](https://arxiv.org/abs/2510.12621)
*Iñaki Lacunza,Javier Garcia Gilabert,Francesca De Luca Fornaciari,Javier Aula-Blasco,Aitor Gonzalez-Agirre,Maite Melero,Marta Villegas*

Main category: cs.CL

TL;DR: ACADATA是一个高质量的学术翻译平行数据集，包含150万段作者生成的段落对（ACAD-TRAIN）和6000个翻译的评估集（ACAD-BENCH）。通过在该数据集上微调LLM，在学术翻译质量上取得了显著提升，超越了专业机器翻译系统和专有模型。


<details>
  <summary>Details</summary>
Motivation: 为了解决学术翻译领域缺乏高质量数据集的问题，作者创建了ACADATA数据集，旨在推动学术领域和长上下文翻译的研究。

Method: 构建了包含96种语言方向的ACAD-TRAIN训练集和12种方向的ACAD-BENCH评估集。在两个大型语言模型（7B和2B）上使用ACAD-TRAIN进行微调，并与专业机器翻译系统、通用LLM和专有模型进行基准测试。

Result: 微调后的模型在学术翻译质量上平均提升了6.1和12.4 d-BLEU点（7B和2B模型），在从英语翻译时，长上下文翻译质量提升了24.9%。表现最佳的微调模型在学术翻译领域超越了最好的专有和开源模型。

Conclusion: ACADATA数据集和微调模型为学术领域和长上下文翻译研究提供了宝贵资源，证明了在高质量学术数据上微调可以显著提升翻译性能。

Abstract: We present ACADATA, a high-quality parallel dataset for academic translation,
that consists of two subsets: ACAD-TRAIN, which contains approximately 1.5
million author-generated paragraph pairs across 96 language directions and
ACAD-BENCH, a curated evaluation set of almost 6,000 translations covering 12
directions. To validate its utility, we fine-tune two Large Language Models
(LLMs) on ACAD-TRAIN and benchmark them on ACAD-BENCH against specialized
machine-translation systems, general-purpose, open-weight LLMs, and several
large-scale proprietary models. Experimental results demonstrate that
fine-tuning on ACAD-TRAIN leads to improvements in academic translation quality
by +6.1 and +12.4 d-BLEU points on average for 7B and 2B models respectively,
while also improving long-context translation in a general domain by up to
24.9% when translating out of English. The fine-tuned top-performing model
surpasses the best propietary and open-weight models on academic translation
domain. By releasing ACAD-TRAIN, ACAD-BENCH and the fine-tuned models, we
provide the community with a valuable resource to advance research in academic
domain and long-context translation.

</details>


### [54] [COSTAR-A: A prompting framework for enhancing Large Language Model performance on Point-of-View questions](https://arxiv.org/abs/2510.12637)
*Nzubechukwu C. Ohalete,Kevin B. Gittner,Lauren M. Matheny*

Main category: cs.CL

TL;DR: COSTAR-A是一个改进的提示工程框架，在原有COSTAR方法基础上增加了'Answer'组件，能够提升较小本地优化语言模型的输出结构和决策性，特别是在资源受限硬件上的AI部署中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有COSTAR提示框架虽然能提高大型语言模型的输出质量和一致性，但在较小的本地优化模型上表现不稳定，特别是在需要更指令性或约束性输出的任务中。

Method: 通过一系列受控的提示-输出评估，使用参数不超过80亿的微调模型，比较COSTAR和COSTAR-A框架在不同任务中的表现。

Result: COSTAR-A能够提升本地化语言模型的输出结构和决策性，但效果因模型和用例而异。Llama 3.1-8B模型在使用COSTAR-A时表现出性能提升。

Conclusion: COSTAR-A作为一个提示框架具有适应性和可扩展性，特别适用于计算效率高的AI部署和资源受限硬件环境。

Abstract: Large Language Models (LLMs) are highly sensitive to prompt design, and
making optimized prompting techniques is crucial for generating consistent,
high-quality outputs. In this study, we introduce COSTAR-A, a novel prompt
engineering framework that enhances the existing COSTAR method, which stands
for Context, Objective, Style, Tone, Audience, and Response, by adding the
'Answer' component at the end. We demonstrate that while the original COSTAR
framework improves prompt clarity and aligns outputs for larger LLMs, its
performance is less consistent with smaller, locally optimized models,
particularly in tasks that require more directive or constrained outputs.
Through a series of controlled prompt-output assessments with smaller (at most
8 billion parameters), fine-tuned models, we found that COSTAR-A can enhance
the output structure and decisiveness of localized LLMs for certain tasks,
although its effectiveness varies across models and use cases. Notably, the
Llama 3.1-8B model exhibited performance improvements when prompted with
COSTAR-A compared to COSTAR alone. These findings emphasize the adaptability
and scalability of COSTAR-A as a prompting framework, particularly in
computationally efficient AI deployments on resource-constrained hardware.

</details>


### [55] [Reasoning Pattern Matters: Learning to Reason without Human Rationales](https://arxiv.org/abs/2510.12643)
*Chaoxu Pang,Yixuan Cao,Ping Luo*

Main category: cs.CL

TL;DR: 本文提出PARO框架，通过让LLM学习任务特定的推理模式来自动生成标注，无需人工标注推理轨迹，在模式化推理任务中可达到与10倍规模人工标注相当的SFT+RLVR性能。


<details>
  <summary>Details</summary>
Motivation: 当前SFT+RLVR范式需要昂贵的人工标注推理轨迹，本文旨在研究如何在保持推理性能的同时大幅降低标注成本。

Method: 提出PARO框架，识别模式化推理任务中固定的推理模式，让LLM基于这些模式自动生成对齐的推理轨迹，无需人工标注。

Result: 实验表明PARO生成的推理轨迹在SFT+RLVR中能达到与10倍规模人工标注相当的性能。

Conclusion: 在模式化推理任务中，大规模人工标注可被仅需有限人工监督的LLM自动标注所替代，推理模式而非标注数量或质量是性能的关键决定因素。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning
capabilities under the widely adopted SFT+RLVR paradigm, which first performs
Supervised Fine-Tuning (SFT) on human-annotated reasoning trajectories
(rationales) to establish initial reasoning behaviors, then applies
Reinforcement Learning with Verifiable Rewards (RLVR) to optimize the model
using verifiable signals without golden rationales. However, annotating
high-quality rationales for the SFT stage remains prohibitively expensive. This
paper investigates when and how rationale annotation costs can be substantially
reduced without compromising reasoning performance. We identify a broad class
of problems, termed patterned reasoning tasks, where reasoning follows a fixed,
procedural strategy consistent across instances. Although instances vary in
content such as domain knowledge, factual information, or numeric values, the
solution derives from applying a shared reasoning pattern. We argue that the
success of SFT+RLVR on such tasks primarily stems from its ability to enable
models to internalize these reasoning patterns. Using numerical semantic
matching as a representative task, we provide both causal and behavioral
evidence showing that reasoning patterns rather than the quantity or quality of
rationales are the key determinant of performance. Building on these insights,
we propose Pattern-Aware LLMs as Rationale AnnOtators (PARO), a simple yet
effective framework that enables LLMs to generate rationales aligned with
task-specific reasoning patterns without requiring human rationale annotations.
Experiments show that PARO-generated rationales achieve comparable SFT+RLVR
performance to human rationales that are 10 times larger. These results suggest
that large-scale human rationale annotations can be replaced with LLM-based
automatic annotations requiring only limited human supervision over reasoning
patterns.

</details>


### [56] [Generation Space Size: Understanding and Calibrating Open-Endedness of LLM Generations](https://arxiv.org/abs/2510.12699)
*Sunny Yu,Ahmad Jabbar,Robert Hawkins,Dan Jurafsky,Myra Cheng*

Main category: cs.CL

TL;DR: 本文提出了生成空间大小(GSS)概念来解决LLM在创造性任务中输出过于同质化和在事实性任务中产生多样化但错误输出的问题，并开发了GSSBench评估套件和应用方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在生成任务中存在校准问题：在创造性任务中输出过于同质化，在事实性任务中产生多样化但错误的回答。这两种失败模式都可以通过生成空间大小(GSS)概念来统一解决。

Method: 提出了GSSBench任务套件，包含具有真实GSS关系的提示对，用于评估不同指标并理解模型与期望行为的差异。发现幻觉检测指标（特别是EigenScore）优于标准多样性和不确定性量化指标。

Result: 证明了GSS的三个应用：检测提示歧义并预测澄清问题以更好地接地、解释推理模型中的过度思考与思考不足、引导模型扩展生成空间以获得高质量多样化输出。

Conclusion: GSS概念统一了LLM的两种常见失败模式，通过GSSBench评估和三种实际应用展示了其在理解和改进模型生成行为方面的价值。

Abstract: Different open-ended generation tasks require different degrees of output
diversity. However, current LLMs are often miscalibrated. They collapse to
overly homogeneous outputs for creative tasks and hallucinate diverse but
incorrect responses for factual tasks. We argue that these two failure modes
are unified by, and can both be addressed by, the notion of effective
generation space size (GSS) -- the set of semantically distinct outputs a model
considers for a prompt. We present GSSBench, a task suite of prompt pairs with
ground-truth GSS relationships to assess different metrics and understand where
models diverge from desired behavior. We find that hallucination detection
metrics, particularly EigenScore, consistently outperform standard diversity
and uncertainty quantification metrics, while using only model internals,
providing interpretable insights into a model's internal task representations.
We demonstrate three applications of GSS: (1) detecting prompt ambiguity and
predicting clarification questions for better grounding, (2) interpreting
overthinking and underthinking in reasoning models, and (3) steering models to
expand their generation space to yield high-quality and diverse outputs.

</details>


### [57] [Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception](https://arxiv.org/abs/2510.12720)
*Ziyang Ma,Ruiyang Xu,Zhenghao Xing,Yunfei Chu,Yuxuan Wang,Jinzheng He,Jin Xu,Pheng-Ann Heng,Kai Yu,Junyang Lin,Eng Siong Chng,Xie Chen*

Main category: cs.CL

TL;DR: 本文提出了Omni-Detective数据生成管道和Omni-Cloze评估基准，用于提升全模态语言模型在细粒度感知方面的能力，解决了细节与幻觉共增长的问题。


<details>
  <summary>Details</summary>
Motivation: 当前全模态语言模型在捕捉和描述细粒度细节方面的能力有限，且存在细节与幻觉共增长的问题，需要系统性的解决方案。

Method: 提出了Omni-Detective代理式数据生成管道，集成工具调用来自主生成高质量的多模态数据；基于此训练了Audio-Captioner和Omni-Captioner两个模型；设计了Omni-Cloze填空式评估基准。

Result: Audio-Captioner在MMAU和MMAR上表现最佳，超越Gemini 2.5 Flash，与Gemini 2.5 Pro相当；Omni-Captioner在VDC上达到新SOTA，在video-SALMONN 2上实现细节与幻觉的最佳平衡。

Conclusion: Omni-Detective能有效生成高质量细粒度描述，Omni-Cloze为全模态细粒度感知提供了稳定可靠的评估方法，推动了全模态语言模型在细粒度理解方面的发展。

Abstract: Fine-grained perception of multimodal information is critical for advancing
human-AI interaction. With recent progress in audio-visual technologies, Omni
Language Models (OLMs), capable of processing audio and video signals in
parallel, have emerged as a promising paradigm for achieving richer
understanding and reasoning. However, their capacity to capture and describe
fine-grained details remains limited explored. In this work, we present a
systematic and comprehensive investigation of omni detailed perception from the
perspectives of the data pipeline, models, and benchmark. We first identify an
inherent "co-growth" between detail and hallucination in current OLMs. To
address this, we propose Omni-Detective, an agentic data generation pipeline
integrating tool-calling, to autonomously produce highly detailed yet minimally
hallucinatory multimodal data. Based on the data generated with Omni-Detective,
we train two captioning models: Audio-Captioner for audio-only detailed
perception, and Omni-Captioner for audio-visual detailed perception. Under the
cascade evaluation protocol, Audio-Captioner achieves the best performance on
MMAU and MMAR among all open-source models, surpassing Gemini 2.5 Flash and
delivering performance comparable to Gemini 2.5 Pro. On existing detailed
captioning benchmarks, Omni-Captioner sets a new state-of-the-art on VDC and
achieves the best trade-off between detail and hallucination on the
video-SALMONN 2 testset. Given the absence of a dedicated benchmark for omni
detailed perception, we design Omni-Cloze, a novel cloze-style evaluation for
detailed audio, visual, and audio-visual captioning that ensures stable,
efficient, and reliable assessment. Experimental results and analysis
demonstrate the effectiveness of Omni-Detective in generating high-quality
detailed captions, as well as the superiority of Omni-Cloze in evaluating such
detailed captions.

</details>


### [58] [Which Word Orders Facilitate Length Generalization in LMs? An Investigation with GCG-Based Artificial Languages](https://arxiv.org/abs/2510.12722)
*Nadine El-Naggar,Tatsuki Kuribayashi,Ted Briscoe*

Main category: cs.CL

TL;DR: 本研究扩展了先前关于语言模型归纳偏好的工作，采用广义范畴语法构建更自然的实验语言，并关注模型对未见长句的泛化能力，发现类型学上合理的词序更容易被语言模型泛化。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要使用人工语言来探究语言模型是否偏好类型学上常见的语法属性，但这些人工语言形式化有限，无法覆盖自然语言中的某些结构（如无界依赖和轻度上下文敏感结构），且评估主要关注短句理解而非泛化能力。

Method: 采用广义范畴语法（GCG）构建人工语言，覆盖了先前被忽视的自然语言结构（如无界依赖和轻度上下文敏感结构），并设计实验重点评估语言模型对未见长句的泛化能力。

Result: 实验结果表明，类型学上合理的词序更容易被语言模型进行生产性泛化，即模型在处理未见的长句时，对类型学上常见的语法结构表现出更好的学习能力。

Conclusion: 通过使用更自然的人工语言形式和关注泛化能力的评估范式，本研究为语言模型具有类型学偏好的假设提供了更清晰的证据，表明语言模型确实倾向于学习类型学上合理的语法结构。

Abstract: Whether language models (LMs) have inductive biases that favor typologically
frequent grammatical properties over rare, implausible ones has been
investigated, typically using artificial languages (ALs) (White and Cotterell,
2021; Kuribayashi et al., 2024). In this paper, we extend these works from two
perspectives. First, we extend their context-free AL formalization by adopting
Generalized Categorial Grammar (GCG) (Wood, 2014), which allows ALs to cover
attested but previously overlooked constructions, such as unbounded dependency
and mildly context-sensitive structures. Second, our evaluation focuses more on
the generalization ability of LMs to process unseen longer test sentences.
Thus, our ALs better capture features of natural languages and our experimental
paradigm leads to clearer conclusions -- typologically plausible word orders
tend to be easier for LMs to productively generalize.

</details>


### [59] [Hey, wait a minute: on at-issue sensitivity in Language Models](https://arxiv.org/abs/2510.12740)
*Sanghee J. Kim,Kanishka Misra*

Main category: cs.CL

TL;DR: 本文提出了一种名为DGRC的新方法来评估语言模型对话的自然性，该方法通过分解、生成、重组和比较对话序列来系统测试语言模型对语篇敏感行为的表现。


<details>
  <summary>Details</summary>
Motivation: 评估语言模型对话的自然性具有挑战性，因为'自然性'的概念因人而异，且可扩展的定量指标有限。

Method: DGRC方法：(i)将对话分解为提示，(ii)使用语言模型为子部分生成延续，(iii)重组对话和延续，(iv)比较重组序列的可能性。

Result: 研究发现语言模型更倾向于继续对话中的核心内容，这种效应在指令调优模型中更为明显。当存在相关提示时，模型会减少对核心内容的偏好。

Conclusion: DGRC方法能够减轻语言模型语言分析中的偏见，并系统测试语篇敏感行为。指令调优虽然不会进一步放大这种调节，但这种模式反映了成功对话动态的特征。

Abstract: Evaluating the naturalness of dialogue in language models (LMs) is not
trivial: notions of 'naturalness' vary, and scalable quantitative metrics
remain limited. This study leverages the linguistic notion of 'at-issueness' to
assess dialogue naturalness and introduces a new method: Divide, Generate,
Recombine, and Compare (DGRC). DGRC (i) divides a dialogue as a prompt, (ii)
generates continuations for subparts using LMs, (iii) recombines the dialogue
and continuations, and (iv) compares the likelihoods of the recombined
sequences. This approach mitigates bias in linguistic analyses of LMs and
enables systematic testing of discourse-sensitive behavior. Applying DGRC, we
find that LMs prefer to continue dialogue on at-issue content, with this effect
enhanced in instruct-tuned models. They also reduce their at-issue preference
when relevant cues (e.g., "Hey, wait a minute") are present. Although
instruct-tuning does not further amplify this modulation, the pattern reflects
a hallmark of successful dialogue dynamics.

</details>


### [60] [Language Models Model Language](https://arxiv.org/abs/2510.12766)
*Łukasz Borchmann*

Main category: cs.CL

TL;DR: 该论文主张从索绪尔和乔姆斯基的理论框架转向曼扎克的实证主义语言学视角，将语言定义为所有口头和书面表达的总和，并以使用频率为主要原则，为LLMs的设计、评估和解释提供建设性指导。


<details>
  <summary>Details</summary>
Motivation: 当前基于索绪尔和乔姆斯基理论框架的LLMs语言评论往往是推测性和非建设性的，批评者质疑LLMs能否真正模拟语言，认为需要"深层结构"或"基础"来实现理想化的语言"能力"。

Method: 采用波兰著名语言学家Witold Mańczak的实证主义原则，将语言定义为所有口头和书面表达的总和，并将特定语言元素的使用频率视为语言的主要支配原则。

Result: 基于曼扎克的框架，挑战了先前对LLMs的批评，并为语言模型的设计、评估和解释提供了建设性指南。

Conclusion: 需要从理论推测转向实证主义视角，承认LLMs作为基于使用频率的语言模型的合法性，这为语言建模提供了更实用和建设性的理论基础。

Abstract: Linguistic commentary on LLMs, heavily influenced by the theoretical
frameworks of de Saussure and Chomsky, is often speculative and unproductive.
Critics challenge whether LLMs can legitimately model language, citing the need
for "deep structure" or "grounding" to achieve an idealized linguistic
"competence." We argue for a radical shift in perspective towards the
empiricist principles of Witold Ma\'nczak, a prominent general and historical
linguist. He defines language not as a "system of signs" or a "computational
system of the brain" but as the totality of all that is said and written. Above
all, he identifies frequency of use of particular language elements as
language's primary governing principle. Using his framework, we challenge prior
critiques of LLMs and provide a constructive guide for designing, evaluating,
and interpreting language models.

</details>


### [61] [Dr.LLM: Dynamic Layer Routing in LLMs](https://arxiv.org/abs/2510.12773)
*Ahmed Heakl,Martin Gubri,Salman Khan,Sangdoo Yun,Seong Joon Oh*

Main category: cs.CL

TL;DR: Dr.LLM是一个可改造的动态层路由框架，通过轻量级路由器决定跳过、执行或重复Transformer层，在保持或提高准确性的同时节省计算资源。


<details>
  <summary>Details</summary>
Motivation: 传统LLM对所有token都经过所有Transformer层，导致简单查询浪费计算资源，复杂推理又缺乏深度灵活性。现有自适应深度方法通常需要昂贵的推理时搜索、架构修改或大规模重训练，且往往以牺牲准确性为代价。

Method: 使用蒙特卡洛树搜索(MCTS)推导高质量层配置，在计算预算下保持或提高准确性。采用窗口池化实现稳定路由，焦点损失与类别平衡，以及瓶颈MLP路由器，确保在类别不平衡和长序列下的鲁棒性。

Result: 在ARC(逻辑)和DART(数学)任务上，Dr.LLM准确率提升高达+3.4%，平均每个示例节省5层。路由器可泛化到域外任务，准确率仅下降0.85%同时保持效率，比现有路由方法性能提升高达+7.7%。

Conclusion: Dr.LLM证明通过显式监督的路由器可以改造冻结的LLM，实现预算感知、准确性驱动的推理，而无需修改基础权重。

Abstract: Large Language Models (LLMs) process every token through all layers of a
transformer stack, causing wasted computation on simple queries and
insufficient flexibility for harder ones that need deeper reasoning.
Adaptive-depth methods can improve efficiency, but prior approaches rely on
costly inference-time search, architectural changes, or large-scale retraining,
and in practice often degrade accuracy despite efficiency gains. We introduce
Dr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that
equips pretrained models with lightweight per-layer routers deciding to skip,
execute, or repeat a block. Routers are trained with explicit supervision:
using Monte Carlo Tree Search (MCTS), we derive high-quality layer
configurations that preserve or improve accuracy under a compute budget. Our
design, windowed pooling for stable routing, focal loss with class balancing,
and bottleneck MLP routers, ensures robustness under class imbalance and long
sequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to
+3.4%p while saving 5 layers per example on average. Routers generalize to
out-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA,
AGIEval) with only 0.85% accuracy drop while retaining efficiency, and
outperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that
explicitly supervised routers retrofit frozen LLMs for budget-aware,
accuracy-driven inference without altering base weights.

</details>


### [62] [Cost Analysis of Human-corrected Transcription for Predominately Oral Languages](https://arxiv.org/abs/2510.12781)
*Yacouba Diarra,Nouhoum Souleymane Coulibaly,Michael Leventhal*

Main category: cs.CL

TL;DR: 该论文研究了为低资源语言创建高质量语音数据集所需的人力成本，通过对53小时班巴拉语音数据的ASR转录修正分析，发现在实验室条件下平均需要30小时人工来准确转录1小时语音，在实地条件下需要36小时。


<details>
  <summary>Details</summary>
Motivation: 为低资源语言创建语音数据集是一个关键但理解不足的挑战，特别是关于实际的人力成本。论文旨在调查为低资源、低识字率的口头语言（如班巴拉语）生产高质量标注语音数据所需的时间和复杂性。

Method: 通过为期一个月的实地研究，涉及10名母语转录员，分析了对53小时班巴拉语音数据的ASR生成转录的修正过程。

Result: 研究发现，在实验室条件下平均需要30小时人工来准确转录1小时语音数据，在实地条件下需要36小时。

Conclusion: 该研究为具有类似特征的大量语言创建NLP资源提供了基准和实用见解，揭示了为低资源语言构建语音数据集的实际人力成本。

Abstract: Creating speech datasets for low-resource languages is a critical yet poorly
understood challenge, particularly regarding the actual cost in human labor.
This paper investigates the time and complexity required to produce
high-quality annotated speech data for a subset of low-resource languages, low
literacy Predominately Oral Languages, focusing on Bambara, a Manding language
of Mali. Through a one-month field study involving ten transcribers with native
proficiency, we analyze the correction of ASR-generated transcriptions of 53
hours of Bambara voice data. We report that it takes, on average, 30 hours of
human labor to accurately transcribe one hour of speech data under laboratory
conditions and 36 hours under field conditions. The study provides a baseline
and practical insights for a large class of languages with comparable profiles
undertaking the creation of NLP resources.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [63] [Embedding the Teacher: Distilling vLLM Preferences for Scalable Image Retrieval](https://arxiv.org/abs/2510.12014)
*Eric He,Akash Gupta,Adian Liusie,Vatsal Raina,Piotr Molenda,Shirom Chabra,Vyas Raina*

Main category: cs.IR

TL;DR: 提出了一种将强大视觉语言模型的偏好排名蒸馏到嵌入系统中的框架，在保持嵌入方法推理效率的同时提升个性化文本-图像检索性能


<details>
  <summary>Details</summary>
Motivation: 现有的嵌入方法（如CLIP）主要训练于字面标题-图像对，难以捕捉产品推荐中常见的抽象或角色驱动属性；而视觉语言模型虽然能灵活对齐文本和图像，但受限于上下文窗口无法直接处理大规模检索

Method: 通过将强大视觉语言模型的偏好排名蒸馏到嵌入系统中，转移其细粒度对齐能力，同时保持嵌入方法的推理可扩展性

Result: 在角色驱动的产品推荐任务上，该方法显著优于现有的嵌入基线方法

Conclusion: 该方法为个性化文本-图像检索提供了一个高效的解决方案

Abstract: Text--image retrieval is necessary for applications such as product
recommendation. Embedding-based approaches like CLIP enable efficient
large-scale retrieval via vector similarity search, but they are primarily
trained on literal caption-like text--image pairs and often fail to capture
abstract or persona-driven attributes common in product recommendation
applications (e.g., ``a gift for a mother who loves gardening''). In contrast,
state-of-the-art vision--language models (vLLMs) can align text with images in
a flexible manner, but their limited context window prevents them from directly
handling retrieval over large catalogs. We propose a framework that distills
the preference rankings of a powerful vLLM into an embedding-based system,
transferring its nuanced alignment abilities while maintaining the
inference-time scalability of an embedding-based approach. Experiments on
persona-driven product recommendation tasks demonstrate that our method
significantly outperforms existing embedding-based baselines, providing an
efficient solution for personalized text--image retrieval.

</details>


### [64] [MIARec: Mutual-influence-aware Heterogeneous Network Embedding for Scientific Paper Recommendation](https://arxiv.org/abs/2510.12054)
*Wenjin Xie,Tao Jia*

Main category: cs.IR

TL;DR: MIARec模型通过重力方法衡量学者间的相互学术影响力，并将其融入图表示学习中的特征聚合过程，同时在异构学术网络中采用多通道聚合方法，显著提升了科学论文推荐的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图的推荐方法在学术网络中往往忽略了普遍存在的不对称学术影响力，这限制了推荐系统的精确性和质量。

Method: 提出MIARec模型，使用重力方法量化学者间的相互学术影响力，并在图表示学习的消息传播过程中融入这种影响力；同时采用多通道聚合方法捕获不同单关系子网络的个体嵌入及其相互依赖嵌入。

Result: 在真实数据集上的大量实验表明，MIARec模型在三个主要评估指标上均优于基线模型。

Conclusion: MIARec模型通过有效建模学术网络中的不对称影响力和异构关系，显著提升了科学论文推荐的性能，证明了其在学术推荐任务中的有效性。

Abstract: With the rapid expansion of scientific literature, scholars increasingly
demand precise and high-quality paper recommendations. Among various
recommendation methodologies, graph-based approaches have garnered attention by
effectively exploiting the structural characteristics inherent in scholarly
networks. However, these methods often overlook the asymmetric academic
influence that is prevalent in scholarly networks when learning graph
representations. To address this limitation, this study proposes the
Mutual-Influence-Aware Recommendation (MIARec) model, which employs a
gravity-based approach to measure the mutual academic influence between
scholars and incorporates this influence into the feature aggregation process
during message propagation in graph representation learning. Additionally, the
model utilizes a multi-channel aggregation method to capture both individual
embeddings of distinct single relational sub-networks and their interdependent
embeddings, thereby enabling a more comprehensive understanding of the
heterogeneous scholarly network. Extensive experiments conducted on real-world
datasets demonstrate that the MIARec model outperforms baseline models across
three primary evaluation metrics, indicating its effectiveness in scientific
paper recommendation tasks.

</details>


### [65] [Reinforced Preference Optimization for Recommendation](https://arxiv.org/abs/2510.12211)
*Junfei Tan,Yuxin Chen,An Zhang,Junguang Jiang,Bin Liu,Ziru Xu,Han Zhu,Jian Xu,Bo Zheng,Xiang Wang*

Main category: cs.IR

TL;DR: 提出ReRe框架，通过约束束搜索和改进奖励机制，解决生成式推荐系统中负样本建模不足和奖励稀疏的问题，在多个数据集上显著提升排序性能。


<details>
  <summary>Details</summary>
Motivation: 当前生成式推荐系统存在两个核心限制：缺乏高质量负样本建模和依赖隐式奖励。强化学习虽然提供了解决方案，但应用于生成式推荐时面临采样效率低和奖励稀疏的挑战。

Method: 提出ReRe框架，结合约束束搜索来提高采样效率和多样化硬负样本，同时使用基于规则的准确性奖励和辅助排序奖励来提供更细粒度的监督。

Result: 在三个真实世界数据集上的实验表明，ReRe在排序性能上持续优于传统和基于LLM的推荐系统，并且在基础模型和SFT初始化模型上都能提升性能，在不同骨干网络家族和规模上具有鲁棒性。

Conclusion: ReRe不仅提供了实证性能提升，还系统性地探索了推荐系统中RLVR的设计空间，为未来研究提供了见解，证明了强化学习在生成式推荐中的有效性。

Abstract: Recent breakthroughs in large language models (LLMs) have fundamentally
shifted recommender systems from discriminative to generative paradigms, where
user behavior modeling is achieved by generating target items conditioned on
historical interactions. Yet current generative recommenders still suffer from
two core limitations: the lack of high-quality negative modeling and the
reliance on implicit rewards. Reinforcement learning with verifiable rewards
(RLVR) offers a natural solution by enabling on-policy sampling of harder
negatives and grounding optimization in explicit reward signals. However,
applying RLVR to generative recommenders remains non-trivial. Its unique
generation space often leads to invalid or repetitive items that undermine
sampling efficiency, and ranking supervision is sparse since most items receive
identical zero rewards. To address these challenges, we propose Reinforced
Preference Optimization for Recommendation (ReRe), a reinforcement-based
paradigm tailored to LLM-based recommenders, an important direction in
generative recommendation. ReRe incorporates constrained beam search to improve
sampling efficiency and diversify hard negatives, while augmenting rule-based
accuracy rewards with auxiliary ranking rewards for finer-grained supervision.
Extensive experiments on three real-world datasets demonstrate that ReRe
consistently outperforms both traditional and LLM-based recommenders in ranking
performance. Further analysis shows that ReRe not only enhances performance
across both base and SFT-initialized models but also generalizes robustly
across different backbone families and scales. Beyond empirical gains, we
systematically investigate the design space of RLVR in recommendation across
generation, sampling strategy, reward modeling, and optimization algorithm,
offering insights for future research.

</details>


### [66] [An Empirical Study for Representations of Videos in Video Question Answering via MLLMs](https://arxiv.org/abs/2510.12299)
*Zhi Li,Yanan Wang,Hao Niu,Julio Vizcarra,Masato Taya*

Main category: cs.IR

TL;DR: 对多模态大语言模型在视频问答中的视频表示方法进行实证研究，发现视觉帧显著提升准确性但计算成本高，字幕提供轻量级有效替代方案。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视频问答方面取得显著进展，但尚不清楚哪种视频表示最有效，以及不同模态如何在任务准确性和计算效率之间取得平衡。

Method: 系统评估单模态输入（仅问题、字幕、视觉帧、音频信号）以及多模态组合，在VideoMME和LongVideoBench两个基准上进行测试。

Result: 视觉帧大幅提升准确性但带来GPU内存和推理延迟的沉重负担，字幕为长视频提供轻量级但有效的替代方案。

Conclusion: 研究揭示了有效性和效率之间的明确权衡，为设计资源感知的MLLM视频问答系统提供了实用见解。

Abstract: Multimodal large language models have recently achieved remarkable progress
in video question answering (VideoQA) by jointly processing visual, textual,
and audio information. However, it remains unclear which video representations
are most effective for MLLMs, and how different modalities balance task
accuracy against computational efficiency. In this work, we present a
comprehensive empirical study of video representation methods for VideoQA with
MLLMs. We systematically evaluate single modality inputs question only,
subtitles, visual frames, and audio signals as well as multimodal combinations,
on two widely used benchmarks: VideoMME and LongVideoBench. Our results show
that visual frames substantially enhance accuracy but impose heavy costs in GPU
memory and inference latency, while subtitles provide a lightweight yet
effective alternative, particularly for long videos. These findings highlight
clear trade-offs between effectiveness and efficiency and provide practical
insights for designing resource-aware MLLM-based VideoQA systems.

</details>


### [67] [Causal Inspired Multi Modal Recommendation](https://arxiv.org/abs/2510.12325)
*Jie Yang,Chenyang Gu,Zixuan Liu*

Main category: cs.IR

TL;DR: 提出了一种因果启发的多模态推荐框架，通过双通道跨模态扩散模块识别隐藏模态混杂因子，使用后门调整和前门调整来消除模态混杂和交互偏差，在三个真实电商数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了模态混杂（多个模态受相同潜在因素驱动）和交互偏差（真实偏好与曝光效应和误点击噪声混合）这两个关键偏差问题。

Method: 引入双通道跨模态扩散模块识别隐藏模态混杂因子，利用后门调整（分层匹配和向量量化码本）阻断混杂路径，应用前门调整结合因果拓扑重建构建去混杂因果子图。

Result: 在三个真实电商数据集上的广泛实验表明，该方法显著优于最先进的基线方法，同时保持了强大的可解释性。

Conclusion: 提出的因果启发的多模态推荐框架有效解决了模态混杂和交互偏差问题，在提升推荐性能的同时保持了良好的可解释性。

Abstract: Multimodal recommender systems enhance personalized recommendations in
e-commerce and online advertising by integrating visual, textual, and user-item
interaction data. However, existing methods often overlook two critical biases:
(i) modal confounding, where latent factors (e.g., brand style or product
category) simultaneously drive multiple modalities and influence user
preference, leading to spurious feature-preference associations; (ii)
interaction bias, where genuine user preferences are mixed with noise from
exposure effects and accidental clicks. To address these challenges, we propose
a Causal-inspired multimodal Recommendation framework. Specifically, we
introduce a dual-channel cross-modal diffusion module to identify hidden modal
confounders, utilize back-door adjustment with hierarchical matching and
vector-quantized codebooks to block confounding paths, and apply front-door
adjustment combined with causal topology reconstruction to build a deconfounded
causal subgraph. Extensive experiments on three real-world e-commerce datasets
demonstrate that our method significantly outperforms state-of-the-art
baselines while maintaining strong interpretability.

</details>


### [68] [Simple Projection Variants Improve ColBERT Performance](https://arxiv.org/abs/2510.12327)
*Benjamin Clavié,Sean Lee,Rikiya Takehi,Aamir Shakir,Makoto P. Kato*

Main category: cs.IR

TL;DR: 本研究探索了在ColBERT等多向量稠密检索模型中，用更复杂的投影块（如深度非线性FFN、GLU块和跳跃连接）替代传统的单层线性投影，可以显著提升检索性能，平均NDCG@10提高超过2个点。


<details>
  <summary>Details</summary>
Motivation: 传统的多向量检索方法使用单层线性投影来降维，但MaxSim算子在这种设置下存在梯度流动的固有局限性，需要更复杂的投影结构来改善性能。

Method: 设计和系统评估了多种替代投影块，包括深度非线性前馈网络、GLU块和跳跃连接等，通过消融研究分析各参数对性能的影响。

Result: 许多投影变体优于原始线性投影，最佳变体在多个检索基准测试中平均NDCG@10性能提升超过2个点，且效果在不同随机种子下保持一致。

Conclusion: 用更复杂的投影块替代ColBERT模型的线性层是一个稳健的即插即用升级方案，能够显著提升下游检索性能。

Abstract: Multi-vector dense retrieval methods like ColBERT systematically use a
single-layer linear projection to reduce the dimensionality of individual
vectors. In this study, we explore the implications of the MaxSim operator on
the gradient flows of the training of multi-vector models and show that such a
simple linear projection has inherent, if non-critical, limitations in this
setting. We then discuss the theoretical improvements that could result from
replacing this single-layer projection with well-studied alternative
feedforward linear networks (FFN), such as deeper, non-linear FFN blocks, GLU
blocks, and skip-connections, could alleviate these limitations. Through the
design and systematic evaluation of alternate projection blocks, we show that
better-designed final projections positively impact the downstream performance
of ColBERT models. We highlight that many projection variants outperform the
original linear projections, with the best-performing variants increasing
average performance on a range of retrieval benchmarks across domains by over 2
NDCG@10 points. We then conduct further exploration on the individual
parameters of these projections block in order to understand what drives this
empirical performance, highlighting the particular importance of upscaled
intermediate projections and residual connections. As part of these ablation
studies, we show that numerous suboptimal projection variants still outperform
the traditional single-layer projection across multiple benchmarks, confirming
our hypothesis. Finally, we observe that this effect is consistent across
random seeds, further confirming that replacing the linear layer of ColBERT
models is a robust, drop-in upgrade.

</details>


### [69] [A Hierarchical Quantized Tokenization Framework for Task-Adaptive Graph Representation Learning](https://arxiv.org/abs/2510.12369)
*Yang Xiang,Li Fan,Chenke Yin,Chengtao Ji*

Main category: cs.IR

TL;DR: 提出了一种分层量化框架，通过自加权机制实现多尺度任务自适应聚合，在保持编码器冻结的同时通过轻量门控过程调节信息流，在节点分类和链接预测任务上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有图标记化方法（线性化、连续和量化）在适应性和效率方面存在局限，特别是基于量化的标记器以固定或任务无关方式组织层次信息，可能过度表示或未充分利用结构线索，且无法在不重新训练编码器的情况下动态重新加权不同层次的贡献。

Method: 分层量化框架，引入自加权机制实现多尺度任务自适应聚合，保持编码器冻结，通过轻量门控过程调节信息流，实现参数高效的下游任务适应。

Result: 在节点分类和链接预测的基准数据集实验中，在可比较的计算预算下，相比强基线方法取得了持续改进。

Conclusion: 该分层量化框架通过自加权机制有效解决了现有图标记化方法的局限性，实现了参数高效的多尺度任务自适应，在保持计算效率的同时提升了性能。

Abstract: Recent progress in language and vision foundation models demonstrates the
importance of discrete token interfaces that transform complex inputs into
compact sequences for large-scale modeling. Extending this paradigm to graphs
requires a tokenization scheme that handles non-Euclidean structures and
multi-scale dependencies efficiently. Existing approaches to graph
tokenization, linearized, continuous, and quantized, remain limited in
adaptability and efficiency. In particular, most current quantization-based
tokenizers organize hierarchical information in fixed or task-agnostic ways,
which may either over-represent or under-utilize structural cues, and lack the
ability to dynamically reweight contributions from different levels without
retraining the encoder. This work presents a hierarchical quantization
framework that introduces a self-weighted mechanism for task-adaptive
aggregation across multiple scales. The proposed method maintains a frozen
encoder while modulating information flow through a lightweight gating process,
enabling parameter-efficient adaptation to diverse downstream tasks.
Experiments on benchmark datasets for node classification and link prediction
demonstrate consistent improvements over strong baselines under comparable
computational budgets.

</details>


### [70] [Leveraging Language Semantics for Collaborative Filtering with TextGCN and TextGCN-MLP: Zero-Shot vs In-Domain Performance](https://arxiv.org/abs/2510.12461)
*Andrei Chernov,Haroon Wahab,Oleg Novitskij*

Main category: cs.IR

TL;DR: 提出了TextGCN和TextGCN-MLP两种方法，通过在图卷积中直接使用LLM生成的物品标题嵌入，在推荐系统中实现了最先进的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注微调LLM生成推荐或将LLM嵌入集成到下游模型中，作者希望探索直接在LLM嵌入上应用图卷积的方法。

Method: TextGCN在LLM生成的物品标题嵌入上应用参数无关的图卷积层，TextGCN-MLP在此基础上增加了可训练的多层感知机和对比损失。

Result: TextGCN在零样本推荐中表现最佳，TextGCN-MLP在领域内基准测试中达到最先进水平，但零样本性能低于TextGCN。

Conclusion: 该方法展示了语言语义与图消息传递结合的优势，但在领域内专业化和零样本泛化之间存在权衡。

Abstract: In recent years, various approaches have been proposed to leverage large
language models (LLMs) for incorporating textual information about items into
recommender systems. Existing methods primarily focus on either fine-tuning
LLMs to generate recommendations or integrating LLM-based embeddings into
downstream models. In this work, we follow the latter direction and propose
\textbf{TextGCN}, which applies parameter-free graph convolution layers
directly over LLM-based item-title embeddings, instead of learning ID-based
embeddings as in traditional methods. By combining language semantics with
graph message passing, this architecture achieves state-of-the-art zero-shot
performance, significantly outperforming prior approaches. Furthermore, we
introduce \textbf{TextGCN-MLP}, which extends TextGCN with a trainable
multilayer perceptron trained using a contrastive loss, achieving
state-of-the-art in-domain performance on recommendation benchmarks. However,
the zero-shot performance of TextGCN-MLP remains lower than that of TextGCN,
highlighting the trade-off between in-domain specialization and zero-shot
generalization. We release our code on github at
\href{https://github.com/ChernovAndrey/TFCE}{github.com/ChernovAndrey/TFCE}.

</details>


### [71] [SMILE: SeMantic Ids Enhanced CoLd Item Representation for Click-through Rate Prediction in E-commerce SEarch](https://arxiv.org/abs/2510.12604)
*Qihang Zhao,Zhongbo Sun,Xiaoyang Zheng,Xian Guo,Siyuan Wang,Zihan Liang,Mingcan Peng,Ben Chen,Chenyi Lei*

Main category: cs.IR

TL;DR: SMILE是一种基于语义ID融合对齐的物品表示增强方法，通过RQ-OPQ编码量化物品内容和协同信息，实现两阶段对齐来提升冷启动物品推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现代搜索和推荐平台中，冷启动物品缺乏协同信息，加剧了平台物品的马太效应，挑战平台多样性。现有方法未能考虑协同与内容之间的不对称性以及物品间的细粒度差异。

Method: 使用RQ-OPQ编码量化物品内容和协同信息，进行两阶段对齐：RQ编码传递物品间共享的协同信号，OPQ编码学习物品的差异化信息。

Result: 在大规模工业数据集上的离线实验显示SMILE的优越性，严格的在线A/B测试证实了统计显著的改进：物品CTR +1.66%，买家数 +1.57%，订单量 +2.17%。

Conclusion: SMILE通过语义ID的融合对齐有效解决了冷启动物品推荐问题，在工业场景中取得了显著的效果提升。

Abstract: With the rise of modern search and recommendation platforms, insufficient
collaborative information of cold-start items exacerbates the Matthew effect of
existing platform items, challenging platform diversity and becoming a
longstanding issue. Existing methods align items' side content with
collaborative information to transfer collaborative signals from
high-popularity items to cold-start items. However, these methods fail to
account for the asymmetry between collaboration and content, nor the
fine-grained differences among items. To address these issues, we propose
SMILE, an item representation enhancement approach based on fused alignment of
semantic IDs. Specifically, we use RQ-OPQ encoding to quantize item content and
collaborative information, followed by a two-step alignment: RQ encoding
transfers shared collaborative signals across items, while OPQ encoding learns
differentiated information of items. Comprehensive offline experiments on
large-scale industrial datasets demonstrate superiority of SMILE, and rigorous
online A/B tests confirm statistically significant improvements: item CTR
+1.66%, buyers +1.57%, and order volume +2.17%.

</details>


### [72] [The Role of Parametric Injection-A Systematic Study of Parametric Retrieval-Augmented Generation](https://arxiv.org/abs/2510.12668)
*Minghao Tang,Shiyu Ni,Jingtong Wu,Zengxin Han,Keping Bi*

Main category: cs.IR

TL;DR: 本文系统研究了参数化检索增强生成(PRAG)，发现参数化文档仅捕获文档的部分语义信息，单独使用性能不如文本级交互。但当参数化表示与文本文档结合使用时，模型能更有效地利用相关信息，对噪声输入更鲁棒，获得比单独使用任一来源更好的性能。


<details>
  <summary>Details</summary>
Motivation: 参数化检索增强生成(PRAG)通过将文档编码为模型参数（LoRA模块）并在推理时注入这些表示，实现了LLM与文档在参数级别的交互。相比直接将文档放入输入上下文，PRAG更高效且具有更深层次模型-文档交互的潜力。然而，参数注入的机制仍未被充分理解。

Method: 对PRAG进行系统研究，阐明参数注入的作用。分析参数化文档捕获的语义信息特性，比较单独使用参数化文档与文本级交互的性能差异，并研究参数化表示与文本文档结合使用的效果。

Result: 参数化文档仅捕获文档的部分语义信息，单独使用时性能不如文本级交互。但这些参数化表示编码了高级文档信息，可以增强模型对输入上下文中文档的理解。当参数化文档与文本文档结合使用时，模型能更有效地利用相关信息，对噪声输入更鲁棒，性能优于单独使用任一来源。

Conclusion: 建议联合使用参数化和文本文档，并提倡增加参数化表示的信息内容以推进PRAG的发展。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
retrieving external documents. As an emerging form of RAG, parametric
retrieval-augmented generation (PRAG) encodes documents as model parameters
(i.e., LoRA modules) and injects these representations into the model during
inference, enabling interaction between the LLM and documents at parametric
level. Compared with directly placing documents in the input context, PRAG is
more efficient and has the potential to offer deeper model-document
interaction. Despite its growing attention, the mechanism underlying parametric
injection remains poorly understood. In this work, we present a systematic
study of PRAG to clarify the role of parametric injection, showing that
parameterized documents capture only partial semantic information of documents,
and relying on them alone yields inferior performance compared to interaction
at text level. However, these parametric representations encode high-level
document information that can enhance the model's understanding of documents
within the input context. When combined parameterized documents with textual
documents, the model can leverage relevant information more effectively and
become more robust to noisy inputs, achieving better performance than either
source alone. We recommend jointly using parameterized and textual documents
and advocate for increasing the information content of parametric
representations to advance PRAG.

</details>


### [73] [SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model](https://arxiv.org/abs/2510.12709)
*Lin Lin,Jiefeng Long,Zhihe Wan,Yuchi Wang,Dingkang Yang,Shuang Yang,Yueyang Yao,Xu Chen,Zirui Guo,Shengqiang Li,Weiran Li,Hanyu Li,Yaling Mou,Yan Qiu,Haiyang Yu,Xiao Liang,Hongsheng Li,Chao Feng*

Main category: cs.IR

TL;DR: SAIL-Embedding是一个全模态嵌入基础模型，通过多阶段训练策略解决多模态表示学习中的挑战，在检索任务中达到SOTA性能，并在推荐场景中显著提升用户体验指标。


<details>
  <summary>Details</summary>
Motivation: 现有多模态嵌入模型在真实应用和业务场景中面临模态支持有限、训练机制不稳定和工业领域差距等挑战。

Method: 提出多阶段训练方案：内容感知渐进训练增强模型对下游任务的适应性和跨模态能力；协作感知推荐增强训练通过从序列到项目和ID到项目嵌入中蒸馏知识来适应推荐场景；同时开发随机专业化和数据集驱动的模式匹配来增强训练灵活性和泛化性。

Result: 在不同检索任务中达到SOTA性能；在Douyin-Selected场景中实现7天LT增益+0.158%和14天LT增益+0.144%；在Douyin feed rank模型中，匹配特征产生+0.08% AUC增益。

Conclusion: SAIL-Embedding通过定制的训练策略和架构设计成功解决了多模态嵌入模型在工业应用中的关键挑战，显著提升了推荐系统的用户体验和性能。

Abstract: Multimodal embedding models aim to yield informative unified representations
that empower diverse cross-modal tasks. Despite promising developments in the
evolution from CLIP-based dual-tower architectures to large vision-language
models, prior works still face unavoidable challenges in real-world
applications and business scenarios, such as the limited modality support,
unstable training mechanisms, and industrial domain gaps. In this work, we
introduce SAIL-Embedding, an omni-modal embedding foundation model that
addresses these issues through tailored training strategies and architectural
design. In the optimization procedure, we propose a multi-stage training scheme
to boost the multifaceted effectiveness of representation learning.
Specifically, the content-aware progressive training aims to enhance the
model's adaptability to diverse downstream tasks and master enriched
cross-modal proficiency. The collaboration-aware recommendation enhancement
training further adapts multimodal representations for recommendation scenarios
by distilling knowledge from sequence-to-item and ID-to-item embeddings while
mining user historical interests. Concurrently, we develop the stochastic
specialization and dataset-driven pattern matching to strengthen model training
flexibility and generalizability. Experimental results show that SAIL-Embedding
achieves SOTA performance compared to other methods in different retrieval
tasks. In online experiments across various real-world scenarios integrated
with our model, we observe a significant increase in Lifetime (LT), which is a
crucial indicator for the recommendation experience. For instance, the model
delivers the 7-day LT gain of +0.158% and the 14-day LT gain of +0.144% in the
Douyin-Selected scenario. For the Douyin feed rank model, the match features
produced by SAIL-Embedding yield a +0.08% AUC gain.

</details>
