<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 69]
- [cs.IR](#cs.IR) [Total: 8]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [The Perplexity Paradox: Why Code Compresses Better Than Math in LLM Prompts](https://arxiv.org/abs/2602.15843)
*Warren Johnson*

Main category: cs.CL

TL;DR: 该论文扩展了之前的提示压缩研究，通过多基准验证、揭示困惑度悖论机制并提出自适应压缩算法TAAC，实现了成本降低与质量保持的平衡。


<details>
  <summary>Details</summary>
Motivation: 先前研究（Johnson, 2026）发现代码生成能容忍激进提示压缩，但存在三个局限：仅使用HumanEval基准、未验证"困惑度悖论"机制、缺乏自适应算法。本文旨在填补这些研究空白。

Method: 1. 在6个代码基准和4个推理基准上验证压缩阈值；2. 首次进行每令牌困惑度分析（n=723令牌），揭示"困惑度悖论"；3. 提出TAAC（任务感知自适应压缩）算法，通过签名注入等技术实现自适应压缩。

Result: 1. 压缩阈值在不同语言和难度任务中具有普适性；2. 发现代码语法令牌被保留（高困惑度）而数学问题中的数值被修剪（低困惑度），签名注入使通过率从5.3%提升至39.3%；3. TAAC实现22%成本降低和96%质量保持，优于固定比率压缩7%。

Conclusion: 本文系统性地解决了提示压缩中的关键问题，揭示了困惑度悖论机制，并提出的TAAC算法在实际应用中实现了成本与性能的有效平衡，为大规模语言模型部署提供了实用解决方案。

Abstract: In "Compress or Route?" (Johnson, 2026), we found that code generation tolerates aggressive prompt compression (r >= 0.6) while chain-of-thought reasoning degrades gradually. That study was limited to HumanEval (164 problems), left the "perplexity paradox" mechanism unvalidated, and provided no adaptive algorithm. This paper addresses all three gaps. First, we validate across six code benchmarks (HumanEval, MBPP, HumanEval+, MultiPL-E) and four reasoning benchmarks (GSM8K, MATH, ARC-Challenge, MMLU-STEM), confirming the compression threshold generalizes across languages and difficulties. Second, we conduct the first per-token perplexity analysis (n=723 tokens), revealing a "perplexity paradox": code syntax tokens are preserved (high perplexity) while numerical values in math problems are pruned despite being task-critical (low perplexity). Signature injection recovers +34 percentage points in pass rate (5.3% to 39.3%; Cohen's h=0.890). Third, we propose TAAC (Task-Aware Adaptive Compression), achieving 22% cost reduction with 96% quality preservation, outperforming fixed-ratio compression by 7%. MBPP validation (n=1,800 trials) confirms systematic variation: 3.6% at r=0.3 to 54.6% at r=1.0.

</details>


### [2] [Language Model Representations for Efficient Few-Shot Tabular Classification](https://arxiv.org/abs/2602.15844)
*Inwon Kang,Parikshit Ram,Yi Zhou,Horst Samulowitz,Oshani Seneviratne*

Main category: cs.CL

TL;DR: TaRL是一种轻量级表格分类方法，利用LLM的行级语义嵌入，通过去除共同成分和校准softmax温度，在少样本场景下达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: Web表格数据丰富但结构语义异构，难以构建统一处理方法。LLM已广泛部署于Web基础设施，能否直接利用已部署的LLM进行表格分类，避免专用模型或大量重训练？

Method: 提出TaRL（Table Representation with Language Model）范式：1) 利用LLM获取表格行级语义嵌入；2) 去除嵌入中的共同成分；3) 校准softmax温度；4) 使用元学习器预测合适温度。

Result: 在少样本场景（k≤32）下，TaRL在语义丰富的表格上达到与SOTA模型相当的性能，而朴素应用LLM嵌入效果不佳。

Conclusion: 证明可以利用现有LLM基础设施进行高效的表格理解，为Web表格分类提供了轻量级解决方案。

Abstract: The Web is a rich source of structured data in the form of tables, from product catalogs and knowledge bases to scientific datasets. However, the heterogeneity of the structure and semantics of these tables makes it challenging to build a unified method that can effectively leverage the information they contain. Meanwhile, Large language models (LLMs) are becoming an increasingly integral component of web infrastructure for tasks like semantic search. This raises a crucial question: can we leverage these already-deployed LLMs to classify structured data in web-native tables (e.g., product catalogs, knowledge base exports, scientific data portals), avoiding the need for specialized models or extensive retraining? This work investigates a lightweight paradigm, $\textbf{Ta}$ble $\textbf{R}$epresentation with $\textbf{L}$anguage Model~($\textbf{TaRL}$), for few-shot tabular classification that directly utilizes semantic embeddings of individual table rows. We first show that naive application of these embeddings underperforms compared to specialized tabular models. We then demonstrate that their potentials can be unlocked with two key techniques: removing the common component from all embeddings and calibrating the softmax temperature. We show that a simple meta-learner, trained on handcrafted features, can learn to predict an appropriate temperature. This approach achieves performance comparable to state-of-the-art models in low-data regimes ($k \leq 32$) of semantically-rich tables. Our findings demonstrate the viability of reusing existing LLM infrastructure for efficient semantics-driven pathway to reuse existing LLM infrastructure for Web table understanding.

</details>


### [3] [KD4MT: A Survey of Knowledge Distillation for Machine Translation](https://arxiv.org/abs/2602.15845)
*Ona de Gibert,Joseph Attieh,Timothee Mickus,Yves Scherrer,Jörg Tiedemann*

Main category: cs.CL

TL;DR: 这篇综述论文系统梳理了机器翻译中的知识蒸馏研究，分析了105篇相关文献，提供了方法分类、应用指南和风险提示。


<details>
  <summary>Details</summary>
Motivation: 机器翻译中的知识蒸馏不仅作为模型压缩工具，更是一种通用的知识转移机制，影响监督、翻译质量和效率。然而该领域缺乏系统综述和统一的评估实践。

Method: 作者首先介绍了MT和KD基础知识，然后基于方法贡献和实际应用对KD4MT文献进行分类，进行定性和定量分析，识别趋势和研究空白。

Result: 研究发现KD4MT领域存在缺乏统一评估实践的问题，识别了常见趋势和关键研究空白，提供了具体场景下选择KD方法的实用指南，并指出了幻觉增加和偏见放大等潜在风险。

Conclusion: 该综述为KD4MT研究提供了系统性框架，讨论了LLMs对该领域的影响，并提供了公开数据库和术语表以支持进一步研究。

Abstract: Knowledge Distillation (KD) as a research area has gained a lot of traction in recent years as a compression tool to address challenges related to ever-larger models in NLP. Remarkably, Machine Translation (MT) offers a much more nuanced take on this narrative: in MT, KD also functions as a general-purpose knowledge transfer mechanism that shapes supervision and translation quality as well as efficiency.
  This survey synthesizes KD for MT (KD4MT) across 105 papers (through October 1, 2025). We begin by introducing both MT and KD for non-experts, followed by an overview of the standard KD approaches relevant to MT applications. Subsequently, we categorize advances in the KD4MT literature based on (i) their methodological contributions and (ii) their practical applications. Our qualitative and quantitative analyses identify common trends in the field and highlight key research gaps as well as the absence of unified evaluation practice for KD methods in MT. We further provide practical guidelines for selecting a KD method in concrete settings and highlight potential risks associated with the application of KD to MT such as increased hallucination and bias amplification. Finally, we discuss the role of LLMs in re-shaping the KD4MT field. To support further research, we complement our survey with a publicly available database summarizing the main characteristics of the surveyed KD methods and a glossary of key terms.

</details>


### [4] [Gated Tree Cross-attention for Checkpoint-Compatible Syntax Injection in Decoder-Only LLMs](https://arxiv.org/abs/2602.15846)
*Xinyu Gao,Shaonan Wang,Nai Ding*

Main category: cs.CL

TL;DR: 通过添加门控树交叉注意力分支，在保持骨干架构不变的情况下增强解码器LLM的句法鲁棒性，而不损害其预训练能力。


<details>
  <summary>Details</summary>
Motivation: 解码器大语言模型在广泛任务上表现良好，但对轻微语法扰动敏感，这削弱了下游推理的可靠性。然而，直接将显式句法结构注入现有检查点可能会干扰其预训练能力。

Method: 引入检查点兼容的门控树交叉注意力分支，读取预计算的成分块记忆，同时保持骨干架构不变。使用令牌更新掩码和分阶段训练来控制结构更新的范围和时机。

Result: 在各种基准测试和Transformer骨干网络中，GTCA增强了句法鲁棒性，超越了持续训练基线，同时没有损害多项选择QA性能或常识推理。

Conclusion: GTCA为解码器LLM提供了一种实用的检查点兼容路径，使其具备更强的句法鲁棒性，而无需牺牲预训练能力。

Abstract: Decoder-only large language models achieve strong broad performance but are brittle to minor grammatical perturbations, undermining reliability for downstream reasoning. However, directly injecting explicit syntactic structure into an existing checkpoint can interfere with its pretrained competence. We introduce a checkpoint-compatible gated tree cross-attention (GTCA) branch that reads precomputed constituency chunk memory while leaving backbone architecture unchanged. Our design uses a token update mask and staged training to control the scope and timing of structural updates. Across benchmarks and Transformer backbones, GTCA strengthens syntactic robustness beyond continued-training baselines without compromising Multiple-Choice QA performance or commonsense reasoning, providing a practical checkpoint-compatible route to more syntax-robust decoder-only LLMs.

</details>


### [5] [Do Personality Traits Interfere? Geometric Limitations of Steering in Large Language Models](https://arxiv.org/abs/2602.15847)
*Pranav Bhandari,Usman Naseem,Mehwish Nasim*

Main category: cs.CL

TL;DR: 大型语言模型的人格引导向量之间存在几何依赖关系，即使去除线性重叠后，引导一个特质仍会影响其他特质，表明人格特质在LLMs中占据轻微耦合的子空间。


<details>
  <summary>Details</summary>
Motivation: 现有的人格引导方法通常假设人格特质可以独立控制，通过注入特质特定的引导向量来实现。本文旨在检验这一假设是否成立，分析大五人格引导方向之间的几何关系。

Method: 研究从两个模型家族（LLaMA-3-8B和Mistral-8B）中提取人格引导向量，应用多种几何调节方案：从无约束方向到软正交化和硬正交化，分析这些方向之间的几何关系。

Result: 人格引导方向表现出显著的几何依赖性：引导一个特质会一致地引起其他特质的变化，即使明确去除线性重叠后也是如此。硬正交化虽然强制几何独立性，但不能消除跨特质行为效应，并且会降低引导强度。

Conclusion: 人格特质在大型语言模型中占据一个轻微耦合的子空间，这限制了完全独立的特质控制。研究结果对人格引导方法的假设提出了质疑，表明需要更复杂的建模方法。

Abstract: Personality steering in large language models (LLMs) commonly relies on injecting trait-specific steering vectors, implicitly assuming that personality traits can be controlled independently. In this work, we examine whether this assumption holds by analysing the geometric relationships between Big Five personality steering directions. We study steering vectors extracted from two model families (LLaMA-3-8B and Mistral-8B) and apply a range of geometric conditioning schemes, from unconstrained directions to soft and hard orthonormalisation. Our results show that personality steering directions exhibit substantial geometric dependence: steering one trait consistently induces changes in others, even when linear overlap is explicitly removed. While hard orthonormalisation enforces geometric independence, it does not eliminate cross-trait behavioural effects and can reduce steering strength. These findings suggest that personality traits in LLMs occupy a slightly coupled subspace, limiting fully independent trait control.

</details>


### [6] [Rethinking Soft Compression in Retrieval-Augmented Generation: A Query-Conditioned Selector Perspective](https://arxiv.org/abs/2602.15856)
*Yunhao Liu,Zian Jia,Xinyu Gao,Kanjun Xu,Yun Xiong*

Main category: cs.CL

TL;DR: SeleCom：基于选择器的软压缩框架，通过查询条件信息选择而非全压缩，显著提升RAG性能并大幅降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统软上下文压缩方法采用类似自编码器的全压缩方式，无论文档信息与查询是否相关都强制压缩，导致信息密度稀释和性能下降，无法与非压缩RAG竞争。

Method: 提出SeleCom框架，将编码器重新定义为查询条件信息选择器而非压缩器。采用仅解码器架构，使用大规模、多样化、难度分级的合成QA数据集进行课程学习训练。

Result: SeleCom显著优于现有软压缩方法，达到甚至超越非压缩基线性能，同时将计算和延迟降低33.8%~84.6%。

Conclusion: 通过将压缩范式从全压缩转变为查询条件信息选择，SeleCom有效解决了传统软压缩方法的根本局限性，实现了性能与效率的双重提升。

Abstract: Retrieval-Augmented Generation (RAG) effectively grounds Large Language Models (LLMs) with external knowledge and is widely applied to Web-related tasks. However, its scalability is hindered by excessive context length and redundant retrievals. Recent research on soft context compression aims to address this by encoding long documents into compact embeddings, yet they often underperform non-compressed RAG due to their reliance on auto-encoder-like full-compression that forces the encoder to compress all document information regardless of relevance to the input query.
  In this work, we conduct an analysis on this paradigm and reveal two fundamental limitations: (I) Infeasibility, full-compression conflicts with the LLM's downstream generation behavior; and (II) Non-necessity: full-compression is unnecessary and dilutes task-relevant information density. Motivated by these insights, we introduce SeleCom, a selector-based soft compression framework for RAG that redefines the encoder's role as query-conditioned information selector. The selector is decoder-only and is trained with a massive, diverse and difficulty-graded synthetic QA dataset with curriculum learning.
  Extensive experiments show that SeleCom significantly outperforms existing soft compression approaches and achieves competitive or superior performance to non-compression baselines, while reducing computation and latency by 33.8%~84.6%.

</details>


### [7] [Can LLMs Assess Personality? Validating Conversational AI for Trait Profiling](https://arxiv.org/abs/2602.15848)
*Andrius Matšenas,Anet Lello,Tõnis Lees,Hans Peep,Kim Lilii Tamm*

Main category: cs.CL

TL;DR: 验证LLM作为人格评估问卷替代方案的可行性，通过对话式LLM评估与标准问卷对比，发现中等收敛效度，部分特质评估效果良好。


<details>
  <summary>Details</summary>
Motivation: 传统的人格评估主要依赖问卷，存在静态、耗时等局限，需要探索动态、交互式的评估方法。LLM作为新兴技术，能否提供有效的替代方案值得研究。

Method: 采用被试内实验设计（N=33），对比基于引导式LLM对话生成的大五人格分数与金标准IPIP-50问卷结果，同时测量用户感知的准确性。

Result: 结果显示中等收敛效度（r=0.38-0.58），尽责性、开放性和神经质分数在两种方法间无显著差异，但宜人性和外向性存在显著差异。参与者认为LLM生成的人格画像与传统问卷结果同样准确。

Conclusion: 对话式AI为传统心理测量学提供了有前景的新方法，但需要对某些特质进行特定校准，未来可进一步优化LLM在人格评估中的应用。

Abstract: This study validates Large Language Models (LLMs) as a dynamic alternative to questionnaire-based personality assessment. Using a within-subjects experiment (N=33), we compared Big Five personality scores derived from guided LLM conversations against the gold-standard IPIP-50 questionnaire, while also measuring user-perceived accuracy. Results indicate moderate convergent validity (r=0.38-0.58), with Conscientiousness, Openness, and Neuroticism scores statistically equivalent between methods. Agreeableness and Extraversion showed significant differences, suggesting trait-specific calibration is needed. Notably, participants rated LLM-generated profiles as equally accurate as traditional questionnaire results. These findings suggest conversational AI offers a promising new approach to traditional psychometrics.

</details>


### [8] [ColBERT-Zero: To Pre-train Or Not To Pre-train ColBERT models](https://arxiv.org/abs/2602.16609)
*Antoine Chaffin,Luca Arnaboldi,Amélie Chatelain,Florent Krzakala*

Main category: cs.CL

TL;DR: 大规模多向量预训练显著提升模型性能，ColBERT-Zero仅用公开数据就超越了使用更强闭源数据的模型，并提出了更高效的训练方法。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的多向量模型是在强单向量模型基础上通过小规模知识蒸馏得到的，本研究探索大规模多向量预训练是否能产生更强的多向量模型。

Method: 对多向量模型进行大规模预训练，开发了ColBERT-Zero模型，并探索了监督训练与知识蒸馏结合的方法来替代成本高昂的无监督预训练阶段。

Result: ColBERT-Zero仅用公开数据就超越了使用更强闭源数据的GTE-ModernColBERT及其基础模型GTE-ModernBERT，创造了该尺寸模型的新SOTA。研究发现先进行监督训练再进行知识蒸馏可以接近完全预训练的性能。

Conclusion: 大规模多向量预训练能产生更强大的模型，调整微调和预训练设置对齐很重要，同时提供了更高效的训练方法并开源了相关代码和检查点。

Abstract: Current state-of-the-art multi-vector models are obtained through a small Knowledge Distillation (KD) training step on top of strong single-vector models, leveraging the large-scale pre-training of these models. In this paper, we study the pre-training of multi-vector models and show that large-scale multi-vector pre-training yields much stronger multi-vector models. Notably, a fully ColBERT-pre-trained model, ColBERT-Zero, trained only on public data, outperforms GTE-ModernColBERT as well as its base model, GTE-ModernBERT, which leverages closed and much stronger data, setting new state-of-the-art for model this size. We also find that, although performing only a small KD step is not enough to achieve results close to full pre-training, adding a supervised step beforehand allows to achieve much closer performance while skipping the most costly unsupervised phase. Finally, we find that aligning the fine-tuning and pre-training setups is crucial when repurposing existing models. To enable exploration of our results, we release various checkpoints as well as code used to train them.

</details>


### [9] [Preference Optimization for Review Question Generation Improves Writing Quality](https://arxiv.org/abs/2602.15849)
*Karun Sharma,Vidushee Vats,Shengzhi Li,Yuxiang Wang,Zhongtian Sun,Prayag Tiwari*

Main category: cs.CL

TL;DR: IntelliReward奖励模型结合DAPO训练IntelliAsk模型，显著提升LLM生成同行评审问题的质量，使其更符合专家对努力程度、证据和依据的标准。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的同行评审问题生成方法往往产生表面化问题，超过50%的问题标记来自论文第一页，缺乏实质性和基于证据的提问。

Method: 开发IntelliReward奖励模型（基于冻结自回归LLM+可训练多头transformer），结合Decoupled Clip和DAPO策略优化训练IntelliAsk问题生成模型。

Result: IntelliAsk在推理和写作基准上表现一致提升：MuSR准确率从64.7提升至68.3，WritingBench从8.07提升至8.31，表明评审问题质量与更广泛能力相关。

Conclusion: 通过IntelliReward和DAPO训练的问题生成模型能产生更符合人类专家标准的评审问题，并发布了实现、专家偏好标注和奖励模型作为自动评估基准。

Abstract: Peer review relies on substantive, evidence-based questions, yet existing LLM-based approaches often generate surface-level queries, drawing over 50\% of their question tokens from a paper's first page. To bridge this gap, we develop IntelliReward, a novel reward model built from a frozen autoregressive LLM with trainable multi-head transformers over the final 50 token states, which outperforms API-based SFT baselines in predicting expert-level human preferences. By applying Decoupled Clip and Dynamic Sampling Policy Optimization (DAPO) with IntelliReward, we train IntelliAsk, a question-generation model aligned with human standards of effort, evidence, and grounding. We find consistent improvements on reasoning and writing benchmarks, suggesting reviewer-question quality correlates with broader capabilities. Compared to the Qwen3-32B base model, IntelliAsk shows measurable gains across diverse benchmarks, specifically improving performance on reasoning tasks like MuSR (68.3 vs 64.7 Acc) and complex writing evaluations such as WritingBench (8.31 vs 8.07). We release our implementation, expert preference annotations, and the IntelliReward model to provide an automatic evaluation benchmark for grounding, effort, and evidence in LLM-generated review questions.

</details>


### [10] [Large Language Models for Assisting American College Applications](https://arxiv.org/abs/2602.15850)
*Zhengliang Liu,Weihang You,Peng Shu,Junhao Chen,Yi Pan,Hanqi Jiang,Yiwei Li,Zhaojun Ding,Chao Cao,Xinliang Li,Yifan Zhou,Ruidong Zhang,Shaochen Xu,Wei Ruan,Huaqin Zhao,Dajiang Zhu,Tianming Liu*

Main category: cs.CL

TL;DR: EZCollegeApp是一个基于大语言模型的系统，帮助高中生简化美国大学申请流程，通过结构化表格、基于权威文档的答案建议，同时保持人类对最终回复的完全控制。


<details>
  <summary>Details</summary>
Motivation: 美国大学申请流程存在碎片化的招生政策、重复且有条件的表格、以及需要交叉参考多个来源的模糊问题，给学生带来了很大负担。

Method: 系统采用"映射优先"范式，将表格理解与答案生成分离，集成官方招生网站文档摄取、检索增强的问答系统，以及人在回路的聊天机器人界面，展示建议但不自动提交。

Result: 系统包含完整的架构、数据管道、内部表示、安全和隐私措施，并通过自动化测试和人工质量评估进行验证，源代码已在GitHub开源。

Conclusion: EZCollegeApp通过LLM技术简化了大学申请流程，保持了人类控制，开源代码有助于扩大该工作的影响力。

Abstract: American college applications require students to navigate fragmented admissions policies, repetitive and conditional forms, and ambiguous questions that often demand cross-referencing multiple sources. We present EZCollegeApp, a large language model (LLM)-powered system that assists high-school students by structuring application forms, grounding suggested answers in authoritative admissions documents, and maintaining full human control over final responses. The system introduces a mapping-first paradigm that separates form understanding from answer generation, enabling consistent reasoning across heterogeneous application portals. EZCollegeApp integrates document ingestion from official admissions websites, retrieval-augmented question answering, and a human-in-the-loop chatbot interface that presents suggestions alongside application fields without automated submission. We describe the system architecture, data pipeline, internal representations, security and privacy measures, and evaluation through automated testing and human quality assessment. Our source code is released on GitHub (https://github.com/ezcollegeapp-public/ezcollegeapp-public) to facilitate the broader impact of this work.

</details>


### [11] [Narrative Theory-Driven LLM Methods for Automatic Story Generation and Understanding: A Survey](https://arxiv.org/abs/2602.15851)
*David Y. Liu,Aditya Joshi,Paul Dawson*

Main category: cs.CL

TL;DR: 该论文调查了NLP领域如何与叙事学研究交叉，提出了一个反映叙事学区别的分类法，分析了叙事数据集、任务、理论和方法趋势，探讨了LLMs在连接NLP与抽象叙事概念方面的潜力，并指出了当前挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在自动故事生成和理解任务中展现出潜力，需要系统性地考察NLP研究如何与叙事学领域交叉，为更系统化和理论化的叙事研究提供基础。

Method: 通过对现有研究的调查分析，提出反映叙事学区别的分类法，考察叙事数据集、任务、理论、NLP流程以及提示和微调的方法趋势。

Result: 发现LLMs能够轻松连接NLP流程与抽象叙事概念，为跨学科合作提供机会。同时指出在统一叙事任务定义或基准方面存在挑战，使得模型比较困难。

Conclusion: 未来方向应聚焦于：定义和改进基于理论的个体叙事属性指标以逐步提升模型性能；进行大规模、理论驱动的文学/社会/文化分析；创建能够验证或完善叙事理论的实验，而非追求单一的"叙事质量"通用基准。

Abstract: Applications of narrative theories using large language models (LLMs) deliver promising use-cases in automatic story generation and understanding tasks. Our survey examines how natural language processing (NLP) research engages with fields of narrative studies, and proposes a taxonomy for ongoing efforts that reflect established distinctions in narratology. We discover patterns in the following: narrative datasets and tasks, narrative theories and NLP pipeline and methodological trends in prompting and fine-tuning. We highlight how LLMs enable easy connections of NLP pipelines with abstract narrative concepts and opportunities for interdisciplinary collaboration. Challenges remain in attempts to work towards any unified definition or benchmark of narrative related tasks, making model comparison difficult. For future directions, instead of the pursuit of a single, generalised benchmark for 'narrative quality', we believe that progress benefits more from efforts that focus on the following: defining and improving theory-based metrics for individual narrative attributes to incrementally improve model performance; conducting large-scale, theory-driven literary/social/cultural analysis; and creating experiments where outputs can be used to validate or refine narrative theories. This work provides a contextual foundation for more systematic and theoretically informed narrative research in NLP by providing an overview to ongoing research efforts and the broader narrative studies landscape.

</details>


### [12] [Building Safe and Deployable Clinical Natural Language Processing under Temporal Leakage Constraints](https://arxiv.org/abs/2602.15852)
*Ha Na Cho,Sairam Sutari,Alexander Lopez,Hansen Bow,Kai Zheng*

Main category: cs.CL

TL;DR: 论文提出了一种轻量级审计流程，用于识别和抑制临床NLP模型中的时间泄漏，以构建可部署的临床NLP系统。


<details>
  <summary>Details</summary>
Motivation: 临床NLP模型在出院规划中具有潜力，但基于临床记录的模型容易受到时间和词汇泄漏的影响，其中文档中的未来信息会夸大预测性能，对实际部署构成风险。

Method: 提出一个轻量级审计流程，将可解释性集成到模型开发过程中，在最终训练前识别和抑制泄漏倾向的信号，以脊柱手术后的次日出院预测为案例进行研究。

Result: 审计后的模型表现出更保守、校准更好的概率估计，减少了对出院相关词汇线索的依赖。

Conclusion: 可部署的临床NLP系统应优先考虑时间有效性、校准和行为鲁棒性，而不是乐观的性能指标。

Abstract: Clinical natural language processing (NLP) models have shown promise for supporting hospital discharge planning by leveraging narrative clinical documentation. However, note-based models are particularly vulnerable to temporal and lexical leakage, where documentation artifacts encode future clinical decisions and inflate apparent predictive performance. Such behavior poses substantial risks for real-world deployment, where overconfident or temporally invalid predictions can disrupt clinical workflows and compromise patient safety. This study focuses on system-level design choices required to build safe and deployable clinical NLP under temporal leakage constraints. We present a lightweight auditing pipeline that integrates interpretability into the model development process to identify and suppress leakage-prone signals prior to final training. Using next-day discharge prediction after elective spine surgery as a case study, we evaluate how auditing affects predictive behavior, calibration, and safety-relevant trade-offs. Results show that audited models exhibit more conservative and better-calibrated probability estimates, with reduced reliance on discharge-related lexical cues. These findings emphasize that deployment-ready clinical NLP systems should prioritize temporal validity, calibration, and behavioral robustness over optimistic performance.

</details>


### [13] [A Lightweight Explainable Guardrail for Prompt Safety](https://arxiv.org/abs/2602.15853)
*Md Asiful Islam,Mihai Surdeanu*

Main category: cs.CL

TL;DR: LEG是一个轻量级可解释护栏方法，通过多任务学习架构联合学习提示分类器和解释分类器，使用合成数据训练，在三个数据集上取得了与最先进方法相当或更好的性能，同时模型尺寸显著更小。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型护栏方法通常模型尺寸大且缺乏可解释性，难以理解为什么某些提示被分类为不安全。需要一种轻量级且能提供解释的方法来帮助用户理解分类决策。

Method: 1) 使用多任务学习架构同时训练提示分类器和解释分类器；2) 采用新颖的合成数据生成策略对抗LLM的确认偏误；3) 设计了结合交叉熵损失和焦点损失的新型损失函数，并加入不确定性加权来捕捉全局解释信号。

Result: 在三个数据集上，LEG在提示分类和可解释性方面均取得了与最先进方法相当或更好的性能（包括域内和域外测试），同时模型尺寸显著小于现有方法。

Conclusion: LEG提供了一种轻量级、可解释的护栏方法，通过多任务学习和创新的训练策略，在保持高性能的同时大大减少了模型尺寸，为解决LLM安全分类问题提供了新思路。

Abstract: We propose a lightweight explainable guardrail (LEG) method for the classification of unsafe prompts. LEG uses a multi-task learning architecture to jointly learn a prompt classifier and an explanation classifier, where the latter labels prompt words that explain the safe/unsafe overall decision. LEG is trained using synthetic data for explainability, which is generated using a novel strategy that counteracts the confirmation biases of LLMs. Lastly, LEG's training process uses a novel loss that captures global explanation signals and combines cross-entropy and focal losses with uncertainty-based weighting. LEG obtains equivalent or better performance than the state-of-the-art for both prompt classification and explainability, both in-domain and out-of-domain on three datasets, despite the fact that its model size is considerably smaller than current approaches. If accepted, we will release all models and the annotated dataset publicly.

</details>


### [14] [Decoupling Strategy and Execution in Task-Focused Dialogue via Goal-Oriented Preference Optimization](https://arxiv.org/abs/2602.15854)
*Jingyi Xu,Xingyu Ren,Zhiqiang You,Yumeng Zhang,Zhoupeng Shou*

Main category: cs.CL

TL;DR: 提出GOPO框架，通过分层强化学习将策略规划与响应生成解耦，显著提升任务导向对话系统的长期任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型训练方法依赖token级似然或偏好优化，与长期任务成功对齐不佳，需要更好的任务导向对话优化方法。

Method: 提出Goal-Oriented Preference Optimization (GOPO)分层强化学习框架，包含专家代理优化对话轨迹级目标偏好，客服代理严格按选定策略生成响应。

Result: 在Mgshop数据集上，GOPO比PPO和Memento分别提升TSE指标7.7%和10.3%；14B模型训练后TSE比Qwen-235B和GPT-5.2分别高2.7%和1.5%。

Conclusion: GOPO为商业场景任务导向对话系统建立了新范式，在多个数据集上表现一致改进，专家代理在长期优化中起关键作用。

Abstract: Large language models show potential in task-oriented dialogue systems, yet existing training methods often rely on token-level likelihood or preference optimization, which poorly align with long-horizon task success. To address this, we propose Goal-Oriented Preference Optimization (GOPO), a hierarchical reinforcement learning framework that decouples strategy planning from response generation via an Expert Agent and a Customer Service Agent. The Expert Agent optimizes multi-turn goal preferences at the dialogue-trajectory level, while the Customer Service Agent generates responses strictly aligned with the selected strategy. We evaluate GOPO on public benchmarks and e-commerce customer service datasets, and introduce Task-focused Sequential Engagement (TSE), a sequence-level metric derived from real e-commerce interaction data. On the Mgshop dataset, GOPO improves TSE by 7.7% and 10.3% over PPO and Memento, with consistent gains in sequence-level reward and generation quality. Furthermore, a 14B model trained with GOPO achieves 2.7% and 1.5% higher TSE than Qwen-235B and GPT-5.2, respectively. Ablation studies confirm the Expert Agent's critical role in long-horizon optimization. GOPO demonstrates consistent improvements across other datasets as well. This work establishes a new paradigm for task-oriented dialogue systems in commercial scenarios, with code and datasets to be made public.

</details>


### [15] [Multi-source Heterogeneous Public Opinion Analysis via Collaborative Reasoning and Adaptive Fusion: A Systematically Integrated Approach](https://arxiv.org/abs/2602.15857)
*Yi Liu*

Main category: cs.CL

TL;DR: CRAF框架通过融合传统特征方法与LLM，采用多阶段推理机制处理多源异构舆情分析，提升主题聚类和情感分析性能。


<details>
  <summary>Details</summary>
Motivation: 多源异构舆情分析面临结构差异、语义变化和平台特定偏见的挑战，需要有效整合不同来源信息并处理平台特定内容。

Method: 提出CRAF框架，包含四个关键创新：跨平台协作注意力模块、层次化自适应融合机制、联合优化策略以及多模态提取能力，整合OCR、ASR和视觉情感分析。

Result: 理论分析显示CRAF比独立源建模有更紧的泛化界；在三个多平台数据集上，主题聚类ARI达到0.76（提升4.1%），情感分析F1-score达到0.84（提升3.8%），新平台标注数据需求减少75%。

Conclusion: CRAF框架有效解决了多源异构舆情分析的挑战，在跨平台适应性、性能提升和数据效率方面表现优异，为多平台舆情监控提供了系统化解决方案。

Abstract: The analysis of public opinion from multiple heterogeneous sources presents significant challenges due to structural differences, semantic variations, and platform-specific biases. This paper introduces a novel Collaborative Reasoning and Adaptive Fusion (CRAF) framework that systematically integrates traditional feature-based methods with large language models (LLMs) through a structured multi-stage reasoning mechanism. Our approach features four key innovations: (1) a cross-platform collaborative attention module that aligns semantic representations while preserving source-specific characteristics, (2) a hierarchical adaptive fusion mechanism that dynamically weights features based on both data quality and task requirements, (3) a joint optimization strategy that simultaneously learns topic representations and sentiment distributions through shared latent spaces, and (4) a novel multimodal extraction capability that processes video content from platforms like Douyin and Kuaishou by integrating OCR, ASR, and visual sentiment analysis. Theoretical analysis demonstrates that CRAF achieves a tighter generalization bound with a reduction of O(sqrt(d log K / m)) compared to independent source modeling, where d is feature dimensionality, K is the number of sources, and m is sample size. Comprehensive experiments on three multi-platform datasets (Weibo-12, CrossPlatform-15, NewsForum-8) show that CRAF achieves an average topic clustering ARI of 0.76 (4.1% improvement over best baseline) and sentiment analysis F1-score of 0.84 (3.8% improvement). The framework exhibits strong cross-platform adaptability, reducing the labeled data requirement for new platforms by 75%.

</details>


### [16] [State Design Matters: How Representations Shape Dynamic Reasoning in Large Language Models](https://arxiv.org/abs/2602.15858)
*Annie Wong,Aske Plaat,Thomas Bäck,Niki van Stein,Anna V. Kononova*

Main category: cs.CL

TL;DR: 论文研究了在动态环境中，LLMs状态表示的三个关键方面（粒度、结构、空间基础）对性能的影响，发现轨迹总结、自然语言表示和文本空间编码最为有效，但当前模型在长时程推理中仍存在脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型从静态推理任务转向动态环境，其成功取决于在推理时与环境交互并响应环境变化的能力。在这些环境中，状态表示是一个未被充分探索的因素。

Method: 在保持模型参数固定的条件下，系统地研究了状态表示的三个关键方面：(1) 状态粒度（长形式 vs 总结），(2) 结构（自然语言 vs 符号），(3) 空间基础（纯文本 vs 图像或文本地图编码），并在顺序决策制定基准上进行测试。

Result: 1. 轨迹总结通过减少噪声和稳定长时程推理提高了性能；2. 自然语言表示在各种模型中最具鲁棒性，而结构化编码主要对具有强代码或结构化输出先验的模型有帮助；3. 文本空间编码最为有效，其优势并非来自空间信息本身，而是来自构建过程迫使模型进行空间推理；4. 即使改进表示，当前LLMs和VLMs在长时程推理中仍显脆弱。

Conclusion: 状态表示的设计选择是性能的决定性因素，与信息可用性本身不同。然而，即使改进表示，当前LLMs和VLMs在需要综合信息管理多个子任务以达到目标的长期推理中仍然脆弱。

Abstract: As large language models (LLMs) move from static reasoning tasks toward dynamic environments, their success depends on the ability to navigate and respond to an environment that changes as they interact at inference time. An underexplored factor in these settings is the representation of the state. Holding model parameters fixed, we systematically vary three key aspects: (1) state granularity (long form versus summary), (2) structure (natural language versus symbolic), and (3) spatial grounding (text-only versus images or textual map encodings) across sequential decision-making benchmarks. We find that trajectory summarisation improves performance by reducing noise and stabilising long-horizon reasoning. Second, natural language representations are the most robust across models, whereas structured encodings help mainly for models with strong code or structured output priors, such as JSON schemas. Third, while image-inputs show some benefit, text-based spatial encodings prove most effective. This advantage stems not from the spatial information itself, but from the act of construction, which compels the model to perform the spatial reasoning that static input does not elicit. Overall, we demonstrate that design choices for representing state are a decisive factor in performance, distinct from the availability of information itself. We note, however, that even with improved representations, current LLMs and VLMs remain brittle over long horizons, particularly when they must synthesise information to manage multiple subtasks to reach a goal.

</details>


### [17] [From Transcripts to AI Agents: Knowledge Extraction, RAG Integration, and Robust Evaluation of Conversational AI Assistants](https://arxiv.org/abs/2602.15859)
*Krittin Pachtrachai,Petmongkon Pornpichitsuwan,Wachiravit Modecrua,Touchapon Kraisingkorn*

Main category: cs.CL

TL;DR: 该论文提出一个端到端框架，直接从历史通话记录构建和评估对话AI助手，在房地产和专业招聘领域实现了约30%的自主通话处理能力、近乎完美的事实准确性和强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 面对客户行业的对话AI助手构建面临三大挑战：嘈杂的对话数据、碎片化的知识、以及需要准确的人工交接，特别是在依赖实时信息的领域。现有解决方案在这些挑战性领域中表现不佳。

Method: 1) 使用简化的PIPA框架对通话记录进行质量分级和筛选；2) 利用LLM从高质量记录中提取结构化知识；3) 将知识作为唯一基础部署在RAG管道中；4) 通过系统提示调优管理助手行为；5) 使用基于记录的模拟器进行定量评估；6) 进行红队测试评估鲁棒性。

Result: 在房地产和专业招聘领域，助手能够自主处理约30%的通话，实现近乎完美的事实准确性和拒绝行为，在对抗性测试中表现出强鲁棒性。

Conclusion: 该框架证明即使在高挑战性、依赖实时信息的领域，也能构建出可靠、准确的对话AI助手，为行业自动化提供了可行路径。

Abstract: Building reliable conversational AI assistants for customer-facing industries remains challenging due to noisy conversational data, fragmented knowledge, and the requirement for accurate human hand-off - particularly in domains that depend heavily on real-time information. This paper presents an end-to-end framework for constructing and evaluating a conversational AI assistant directly from historical call transcripts. Incoming transcripts are first graded using a simplified adaptation of the PIPA framework, focusing on observation alignment and appropriate response behavior, and are filtered to retain only high-quality interactions exhibiting coherent flow and effective human agent responses. Structured knowledge is then extracted from curated transcripts using large language models (LLMs) and deployed as the sole grounding source in a Retrieval-Augmented Generation (RAG) pipeline. Assistant behavior is governed through systematic prompt tuning, progressing from monolithic prompts to lean, modular, and governed designs that ensure consistency, safety, and controllable execution. Evaluation is conducted using a transcript-grounded user simulator, enabling quantitative measurement of call coverage, factual accuracy, and human escalation behavior. Additional red teaming assesses robustness against prompt injection, out-of-scope, and out-of-context attacks. Experiments are conducted in the Real Estate and Specialist Recruitment domains, which are intentionally challenging and currently suboptimal for automation due to their reliance on real-time data. Despite these constraints, the assistant autonomously handles approximately 30 percents of calls, achieves near-perfect factual accuracy and rejection behavior, and demonstrates strong robustness under adversarial testing.

</details>


### [18] [Reranker Optimization via Geodesic Distances on k-NN Manifolds](https://arxiv.org/abs/2602.15860)
*Wen G. Gong*

Main category: cs.CL

TL;DR: Maniscope是一种基于k-NN流形几何距离的轻量级重排序方法，比现有方法快10-45倍，在保持相近精度的同时实现亚10毫秒延迟。


<details>
  <summary>Details</summary>
Motivation: 当前基于交叉编码器或大语言模型的神经重排序方法计算资源消耗大、延迟高（3-5秒/查询），不适用于实时RAG部署。需要一种既高效又能捕捉语义结构的重排序方法。

Method: 提出Maniscope方法，在检索到的文档候选项上构建k-NN流形，计算测地线距离。该方法结合全局余弦相似度和局部流形几何，捕捉平坦欧几里得度量忽略的语义结构。

Result: 在8个BEIR基准数据集（1,233个查询）上评估，Maniscope在三个最难数据集上优于HNSW图基线（NFCorpus:+7.0%，TREC-COVID:+1.6%，AorB:+2.8% NDCG@3），速度快3.2倍。相比交叉编码器重排序器，精度相差2%以内，延迟低10-45倍。在TREC-COVID上，LLM-Reranker仅比Maniscope提高0.5% NDCG@3，但延迟高840倍。

Conclusion: Maniscope提供了一种实用的实时RAG部署替代方案，具有O(N D + M^2 D + M k log k)复杂度（M << N），可实现亚10毫秒延迟。作者计划将其作为开源软件发布。

Abstract: Current neural reranking approaches for retrieval-augmented generation (RAG) rely on cross-encoders or large language models (LLMs), requiring substantial computational resources and exhibiting latencies of 3-5 seconds per query. We propose Maniscope, a geometric reranking method that computes geodesic distances on k-nearest neighbor (k-NN) manifolds constructed over retrieved document candidates. This approach combines global cosine similarity with local manifold geometry to capture semantic structure that flat Euclidean metrics miss. Evaluating on eight BEIR benchmark datasets (1,233 queries), Maniscope outperforms HNSW graph-based baseline on the three hardest datasets (NFCorpus: +7.0%, TREC-COVID: +1.6%, AorB: +2.8% NDCG@3) while being 3.2x faster (4.7 ms vs 14.8 ms average). Compared to cross-encoder rerankers, Maniscope achieves within 2% accuracy at 10-45x lower latency. On TREC-COVID, LLM-Reranker provides only +0.5% NDCG@3 improvement over Maniscope at 840x higher latency, positioning Maniscope as a practical alternative for real-time RAG deployment. The method requires O(N D + M^2 D + M k log k) complexity where M << N , enabling sub-10 ms latency. We plan to release Maniscope as open-source software.

</details>


### [19] [CAST: Achieving Stable LLM-based Text Analysis for Data Analytics](https://arxiv.org/abs/2602.15861)
*Jinxiang Xie,Zihao Li,Wei He,Rui Ding,Shi Han,Dongmei Zhang*

Main category: cs.CL

TL;DR: CAST框架通过算法提示和稳定思考机制提升LLM在表格数据分析中的输出稳定性，显著改善稳定性评分16.2%的同时保持或提升输出质量。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在表格数据的文本分析任务（如摘要和标记）中，无法满足数据分析所需的高标准输出稳定性要求。

Method: 提出CAST框架，包含两个核心组件：1）算法提示，为有效推理转换提供程序化支架；2）思考-后说话，在最终生成前强制明确的中间承诺。还提出了CAST-S和CAST-T两个稳定性评估指标。

Result: 在多个公开基准测试和LLM骨干网络上，CAST在所有基线方法中始终达到最佳稳定性，稳定性评分提升最高达16.2%，同时保持或改善了输出质量。

Conclusion: CAST框架通过约束模型的潜在推理路径，有效解决了LLM在表格数据分析中的输出稳定性问题，为实现可靠的数据分析应用提供了实用解决方案。

Abstract: Text analysis of tabular data relies on two core operations: \emph{summarization} for corpus-level theme extraction and \emph{tagging} for row-level labeling. A critical limitation of employing large language models (LLMs) for these tasks is their inability to meet the high standards of output stability demanded by data analytics. To address this challenge, we introduce \textbf{CAST} (\textbf{C}onsistency via \textbf{A}lgorithmic Prompting and \textbf{S}table \textbf{T}hinking), a framework that enhances output stability by constraining the model's latent reasoning path. CAST combines (i) Algorithmic Prompting to impose a procedural scaffold over valid reasoning transitions and (ii) Thinking-before-Speaking to enforce explicit intermediate commitments before final generation. To measure progress, we introduce \textbf{CAST-S} and \textbf{CAST-T}, stability metrics for bulleted summarization and tagging, and validate their alignment with human judgments. Experiments across publicly available benchmarks on multiple LLM backbones show that CAST consistently achieves the best stability among all baselines, improving Stability Score by up to 16.2\%, while maintaining or improving output quality.

</details>


### [20] [Enhancing Action and Ingredient Modeling for Semantically Grounded Recipe Generation](https://arxiv.org/abs/2602.15862)
*Guoshan Liu,Bin Zhu,Yian Li,Jingjing Chen,Chong-Wah Ngo,Yu-Gang Jiang*

Main category: cs.CL

TL;DR: 提出一个语义基础的两阶段框架，通过监督微调和强化微调结合，提升食谱生成的语义准确性


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在从食物图像生成食谱时，尽管词汇评分（如BLEU、ROUGE）很高，但经常产生语义不正确的动作或食材

Method: 两阶段流水线：监督微调（SFT）使用动作推理数据集和食材语料库建立基础准确性；强化微调（RFT）采用频率感知奖励来改进长尾动作预测和食材泛化；还包含语义置信度评分与纠正模块进行预测过滤和校正

Result: 在Recipe1M数据集上实现了最先进的性能，并显著提高了语义保真度

Conclusion: 提出的语义基础框架有效解决了食谱生成中的语义不准确问题，通过预测和验证动作与食材作为指令生成的内在上下文，显著提升了语义质量

Abstract: Recent advances in Multimodal Large Language Models (MLMMs) have enabled recipe generation from food images, yet outputs often contain semantically incorrect actions or ingredients despite high lexical scores (e.g., BLEU, ROUGE). To address this gap, we propose a semantically grounded framework that predicts and validates actions and ingredients as internal context for instruction generation. Our two-stage pipeline combines supervised fine-tuning (SFT) with reinforcement fine-tuning (RFT): SFT builds foundational accuracy using an Action-Reasoning dataset and ingredient corpus, while RFT employs frequency-aware rewards to improve long-tail action prediction and ingredient generalization. A Semantic Confidence Scoring and Rectification (SCSR) module further filters and corrects predictions. Experiments on Recipe1M show state-of-the-art performance and markedly improved semantic fidelity.

</details>


### [21] [Not the Example, but the Process: How Self-Generated Examples Enhance LLM Reasoning](https://arxiv.org/abs/2602.15863)
*Daehoon Gwak,Minseo Jung,Junwoo Park,Minho Park,ChaeHun Park,Junha Hyung,Jaegul Choo*

Main category: cs.CL

TL;DR: 研究表明LLM自我生成示例的收益主要来自创建过程而非示例本身，集成提示（创建与解决统一）优于零样本和解耦提示。


<details>
  <summary>Details</summary>
Motivation: 虽然先前研究发现LLM通过自生成few-shot示例能达到与人工示例相当的性能，但其背后的机制尚不明确，难以决定何时以及如何有效应用该技术。

Method: 在多种推理密集型任务和LLM架构上，系统评估三种提示策略：零样本提示、集成提示（LLM在单一统一提示中创建并解决问题）、解耦提示（重用自生成示例但排除其创建上下文）。还进行了注意力模式分析。

Result: 集成提示始终优于零样本和解耦提示，而解耦提示相比零样本仅有边际提升。注意力分析显示集成与解耦提示之间存在显著差异的注意力模式。

Conclusion: 自生成提示的优势主要来自问题创建过程而非示例本身，这为设计更有效的提示策略提供了重要见解。

Abstract: Recent studies have shown that Large Language Models (LLMs) can improve their reasoning performance through self-generated few-shot examples, achieving results comparable to manually curated in-context examples. However, the underlying mechanism behind these gains remains unclear, making it hard to decide when and how to apply the technique effectively. In this work, we argue that the key benefit arises not from the generated examples themselves but from the act of creating them. To validate this, on reasoning-intensive tasks across diverse LLM architectures, we systematically evaluate three prompting strategies for in-context learning: (1) Zero-shot prompting; (2) Integrated prompting, where LLMs create and solve problems within a single, unified prompt; and (3) Decoupled prompting, where self-generated examples are reused as in-context examples, but the context of their creation itself is excluded. We conduct experiments across five widely used model architectures, demonstrating that Integrated prompting consistently outperforms both Zero-shot and Decoupled prompting. In contrast, Decoupled prompting offers only marginal gains over Zero-shot. Further, for a more in-depth analysis, we conduct an attention analysis and observe significant differences in attention patterns between Integrated and Decoupled prompting. These findings suggest that the advantage of self-generation prompting comes from the process of problem creation, not the examples themselves, providing valuable insights for designing more effective prompting strategies.

</details>


### [22] [NLP Privacy Risk Identification in Social Media (NLP-PRISM): A Survey](https://arxiv.org/abs/2602.15866)
*Dhiman Goswami,Jai Kruthunz Naveen Kumar,Sanchari Das*

Main category: cs.CL

TL;DR: NLP-PRISM框架系统评估社交媒体NLP中的隐私风险，发现Transformer模型在隐私保护微调下性能下降1%-23%，隐私与效用存在2%-9%权衡，提出加强匿名化、隐私感知学习和公平性训练的建议。


<details>
  <summary>Details</summary>
Motivation: 社交媒体分析中的NLP经常处理包含个人身份信息、行为线索和元数据的内容，存在监控、画像和定向广告等隐私风险，需要系统评估这些风险。

Method: 回顾203篇同行评审论文，提出NLP-PRISM框架，从六个维度评估隐私风险：数据收集、预处理、可见性、公平性、计算风险和法规遵从性。分析六个NLP任务中的隐私覆盖情况。

Result: Transformer模型F1分数0.58-0.84，隐私保护微调下性能下降1%-23%；隐私研究存在显著空白；模型效用降低2%-9%，成员推理攻击AUC 0.81，属性推理攻击准确率0.75。

Conclusion: 需要加强匿名化、隐私感知学习和公平性驱动训练，以实现社交媒体环境中的道德NLP应用。

Abstract: Natural Language Processing (NLP) is integral to social media analytics but often processes content containing Personally Identifiable Information (PII), behavioral cues, and metadata raising privacy risks such as surveillance, profiling, and targeted advertising. To systematically assess these risks, we review 203 peer-reviewed papers and propose the NLP Privacy Risk Identification in Social Media (NLP-PRISM) framework, which evaluates vulnerabilities across six dimensions: data collection, preprocessing, visibility, fairness, computational risk, and regulatory compliance. Our analysis shows that transformer models achieve F1-scores ranging from 0.58-0.84, but incur a 1% - 23% drop under privacy-preserving fine-tuning. Using NLP-PRISM, we examine privacy coverage in six NLP tasks: sentiment analysis (16), emotion detection (14), offensive language identification (19), code-mixed processing (39), native language identification (29), and dialect detection (24) revealing substantial gaps in privacy research. We further found a (reduced by 2% - 9%) trade-off in model utility, MIA AUC (membership inference attacks) 0.81, AIA accuracy 0.75 (attribute inference attacks). Finally, we advocate for stronger anonymization, privacy-aware learning, and fairness-driven training to enable ethical NLP in social media contexts.

</details>


### [23] [Playing With AI: How Do State-Of-The-Art Large Language Models Perform in the 1977 Text-Based Adventure Game Zork?](https://arxiv.org/abs/2602.15867)
*Berry Gerrits*

Main category: cs.CL

TL;DR: 大型语言模型在经典文本冒险游戏Zork中表现不佳，平均完成度低于10%，即使最佳模型也只获得约75/350分，显示其推理和元认知能力存在根本性限制。


<details>
  <summary>Details</summary>
Motivation: 评估当代大型语言模型在问题解决和推理方面的能力，通过它们在Zork这款开创性文本冒险游戏中的表现来测试其自然语言理解、行动序列生成和游戏进展能力。

Method: 在Zork游戏中测试领先的专有模型（ChatGPT、Claude、Gemini），使用最小化和详细化两种指令设置，以游戏得分作为主要评估指标，并进行定性分析模型推理过程。

Result: 所有测试模型平均完成度低于10%，最佳模型Claude Opus 4.5仅获得约75/350分。详细游戏指令和"扩展思考"功能均未带来改进。定性分析显示模型存在重复无效行动、策略不一致、无法从历史中学习等根本性限制。

Conclusion: 当前大型语言模型在文本游戏领域的元认知能力和问题解决能力存在实质性限制，这对其推理能力的性质和范围提出了质疑。

Abstract: In this positioning paper, we evaluate the problem-solving and reasoning capabilities of contemporary Large Language Models (LLMs) through their performance in Zork, the seminal text-based adventure game first released in 1977. The game's dialogue-based structure provides a controlled environment for assessing how LLM-based chatbots interpret natural language descriptions and generate appropriate action sequences to succeed in the game. We test the performance of leading proprietary models - ChatGPT, Claude, and Gemini - under both minimal and detailed instructions, measuring game progress through achieved scores as the primary metric. Our results reveal that all tested models achieve less than 10% completion on average, with even the best-performing model (Claude Opus 4.5) reaching only approximately 75 out of 350 possible points. Notably, providing detailed game instructions offers no improvement, nor does enabling ''extended thinking''. Qualitative analysis of the models' reasoning processes reveals fundamental limitations: repeated unsuccessful actions suggesting an inability to reflect on one's own thinking, inconsistent persistence of strategies, and failure to learn from previous attempts despite access to conversation history. These findings suggest substantial limitations in current LLMs' metacognitive abilities and problem-solving capabilities within the domain of text-based games, raising questions about the nature and extent of their reasoning capabilities.

</details>


### [24] [Understanding LLM Failures: A Multi-Tape Turing Machine Analysis of Systematic Errors in Language Model Reasoning](https://arxiv.org/abs/2602.15868)
*Magnus Boman*

Main category: cs.CL

TL;DR: 该论文提出了一个基于确定性多带图灵机的LLM交互形式化模型，用于精确定位LLM在简单任务上的失败模式


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在看似简单的任务上表现出失败模式，需要一种形式化方法来精确定位这些失败的根本原因，超越几何隐喻和纯经验分析

Method: 提出一个确定性多带图灵机模型，用不同的磁带表示LLM交互的各个组件：输入字符、标记、词汇表、模型参数、激活值、概率分布和输出文本

Result: 该模型能够将失败模式精确定位到特定处理阶段，例如揭示标记化如何掩盖计数任务所需的字符级结构；解释了思维链提示等技术的工作原理及其根本局限性

Conclusion: 这种方法为LLM分析提供了严谨、可证伪的形式化框架，补充了经验性缩放定律，并为理解LLM失败模式提供了原则性错误分析工具

Abstract: Large language models (LLMs) exhibit failure modes on seemingly trivial tasks. We propose a formalisation of LLM interaction using a deterministic multi-tape Turing machine, where each tape represents a distinct component: input characters, tokens, vocabulary, model parameters, activations, probability distributions, and output text. The model enables precise localisation of failure modes to specific pipeline stages, revealing, e.g., how tokenisation obscures character-level structure needed for counting tasks. The model clarifies why techniques like chain-of-thought prompting help, by externalising computation on the output tape, while also revealing their fundamental limitations. This approach provides a rigorous, falsifiable alternative to geometric metaphors and complements empirical scaling laws with principled error analysis.

</details>


### [25] [Towards Fair and Efficient De-identification: Quantifying the Efficiency and Generalizability of De-identification Approaches](https://arxiv.org/abs/2602.15869)
*Noopur Zambare,Kiana Aghakasiri,Carissa Lin,Carrie Ye,J. Ross Mitchell,Mohamed Abdalla*

Main category: cs.CL

TL;DR: 研究评估了不同规模语言模型在临床去识别化任务上的表现，发现小型模型在降低成本的同时能达到可比性能，且在多语言、跨文化场景中表现更好，并发布了多文化BERT去识别化模型。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在临床去识别化任务上表现出色，但之前的研究没有系统评估它们在格式、文化和性别之间的泛化能力，特别是在多语言和跨文化场景下的表现。

Method: 系统评估了三种类型模型：微调后的Transformer模型（BERT、ClinicalBERT、ModernBERT）、小型LLM（Llama 1-8B、Qwen 1.5-7B）和大型LLM（Llama-70B、Qwen-72B）。创建了BERT-MultiCulture-DEID模型集，基于BERT、ClinicalBERT和ModernBERT在MIMIC数据集上使用多语言变体标识符进行微调。

Result: 小型模型在降低推理成本的同时能达到可比性能；在有限数据微调后，小型模型在去识别化中文、印地语、西班牙语、法语、孟加拉语等语言变体以及性别化姓名方面优于大型模型；发布了多文化BERT去识别化模型集。

Conclusion: 首次全面量化了去识别化任务中效率与泛化能力的权衡，为公平高效的临床去识别化提供了实用路径，小型模型在多语言跨文化场景中更具优势。

Abstract: Large language models (LLMs) have shown strong performance on clinical de-identification, the task of identifying sensitive identifiers to protect privacy. However, previous work has not examined their generalizability between formats, cultures, and genders. In this work, we systematically evaluate fine-tuned transformer models (BERT, ClinicalBERT, ModernBERT), small LLMs (Llama 1-8B, Qwen 1.5-7B), and large LLMs (Llama-70B, Qwen-72B) at de-identification. We show that smaller models achieve comparable performance while substantially reducing inference cost, making them more practical for deployment. Moreover, we demonstrate that smaller models can be fine-tuned with limited data to outperform larger models in de-identifying identifiers drawn from Mandarin, Hindi, Spanish, French, Bengali, and regional variations of English, in addition to gendered names. To improve robustness in multi-cultural contexts, we introduce and publicly release BERT-MultiCulture-DEID, a set of de-identification models based on BERT, ClinicalBERT, and ModernBERT, fine-tuned on MIMIC with identifiers from multiple language variants. Our findings provide the first comprehensive quantification of the efficiency-generalizability trade-off in de-identification and establish practical pathways for fair and efficient clinical de-identification.
  Details on accessing the models are available at: https://doi.org/10.5281/zenodo.18342291

</details>


### [26] [VDLM: Variable Diffusion LMs via Robust Latent-to-Text Rendering](https://arxiv.org/abs/2602.15870)
*Shuhui Qu*

Main category: cs.CL

TL;DR: VDLM是一种模块化可变扩散语言模型，通过分离语义规划和文本渲染，实现潜在空间的迭代优化，在多个推理任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统自回归语言模型从左到右解码且不可逆，限制了多步推理过程中的修订能力。作者希望开发一种能够进行迭代优化的语言模型。

Method: 提出VDLM模型：1）使用LLaDA风格掩码扩散在语义变量嵌入上进行迭代优化；2）通过轨迹感知优化对规划器进行后训练，使用嵌入空间奖励和值函数，避免在RL循环中进行文本解码；3）使用Vec2Text渲染器将规划嵌入转换回文本，并引入嵌入扰动来增强解码鲁棒性。

Result: 在9个涵盖通用推理、数学和代码的基准测试中，VDLM在预训练阶段具有竞争力，在长文本生成任务的后训练阶段显著优于其他基线方法。

Conclusion: VDLM展示了嵌入空间后训练和鲁棒的潜在到文本渲染在扩散语言建模中的有效性，为语言模型的迭代优化提供了新思路。

Abstract: Autoregressive language models decode left-to-right with irreversible commitments, limiting revision during multi-step reasoning. We propose \textbf{VDLM}, a modular variable diffusion language model that separates semantic planning from text rendering. VDLM applies LLaDA-style masked diffusion over semantic variable embeddings to enable iterative refinement in latent space, then post-trains the planner with trajectory-aware optimization using embedding-space rewards and values, avoiding text decoding inside the RL loop. To convert planned embeddings back to text, we use a \textbf{Vec2Text} renderer and introduce \textbf{embedding perturbations} to robustify decoding under planner noise. Across nine benchmarks spanning general reasoning, math, and code, VDLM is competitive in pre-training and yields substantial post-training improvements on long-form generation tasks, outperforming other baselines. These results highlight the effectiveness of embedding-space post-training and robust latent-to-text rendering for diffusion language modeling.

</details>


### [27] [CheckIfExist: Detecting Citation Hallucinations in the Era of AI-Generated Content](https://arxiv.org/abs/2602.15871)
*Diletta Abbonato*

Main category: cs.CL

TL;DR: 开发了一个开源网络工具CheckIfExist，用于实时验证参考文献真实性，通过多源数据库交叉验证和字符串相似度算法检测AI幻觉引用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在学术工作流中的普及导致参考文献幻觉问题日益严重，即使在顶级机器学习会议上也发现了AI生成的虚假引用，现有工具缺乏实时验证功能，商业服务又存在使用限制和高昂费用。

Method: 采用级联验证架构，通过CrossRef、Semantic Scholar和OpenAlex等多个学术数据库进行多源验证，使用字符串相似度算法计算多维匹配置信度分数，支持单条验证和BibTeX批量处理。

Result: 开发了CheckIfExist工具，提供即时反馈的参考文献真实性验证，能够返回验证后的APA引用和可导出的BibTeX记录，支持统一接口的批量处理。

Conclusion: 该工具填补了现有参考文献管理工具缺乏实时验证功能的空白，为学术界提供了免费、开源的解决方案，有助于维护学术引用的完整性。

Abstract: The proliferation of large language models (LLMs) in academic workflows has introduced unprecedented challenges to bibliographic integrity, particularly through reference hallucination -- the generation of plausible but non-existent citations. Recent investigations have documented the presence of AI-hallucinated citations even in papers accepted at premier machine learning conferences such as NeurIPS and ICLR, underscoring the urgency of automated verification mechanisms. This paper presents "CheckIfExist", an open-source web-based tool designed to provide immediate verification of bibliographic references through multi-source validation against CrossRef, Semantic Scholar, and OpenAlex scholarly databases. While existing reference management tools offer bibliographic organization capabilities, they do not provide real-time validation of citation authenticity. Commercial hallucination detection services, though increasingly available, often impose restrictive usage limits on free tiers or require substantial subscription fees. The proposed tool fills this gap by employing a cascading validation architecture with string similarity algorithms to compute multi-dimensional match confidence scores, delivering instant feedback on reference authenticity. The system supports both single-reference verification and batch processing of BibTeX entries through a unified interface, returning validated APA citations and exportable BibTeX records within seconds.

</details>


### [28] [P-RAG: Prompt-Enhanced Parametric RAG with LoRA and Selective CoT for Biomedical and Multi-Hop QA](https://arxiv.org/abs/2602.15874)
*Xingda Lyu,Gongfu Lyu,Zitai Yan,Yuxin Jiang*

Main category: cs.CL

TL;DR: 论文提出了一种名为Prompt-Enhanced Parametric RAG (P-RAG)的混合架构，通过整合LLM内部参数化知识和检索证据，结合思维链提示和LoRA微调，在生物医学问答任务上显著超越了标准RAG方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型表现出强大能力，但受限于静态训练数据。检索增强生成通过检索外部知识缓解这一问题，但仍高度依赖知识库质量。需要探索更有效的改进方法。

Method: 提出了P-RAG混合架构，整合LLM参数化知识和检索证据，使用思维链提示和LoRA微调。评估了三种RAG变体：标准RAG、DA-RAG和P-RAG，使用LLaMA-3.2-1B-Instruct模型在PubMedQA和2WikiMultihopQA数据集上进行实验。

Result: P-RAG在PubMedQA上比标准RAG提升10.47个百分点F1分数（93.33% vs. 82.86%）；在2WikiMultihopQA上，P-RAG整体分数接近翻倍（33.44% vs. 17.83%），在Compare子集达到44.03%。思维链提示显著改善多跳推理，但对单跳查询效果混合。

Conclusion: P-RAG在生物医学问答任务中展现出准确、可扩展和上下文自适应的潜力，通过LoRA微调、思维链提示和混合架构实现了最先进的结果。

Abstract: Large Language Models (LLMs) demonstrate remarkable capabilities but remain limited by their reliance on static training data. Retrieval-Augmented Generation (RAG) addresses this constraint by retrieving external knowledge during inference, though it still depends heavily on knowledge base quality. To explore potential improvements, we evaluated three RAG variants-Standard RAG, DA-RAG, and our proposed Prompt-Enhanced Parametric RAG (P-RAG), a hybrid architecture that integrates parametric knowledge within the LLM and retrieved evidence, guided by Chain-of-Thought (CoT) prompting and Low-Rank Adaptation (LoRA) fine-tuning-on both general and biomedical datasets. Using LLaMA-3.2-1B-Instruct fine-tuned via LoRA, we evaluate on PubMedQA and 2WikiMultihopQA. P-RAG outperforms Standard RAG on PubMedQA by 10.47 percentage points in F1 (93.33% vs. 82.86%; 12.64% relative). On 2WikiMultihopQA, P-RAG nearly doubles the overall score vs. Standard RAG (33.44% vs. 17.83%) and achieves 44.03% on the Compare subset (with 42.74% Bridge, 21.84% Inference, 8.60% Compose). CoT prompting substantially improves multi-hop reasoning but yields mixed results for simpler, single-hop queries. These findings underscore P-RAG's potential for accurate, scalable, and contextually adaptive biomedical question answering. Our contributions include: (1) LoRA-based fine-tuning of LLaMA-3.2-1B-Instruct for biomedical QA, (2) introduction of P-RAG with Chain-of-Thought prompting, and (3) state-of-the-art results on PubMedQA and 2WikiMultihopQA.

</details>


### [29] [Quality-constrained Entropy Maximization Policy Optimization for LLM Diversity](https://arxiv.org/abs/2602.15894)
*Haihui Pan,Yuzhong Hong,Shaoke Lv,Junwei Bao,Hongfei Jiang,Yang Song*

Main category: cs.CL

TL;DR: QEMPO方法通过质量约束的熵最大化策略优化，在保持LLM输出质量的同时提升输出多样性


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法虽然提升了大型语言模型的输出质量，但会降低输出多样性，而现有提升多样性的方法往往以性能下降为代价

Method: 提出质量约束的熵最大化策略优化(QEMPO)，将对齐任务分解为质量和多样性两个分布，通过最大化策略输出熵同时确保输出质量，并提出了在线和离线训练方法

Result: 实验验证QEMPO在保持或超越RLHF性能的同时，显著提升了输出多样性

Conclusion: QEMPO成功解决了对齐方法中质量与多样性的权衡问题，为LLM对齐提供了新的有效方法

Abstract: Recent research indicates that while alignment methods significantly improve the quality of large language model(LLM) outputs, they simultaneously reduce the diversity of the models' output. Although some methods have been proposed to enhance LLM output diversity, they often come at the cost of reduced performance. In this work, we first theoretically demonstrate that the alignment task can be decomposed into two distributions: quality and diversity. To enhance the diversity of LLM outputs while ensuring quality, we propose the Quality-constrained Entropy Maximization Policy Optimization (QEMPO). QEMPO aims to maximize the output entropy of the policy while ensuring output quality. By adding different constraints to QEMPO, we obtain different policies. To optimize policies, we propose both online and offline training methods. Experiments validate that QEMPO achieves performance comparable to or even better than RLHF while improving output diversity.

</details>


### [30] [Understand Then Memory: A Cognitive Gist-Driven RAG Framework with Global Semantic Diffusion](https://arxiv.org/abs/2602.15895)
*Pengcheng Zhou,Haochen Li,Zhiqiang Nie,JiaLe Chen,Qing Gong,Weizhen Zhang,Chun Yu*

Main category: cs.CL

TL;DR: CogitoRAG是一个受人类情景记忆机制启发的RAG框架，通过提取和演化语义要点，构建多维知识图谱，结合查询分解和实体扩散进行检索，显著提升了复杂知识整合和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG框架使用文本的离散表示，导致语义完整性损失和检索偏差。为了解决这个问题，研究者受到人类情景记忆机制的启发，提出模拟人类认知记忆过程的RAG框架。

Method: CogitoRAG包含离线索引和在线检索两个阶段。离线阶段将非结构化语料库推导为要点记忆语料库，并转换为包含实体、关系事实和记忆节点的多维知识图谱。在线阶段通过查询分解模块处理复杂查询，实体扩散模块在图谱上进行关联检索，最后使用CogniRank算法融合扩散得分和语义相似度对候选段落进行重新排序。

Result: 在五个主流QA基准测试和GraphBench的多任务生成任务上，CogitoRAG显著优于现有的最先进RAG方法，在复杂知识整合和推理方面展现出卓越能力。

Conclusion: CogitoRAG通过模拟人类认知记忆过程，有效解决了现有RAG框架中的语义完整性损失问题，为复杂知识整合和推理提供了更有效的解决方案，展示了基于认知启发的RAG方法的优越性。

Abstract: Retrieval-Augmented Generation (RAG) effectively mitigates hallucinations in LLMs by incorporating external knowledge. However, the inherent discrete representation of text in existing frameworks often results in a loss of semantic integrity, leading to retrieval deviations. Inspired by the human episodic memory mechanism, we propose CogitoRAG, a RAG framework that simulates human cognitive memory processes. The core of this framework lies in the extraction and evolution of the Semantic Gist. During the offline indexing stage, CogitoRAG first deduces unstructured corpora into gist memory corpora, which are then transformed into a multi-dimensional knowledge graph integrating entities, relational facts, and memory nodes. In the online retrieval stage, the framework handles complex queries via Query Decomposition Module that breaks them into comprehensive sub-queries, mimicking the cognitive decomposition humans employ for complex information. Subsequently, Entity Diffusion Module performs associative retrieval across the graph, guided by structural relevance and an entity-frequency reward mechanism. Furthermore, we propose the CogniRank algorithm, which precisely reranks candidate passages by fusing diffusion-derived scores with semantic similarity. The final evidence is delivered to the generator in a passage-memory pairing format, providing high-density information support. Experimental results across five mainstream QA benchmarks and multi-task generation on GraphBench demonstrate that CogitoRAG significantly outperforms state-of-the-art RAG methods, showcasing superior capabilities in complex knowledge integration and reasoning.

</details>


### [31] [Every Little Helps: Building Knowledge Graph Foundation Model with Fine-grained Transferable Multi-modal Tokens](https://arxiv.org/abs/2602.15896)
*Yichi Zhang,Zhuo Chen,Lingbing Guo,Wen Zhang,Huajun Chen*

Main category: cs.CL

TL;DR: TOFU：一种基于token的多模态知识图谱推理基础模型，通过模态特定token和层次融合架构实现跨不同MMKG的强泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有MMKGR方法多为特定数据集设计，缺乏泛化能力；而知识图谱基础模型主要关注结构模式，忽略了丰富的多模态信号。需要一种能够跨不同多模态知识图谱泛化的解决方案。

Method: 提出TOFU模型：1）将结构、视觉和文本信息离散化为模态特定token；2）采用具有混合消息机制的层次融合架构处理这些token；3）获得可用于MMKGR的可迁移特征。

Result: 在17个转导、归纳和完全归纳的MMKG上的实验表明，TOFU始终优于强KGFM和MMKGR基线，在未见过的MMKG上表现出色。

Conclusion: TOFU通过token化多模态信息和层次融合架构，有效解决了MMKGR的跨知识图谱泛化问题，为多模态知识图谱推理提供了强泛化能力的基础模型。

Abstract: Multi-modal knowledge graph reasoning (MMKGR) aims to predict the missing links by exploiting both graph structure information and multi-modal entity contents. Most existing works are designed for a transductive setting, which learns dataset-specific embeddings and struggles to generalize to new KGs. Recent knowledge graph foundation models (KGFMs) improve cross-KG transfer, but they mainly exploit structural patterns and ignore rich multi-modal signals. We address these gaps by proposing a token-based foundation model (TOFU) for MMKGR, which exhibits strong generalization across different MMKGs. TOFU discretizes structural, visual, and textual information into modality-specific tokens. TOFU then employs a hierarchical fusion architecture with mixture-of-message mechanisms, aiming to process these tokens and obtain transferable features for MMKGR. Experimental results on 17 transductive, inductive, and fully-inductive MMKGs show that TOFU consistently outperforms strong KGFM and MMKGR baselines, delivering strong performance on unseen MMKGs.

</details>


### [32] [Mitigating Gradient Inversion Risks in Language Models via Token Obfuscation](https://arxiv.org/abs/2602.15897)
*Xinguo Feng,Zhongkui Ma,Zihan Wang,Alsharif Abuadbba,Guangdong Bai*

Main category: cs.CL

TL;DR: GHOST是一种新的梯度反演攻击防御机制，通过token级别的混淆来切断梯度、嵌入和token空间之间的语义联系，从而保护隐私数据。


<details>
  <summary>Details</summary>
Motivation: 现有的梯度扰动防御方法（如噪声注入、梯度剪枝）在破坏梯度反演攻击的直接映射方面效果有限，因为它们无法完全消除梯度、嵌入和token空间之间的语义相似性。

Method: GHOST采用token级别的混淆机制，包含两个步骤：1) 搜索步骤：使用多标准搜索过程找到语义不同但嵌入接近的候选token作为原始token的影子替代品；2) 选择步骤：选择最优影子token以最小化对训练关键特征的干扰，同时保持与原始token产生的内部输出对齐。

Result: 评估显示GHOST在多种模型架构（从BERT到Llama）和数据集上表现优异：隐私保护方面恢复率低至1%，效用保持方面分类F1高达0.92，困惑度低至5.45，在分类和生成任务中都能有效防御最先进的梯度反演攻击和自适应攻击。

Conclusion: GHOST通过token级别的混淆机制成功切断了梯度、嵌入和token空间之间的内在联系，在保持模型效用的同时提供了强大的隐私保护，为协作学习中的梯度反演攻击防御提供了新思路。

Abstract: Training and fine-tuning large-scale language models largely benefit from collaborative learning, but the approach has been proven vulnerable to gradient inversion attacks (GIAs), which allow adversaries to reconstruct private training data from shared gradients. Existing defenses mainly employ gradient perturbation techniques, e.g., noise injection or gradient pruning, to disrupt GIAs' direct mapping from gradient space to token space. However, these methods often fall short due to the retention of semantics similarity across gradient, embedding, and token spaces. In this work, we propose a novel defense mechanism named GHOST (gradient shield with obfuscated tokens), a token-level obfuscation mechanism that neutralizes GIAs by decoupling the inherent connections across gradient, embedding, and token spaces. GHOST is built upon an important insight: due to the large scale of the token space, there exist semantically distinct yet embedding-proximate tokens that can serve as the shadow substitutes of the original tokens, which enables a semantic disconnection in the token space while preserving the connection in the embedding and gradient spaces. GHOST comprises a searching step, which identifies semantically distinct candidate tokens using a multi-criteria searching process, and a selection step, which selects optimal shadow tokens to ensure minimal disruption to features critical for training by preserving alignment with the internal outputs produced by original tokens. Evaluation across diverse model architectures (from BERT to Llama) and datasets demonstrates the remarkable effectiveness of GHOST in protecting privacy (as low as 1% in recovery rate) and preserving utility (up to 0.92 in classification F1 and 5.45 in perplexity), in both classification and generation tasks against state-of-the-art GIAs and adaptive attack scenarios.

</details>


### [33] [MultiCube-RAG for Multi-hop Question Answering](https://arxiv.org/abs/2602.15898)
*Jimeng Shi,Wei Hu,Runchu Tian,Bowen Jin,Wonbin Kweon,SeongKu Kang,Yunfan Kang,Dingqi Ye,Sizhe Zhou,Shaowen Wang,Jiawei Han*

Main category: cs.CL

TL;DR: MultiCube-RAG：一种基于本体立方体结构的训练免费方法，通过多维度建模和多立方体选择机制，显著提升多跳问答的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成方法在处理多跳问答时面临挑战：传统RAG难以捕捉结构语义，基于图的方法存在噪声和计算成本高的问题，多数方法依赖单步检索而忽略多跳推理过程，基于训练的方法则存在收敛不稳定和计算开销大的问题。

Method: 提出基于本体论的立方体结构，通过多个正交维度建模主题、属性和关系；在此基础上构建MultiCube-RAG，使用多个立方体分别建模不同主题类别，灵活选择最合适的立方体获取相关知识；将复杂多跳查询沿立方体维度分解为简单子查询，并顺序解决每个子查询。

Result: 在四个多跳问答数据集上的实验表明，MultiCube-RAG比各种基线的平均性能提高了8.9%的响应准确率；同时表现出更高的效率和固有的可解释性。

Conclusion: MultiCube-RAG通过本体立方体结构和训练免费的多立方体方法，有效解决了多跳问答中的结构语义建模和多步推理问题，在准确性、效率和可解释性方面均优于现有方法。

Abstract: Multi-hop question answering (QA) necessitates multi-step reasoning and retrieval across interconnected subjects, attributes, and relations. Existing retrieval-augmented generation (RAG) methods struggle to capture these structural semantics accurately, resulting in suboptimal performance. Graph-based RAGs structure such information in graphs, but the resulting graphs are often noisy and computationally expensive. Moreover, most methods rely on single-step retrieval, neglecting the need for multi-hop reasoning processes. Recent training-based approaches attempt to incentivize the large language models (LLMs) for iterative reasoning and retrieval, but their training processes are prone to unstable convergence and high computational overhead. To address these limitations, we devise an ontology-based cube structure with multiple and orthogonal dimensions to model structural subjects, attributes, and relations. Built on the cube structure, we propose MultiCube-RAG, a training-free method consisting of multiple cubes for multi-step reasoning and retrieval. Each cube specializes in modeling a class of subjects, so that MultiCube-RAG flexibly selects the most suitable cubes to acquire the relevant knowledge precisely. To enhance the query-based reasoning and retrieval, our method decomposes a complex multi-hop query into a set of simple subqueries along cube dimensions and conquers each of them sequentially. Experiments on four multi-hop QA datasets show that MultiCube-RAG improves response accuracy by 8.9% over the average performance of various baselines. Notably, we also demonstrate that our method performs with greater efficiency and inherent explainability.

</details>


### [34] [Doc-to-LoRA: Learning to Instantly Internalize Contexts](https://arxiv.org/abs/2602.15902)
*Rujikorn Charakorn,Edoardo Cetin,Shinnosuke Uesaka,Robert Tjarko Lange*

Main category: cs.CL

TL;DR: D2L提出了一种轻量级超网络，能够通过单次前向传播实现近似上下文蒸馏，为LLM生成LoRA适配器，从而在推理时减少延迟和KV缓存内存消耗。


<details>
  <summary>Details</summary>
Motivation: 长输入序列对LLM的上下文学习、文档理解和多步推理至关重要，但Transformer的二次注意力成本使得推理内存密集且缓慢。虽然上下文蒸馏可以将信息转移到模型参数中，但每个提示的蒸馏由于训练成本和延迟而不切实际。

Method: 提出Doc-to-LoRA（D2L），一种轻量级超网络，通过元学习在单次前向传播中执行近似上下文蒸馏。对于未见过的提示，D2L为目标LLM生成LoRA适配器，使后续查询无需重新消耗原始上下文。

Result: 在长上下文"大海捞针"任务中，D2L成功学习将上下文映射到存储针信息的适配器中，在序列长度超过目标LLM原生上下文窗口4倍以上时实现接近完美的零样本准确率。在有限计算资源的真实世界QA数据集上，D2L优于标准上下文蒸馏，同时显著降低峰值内存消耗和更新延迟。

Conclusion: D2L可以促进LLM的快速适应，为频繁知识更新和个性化聊天行为开辟可能性。该方法通过将上下文信息高效压缩到适配器中，解决了长序列推理的内存和延迟问题。

Abstract: Long input sequences are central to in-context learning, document understanding, and multi-step reasoning of Large Language Models (LLMs). However, the quadratic attention cost of Transformers makes inference memory-intensive and slow. While context distillation (CD) can transfer information into model parameters, per-prompt distillation is impractical due to training costs and latency. To address these limitations, we propose Doc-to-LoRA (D2L), a lightweight hypernetwork that meta-learns to perform approximate CD within a single forward pass. Given an unseen prompt, D2L generates a LoRA adapter for a target LLM, enabling subsequent queries to be answered without re-consuming the original context, reducing latency and KV-cache memory consumption during inference of the target LLM. On a long-context needle-in-a-haystack task, D2L successfully learns to map contexts into adapters that store the needle information, achieving near-perfect zero-shot accuracy at sequence lengths exceeding the target LLM's native context window by more than 4x. On real-world QA datasets with limited compute, D2L outperforms standard CD while significantly reducing peak memory consumption and update latency. We envision that D2L can facilitate rapid adaptation of LLMs, opening up the possibility of frequent knowledge updates and personalized chat behavior.

</details>


### [35] [DocSplit: A Comprehensive Benchmark Dataset and Evaluation Approach for Document Packet Recognition and Splitting](https://arxiv.org/abs/2602.15958)
*Md Mofijul Islam,Md Sirajus Salekin,Nivedha Balakrishnan,Vincil C. Bishop,Niharika Jain,Spencer Romo,Bob Strahan,Boyi Xie,Diego A. Socolinsky*

Main category: cs.CL

TL;DR: DocSplit：首个用于评估大语言模型文档包拆分能力的综合基准数据集，包含五个复杂度不同的数据集，涵盖多种文档类型和布局。


<details>
  <summary>Details</summary>
Motivation: 现实应用中的文档理解需要处理异构、多页的文档包，但文档包拆分这一基础任务尚未得到充分研究。目前缺乏系统性评估大语言模型处理此类任务能力的基准。

Method: 创建DocSplit基准数据集，包含五个复杂度不同的数据集，涵盖多种文档类型、布局和多模态设置。提出新的评估指标，并将文档包拆分任务形式化为识别文档边界、分类文档类型和保持正确页面顺序。

Result: 通过对多模态大语言模型进行广泛实验，发现现有模型在处理复杂文档拆分任务时存在显著性能差距。DocSplit基准为评估和提升文档理解能力提供了系统性框架。

Conclusion: DocSplit基准数据集和新颖的评估指标为推进文档理解能力提供了重要工具，特别是在法律、金融、医疗等文档密集型领域。作者已发布数据集以促进未来研究。

Abstract: Document understanding in real-world applications often requires processing heterogeneous, multi-page document packets containing multiple documents stitched together. Despite recent advances in visual document understanding, the fundamental task of document packet splitting, which involves separating a document packet into individual units, remains largely unaddressed. We present the first comprehensive benchmark dataset, DocSplit, along with novel evaluation metrics for assessing the document packet splitting capabilities of large language models. DocSplit comprises five datasets of varying complexity, covering diverse document types, layouts, and multimodal settings. We formalize the DocSplit task, which requires models to identify document boundaries, classify document types, and maintain correct page ordering within a document packet. The benchmark addresses real-world challenges, including out-of-order pages, interleaved documents, and documents lacking clear demarcations. We conduct extensive experiments evaluating multimodal LLMs on our datasets, revealing significant performance gaps in current models' ability to handle complex document splitting tasks. The DocSplit benchmark datasets and proposed novel evaluation metrics provide a systematic framework for advancing document understanding capabilities essential for legal, financial, healthcare, and other document-intensive domains. We release the datasets to facilitate future research in document packet processing.

</details>


### [36] [A Curious Class of Adpositional Multiword Expressions in Korean](https://arxiv.org/abs/2602.16023)
*Junghyun Min,Na-Rae Han,Jena D. Hwang,Nathan Schneider*

Main category: cs.CL

TL;DR: 该论文研究韩语中的功能多词表达式——后置动词结构（PVCs），提出了韩语多词介词注释指南，旨在改善韩语在多语言框架中的代表性。


<details>
  <summary>Details</summary>
Motivation: 韩语多词表达式（特别是多词介词）在跨语言注释框架（如PARSEME）中代表性不足，缺乏系统性分析、注释资源和与现有多语言框架的整合。

Method: 使用韩语维基百科数据，调查和分析多个PVC表达式，并将其与非MWE和结构相似的光动词结构（LVCs）进行对比。

Result: 基于分析提出了注释指南，旨在支持未来韩语多词介词的研究，并促进与跨语言框架的对接。

Conclusion: 该研究为韩语多词表达式的系统分析提供了基础，提出的注释指南将有助于改善韩语在跨语言MWE框架中的整合。

Abstract: Multiword expressions (MWEs) have been widely studied in cross-lingual annotation frameworks such as PARSEME. However, Korean MWEs remain underrepresented in these efforts. In particular, Korean multiword adpositions lack systematic analysis, annotated resources, and integration into existing multilingual frameworks. In this paper, we study a class of Korean functional multiword expressions: postpositional verb-based constructions (PVCs). Using data from Korean Wikipedia, we survey and analyze several PVC expressions and contrast them with non-MWEs and light verb constructions (LVCs) with similar structure. Building on this analysis, we propose annotation guidelines designed to support future work in Korean multiword adpositions and facilitate alignment with cross-lingual frameworks.

</details>


### [37] [CLAA: Cross-Layer Attention Aggregation for Accelerating LLM Prefill](https://arxiv.org/abs/2602.16054)
*Bradley McDanel,Steven Li,Harshit Khaitan*

Main category: cs.CL

TL;DR: 提出了一种基于跨层注意力聚合的预填充加速方法，通过聚合多层注意力分数来稳定token重要性估计，显著减少长上下文LLM推理的首token时间。


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM推理中的预填充阶段存在计算瓶颈，现有的token排序启发式方法通过选择性处理语义相关token来加速，但存在token重要性估计不稳定、在不同层间变化大的问题，且难以独立于特定启发式架构评估排序质量。

Method: 引入Answer-Informed Oracle来定义ground-truth token重要性（通过测量生成答案对提示的注意力），发现现有启发式方法在层间存在高方差；提出跨层注意力聚合（CLAA），通过聚合跨层注意力分数而非依赖单层来稳定token重要性估计。

Result: CLAA方法接近Oracle上界，相比完整KV缓存基线，将首token时间（TTFT）减少高达39%。

Conclusion: 跨层注意力聚合是解决现有token排序启发式方法层间不稳定性问题的有效方案，能显著提升长上下文LLM推理效率。

Abstract: The prefill stage in long-context LLM inference remains a computational bottleneck. Recent token-ranking heuristics accelerate inference by selectively processing a subset of semantically relevant tokens. However, existing methods suffer from unstable token importance estimation, often varying between layers. Evaluating token-ranking quality independently from heuristic-specific architectures is challenging. To address this, we introduce an Answer-Informed Oracle, which defines ground-truth token importance by measuring attention from generated answers back to the prompt. This oracle reveals that existing heuristics exhibit high variance across layers: rankings can degrade sharply at specific layers, a failure mode invisible to end-to-end benchmarks. The diagnosis suggests a simple fix: aggregate scores across layers rather than relying on any single one. We implement this as Cross-Layer Attention Aggregation (CLAA), which closes the gap to the oracle upper bound and reduces Time-to-First-Token (TTFT) by up to 39\% compared to the Full KV Cache baseline.

</details>


### [38] [Surgical Activation Steering via Generative Causal Mediation](https://arxiv.org/abs/2602.16080)
*Aruna Sankaranarayanan,Amir Zur,Atticus Geiger,Dylan Hadfield-Menell*

Main category: cs.CL

TL;DR: GCM（生成因果中介）是一种通过定位和干预语言模型特定组件（如注意力头）来控制长文本生成行为的方法，在拒绝、奉承和风格转换任务上表现优于相关探针基线。


<details>
  <summary>Details</summary>
Motivation: 当需要在语言模型的长文本响应中控制特定行为时，传统方法难以确定干预位置，因为这些行为可能分散在多个标记中。需要一种方法来定位和干预表达这些概念的关键模型组件。

Method: 首先构建对比输入和响应的数据集，然后量化单个模型组件（如注意力头）如何中介对比概念，选择最强的中介组件进行控制。通过干预这些选定的组件来引导模型生成。

Result: GCM在三个语言模型的三个任务（拒绝、奉承和风格转换）上成功定位了长文本响应中表达的概念，并且在使用稀疏注意力头集进行控制时，始终优于基于相关探针的基线方法。

Conclusion: GCM为定位和控制语言模型的长文本响应提供了有效方法，通过因果中介分析实现更精确的模型行为控制。

Abstract: Where should we intervene in a language model (LM) to control behaviors that are diffused across many tokens of a long-form response? We introduce Generative Causal Mediation (GCM), a procedure for selecting model components, e.g., attention heads, to steer a binary concept (e.g., talk in verse vs. talk in prose) from contrastive long-form responses. In GCM, we first construct a dataset of contrasting inputs and responses. Then, we quantify how individual model components mediate the contrastive concept and select the strongest mediators for steering. We evaluate GCM on three tasks--refusal, sycophancy, and style transfer--across three language models. GCM successfully localizes concepts expressed in long-form responses and consistently outperforms correlational probe-based baselines when steering with a sparse set of attention heads. Together, these results demonstrate that GCM provides an effective approach for localizing and controlling the long-form responses of LMs.

</details>


### [39] [Language Statistics and False Belief Reasoning: Evidence from 41 Open-Weight LMs](https://arxiv.org/abs/2602.16085)
*Sean Trott,Samuel Taylor,Cameron Jones,James A. Michaelov,Pamela D. Rivière*

Main category: cs.CL

TL;DR: 该研究通过测试41个开源语言模型在错误信念任务上的表现，发现34%的模型对知识状态敏感，但没有任何模型能完全解释人类效应，同时利用模型行为生成了关于人类认知的新假设。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型的心智状态推理能力，既能促进对人类社会认知理论（如心智状态推理部分源于语言暴露）的理解，也能增进对语言模型自身的认识。现有研究大多依赖少数闭源模型，限制了理论检验和模型能力评估的严谨性。

Method: 复制并扩展已发表的错误信念任务研究，对来自不同模型家族的41个开源权重模型进行测试，分析模型对知识状态的敏感性、模型规模与性能的关系，并利用模型行为生成和检验关于人类认知的新假设。

Result: 34%的测试模型对隐含知识状态表现出敏感性；更大规模的模型显示出更高的敏感性和心理测量预测能力；人类和语言模型都表现出使用非事实动词（如"约翰认为..."）时比间接提示时更容易归因错误信念的偏见；人类的知识状态敏感性效应超过语言模型，但人类知识提示效应的大小正好落在语言模型效应大小的分布范围内。

Conclusion: 研究结果表明，使用更大规模的开源语言模型样本来检验人类认知理论和评估语言模型能力具有重要价值，语言的分布统计特征可能解释人类知识提示效应，但无法完全解释人类对知识状态的主要敏感性效应。

Abstract: Research on mental state reasoning in language models (LMs) has the potential to inform theories of human social cognition--such as the theory that mental state reasoning emerges in part from language exposure--and our understanding of LMs themselves. Yet much published work on LMs relies on a relatively small sample of closed-source LMs, limiting our ability to rigorously test psychological theories and evaluate LM capacities. Here, we replicate and extend published work on the false belief task by assessing LM mental state reasoning behavior across 41 open-weight models (from distinct model families). We find sensitivity to implied knowledge states in 34% of the LMs tested; however, consistent with prior work, none fully ``explain away'' the effect in humans. Larger LMs show increased sensitivity and also exhibit higher psychometric predictive power. Finally, we use LM behavior to generate and test a novel hypothesis about human cognition: both humans and LMs show a bias towards attributing false beliefs when knowledge states are cued using a non-factive verb (``John thinks...'') than when cued indirectly (``John looks in the...''). Unlike the primary effect of knowledge states, where human sensitivity exceeds that of LMs, the magnitude of the human knowledge cue effect falls squarely within the distribution of LM effect sizes-suggesting that distributional statistics of language can in principle account for the latter but not the former in humans. These results demonstrate the value of using larger samples of open-weight LMs to test theories of human cognition and evaluate LM capacities.

</details>


### [40] [Updating Parametric Knowledge with Context Distillation Retains Post-Training Capabilities](https://arxiv.org/abs/2602.16093)
*Shankar Padmanabhan,Mustafa Omer Gul,Tanya Goyal*

Main category: cs.CL

TL;DR: DiSC：一种通过分割上下文进行蒸馏的持续知识适应方法，在适应新知识的同时有效减少遗忘先前技能


<details>
  <summary>Details</summary>
Motivation: 现有方法无法同时学习适应文档中的新知识并减轻对先前学习能力的遗忘，需要一种持续知识适应解决方案

Method: DiSC通过分割训练示例的不同部分来推导学生和教师分布，最小化共享标记之间的KL散度，无需训练时显式生成步骤

Result: 在四个后训练模型和两个适应领域的实验中，DiSC相比现有微调和蒸馏方法，在学习新知识和减轻遗忘方面达到最佳平衡

Conclusion: DiSC提供了一种简单有效的持续知识适应方法，能够同时学习新知识并保持先前习得的技能

Abstract: Post-training endows pretrained LLMs with a variety of desirable skills, including instruction-following, reasoning, and others. However, these post-trained LLMs only encode knowledge up to a cut-off date, necessitating continual adaptation. Unfortunately, existing solutions cannot simultaneously learn new knowledge from an adaptation document corpora and mitigate the forgetting of earlier learned capabilities. To address this, we introduce Distillation via Split Contexts (DiSC), a simple context-distillation based approach for continual knowledge adaptation. \methodname~derives student and teacher distributions by conditioning on distinct segments of the training example and minimizes the KL divergence between the shared tokens. This allows us to efficiently apply context-distillation without requiring explicit generation steps during training. We run experiments on four post-trained models and two adaptation domains. Compared to prior finetuning and distillation methods for continual adaptation, DiSC consistently reports the best trade-off between learning new knowledge and mitigating forgetting of previously learned skills like instruction-following, reasoning, and factual knowledge.

</details>


### [41] [Missing-by-Design: Certifiable Modality Deletion for Revocable Multimodal Sentiment Analysis](https://arxiv.org/abs/2602.16144)
*Rong Fu,Wenxin Zhang,Ziming Wang,Chunlei Meng,Jiaxuan Lu,Jiekai Wu,Kangan Qian,Hao Zhang,Simon Fong*

Main category: cs.CL

TL;DR: MBD框架通过结构化表示学习和可验证参数修改实现多模态情感分析的可撤销性，为隐私敏感应用提供高效替代完全重训练的方案。


<details>
  <summary>Details</summary>
Motivation: 随着多模态系统处理敏感个人数据增多，选择性撤销特定数据模态的能力成为隐私合规和用户自主权的关键需求。

Method: MBD框架结合结构化表示学习与可验证参数修改管道，学习属性感知嵌入并使用生成器重建缺失通道，同时保留任务相关信号。对于删除请求，采用显著性驱动的候选选择和校准高斯更新生成机器可验证的模态删除证书。

Result: 在基准数据集上的实验表明，MBD在不完整输入下实现强大的预测性能，并提供实用的隐私-效用权衡，将手术式遗忘定位为完全重训练的高效替代方案。

Conclusion: MBD为多模态情感分析提供了一种统一的、可验证的撤销框架，解决了隐私敏感应用中模态特定信息删除的关键需求，展示了手术式遗忘在实际应用中的可行性。

Abstract: As multimodal systems increasingly process sensitive personal data, the ability to selectively revoke specific data modalities has become a critical requirement for privacy compliance and user autonomy. We present Missing-by-Design (MBD), a unified framework for revocable multimodal sentiment analysis that combines structured representation learning with a certifiable parameter-modification pipeline. Revocability is critical in privacy-sensitive applications where users or regulators may request removal of modality-specific information. MBD learns property-aware embeddings and employs generator-based reconstruction to recover missing channels while preserving task-relevant signals. For deletion requests, the framework applies saliency-driven candidate selection and a calibrated Gaussian update to produce a machine-verifiable Modality Deletion Certificate. Experiments on benchmark datasets show that MBD achieves strong predictive performance under incomplete inputs and delivers a practical privacy-utility trade-off, positioning surgical unlearning as an efficient alternative to full retraining.

</details>


### [42] [Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution](https://arxiv.org/abs/2602.16154)
*Nithin Sivakumaran,Shoubin Yu,Hyunji Lee,Yue Zhang,Ali Payani,Mohit Bansal,Elias Stengel-Eskin*

Main category: cs.CL

TL;DR: REMUL通过多智能体强化学习方法提高CoT推理的忠实性，在保持任务性能的同时显著提升了多个忠实性指标


<details>
  <summary>Details</summary>
Motivation: 现有CoT推理存在忠实性问题，不能真实反映LLM的计算过程，同时优化忠实性和可解释性会降低任务性能，需要解决这种权衡

Method: 提出REMUL多智能体强化学习方法，包含说话者模型生成推理轨迹，多个听者模型执行该轨迹并继续推理到答案，通过奖励机制激励说话者生成清晰的推理

Result: 在多个推理基准测试中，REMUL显著提升了三个忠实性指标（提示归因、早期回答AOC、错误注入AOC），同时提高了准确性

Conclusion: REMUL有效解决了忠实性与性能的权衡问题，通过多智能体强化学习实现了更忠实、更简洁直接的CoT推理

Abstract: Chain-of-thought (CoT) reasoning sometimes fails to faithfully reflect the true computation of a large language model (LLM), hampering its utility in explaining how LLMs arrive at their answers. Moreover, optimizing for faithfulness and interpretability in reasoning often degrades task performance. To address this tradeoff and improve CoT faithfulness, we propose Reasoning Execution by Multiple Listeners (REMUL), a multi-party reinforcement learning approach. REMUL builds on the hypothesis that reasoning traces which other parties can follow will be more faithful. A speaker model generates a reasoning trace, which is truncated and passed to a pool of listener models who "execute" the trace, continuing the trace to an answer. Speakers are rewarded for producing reasoning that is clear to listeners, with additional correctness regularization via masked supervised finetuning to counter the tradeoff between faithfulness and performance. On multiple reasoning benchmarks (BIG-Bench Extra Hard, MuSR, ZebraLogicBench, and FOLIO), REMUL consistently and substantially improves three measures of faithfulness -- hint attribution, early answering area over the curve (AOC), and mistake injection AOC -- while also improving accuracy. Our analysis finds that these gains are robust across training domains, translate to legibility gains, and are associated with shorter and more direct CoTs.

</details>


### [43] [LLMs Exhibit Significantly Lower Uncertainty in Creative Writing Than Professional Writers](https://arxiv.org/abs/2602.16162)
*Peiqi Sui*

Main category: cs.CL

TL;DR: LLMs在创意写作中因不确定性不足而表现平庸，文学理论认为不确定性是创意的必要条件，但现有对齐策略却抑制了不确定性。研究通过量化人类创作与模型生成之间的"不确定性差距"，发现人类写作的不确定性显著高于模型输出，尤其在创意写作领域。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在创意写作中常被批评为陈词滥调，缺乏新意。文学理论指出不确定性是创造性表达的必要条件，但现有的对齐策略为了确保事实准确性和减少幻觉，反而引导模型远离不确定性输出。这种矛盾限制了LLMs的创意潜力。

Method: 通过信息论分析量化人类创作与模型生成之间的"不确定性差距"。对28个LLMs在高质量故事数据集上进行受控分析，比较人类写作与模型续写的不确定性水平。

Result: 人类写作的不确定性显著高于模型输出；指令微调和推理模型比其基础版本加剧了这一差距；创意写作领域的不确定性差距比功能领域更明显；不确定性差距与写作质量呈强相关。

Conclusion: 要实现人类水平的创造力，需要新的不确定性感知对齐范式，能够区分破坏性的幻觉和文学丰富性所需的建设性模糊性。这要求重新思考LLMs对齐策略，在确保事实准确性的同时保留创意所需的不确定性。

Abstract: We argue that uncertainty is a key and understudied limitation of LLMs' performance in creative writing, which is often characterized as trite and cliché-ridden. Literary theory identifies uncertainty as a necessary condition for creative expression, while current alignment strategies steer models away from uncertain outputs to ensure factuality and reduce hallucination. We formalize this tension by quantifying the "uncertainty gap" between human-authored stories and model-generated continuations. Through a controlled information-theoretic analysis of 28 LLMs on high-quality storytelling datasets, we demonstrate that human writing consistently exhibits significantly higher uncertainty than model outputs. We find that instruction-tuned and reasoning models exacerbate this trend compared to their base counterparts; furthermore, the gap is more pronounced in creative writing than in functional domains, and strongly correlates to writing quality. Achieving human-level creativity requires new uncertainty-aware alignment paradigms that can distinguish between destructive hallucinations and the constructive ambiguity required for literary richness.

</details>


### [44] [Beyond Learning: A Training-Free Alternative to Model Adaptation](https://arxiv.org/abs/2602.16189)
*Namkyung Yoon,Kyeonghyun Yoo,Wooyong Jung,Sanghong Kim,Hwangnam Kim*

Main category: cs.CL

TL;DR: 该论文提出了一种新的语言模型移植技术，通过识别特定任务下激活变化一致的局部模块，并将其移植到目标模型中，无需额外训练即可实现即时功能改进。


<details>
  <summary>Details</summary>
Motivation: 尽管语言模型不断演进，但新版本有时反而性能下降。现有解决方案资源密集，需要能够立即采取行动的替代方法。作者假设每个语言模型内部都存在适合特定功能的局部模块。

Method: 首先通过激活分析识别在推理任务下表现一致且局部激活变化的模块，然后将针对特定任务适当激活的内部模块移植到目标模型中，无需额外训练或微调。

Result: 在跨代移植实验中，移植激活选择的模块可使性能不佳的模型提升至目标基线的两倍，实现100%以上的差距恢复。在基础模型与指令调优模型间的移植中，性能提升可达目标基线的2.33倍，差距恢复最高达100%。

Conclusion: 这项工作为语言模型中任务局部模块化提供了实证证据，并提出了模型移植这一新研究领域，表明通过植入高度局部化的模块可以实现有意义的容量转移。

Abstract: Despite the continuous research and evolution of language models, they sometimes underperform previous versions. Existing approaches to overcome these challenges are resource-intensive, highlighting the need for alternatives that enable immediate action. We assume that each language model has a local module inside that is suitable for a specific function. First, this work identifies a set of modules showing consistent and local activation changes under an inference workload through activation-based analysis. Subsequently, we transplant an internal module that is properly activated for a specific task into the target model, leading to immediate and measurable functional changes without additional training or fine-tuning. To experimentally demonstrate the effectiveness of the transplant technique, we quantify the relationship between transplant strength and performance improvement under different conditions for two language models. In the cross-generation setting, we find that transplanting activation-selected modules can substantially improve the underperforming model, reaching up to twice the target baseline and achieving gap-based recovery above 100%. Moreover, in transplant experiments between a base model and its instruction-tuned counterpart, transplantation improves the underperforming model toward the stronger baseline, yielding up to about 2.33 times the target baseline with gap-based recovery reaching up to 100% in the best case. These results show that meaningful capacity transfer can be realized through the implantation of highly localized modules implied by language models. Overall, this work provides empirical evidence for task-localized modularity in language models and presents a new research area: model transplantation.

</details>


### [45] [The Validity of Coreference-based Evaluations of Natural Language Understanding](https://arxiv.org/abs/2602.16200)
*Ian Porada*

Main category: cs.CL

TL;DR: 该论文分析了指代消解评估中的测量效度问题，提出了基于事件相对合理性推断的新评估方法，揭示了当前语言模型在标准基准上表现良好但泛化能力有限的问题。


<details>
  <summary>Details</summary>
Motivation: 指代消解评估存在测量效度问题，包括定义争议和收敛效度不足，导致评估结果可能无法得出普遍适用的结论。作者旨在通过扩展评估实践来更好地理解指代消解评估的有效性。

Method: 1) 分析标准指代消解评估的设计问题，识别测量效度问题；2) 提出并实施基于事件相对合理性推断的新评估方法，测试系统在指代消解关键方面的能力。

Result: 当代语言模型在标准基准测试中表现良好，在特定领域和指代类型上超越了早期基线系统。然而，当评估语境稍有变化时，它们缺乏人类应有的泛化能力，对评估条件敏感。

Conclusion: 研究阐明了当前NLP范式的优势（在广泛使用的评估中准确性提高）和局限性（测量效度弱点），为开发更好的评估方法和真正可泛化的系统指明了方向。

Abstract: In this thesis, I refine our understanding as to what conclusions we can reach from coreference-based evaluations by expanding existing evaluation practices and considering the extent to which evaluation results are either converging or conflicting. First, I analyze standard coreference evaluations and show that their design often leads to non-generalizable conclusions due to issues of measurement validity - including contestedness (multiple, competing definitions of coreference) and convergent validity (evaluation results that rank models differently across benchmarks). Second, I propose and implement a novel evaluation focused on testing systems' ability to infer the relative plausibility of events, a key aspect of resolving coreference. Through this extended evaluation, I find that contemporary language models demonstrate strong performance on standard benchmarks - improving over earlier baseline systems within certain domains and types of coreference - but remain sensitive to the evaluation conditions: they often fail to generalize in ways one would expect a human to be capable of when evaluation contexts are slightly modified. Taken together, these findings clarify both the strengths, such as improved accuracy over baselines on widely used evaluations, and the limitations of the current NLP paradigm, including weaknesses in measurement validity, and suggest directions for future work in developing better evaluation methods and more genuinely generalizable systems.

</details>


### [46] [Long-Tail Knowledge in Large Language Models: Taxonomy, Mechanisms, Interventions and Implications](https://arxiv.org/abs/2602.16201)
*Sanket Badhe,Deep Shah,Nehal Kathrotia*

Main category: cs.CL

TL;DR: 本文提出了一个分析大语言模型中长尾知识的系统框架，综合了技术和社会技术视角，探讨了长尾知识的定义、丢失机制、缓解方法及其对公平性、问责制和透明度的影响。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在网页规模语料上训练，这些语料呈现陡峭的幂律分布，导致大部分知识出现频率很低。虽然模型规模扩大提高了平均性能，但在低频、领域特定、文化和时间性知识上的持续失败问题仍未得到充分表征。需要系统分析长尾知识在大语言模型中的表现和影响。

Method: 开发了一个结构化分析框架，综合了先前工作，从四个互补维度进行分析：1) 长尾知识的定义方式；2) 训练和推理过程中知识丢失或扭曲的机制；3) 缓解这些失败的技术干预措施；4) 这些失败对公平性、问责制、透明度和用户信任的影响。同时考察现有评估实践如何掩盖尾部行为并复杂化罕见但重要失败的问责。

Result: 提出了一个统一的概念框架，用于理解长尾知识在部署的语言模型系统中如何被定义、丢失、评估和表现。该框架综合了技术和社会技术视角，为理解大语言模型中的长尾知识提供了系统性分析工具。

Conclusion: 本文为理解大语言模型中的长尾知识提供了一个统一的概念框架，指出了与隐私、可持续性和治理相关的开放挑战，这些挑战限制了长尾知识的表示。框架有助于更好地理解长尾知识在语言模型系统中的定义、丢失、评估和表现方式。

Abstract: Large language models (LLMs) are trained on web-scale corpora that exhibit steep power-law distributions, in which the distribution of knowledge is highly long-tailed, with most appearing infrequently. While scaling has improved average-case performance, persistent failures on low-frequency, domain-specific, cultural, and temporal knowledge remain poorly characterized. This paper develops a structured taxonomy and analysis of long-Tail Knowledge in large language models, synthesizing prior work across technical and sociotechnical perspectives.
  We introduce a structured analytical framework that synthesizes prior work across four complementary axes: how long-Tail Knowledge is defined, the mechanisms by which it is lost or distorted during training and inference, the technical interventions proposed to mitigate these failures, and the implications of these failures for fairness, accountability, transparency, and user trust. We further examine how existing evaluation practices obscure tail behavior and complicate accountability for rare but consequential failures. The paper concludes by identifying open challenges related to privacy, sustainability, and governance that constrain long-Tail Knowledge representation. Taken together, this paper provides a unifying conceptual framework for understanding how long-Tail Knowledge is defined, lost, evaluated, and manifested in deployed language model systems.

</details>


### [47] [Are LLMs Ready to Replace Bangla Annotators?](https://arxiv.org/abs/2602.16241)
*Md. Najib Hasan,Touseef Hasan,Souvika Sarkar*

Main category: cs.CL

TL;DR: LLMs作为零样本标注器在孟加拉语仇恨言论任务中表现出显著偏见和不稳定性，模型规模增加并不能保证标注质量提升


<details>
  <summary>Details</summary>
Motivation: LLMs越来越多地用于自动化数据集标注，但它们在低资源和身份敏感设置中作为无偏见标注器的可靠性仍然知之甚少，特别是在人类标注者也难以达成一致的任务中

Method: 使用统一评估框架对17个LLMs进行系统性基准测试，研究它们在孟加拉语仇恨言论检测任务中的零样本标注行为

Result: 发现LLMs存在标注偏见和显著的不稳定性，令人惊讶的是模型规模增加并不能保证标注质量提升，较小的、与任务更对齐的模型通常表现出比大型模型更一致的行为

Conclusion: 当前LLMs在低资源语言敏感标注任务中存在重要局限性，强调了在部署前需要进行仔细评估的必要性

Abstract: Large Language Models (LLMs) are increasingly used as automated annotators to scale dataset creation, yet their reliability as unbiased annotators--especially for low-resource and identity-sensitive settings--remains poorly understood. In this work, we study the behavior of LLMs as zero-shot annotators for Bangla hate speech, a task where even human agreement is challenging, and annotator bias can have serious downstream consequences. We conduct a systematic benchmark of 17 LLMs using a unified evaluation framework. Our analysis uncovers annotator bias and substantial instability in model judgments. Surprisingly, increased model scale does not guarantee improved annotation quality--smaller, more task-aligned models frequently exhibit more consistent behavior than their larger counterparts. These results highlight important limitations of current LLMs for sensitive annotation tasks in low-resource languages and underscore the need for careful evaluation before deployment.

</details>


### [48] [Aladdin-FTI @ AMIYA Three Wishes for Arabic NLP: Fidelity, Diglossia, and Multidialectal Generation](https://arxiv.org/abs/2602.16290)
*Jonathan Mutal,Perla Al Almaoui,Simon Hengchen,Pierrette Bouillon*

Main category: cs.CL

TL;DR: 本文提出了Aladdin-FTI系统，用于阿拉伯方言的生成和翻译，支持摩洛哥、埃及、巴勒斯坦、叙利亚、沙特等五种方言与现代标准阿拉伯语及英语之间的双向翻译。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯方言由于非标准化和高度变异性，在自然语言处理研究中长期被忽视，而大型语言模型为将其建模为多中心语言而非单一系统提供了新机遇。

Method: 提出Aladdin-FTI系统，作为AMIYA共享任务的提交方案，专门设计用于生成和翻译阿拉伯方言文本。

Result: 系统支持摩洛哥、埃及、巴勒斯坦、叙利亚、沙特五种阿拉伯方言的文本生成，以及这些方言与现代标准阿拉伯语、英语之间的双向翻译。

Conclusion: 该工作通过公开代码和训练模型，为阿拉伯方言的自然语言处理研究提供了实用工具，有助于弥合阿拉伯语言变体在计算建模中的研究差距。

Abstract: Arabic dialects have long been under-represented in Natural Language Processing (NLP) research due to their non-standardization and high variability, which pose challenges for computational modeling. Recent advances in the field, such as Large Language Models (LLMs), offer promising avenues to address this gap by enabling Arabic to be modeled as a pluricentric language rather than a monolithic system. This paper presents Aladdin-FTI, our submission to the AMIYA shared task. The proposed system is designed to both generate and translate dialectal Arabic (DA). Specifically, the model supports text generation in Moroccan, Egyptian, Palestinian, Syrian, and Saudi dialects, as well as bidirectional translation between these dialects, Modern Standard Arabic (MSA), and English. The code and trained model are publicly available.

</details>


### [49] [MultiCW: A Large-Scale Balanced Benchmark Dataset for Training Robust Check-Worthiness Detection Models](https://arxiv.org/abs/2602.16298)
*Martin Hyben,Sebastian Kula,Jan Cegin,Jakub Simko,Ivan Srba,Robert Moro*

Main category: cs.CL

TL;DR: MultiCW数据集：首个平衡的多语言可核查声明检测基准，涵盖16种语言、7个主题领域和2种写作风格，用于评估微调模型与零样本LLM在事实核查中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型正在改变媒体专业人员验证信息的方式，但在事实核查的关键步骤——检测值得核查的声明方面，自动化支持仍然有限。现有数据集存在语言覆盖不足、类别不平衡等问题。

Method: 提出了MultiCW数据集，包含123,722个样本，均匀分布在16种语言、7个主题领域和2种写作风格（非正式/正式）中，并平衡了可核查和不可核查类别。同时创建了27,761个样本的OOD评估集，涵盖4种额外语言。基准测试包括3个微调多语言transformer模型和15个商业/开源LLM的零样本设置。

Result: 微调模型在声明分类任务上持续优于零样本LLMs，并在跨语言、领域和风格的OOD泛化方面表现强劲。MultiCW为系统比较微调模型与前沿LLM提供了可靠基准。

Conclusion: MultiCW数据集为推进自动化事实核查提供了严格的多语言资源，能够系统比较微调模型与前沿LLM在可核查声明检测任务上的性能，表明微调方法在当前任务中仍具优势。

Abstract: Large Language Models (LLMs) are beginning to reshape how media professionals verify information, yet automated support for detecting check-worthy claims a key step in the fact-checking process remains limited. We introduce the Multi-Check-Worthy (MultiCW) dataset, a balanced multilingual benchmark for check-worthy claim detection spanning 16 languages, 7 topical domains, and 2 writing styles. It consists of 123,722 samples, evenly distributed between noisy (informal) and structured (formal) texts, with balanced representation of check-worthy and non-check-worthy classes across all languages. To probe robustness, we also introduce an equally balanced out-of-distribution evaluation set of 27,761 samples in 4 additional languages. To provide baselines, we benchmark 3 common fine-tuned multilingual transformers against a diverse set of 15 commercial and open LLMs under zero-shot settings. Our findings show that fine-tuned models consistently outperform zero-shot LLMs on claim classification and show strong out-of-distribution generalization across languages, domains, and styles. MultiCW provides a rigorous multilingual resource for advancing automated fact-checking and enables systematic comparisons between fine-tuned models and cutting-edge LLMs on the check-worthy claim detection task.

</details>


### [50] [MemoryArena: Benchmarking Agent Memory in Interdependent Multi-Session Agentic Tasks](https://arxiv.org/abs/2602.16313)
*Zexue He,Yu Wang,Churan Zhi,Yuanzhe Hu,Tzu-Ping Chen,Lang Yin,Ze Chen,Tong Arthur Wu,Siru Ouyang,Zihan Wang,Jiaxin Pei,Julian McAuley,Yejin Choi,Alex Pentland*

Main category: cs.CL

TL;DR: 提出了MemoryArena基准测试，用于评估智能体在记忆-智能体-环境循环中的记忆能力，发现现有长上下文记忆基准测试表现良好的智能体在真实任务中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法存在局限性：一类基准测试只评估记忆回忆，不考察记忆如何指导未来决策；另一类关注单会话任务，不需要长期记忆。真实场景中记忆与行动紧密耦合，智能体需要在交互中获取记忆并依赖记忆解决未来任务。

Method: 提出MemoryArena，一个统一的评估框架，用于多会话记忆-智能体-环境循环中的智能体记忆基准测试。该基准包含人工设计的具有明确相互依赖子任务的智能体任务。

Result: 在MemoryArena上评估发现，在现有长上下文记忆基准（如LoCoMo）上表现接近饱和的智能体，在智能体设置中表现不佳，暴露了当前智能体记忆评估的差距。

Conclusion: MemoryArena揭示了现有记忆评估方法的不足，为智能体记忆能力提供了更真实的评估框架，支持网络导航、偏好约束规划、渐进信息搜索和序列形式推理等多种任务评估。

Abstract: Existing evaluations of agents with memory typically assess memorization and action in isolation. One class of benchmarks evaluates memorization by testing recall of past conversations or text but fails to capture how memory is used to guide future decisions. Another class focuses on agents acting in single-session tasks without the need for long-term memory. However, in realistic settings, memorization and action are tightly coupled: agents acquire memory while interacting with the environment, and subsequently rely on that memory to solve future tasks. To capture this setting, we introduce MemoryArena, a unified evaluation gym for benchmarking agent memory in multi-session Memory-Agent-Environment loops. The benchmark consists of human-crafted agentic tasks with explicitly interdependent subtasks, where agents must learn from earlier actions and feedback by distilling experiences into memory, and subsequently use that memory to guide later actions to solve the overall task. MemoryArena supports evaluation across web navigation, preference-constrained planning, progressive information search, and sequential formal reasoning, and reveals that agents with near-saturated performance on existing long-context memory benchmarks like LoCoMo perform poorly in our agentic setting, exposing a gap in current evaluations for agents with memory.

</details>


### [51] [Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents](https://arxiv.org/abs/2602.16346)
*Nivya Talokar,Ayush K Tarun,Murari Mandal,Maksym Andriushchenko,Antoine Bosselut*

Main category: cs.CL

TL;DR: STING是一个用于评估AI智能体在多轮交互中执行非法任务风险的自动化红队框架，通过构建基于良性人设的逐步非法计划并进行自适应探测，显著提高了非法任务完成率。


<details>
  <summary>Details</summary>
Motivation: 现有智能体滥用基准主要测试单轮指令，缺乏对多轮交互中智能体如何逐步协助有害或非法任务的测量。现实部署中，智能体交互本质上是多轮且多语言的。

Method: STING框架构建基于良性人设的逐步非法计划，通过自适应后续提问迭代探测目标智能体，使用评估智能体跟踪阶段完成情况。将多轮红队建模为"首次越狱"时间随机变量，支持发现曲线、攻击语言风险比归因等分析工具。

Result: 在AgentHarm场景中，STING比单轮提示和适应工具使用智能体的聊天式多轮基线显著提高非法任务完成率。在六种非英语环境的多语言评估中，攻击成功率和非法任务完成率在低资源语言中并未一致增加，与常见聊天机器人发现不同。

Conclusion: STING为评估和压力测试智能体在现实部署环境中的滥用提供了实用方法，这些环境本质上是多轮交互且经常是多语言的。

Abstract: LLM-based agents execute real-world workflows via tools and memory. These affordances enable ill-intended adversaries to also use these agents to carry out complex misuse scenarios. Existing agent misuse benchmarks largely test single-prompt instructions, leaving a gap in measuring how agents end up helping with harmful or illegal tasks over multiple turns. We introduce STING (Sequential Testing of Illicit N-step Goal execution), an automated red-teaming framework that constructs a step-by-step illicit plan grounded in a benign persona and iteratively probes a target agent with adaptive follow-ups, using judge agents to track phase completion. We further introduce an analysis framework that models multi-turn red-teaming as a time-to-first-jailbreak random variable, enabling analysis tools like discovery curves, hazard-ratio attribution by attack language, and a new metric: Restricted Mean Jailbreak Discovery. Across AgentHarm scenarios, STING yields substantially higher illicit-task completion than single-turn prompting and chat-oriented multi-turn baselines adapted to tool-using agents. In multilingual evaluations across six non-English settings, we find that attack success and illicit-task completion do not consistently increase in lower-resource languages, diverging from common chatbot findings. Overall, STING provides a practical way to evaluate and stress-test agent misuse in realistic deployment settings, where interactions are inherently multi-turn and often multilingual.

</details>


### [52] [Label-Consistent Data Generation for Aspect-Based Sentiment Analysis Using LLM Agents](https://arxiv.org/abs/2602.16379)
*Mohammad H. A. Monfared,Lucie Flek,Akbar Karimi*

Main category: cs.CL

TL;DR: 本文提出了一种用于方面级情感分析（ABSA）的代理数据增强方法，通过迭代生成和验证来产生高质量合成训练样本。与提示方法相比，代理增强在标签保持方面表现更好，特别是在需要生成方面术语的任务中。


<details>
  <summary>Details</summary>
Motivation: 当前ABSA任务面临数据稀缺的挑战，传统数据增强方法难以保证合成数据的质量。本文旨在开发一种能够产生高质量、标签一致的合成训练数据的增强方法。

Method: 提出代理数据增强方法，采用迭代生成和验证机制来创建合成训练样本。同时设计了与该方法紧密匹配的提示基线，使用相同的模型和指令，以隔离代理结构的影响。

Result: 代理增强在标签保持方面优于原始提示方法，特别是在需要生成方面术语的任务中。与真实数据结合时，代理增强提供了更高的性能提升，始终优于基于提示的生成。这些优势在T5-Base上最为明显，而预训练更充分的Tk-Instruct改进较小。

Conclusion: 代理数据增强方法能够有效提升ABSA任务的性能，特别是在数据稀缺场景下。该方法使T5-Base能够达到与更强大模型相当的性能水平，为ABSA任务提供了一种有效的解决方案。

Abstract: We propose an agentic data augmentation method for Aspect-Based Sentiment Analysis (ABSA) that uses iterative generation and verification to produce high quality synthetic training examples. To isolate the effect of agentic structure, we also develop a closely matched prompting-based baseline using the same model and instructions. Both methods are evaluated across three ABSA subtasks (Aspect Term Extraction (ATE), Aspect Sentiment Classification (ATSC), and Aspect Sentiment Pair Extraction (ASPE)), four SemEval datasets, and two encoder-decoder models: T5-Base and Tk-Instruct. Our results show that the agentic augmentation outperforms raw prompting in label preservation of the augmented data, especially when the tasks require aspect term generation. In addition, when combined with real data, agentic augmentation provides higher gains, consistently outperforming prompting-based generation. These benefits are most pronounced for T5-Base, while the more heavily pretrained Tk-Instruct exhibits smaller improvements. As a result, augmented data helps T5-Base achieve comparable performance with its counterpart.

</details>


### [53] [TabAgent: A Framework for Replacing Agentic Generative Components with Tabular-Textual Classifiers](https://arxiv.org/abs/2602.16429)
*Ido Levy,Eilam Shapira,Yinon Goldshtein,Avi Yaeli,Nir Mashkif,Segev Shlomov*

Main category: cs.CL

TL;DR: TabAgent：用轻量级表格分类器替代LLM决策组件，显著降低Agentic系统延迟和成本


<details>
  <summary>Details</summary>
Motivation: 现有Agentic系统依赖重复的LLM调用进行封闭集决策任务（如路由、筛选、门控、验证），导致部署速度慢、成本高，因为累积延迟和token使用量大。

Method: TabAgent框架：1) TabSchema从执行轨迹中提取结构化模式、状态和依赖特征；2) TabSynth通过模式对齐的合成监督增强覆盖范围；3) TabHead使用轻量级分类器对候选方案进行评分。

Result: 在长时域AppWorld基准测试中，TabAgent在保持任务级成功率的同时，消除了短列表时间的LLM调用，延迟降低约95%，推理成本降低85-91%。

Conclusion: TabAgent不仅适用于工具短列表，还能推广到其他Agentic决策头，为生产Agent架构中的生成瓶颈提供了学习型判别替代范式。

Abstract: Agentic systems, AI architectures that autonomously execute multi-step workflows to achieve complex goals, are often built using repeated large language model (LLM) calls for closed-set decision tasks such as routing, shortlisting, gating, and verification. While convenient, this design makes deployments slow and expensive due to cumulative latency and token usage. We propose TabAgent, a framework for replacing generative decision components in closed-set selection tasks with a compact textual-tabular classifier trained on execution traces. TabAgent (i) extracts structured schema, state, and dependency features from trajectories (TabSchema), (ii) augments coverage with schema-aligned synthetic supervision (TabSynth), and (iii) scores candidates with a lightweight classifier (TabHead). On the long-horizon AppWorld benchmark, TabAgent maintains task-level success while eliminating shortlist-time LLM calls, reducing latency by approximately 95% and inference cost by 85-91%. Beyond tool shortlisting, TabAgent generalizes to other agentic decision heads, establishing a paradigm for learned discriminative replacements of generative bottlenecks in production agent architectures.

</details>


### [54] [IndicEval: A Bilingual Indian Educational Evaluation Framework for Large Language Models](https://arxiv.org/abs/2602.16467)
*Saurabh Bharti,Gaurav Azad,Abhinaw Jagtap,Nachiket Tapas*

Main category: cs.CL

TL;DR: IndicEval是一个基于真实高难度考试题目的多语言基准测试平台，用于评估大语言模型在STEM和人文领域的推理能力、领域知识和双语适应性，揭示了思维链提示的有效性、模型性能差异以及多语言能力下降等关键发现。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型评估框架往往使用合成数据，无法反映真实世界的学术严谨性和多语言复杂性。需要基于真实高难度考试题目构建评估基准，以更真实地衡量模型在双语环境下的推理能力和领域知识。

Method: 构建IndicEval平台，使用来自UPSC、JEE和NEET等真实高难度考试中的英语和印地语题目。采用零样本、少样本和思维链提示策略进行自动化评估，支持新模型和语言的模块化集成。

Result: 对Gemini 2.0 Flash、GPT-4、Claude和LLaMA 3-70B的实验显示：1) 思维链提示能显著提升推理准确性；2) 在高复杂度考试中存在显著的模型性能差异；3) 多语言能力下降明显，印地语准确率显著低于英语，特别是在零样本条件下。

Conclusion: IndicEval为多语言教育环境中的大语言模型评估提供了实用、可扩展的基础，揭示了双语推理和领域迁移方面的持续差距，为改进推理鲁棒性和语言适应性提供了可操作的见解。

Abstract: The rapid advancement of large language models (LLMs) necessitates evaluation frameworks that reflect real-world academic rigor and multilingual complexity. This paper introduces IndicEval, a scalable benchmarking platform designed to assess LLM performance using authentic high-stakes examination questions from UPSC, JEE, and NEET across STEM and humanities domains in both English and Hindi. Unlike synthetic benchmarks, IndicEval grounds evaluation in real examination standards, enabling realistic measurement of reasoning, domain knowledge, and bilingual adaptability. The framework automates assessment using Zero-Shot, Few-Shot, and Chain-of-Thought (CoT) prompting strategies and supports modular integration of new models and languages. Experiments conducted on Gemini 2.0 Flash, GPT-4, Claude, and LLaMA 3-70B reveal three major findings. First, CoT prompting consistently improves reasoning accuracy, with substantial gains across subjects and languages. Second, significant cross-model performance disparities persist, particularly in high-complexity examinations. Third, multilingual degradation remains a critical challenge, with marked accuracy drops in Hindi compared to English, especially under Zero-Shot conditions. These results highlight persistent gaps in bilingual reasoning and domain transfer. Overall, IndicEval provides a practice-oriented, extensible foundation for rigorous, equitable evaluation of LLMs in multilingual educational settings and offers actionable insights for improving reasoning robustness and language adaptability.

</details>


### [55] [Training Models on Dialects of Translationese Shows How Lexical Diversity and Source-Target Syntactic Similarity Shape Learning](https://arxiv.org/abs/2602.16469)
*Jenny Kunz*

Main category: cs.CL

TL;DR: 研究机器翻译数据对小型英语语言模型的影响，发现源语言通过翻译语对模型的语言可接受性判断和语言建模产生系统性影响。


<details>
  <summary>Details</summary>
Motivation: 机器翻译数据在多语言NLP中被广泛使用，但翻译文本（翻译语）与原生文本存在系统性差异，包含源语言痕迹和翻译过程本身的特征。研究这种差异如何影响模型学习非常重要。

Method: 使用从24种类型学和资源多样性源语言翻译而来的英语文本训练小型英语语言模型，系统分析源语言和语料库特性如何影响模型学习。

Result: 源语言对模型行为有显著影响：通用困惑度主要受翻译语料库词汇多样性的驱动，而语法性能在足够数据情况下与源语言和英语的类型学相似性高度相关。

Conclusion: 源语言特性通过翻译语显著影响模型学习，词汇多样性和类型学相似性分别驱动模型的不同能力，这对使用机器翻译数据训练模型具有重要意义。

Abstract: Machine-translated data is widely used in multilingual NLP, particularly when native text is scarce. However, translated text differs systematically from native text. This phenomenon is known as translationese, and it reflects both traces of the source language and characteristic properties of translation itself. In this paper, we study how training on machine-translated data affects small English language models, focusing on how translationese from different source languages shapes linguistic acceptability judgments and language modelling for different domains. We train models on English text translated from 24 typologically and resource-diverse source languages, enabling a systematic analysis of how source language and corpus properties influence what models learn. Our results show that the source language has a clear impact on model behavior: general perplexity is more driven by the lexical diversity of the translated corpus, while grammatical performance is strongly correlated to typological similarity to English, given enough data.

</details>


### [56] [Team of Thoughts: Efficient Test-time Scaling of Agentic Systems through Orchestrated Tool Calling](https://arxiv.org/abs/2602.16485)
*Jeffrey T. H. Wong,Zixi Zhang,Junyi Liu,Yiren Zhao*

Main category: cs.CL

TL;DR: Team-of-Thoughts：一种利用异构代理互补能力的多智能体系统架构，通过编排器-工具范式动态激活最适合的工具代理，在推理和代码生成任务上显著优于同质化基线。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统通常依赖静态、同质的模型配置，无法充分利用不同后训练模型的独特优势。需要一种能够利用异构代理互补能力的架构。

Method: 提出Team-of-Thoughts架构，包含两个关键机制：1）编排器校准方案，识别具有卓越协调能力的模型；2）自我评估协议，工具代理分析自身领域专长以考虑后训练技能差异。在推理时，编排器根据能力配置文件动态激活最合适的工具代理。

Result: 在五个推理和代码生成基准测试中，Team-of-Thoughts始终表现出优越的任务性能。在AIME24和LiveCodeBench上分别达到96.67%和72.53%的准确率，显著优于同质化角色扮演基线（80%和65.93%）。

Conclusion: Team-of-Thoughts通过异构代理协作和动态编排，有效解决了传统多智能体系统在利用不同模型优势方面的局限性，为复杂任务提供了更灵活、更有效的解决方案。

Abstract: Existing Multi-Agent Systems (MAS) typically rely on static, homogeneous model configurations, limiting their ability to exploit the distinct strengths of differently post-trained models. To address this, we introduce Team-of-Thoughts, a novel MAS architecture that leverages the complementary capabilities of heterogeneous agents via an orchestrator-tool paradigm. Our framework introduces two key mechanisms to optimize performance: (1) an orchestrator calibration scheme that identifies models with superior coordination capabilities, and (2) a self-assessment protocol where tool agents profile their own domain expertise to account for variations in post-training skills. During inference, the orchestrator dynamically activates the most suitable tool agents based on these proficiency profiles. Experiments on five reasoning and code generation benchmarks show that Team-of-Thoughts delivers consistently superior task performance. Notably, on AIME24 and LiveCodeBench, our approach achieves accuracies of 96.67% and 72.53%, respectively, substantially outperforming homogeneous role-play baselines, which score 80% and 65.93%.

</details>


### [57] [Learning to Learn from Language Feedback with Social Meta-Learning](https://arxiv.org/abs/2602.16488)
*Jonathan Cook,Diego Antognini,Martin Klissarov,Claudiu Musat,Edward Grefenstette*

Main category: cs.CL

TL;DR: 论文提出了一种基于社会元学习（SML）的微调方法，让大语言模型学会在对话中主动寻求反馈并从中学习，从而提升解决复杂问题的能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在对话中难以从纠正性反馈中学习，很少主动寻求反馈（尤其是在面对模糊性时），导致对话显得静态、单向，缺乏人类对话的适应性。

Method: 受人类社会元学习启发，将SML制定为微调方法，在大语言模型上训练其从模拟教学对话中的语言反馈中学习和寻求反馈，将静态任务转化为交互式社会学习问题。

Result: SML训练后的模型能够利用对话解决单轮无法解决的问题，这种能力具有跨领域泛化性（数学问题训练可提升编程问题解决能力，反之亦然）。即使只在明确指定问题上训练，模型也能更好地解决信息在多轮中逐步揭示的模糊任务，减少过早回答尝试，更可能主动询问所需信息。

Conclusion: 这项工作为开发能有效从语言反馈中学习的AI系统提供了一种可扩展的方法。

Abstract: Large language models (LLMs) often struggle to learn from corrective feedback within a conversational context. They are rarely proactive in soliciting this feedback, even when faced with ambiguity, which can make their dialogues feel static, one-sided, and lacking the adaptive qualities of human conversation. To address these limitations, we draw inspiration from social meta-learning (SML) in humans - the process of learning how to learn from others. We formulate SML as a finetuning methodology, training LLMs to solicit and learn from language feedback in simulated pedagogical dialogues, where static tasks are converted into interactive social learning problems. SML effectively teaches models to use conversation to solve problems they are unable to solve in a single turn. This capability generalises across domains; SML on math problems produces models that better use feedback to solve coding problems and vice versa. Furthermore, despite being trained only on fully-specified problems, these models are better able to solve underspecified tasks where critical information is revealed over multiple turns. When faced with this ambiguity, SML-trained models make fewer premature answer attempts and are more likely to ask for the information they need. This work presents a scalable approach to developing AI systems that effectively learn from language feedback.

</details>


### [58] [From Growing to Looping: A Unified View of Iterative Computation in LLMs](https://arxiv.org/abs/2602.16490)
*Ferdinand Kapl,Emmanouil Angelis,Kaitlin Maile,Johannes von Oswald,Stefan Bauer*

Main category: cs.CL

TL;DR: 论文统一了循环架构与深度增长两种方法，揭示了它们在深度维度上的收敛特征，表明两者都通过迭代计算提升推理能力，并且可以相互适配和组合使用。


<details>
  <summary>Details</summary>
Motivation: 循环架构（重复使用层块）和深度增长（从浅到深训练模型）都被认为能增强推理能力，但它们之间的关系尚不清楚。研究者希望从机制上统一这两种方法，探究它们是否共享共同的迭代计算模式。

Method: 通过分析循环模型和深度增长模型在深度维度上的特征，包括对后期层的依赖增加以及与循环块或增长块对齐的重复模式。在此基础上，展示了两种技术的可适应性和可组合性，例如在深度增长模型的中间块上应用推理时循环。

Result: 循环和深度增长模型表现出收敛的深度特征，支持它们通过共同形式的迭代计算获得收益。两种技术可以相互适配和组合：在深度增长模型的中间块应用推理时循环，即使模型从未训练过循环，也能在某些推理原语上提升高达2倍的准确率。两种方法在提供更多上下文示例或额外监督微调数据时，都比基线模型适应得更好。深度增长模型在使用更高质量、数学含量更高的冷却混合数据时获得最大的推理收益，而通过适配中间块进行循环可以进一步提升性能。

Conclusion: 深度增长和循环是互补的实用方法，可以共同用于诱导和扩展迭代计算以改善推理能力。研究结果为理解和利用这两种技术提供了统一的机制视角。

Abstract: Looping, reusing a block of layers across depth, and depth growing, training shallow-to-deep models by duplicating middle layers, have both been linked to stronger reasoning, but their relationship remains unclear. We provide a mechanistic unification: looped and depth-grown models exhibit convergent depth-wise signatures, including increased reliance on late layers and recurring patterns aligned with the looped or grown block. These shared signatures support the view that their gains stem from a common form of iterative computation. Building on this connection, we show that the two techniques are adaptable and composable: applying inference-time looping to the middle blocks of a depth-grown model improves accuracy on some reasoning primitives by up to $2\times$, despite the model never being trained to loop. Both approaches also adapt better than the baseline when given more in-context examples or additional supervised fine-tuning data. Additionally, depth-grown models achieve the largest reasoning gains when using higher-quality, math-heavy cooldown mixtures, which can be further boosted by adapting a middle block to loop. Overall, our results position depth growth and looping as complementary, practical methods for inducing and scaling iterative computation to improve reasoning.

</details>


### [59] [Optimizing Soft Prompt Tuning via Structural Evolution](https://arxiv.org/abs/2602.16500)
*Zhenzhen Huang,Chaoning Zhang,Haoyu Bian,Songbo Zhang,Chi-lok Andy Tai,Jiaquan Zhang,Caiyan Qin,Jingjing Qu,Yalan Ye,Yang Yang,Heng Tao Shen*

Main category: cs.CL

TL;DR: 本文提出了一种基于拓扑形态演化的软提示调优优化方法，通过拓扑数据分析量化软提示的结构表示，并构建TSLoss损失函数来指导模型学习结构稳定的适配。


<details>
  <summary>Details</summary>
Motivation: 软提示调优虽然在少样本设置中表现良好，但依赖于高维隐式表示，缺乏明确的语义和可追踪的训练行为，限制了其可解释性。

Method: 采用拓扑数据分析中的持久同调来量化软提示在连续参数空间中的结构表示及其训练过程演化，并基于拓扑稳定性和紧凑性构建TSLoss损失函数来优化软提示调优。

Result: 定量分析表明拓扑稳定且紧凑的软提示能获得更好的下游性能，使用TSLoss训练可加速收敛并提高调优性能，为从结构和拓扑角度理解和优化软提示调优提供了可解释的方法。

Conclusion: 基于拓扑形态演化的软提示调优优化方法不仅提升了性能，还增强了可解释性，为理解软提示训练过程提供了新的结构视角。

Abstract: Soft prompt tuning leverages continuous embeddings to capture task-specific information in large pre-trained language models (LLMs), achieving competitive performance in few-shot settings. However, soft prompts rely on high-dimensional, implicit representations and lack explicit semantics and traceable training behaviors, which limits their interpretability. To address this limitation, we propose a soft prompt tuning optimization method based on topological morphological evolution. Specifically, we employ persistent homology from topological data analysis (TDA) to quantify the structural representations of soft prompts in continuous parameter space and their training process evolution. Quantitative analysis shows that topologically stable and compact soft prompts achieve better downstream performance. Based on this empirical observation, we construct a loss function for optimizing soft prompt tuning, termed Topological Soft Prompt Loss (TSLoss). TSLoss guides the model to learn structurally stable adaptations by quantifying inter-parameter connectivity and redundancy. Extensive experiments show that training with TSLoss accelerates convergence and improves tuning performance, providing an interpretable method to understand and optimize soft prompt tuning from structural and topological perspectives.

</details>


### [60] [Supercharging Agenda Setting Research: The ParlaCAP Dataset of 28 European Parliaments and a Scalable Multilingual LLM-Based Classification](https://arxiv.org/abs/2602.16516)
*Taja Kuzman Pungeršek,Peter Rupnik,Daniela Širinić,Nikola Ljubešić*

Main category: cs.CL

TL;DR: ParlaCAP是一个用于分析欧洲议会议程设置的大规模数据集，采用教师-学生框架构建特定领域的政策主题分类器，在多语言议会语料上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 需要分析欧洲各国议会议程设置，但缺乏大规模、高质量的领域特定政策主题分类数据集和方法，现有分类器基于领域外手动标注数据效果有限。

Method: 将比较议程项目(CAP)分类框架应用于ParlaMint多语言议会语料库(2800万条演讲)，采用教师-学生框架：使用高性能大语言模型(LLM)标注领域内训练数据，然后微调多语言编码器模型进行可扩展数据标注。

Result: LLM与人类标注者的一致性接近人类间标注一致性，生成的模型优于基于领域外手动标注数据的现有CAP分类器。ParlaCAP数据集提供丰富的演讲者和政党元数据，以及来自ParlaSent多语言transformer模型的情感预测。

Conclusion: 该方法能有效构建特定领域的政策主题分类器，ParlaCAP数据集为跨国家政治关注和代表性比较研究提供了丰富资源，并通过三个用例展示了其分析潜力。

Abstract: This paper introduces ParlaCAP, a large-scale dataset for analyzing parliamentary agenda setting across Europe, and proposes a cost-effective method for building domain-specific policy topic classifiers. Applying the Comparative Agendas Project (CAP) schema to the multilingual ParlaMint corpus of over 8 million speeches from 28 parliaments of European countries and autonomous regions, we follow a teacher-student framework in which a high-performing large language model (LLM) annotates in-domain training data and a multilingual encoder model is fine-tuned on these annotations for scalable data annotation. We show that this approach produces a classifier tailored to the target domain. Agreement between the LLM and human annotators is comparable to inter-annotator agreement among humans, and the resulting model outperforms existing CAP classifiers trained on manually-annotated but out-of-domain data. In addition to the CAP annotations, the ParlaCAP dataset offers rich speaker and party metadata, as well as sentiment predictions coming from the ParlaSent multilingual transformer model, enabling comparative research on political attention and representation across countries. We illustrate the analytical potential of the dataset with three use cases, examining the distribution of parliamentary attention across policy topics, sentiment patterns in parliamentary speech, and gender differences in policy attention.

</details>


### [61] [Utility-Preserving De-Identification for Math Tutoring: Investigating Numeric Ambiguity in the MathEd-PII Benchmark Dataset](https://arxiv.org/abs/2602.16571)
*Zhuqian Zhou,Kirk Vanacore,Bakhtawar Ahtisham,Jinsook Lee,Doug Pietrzak,Daryl Hedley,Jorge Dias,Chris Shaw,Ruth Schäfer,René F. Kizilcec*

Main category: cs.CL

TL;DR: 提出了MathEd-PII基准数据集，用于数学辅导对话中的PII检测，通过数学感知提示显著提升检测性能，减少对数学内容的误删。


<details>
  <summary>Details</summary>
Motivation: 数学辅导对话数据共享对教学研究至关重要，但现有通用PII检测系统会将数字表达式（如日期、ID）误判为个人身份信息，导致过度删减核心教学内容，降低数据集实用性。

Method: 1. 创建MathEd-PII基准数据集：包含1000个辅导会话，通过人机协同LLM工作流审核上游删减并生成隐私保护替代内容；2. 使用基于密度的分割方法分析误删模式；3. 比较四种检测策略：Presidio基线、基础LLM提示、数学感知提示、分段感知提示。

Result: 1. 数学密集区域的误删比例显著更高；2. 数学感知提示相比基线F1分数大幅提升（0.821 vs 0.379）；3. 数学感知提示能有效减少数字误报，保持数据集分析实用性。

Conclusion: 数学辅导数据的隐私保护去标识化需要结合领域知识，数学感知提示能显著提升PII检测性能并保持教育数据实用性，为教学研究数据共享提供了新基准和方法。

Abstract: Large-scale sharing of dialogue-based data is instrumental for advancing the science of teaching and learning, yet rigorous de-identification remains a major barrier. In mathematics tutoring transcripts, numeric expressions frequently resemble structured identifiers (e.g., dates or IDs), leading generic Personally Identifiable Information (PII) detection systems to over-redact core instructional content and reduce dataset utility. This work asks how PII can be detected in math tutoring transcripts while preserving their educational utility. To address this challenge, we investigate the "numeric ambiguity" problem and introduce MathEd-PII, the first benchmark dataset for PII detection in math tutoring dialogues, created through a human-in-the-loop LLM workflow that audits upstream redactions and generates privacy-preserving surrogates. The dataset contains 1,000 tutoring sessions (115,620 messages; 769,628 tokens) with validated PII annotations. Using a density-based segmentation method, we show that false PII redactions are disproportionately concentrated in math-dense regions, confirming numeric ambiguity as a key failure mode. We then compare four detection strategies: a Presidio baseline and LLM-based approaches with basic, math-aware, and segment-aware prompting. Math-aware prompting substantially improves performance over the baseline (F1: 0.821 vs. 0.379) while reducing numeric false positives, demonstrating that de-identification must incorporate domain context to preserve analytic utility. This work provides both a new benchmark and evidence that utility-preserving de-identification for tutoring data requires domain-aware modeling.

</details>


### [62] [CitiLink-Summ: Summarization of Discussion Subjects in European Portuguese Municipal Meeting Minutes](https://arxiv.org/abs/2602.16607)
*Miguel Marques,Ana Luísa Fernandes,Ana Filipa Pacheco,Rute Rebouças,Inês Cantante,José Isidro,Luís Filipe Cunha,Alípio Jorge,Nuno Guimarães,Sérgio Nunes,António Leal,Purificação Silvano,Ricardo Campos*

Main category: cs.CL

TL;DR: CitiLink-Summ是首个欧洲葡萄牙语市政会议纪要摘要数据集，包含100份文档和2322个手工编写的摘要，为低资源语言市政领域摘要研究提供了基准。


<details>
  <summary>Details</summary>
Motivation: 市政会议纪要内容冗长复杂，市民难以理解，而自动摘要可以解决这一问题。但目前缺乏高质量的市政会议纪要摘要数据集，特别是在低资源语言如欧洲葡萄牙语中，这限制了该领域摘要模型的发展。

Method: 构建了CitiLink-Summ数据集，包含100份欧洲葡萄牙语市政会议纪要和2322个手工编写的摘要。使用最先进的生成模型（如BART、PRIMERA）和大语言模型（LLMs）建立基线，并使用ROUGE、BLEU、METEOR和BERTScore等词汇和语义指标进行评估。

Result: 创建了首个欧洲葡萄牙语市政领域摘要数据集CitiLink-Summ，为该领域建立了基准结果，为复杂行政文本的NLP研究提供了宝贵资源。

Conclusion: CitiLink-Summ填补了低资源语言市政会议纪要摘要研究的空白，为开发更有效的自动摘要模型提供了必要的数据基础，有望推动市政文档可访问性的提升。

Abstract: Municipal meeting minutes are formal records documenting the discussions and decisions of local government, yet their content is often lengthy, dense, and difficult for citizens to navigate. Automatic summarization can help address this challenge by producing concise summaries for each discussion subject. Despite its potential, research on summarizing discussion subjects in municipal meeting minutes remains largely unexplored, especially in low-resource languages, where the inherent complexity of these documents adds further challenges. A major bottleneck is the scarcity of datasets containing high-quality, manually crafted summaries, which limits the development and evaluation of effective summarization models for this domain. In this paper, we present CitiLink-Summ, a new corpus of European Portuguese municipal meeting minutes, comprising 100 documents and 2,322 manually hand-written summaries, each corresponding to a distinct discussion subject. Leveraging this dataset, we establish baseline results for automatic summarization in this domain, employing state-of-the-art generative models (e.g., BART, PRIMERA) as well as large language models (LLMs), evaluated with both lexical and semantic metrics such as ROUGE, BLEU, METEOR, and BERTScore. CitiLink-Summ provides the first benchmark for municipal-domain summarization in European Portuguese, offering a valuable resource for advancing NLP research on complex administrative texts.

</details>


### [63] [Explainable AI: Context-Aware Layer-Wise Integrated Gradients for Explaining Transformer Models](https://arxiv.org/abs/2602.16608)
*Melkamu Abay Mersha,Jugal Kalita*

Main category: cs.CL

TL;DR: 提出CA-LIG框架，通过分层集成梯度和注意力梯度，提供Transformer模型更忠实、上下文感知的解释方法。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer解释方法依赖最终层归因，缺乏对跨层相关性演化和结构组件影响的理解，且未能统一局部token级归因与全局注意力模式，缺乏上下文感知能力。

Method: 提出Context-Aware Layer-wise Integrated Gradients框架，在每个Transformer块内计算分层集成梯度，并将这些token级归因与类别特定的注意力梯度融合，生成有符号、上下文敏感的归因图。

Result: 在情感分析、长文档分类、多类文档分类、低资源语言仇恨检测和图像分类等任务中，CA-LIG相比现有方法提供更忠实的归因，对上下文依赖更敏感，产生更清晰、语义更连贯的可视化结果。

Conclusion: CA-LIG为Transformer决策提供了更全面、上下文感知且可靠的解释，推进了深度神经网络的实际可解释性和概念理解。

Abstract: Transformer models achieve state-of-the-art performance across domains and tasks, yet their deeply layered representations make their predictions difficult to interpret. Existing explainability methods rely on final-layer attributions, capture either local token-level attributions or global attention patterns without unification, and lack context-awareness of inter-token dependencies and structural components. They also fail to capture how relevance evolves across layers and how structural components shape decision-making. To address these limitations, we proposed the \textbf{Context-Aware Layer-wise Integrated Gradients (CA-LIG) Framework}, a unified hierarchical attribution framework that computes layer-wise Integrated Gradients within each Transformer block and fuses these token-level attributions with class-specific attention gradients. This integration yields signed, context-sensitive attribution maps that capture supportive and opposing evidence while tracing the hierarchical flow of relevance through the Transformer layers. We evaluate the CA-LIG Framework across diverse tasks, domains, and transformer model families, including sentiment analysis and long and multi-class document classification with BERT, hate speech detection in a low-resource language setting with XLM-R and AfroLM, and image classification with Masked Autoencoder vision Transformer model. Across all tasks and architectures, CA-LIG provides more faithful attributions, shows stronger sensitivity to contextual dependencies, and produces clearer, more semantically coherent visualizations than established explainability methods. These results indicate that CA-LIG provides a more comprehensive, context-aware, and reliable explanation of Transformer decision-making, advancing both the practical interpretability and conceptual understanding of deep neural models.

</details>


### [64] [Who can we trust? LLM-as-a-jury for Comparative Assessment](https://arxiv.org/abs/2602.16610)
*Mengjie Qian,Guangzhi Sun,Mark J. F. Gales,Kate M. Knill*

Main category: cs.CL

TL;DR: 本文提出BT-sigma方法，通过引入判别参数对LLM评委进行可靠性建模，无需人工标注即可从成对比较中联合推断项目排名和评委可靠性，显著优于基于平均的聚合方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM自动评估方法通常依赖单个评委或假设多个评委同等可靠地进行聚合。实际上，LLM评委在不同任务和方面表现差异很大，其判断概率可能存在偏差和不一致，且缺乏人工标注进行校准。

Method: 提出BT-sigma方法，这是Bradley-Terry模型的评委感知扩展，为每个评委引入判别参数，仅从成对比较中联合推断项目排名和评委可靠性。

Result: 在基准NLG评估数据集上的实验表明，BT-sigma始终优于基于平均的聚合方法，学习到的判别参数与LLM判断的循环一致性独立测量强相关。进一步分析显示BT-sigma可解释为无监督校准机制。

Conclusion: BT-sigma通过建模评委可靠性改进了LLM作为评审团的聚合效果，无需人工监督即可处理LLM判断中的不一致性，为自动评估提供了更可靠的方法。

Abstract: Large language models (LLMs) are increasingly applied as automatic evaluators for natural language generation assessment often using pairwise comparative judgements. Existing approaches typically rely on single judges or aggregate multiple judges assuming equal reliability. In practice, LLM judges vary substantially in performance across tasks and aspects, and their judgment probabilities may be biased and inconsistent. Furthermore, human-labelled supervision for judge calibration may be unavailable. We first empirically demonstrate that inconsistencies in LLM comparison probabilities exist and show that it limits the effectiveness of direct probability-based ranking. To address this, we study the LLM-as-a-jury setting and propose BT-sigma, a judge-aware extension of the Bradley-Terry model that introduces a discriminator parameter for each judge to jointly infer item rankings and judge reliability from pairwise comparisons alone. Experiments on benchmark NLG evaluation datasets show that BT-sigma consistently outperforms averaging-based aggregation methods, and that the learned discriminator strongly correlates with independent measures of the cycle consistency of LLM judgments. Further analysis reveals that BT-sigma can be interpreted as an unsupervised calibration mechanism that improves aggregation by modelling judge reliability.

</details>


### [65] [AREG: Adversarial Resource Extraction Game for Evaluating Persuasion and Resistance in Large Language Models](https://arxiv.org/abs/2602.16639)
*Adib Sakhawat,Fardeen Sadab*

Main category: cs.CL

TL;DR: AREG基准测试通过多轮零和谈判评估LLM的社交智能，发现说服与抵抗能力弱相关且不对称，防御能力普遍优于攻击能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM社交智能评估多关注静态文本生成，需要向动态对抗性交互发展，以更全面评估说服和抵抗能力。

Method: 提出AREG基准，将说服和抵抗操作化为多轮零和财务资源谈判，采用循环赛制在多个前沿模型上进行联合评估。

Result: 说服与抵抗能力弱相关（ρ=0.33），防御能力普遍优于攻击能力；增量承诺寻求策略与成功提取相关，验证寻求响应在成功防御中更常见。

Conclusion: LLM的社交影响力不是单一能力，仅关注说服的评估框架可能忽略不对称的行为脆弱性，需要更平衡的评估方法。

Abstract: Evaluating the social intelligence of Large Language Models (LLMs) increasingly requires moving beyond static text generation toward dynamic, adversarial interaction. We introduce the Adversarial Resource Extraction Game (AREG), a benchmark that operationalizes persuasion and resistance as a multi-turn, zero-sum negotiation over financial resources. Using a round-robin tournament across frontier models, AREG enables joint evaluation of offensive (persuasion) and defensive (resistance) capabilities within a single interactional framework. Our analysis provides evidence that these capabilities are weakly correlated ($ρ= 0.33$) and empirically dissociated: strong persuasive performance does not reliably predict strong resistance, and vice versa. Across all evaluated models, resistance scores exceed persuasion scores, indicating a systematic defensive advantage in adversarial dialogue settings. Further linguistic analysis suggests that interaction structure plays a central role in these outcomes. Incremental commitment-seeking strategies are associated with higher extraction success, while verification-seeking responses are more prevalent in successful defenses than explicit refusal. Together, these findings indicate that social influence in LLMs is not a monolithic capability and that evaluation frameworks focusing on persuasion alone may overlook asymmetric behavioral vulnerabilities.

</details>


### [66] [Quecto-V1: Empirical Analysis of 8-bit Quantized Small Language Models for On-Device Legal Retrieval](https://arxiv.org/abs/2602.16640)
*Subrit Dikshit*

Main category: cs.CL

TL;DR: 开发了一个124M参数的印度法律专用小型语言模型Quecto-V1，通过8位量化压缩至150MB，在本地CPU上运行，在法律检索任务上优于通用SLM，提供隐私保护的替代方案。


<details>
  <summary>Details</summary>
Motivation: 当前先进的法律智能系统通常依赖大规模参数（7B+）和云端推理，导致资源受限环境无法访问，存在数据主权风险。需要为印度法律领域开发可访问的专用模型。

Method: 基于GPT-2架构定制配置（124M参数），从头训练于印度法律语料（IPC、CrPC、宪法等）。采用后训练8位量化（GGUF格式）压缩模型大小，专注于法律领域"词汇密度"最大化。

Result: Quecto-V1在法律定义和刑罚条款检索中表现高保真度，在领域特定精确匹配任务上优于通用SLM，完全离线运行于消费级CPU。8位量化使模型大小减少74%，检索准确率下降小于3.5%。

Conclusion: 对于法律等高风险专业领域，领域特定训练结合激进量化提供了一种可行、隐私保护的替代方案，避免了依赖庞大云端模型。

Abstract: The rapid proliferation of Large Language Models (LLMs) has revolutionized Natural Language Processing (NLP) but has simultaneously created a "resource divide." State-of-the-art legal intelligence systems typically rely on massive parameter counts (7B+) and cloud-based inference, rendering them inaccessible to practitioners in resource-constrained environments and posing significant data sovereignty risks. This paper introduces Quecto-V1, a domain-specific Small Language Model (SLM) engineered to democratize access to Indian legal intelligence. Built upon a custom configuration of the GPT-2 architecture (124 million parameters), Quecto-V1 was trained from scratch exclusively on a corpus of Indian statutes, including the Indian Penal Code (IPC), the Code of Criminal Procedure (CrPC), and the Constitution of India. Unlike generalist models, which prioritize broad world knowledge, our approach maximizes "lexical density" within the legal domain. Furthermore, we address the deployment bottleneck by applying post-training 8-bit quantization (GGUF format), compressing the model to a memory footprint of under 150 MB. Our empirical analysis demonstrates that Quecto-V1 achieves high fidelity in retrieving statutory definitions and penal provisions, outperforming general-purpose SLMs in domain-specific exact match tasks while running entirely offline on consumer-grade CPUs. We further present an ablation study showing that 8-bit quantization yields a 74% reduction in model size with less than 3.5% degradation in retrieval accuracy compared to full-precision baselines. These findings suggest that for specialized, high-stakes domains like law, domain-specific training coupled with aggressive quantization offers a viable, privacy-preserving alternative to monolithic cloud models.

</details>


### [67] [Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment](https://arxiv.org/abs/2602.16660)
*Yuyan Bu,Xiaohao Liu,ZhaoXing Ren,Yaodong Yang,Juntao Dai*

Main category: cs.CL

TL;DR: 提出一种资源高效的多语言安全对齐方法MLC，通过提升多语言表示向量的共线性，仅使用多语言提示变体即可实现多语言同时对齐，无需低资源语言的额外监督数据。


<details>
  <summary>Details</summary>
Motivation: 当前将大语言模型安全对齐扩展到多语言环境需要大量资源：要么需要目标语言的大规模高质量监督数据，要么需要与高资源语言进行配对对齐，限制了可扩展性。

Method: 提出可插拔的多语言一致性（MLC）损失函数，可集成到现有的单语对齐流程中。通过改善多语言表示向量的共线性，在单次更新中实现多语言语义层面的方向一致性。

Result: 在不同模型架构和对齐范式上验证了方法的有效性，能够显著提升多语言安全性，同时对模型通用能力影响有限。跨语言和跨任务评估显示改善了跨语言泛化能力。

Conclusion: MLC方法为有限监督下的多语言一致性对齐提供了实用解决方案，能够在资源受限的情况下有效提升多语言安全对齐效果。

Abstract: The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other languages often require substantial resources, either through large-scale, high-quality supervision in the target language or through pairwise alignment with high-resource languages, which limits scalability. In this work, we propose a resource-efficient method for improving multilingual safety alignment. We introduce a plug-and-play Multi-Lingual Consistency (MLC) loss that can be integrated into existing monolingual alignment pipelines. By improving collinearity between multilingual representation vectors, our method encourages directional consistency at the multilingual semantic level in a single update. This allows simultaneous alignment across multiple languages using only multilingual prompt variants without requiring additional response-level supervision in low-resource languages. We validate the proposed method across different model architectures and alignment paradigms, and demonstrate its effectiveness in enhancing multilingual safety with limited impact on general model utility. Further evaluation across languages and tasks indicates improved cross-lingual generalization, suggesting the proposed approach as a practical solution for multilingual consistency alignment under limited supervision.

</details>


### [68] [Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents](https://arxiv.org/abs/2602.16699)
*Wenxuan Ding,Nicholas Tomlin,Greg Durrett*

Main category: cs.CL

TL;DR: LLMs在需要与环境交互的复杂任务中面临成本-不确定性权衡问题，本文提出的Calibrate-Then-Act框架通过让LLM显式推理这种权衡，使其能更优地进行环境探索。


<details>
  <summary>Details</summary>
Motivation: LLMs越来越多地用于需要与环境交互获取信息的复杂问题，在这些场景中，LLMs必须权衡何时停止探索并提交答案的成本-不确定性权衡问题。例如在编程任务中，LLM应该测试不确定的代码片段，虽然测试有成本但通常低于错误的成本。

Method: 将信息检索和编码等任务形式化为不确定性下的顺序决策问题，每个问题都有可以通过先验进行推理的潜在环境状态。提出Calibrate-Then-Act（CTA）框架，为LLM提供额外上下文使其能够更优地行动。

Result: 在信息寻求QA和简化编码任务上的结果表明，通过CTA显式进行成本效益权衡可以帮助智能体发现更优的决策策略，这种改进即使在基线和CTA都经过RL训练后仍然保持。

Conclusion: 让LLM显式推理成本-不确定性权衡可以使它们在需要环境交互的任务中执行更优的探索策略，CTA框架为此提供了有效的解决方案。

Abstract: LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.

</details>


### [69] [Reinforced Fast Weights with Next-Sequence Prediction](https://arxiv.org/abs/2602.16704)
*Hee Seung Hwang,Xindi Wu,Sanghyuk Chun,Olga Russakovsky*

Main category: cs.CL

TL;DR: REFINE：一个基于强化学习的框架，通过下一序列预测目标训练快速权重模型，提升长上下文建模能力。


<details>
  <summary>Details</summary>
Motivation: 快速权重架构在长上下文建模中具有潜力，但受限于下一令牌预测训练范式。NTP只优化单令牌预测，忽略了前缀后的多令牌语义一致性，导致快速权重模型学习到次优表示，无法有效捕捉长程依赖。

Method: 提出REFINE框架，使用强化学习在下一序列预测目标下训练快速权重模型。方法包括：基于预测熵选择信息丰富的令牌位置、生成多令牌rollout、分配自监督序列级奖励、使用组相对策略优化进行模型优化。REFINE可在预训练语言模型的整个训练生命周期中应用：中期训练、后训练和测试时训练。

Result: 在LaCT-760M和DeltaNet-1.3B上的实验表明，REFINE在针在草堆检索、长上下文问题回答和LongBench中的多样化任务上，始终优于使用NTP的监督微调。

Conclusion: REFINE为改进快速权重架构中的长上下文建模提供了一个有效且通用的强化学习框架。

Abstract: Fast weight architectures offer a promising alternative to attention-based transformers for long-context modeling by maintaining constant memory overhead regardless of context length. However, their potential is limited by the next-token prediction (NTP) training paradigm. NTP optimizes single-token predictions and ignores semantic coherence across multiple tokens following a prefix. Consequently, fast weight models, which dynamically update their parameters to store contextual information, learn suboptimal representations that fail to capture long-range dependencies. We introduce REFINE (Reinforced Fast weIghts with Next sEquence prediction), a reinforcement learning framework that trains fast weight models under the next-sequence prediction (NSP) objective. REFINE selects informative token positions based on prediction entropy, generates multi-token rollouts, assigns self-supervised sequence-level rewards, and optimizes the model with group relative policy optimization (GRPO). REFINE is applicable throughout the training lifecycle of pre-trained language models: mid-training, post-training, and test-time training. Our experiments on LaCT-760M and DeltaNet-1.3B demonstrate that REFINE consistently outperforms supervised fine-tuning with NTP across needle-in-a-haystack retrieval, long-context question answering, and diverse tasks in LongBench. REFINE provides an effective and versatile framework for improving long-context modeling in fast weight architectures.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [70] [FeDecider: An LLM-Based Framework for Federated Cross-Domain Recommendation](https://arxiv.org/abs/2602.16034)
*Xinrui He,Ting-Wei Li,Tianxin Wei,Xuying Ning,Xinyu He,Wenxuan Bao,Hanghang Tong,Jingrui He*

Main category: cs.IR

TL;DR: FeDecider是一个基于大语言模型的联邦跨域推荐框架，通过解耦低秩更新和共享方向分量来解决域特定过拟合问题，并学习个性化权重实现数据感知的跨域集成。


<details>
  <summary>Details</summary>
Motivation: 在联邦跨域推荐中采用基于大语言模型的推荐模型面临两个主要挑战：1）域特定本地适配器存在过拟合风险，本地优化参数更新的幅度在不同域间差异导致聚合偏差和域特定分布过拟合；2）与传统推荐模型不同，大语言模型通过自回归文本生成隐式编码知识，难以在异构环境下有效衡量跨域相似性。

Method: 提出FeDecider框架：1）通过解耦每个客户端的低秩更新，仅共享其方向分量来处理尺度特定噪声；2）每个客户端进一步学习个性化权重，实现来自其他域更新的数据感知集成。

Result: 在多个不同数据集上的广泛实验验证了FeDecider框架的有效性。

Conclusion: FeDecider成功解决了基于大语言模型的联邦跨域推荐中的域特定过拟合和跨域相似性衡量问题，通过解耦低秩更新和个性化权重学习实现了有效的跨域协作推荐。

Abstract: Federated cross-domain recommendation (Federated CDR) aims to collaboratively learn personalized recommendation models across heterogeneous domains while preserving data privacy. Recently, large language model (LLM)-based recommendation models have demonstrated impressive performance by leveraging LLMs' strong reasoning capabilities and broad knowledge. However, adopting LLM-based recommendation models in Federated CDR scenarios introduces new challenges. First, there exists a risk of overfitting with domain-specific local adapters. The magnitudes of locally optimized parameter updates often vary across domains, causing biased aggregation and overfitting toward domain-specific distributions. Second, unlike traditional recommendation models (e.g., collaborative filtering, bipartite graph-based methods) that learn explicit and comparable user/item representations, LLMs encode knowledge implicitly through autoregressive text generation training. This poses additional challenges for effectively measuring the cross-domain similarities under heterogeneity. To address these challenges, we propose an LLM-based framework for federated cross-domain recommendation, FeDecider. Specifically, FeDecider tackles the challenge of scale-specific noise by disentangling each client's low-rank updates and sharing only their directional components. To handle the need for flexible and effective integration, each client further learns personalized weights that achieve the data-aware integration of updates from other domains. Extensive experiments across diverse datasets validate the effectiveness of our proposed FeDecider.

</details>


### [71] [Rethinking ANN-based Retrieval: Multifaceted Learnable Index for Large-scale Recommendation System](https://arxiv.org/abs/2602.16124)
*Jiang Zhang,Yubo Wang,Wei Chang,Lu Han,Xingying Cheng,Feng Zhang,Min Li,Songhao Jiang,Wei Zheng,Harry Tran,Zhen Wang,Lei Chen,Yueming Wang,Benyu Zhang,Xiangjun Fan,Bi Xue,Qifan Wang*

Main category: cs.IR

TL;DR: MFLI是一种统一学习嵌入和索引的实时检索范式，通过多层次码本消除ANN搜索，显著提升推荐系统效果和效率。


<details>
  <summary>Details</summary>
Motivation: 现有ANN检索存在两个关键问题：1) 嵌入学习和索引构建分离导致次优检索质量，特别是对新内容；2) 每个查询都需要ANN搜索，在工业规模下计算成本高。

Method: 通过残差量化构建多层次码本，在统一框架中共同训练嵌入和码本，并设计支持实时更新的高效多面索引结构，服务时直接使用学习到的层次索引而无需ANN搜索。

Result: 在数十亿用户的真实数据上，MFLI相比SOTA方法：参与度任务召回提升11.8%，冷内容分发提升57.29%，语义相关性提升13.5%。在线部署显示参与度提升、流行度偏差减少、服务效率更高。

Conclusion: MFLI通过统一学习嵌入和索引的范式，解决了ANN检索的两大限制，实现了高质量、高效率的实时推荐检索，并在大规模工业场景中验证了其有效性。

Abstract: Approximate nearest neighbor (ANN) search is widely used in the retrieval stage of large-scale recommendation systems. In this stage, candidate items are indexed using their learned embedding vectors, and ANN search is executed for each user (or item) query to retrieve a set of relevant items. However, ANN-based retrieval has two key limitations. First, item embeddings and their indices are typically learned in separate stages: indexing is often performed offline after embeddings are trained, which can yield suboptimal retrieval quality-especially for newly created items. Second, although ANN offers sublinear query time, it must still be run for every request, incurring substantial computation cost at industry scale. In this paper, we propose MultiFaceted Learnable Index (MFLI), a scalable, real-time retrieval paradigm that learns multifaceted item embeddings and indices within a unified framework and eliminates ANN search at serving time. Specifically, we construct a multifaceted hierarchical codebook via residual quantization of item embeddings and co-train the codebook with the embeddings. We further introduce an efficient multifaceted indexing structure and mechanisms that support real-time updates. At serving time, the learned hierarchical indices are used directly to identify relevant items, avoiding ANN search altogether. Extensive experiments on real-world data with billions of users show that MFLI improves recall on engagement tasks by up to 11.8\%, cold-content delivery by up to 57.29\%, and semantic relevance by 13.5\% compared with prior state-of-the-art methods. We also deploy MFLI in the system and report online experimental results demonstrating improved engagement, less popularity bias, and higher serving efficiency.

</details>


### [72] [Retrieval Collapses When AI Pollutes the Web](https://arxiv.org/abs/2602.16136)
*Hongyeon Yu,Dongchan Kim,Young-Bum Kim*

Main category: cs.IR

TL;DR: 本文提出"检索崩溃"概念，指AI生成内容主导搜索结果并侵蚀来源多样性，导致检索系统质量下降的两阶段过程。


<details>
  <summary>Details</summary>
Motivation: AI生成内容在Web上的快速扩散对信息检索构成结构性风险，因为搜索引擎和RAG系统越来越多地使用LLM生成的证据。这可能导致检索系统质量下降的恶性循环。

Method: 通过控制实验分析检索崩溃动态，包括高质量SEO风格内容和对抗性内容两种情况。实验涉及BM25和基于LLM的排序器在污染环境下的表现比较。

Result: 在SEO场景中，67%的池污染导致超过80%的暴露污染，形成看似健康但同质化的状态；在对抗性污染中，BM25暴露约19%有害内容，而LLM排序器表现出更强的抑制能力。

Conclusion: 检索管道可能悄然转向合成证据，需要检索感知策略来防止Web基础系统中质量下降的自我强化循环。

Abstract: The rapid proliferation of AI-generated content on the Web presents a structural risk to information retrieval, as search engines and Retrieval-Augmented Generation (RAG) systems increasingly consume evidence produced by the Large Language Models (LLMs). We characterize this ecosystem-level failure mode as Retrieval Collapse, a two-stage process where (1) AI-generated content dominates search results, eroding source diversity, and (2) low-quality or adversarial content infiltrates the retrieval pipeline. We analyzed this dynamic through controlled experiments involving both high-quality SEO-style content and adversarially crafted content. In the SEO scenario, a 67\% pool contamination led to over 80\% exposure contamination, creating a homogenized yet deceptively healthy state where answer accuracy remains stable despite the reliance on synthetic sources. Conversely, under adversarial contamination, baselines like BM25 exposed $\sim$19\% of harmful content, whereas LLM-based rankers demonstrated stronger suppression capabilities. These findings highlight the risk of retrieval pipelines quietly shifting toward synthetic evidence and the need for retrieval-aware strategies to prevent a self-reinforcing cycle of quality decline in Web-grounded systems.

</details>


### [73] [MICE: Minimal Interaction Cross-Encoders for efficient Re-ranking](https://arxiv.org/abs/2602.16299)
*Mathias Vast,Victor Morand,Basile van Cooten,Laure Soulier,Josiane Mothe,Benjamin Piwowarski*

Main category: cs.IR

TL;DR: 提出MICE架构，通过精简交叉编码器的注意力机制，在保持排序效果的同时显著降低推理延迟


<details>
  <summary>Details</summary>
Motivation: 交叉编码器在信息检索中效果最好但推理成本过高，现有方法要么加速交叉编码器推理，要么改进第一阶段的复杂模型，但两者分离。需要结合这两种方法，基于对交叉编码器内部机制的深入理解来解决问题。

Method: 从交叉编码器出发，通过精心移除有害或不必要的交互，推导出新的类似延迟交互架构（MICE）。该方法在领域内外数据集上进行广泛评估。

Result: MICE将推理延迟降低4倍，与ColBERT等延迟交互模型相当，同时保留了大部分交叉编码器的领域内效果，并在领域外表现出更优的泛化能力。

Conclusion: MICE架构成功结合了交叉编码器的效果优势和延迟交互模型的高效性，为高效检索提供了新的解决方案。

Abstract: Cross-encoders deliver state-of-the-art ranking effectiveness in information retrieval, but have a high inference cost. This prevents them from being used as first-stage rankers, but also incurs a cost when re-ranking documents. Prior work has addressed this bottleneck from two largely separate directions: accelerating cross-encoder inference by sparsifying the attention process or improving first-stage retrieval effectiveness using more complex models, e.g. late-interaction ones. In this work, we propose to bridge these two approaches, based on an in-depth understanding of the internal mechanisms of cross-encoders. Starting from cross-encoders, we show that it is possible to derive a new late-interaction-like architecture by carefully removing detrimental or unnecessary interactions. We name this architecture MICE (Minimal Interaction Cross-Encoders). We extensively evaluate MICE across both in-domain (ID) and out-of-domain (OOD) datasets. MICE decreases fourfold the inference latency compared to standard cross-encoders, matching late-interaction models like ColBERT while retaining most of cross-encoder ID effectiveness and demonstrating superior generalization abilities in OOD.

</details>


### [74] [The Diversity Paradox revisited: Systemic Effects of Feedback Loops in Recommender Systems](https://arxiv.org/abs/2602.16315)
*Gabriele Barlacchi,Margherita Lalli,Emanuele Ferragina,Fosca Giannotti,Dino Pedreschi,Luca Pappalardo*

Main category: cs.IR

TL;DR: 推荐系统反馈循环研究：增加推荐采纳率会提高个体消费多样性，但集体需求会重新分配并加剧流行度集中；静态评估中观察到的个体多样性增加是假象，实际时间发展中个体多样性持续下降。


<details>
  <summary>Details</summary>
Motivation: 推荐系统通过反馈循环影响用户选择，但现有研究存在不现实的假设，对反馈循环的系统性效应理解不足。需要更准确地模拟用户行为与算法推荐的共同演化过程。

Method: 提出一个反馈循环模型，包含隐式反馈、定期重新训练、概率性推荐采纳和异构推荐系统。将该框架应用于在线零售和音乐流媒体数据，分析反馈循环的系统性效应。

Result: 1) 增加推荐采纳率可能导致个体消费逐渐多样化；2) 集体需求以模型和领域依赖的方式重新分配，通常加剧流行度集中；3) 时间分析显示静态评估中观察到的个体多样性增加是假象，当采纳率固定且时间推移时，所有模型的个体多样性持续下降。

Conclusion: 需要超越静态评估，在设计推荐系统时明确考虑反馈循环动态。研究强调了理解推荐系统长期系统性效应的重要性。

Abstract: Recommender systems shape individual choices through feedback loops in which user behavior and algorithmic recommendations coevolve over time. The systemic effects of these loops remain poorly understood, in part due to unrealistic assumptions in existing simulation studies. We propose a feedback-loop model that captures implicit feedback, periodic retraining, probabilistic adoption of recommendations, and heterogeneous recommender systems. We apply the framework on online retail and music streaming data and analyze systemic effects of the feedback loop. We find that increasing recommender adoption may lead to a progressive diversification of individual consumption, while collective demand is redistributed in model- and domain-dependent ways, often amplifying popularity concentration. Temporal analyses further reveal that apparent increases in individual diversity observed in static evaluations are illusory: when adoption is fixed and time unfolds, individual diversity consistently decreases across all models. Our results highlight the need to move beyond static evaluations and explicitly account for feedback-loop dynamics when designing recommender systems.

</details>


### [75] [Variable-Length Semantic IDs for Recommender Systems](https://arxiv.org/abs/2602.16375)
*Kirill Khrylchenko*

Main category: cs.IR

TL;DR: 该论文提出了一种用于推荐系统的变长语义标识符方法，通过离散变分自编码器学习自适应长度的物品表示，解决了固定长度语义标识符的低效性问题。


<details>
  <summary>Details</summary>
Motivation: 生成模型在推荐系统中面临物品空间基数极大的挑战，现有语义标识符方法采用固定长度表示所有物品，这效率低下且与自然语言不匹配，忽略了现实世界目录中热门物品和长尾物品具有不同信息需求的特性。

Method: 提出了一种离散变分自编码器，采用Gumbel-Softmax重参数化技术，在概率框架下学习自适应长度的物品表示，避免了基于REINFORCE训练的不稳定性和先前语义ID方法的固定长度限制。

Result: 该方法能够为不同频率的物品生成变长语义标识符，使热门物品获得较短描述，长尾物品获得较长描述，更符合自然语言中的信息压缩原理。

Conclusion: 通过将推荐系统与涌现通信理论相结合，提出的变长语义标识符方法能够更有效地处理大规模物品目录，为生成式推荐系统提供了更灵活的表示学习框架。

Abstract: Generative models are increasingly used in recommender systems, both for modeling user behavior as event sequences and for integrating large language models into recommendation pipelines. A key challenge in this setting is the extremely large cardinality of item spaces, which makes training generative models difficult and introduces a vocabulary gap between natural language and item identifiers. Semantic identifiers (semantic IDs), which represent items as sequences of low-cardinality tokens, have recently emerged as an effective solution to this problem.
  However, existing approaches generate semantic identifiers of fixed length, assigning the same description length to all items. This is inefficient, misaligned with natural language, and ignores the highly skewed frequency structure of real-world catalogs, where popular items and rare long-tail items exhibit fundamentally different information requirements. In parallel, the emergent communication literature studies how agents develop discrete communication protocols, often producing variable-length messages in which frequent concepts receive shorter descriptions. Despite the conceptual similarity, these ideas have not been systematically adopted in recommender systems.
  In this work, we bridge recommender systems and emergent communication by introducing variable-length semantic identifiers for recommendation. We propose a discrete variational autoencoder with Gumbel-Softmax reparameterization that learns item representations of adaptive length under a principled probabilistic framework, avoiding the instability of REINFORCE-based training and the fixed-length constraints of prior semantic ID methods.

</details>


### [76] [From Latent to Observable Position-Based Click Models in Carousel Interfaces](https://arxiv.org/abs/2602.16541)
*Santiago de Leon-Martinez,Robert Moro,Branislav Kveton,Maria Bielikova*

Main category: cs.IR

TL;DR: 该论文研究了轮播界面中的点击模型，提出了三种新颖的基于位置的模型，并比较了不同优化方法的效果。研究发现梯度优化方法表现最佳，但仅靠点击数据难以准确建模用户浏览行为。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统越来越多地使用轮播等复杂界面，但现有点击模型主要针对单一排名列表设计，无法有效建模用户在轮播界面中的复杂浏览行为。

Method: 提出了三种专门针对轮播界面的基于位置点击模型，包括首个无需潜在变量、使用眼动追踪数据的观测检查位置模型(OEPBM)。开发了通用实现框架，比较了梯度优化、期望最大化(EM)和最大似然估计等优化方法。

Result: 梯度优化方法在点击似然度方面表现最好；OEPBM模型在点击预测和用户行为对齐方面表现最强；但研究也发现点击拟合效果好并不代表能准确建模用户检查模式。

Conclusion: 在复杂界面中，仅依赖点击数据的模型存在根本性局限，设计轮播推荐系统的点击模型时需要结合额外的行为信号。

Abstract: Click models are a central component of learning and evaluation in recommender systems, yet most existing models are designed for single ranked-list interfaces. In contrast, modern recommender platforms increasingly use complex interfaces such as carousels, which consist of multiple swipeable lists that enable complex user browsing behaviors.
  In this paper, we study position-based click models in carousel interfaces and examine optimization methods, model structure, and alignment with user behavior. We propose three novel position-based models tailored to carousels, including the first position-based model without latent variables that incorporates observed examination signals derived from eye tracking data, called the Observed Examination Position-Based Model (OEPBM). We develop a general implementation of these carousel click models, supporting multiple optimization techniques and conduct experiments comparing gradient-based methods with classical approaches, namely expectation-maximization and maximum likelihood estimation.
  Our results show that gradient-based optimization consistently achieve better click likelihoods. Among the evaluated models, the OEPBM achieves the strongest performance in click prediction and produces examination patterns that most closely align to user behavior. However, we also demonstrate that strong click fit does not imply realistic modeling of user examination and browsing patterns. This reveals a fundamental limitation of click-only models in complex interfaces and the need for incorporating additional behavioral signals when designing click models for carousel-based recommender systems.

</details>


### [77] [Why Thinking Hurts? Diagnosing and Rectifying the Reasoning Shift in Foundation Recommender Models](https://arxiv.org/abs/2602.16587)
*Luankang Zhang,Yonghao Huang,Hang Lv,Mingjia Yin,Liangyue Li,Zulong Chen,Hao Wang,Enhong Chen*

Main category: cs.IR

TL;DR: 提出推理时子空间对齐框架，通过压缩推理链和偏置减除对比解码，解决在语义ID推荐模型中集成CoT推理导致的性能下降问题


<details>
  <summary>Details</summary>
Motivation: 在基于语义ID的推荐基础模型（如OpenOneRec）中集成CoT推理时，反而会导致推荐性能下降。研究发现根本原因是通用子空间的文本惯性问题，冗长的推理过程主导了推理，使模型忽略了关键的语义ID信息。

Method: 提出无需训练的推理时子空间对齐框架，通过压缩推理链和应用偏置减除对比解码，缓解无基础的文本漂移问题。

Result: 实验表明该方法能有效校准推理过程，使基础模型能够利用推理能力而不牺牲基于ID的准确性。

Conclusion: 推理时子空间对齐框架解决了CoT推理在语义ID推荐模型中的负面影响，实现了推理能力与ID基础准确性的平衡。

Abstract: Integrating Chain-of-Thought (CoT) reasoning into Semantic ID-based recommendation foundation models (such as OpenOneRec) often paradoxically degrades recommendation performance. We identify the root cause as textual inertia from the General Subspace, where verbose reasoning dominates inference and causes the model to neglect critical Semantic ID. To address this, we propose a training-free Inference-Time Subspace Alignment framework. By compressing reasoning chains and applying bias-subtracted contrastive decoding, our approach mitigates ungrounded textual drift. Experiments show this effectively calibrates inference, allowing foundation models to leverage reasoning without sacrificing ID-grounded accuracy.

</details>
