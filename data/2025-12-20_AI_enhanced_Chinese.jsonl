{"id": "2512.16814", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16814", "abs": "https://arxiv.org/abs/2512.16814", "authors": ["William English", "Dominic Simon", "Sumit Kumar Jha", "Rickard Ewetz"], "title": "Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs", "comment": null, "summary": "Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.", "AI": {"tldr": "GraFT\u6846\u67b6\u901a\u8fc7\u9650\u5236\u8bed\u8a00\u6a21\u578b\u5728\u6bcf\u4e2a\u6b65\u9aa4\u7684\u6709\u6548\u8f93\u51fa\u6807\u8bb0\uff0c\u964d\u4f4e\u4ece\u81ea\u7136\u8bed\u8a00\u5230\u65f6\u5e8f\u903b\u8f91\u7ffb\u8bd1\u7684\u590d\u6742\u5ea6\uff0c\u63d0\u5347\u7ffb\u8bd1\u51c6\u786e\u7387\u3002", "motivation": "\u5f53\u524d\u81ea\u7136\u8bed\u8a00\u5230\u65f6\u5e8f\u903b\u8f91\u7684\u7ffb\u8bd1\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u539f\u5b50\u547d\u9898\u63d0\u5347\u4e0d\u51c6\u786e\u3001\u5b58\u5728\u5171\u6307\u73b0\u8c61\u3001\u4ee5\u53ca\u96be\u4ee5\u4ece\u6709\u9650\u6570\u636e\u4e2d\u5b66\u4e60\u3002\u73b0\u6709\u65b9\u6cd5\u8ba9\u8bed\u8a00\u6a21\u578b\u4ece\u5b8c\u6574\u8bcd\u6c47\u8868\u4e2d\u8fed\u4ee3\u9884\u6d4b\u6807\u8bb0\uff0c\u5bfc\u81f4\u4efb\u52a1\u590d\u6742\u5ea6\u9ad8\u3002", "method": "\u63d0\u51faGrammar Forced Translation (GraFT)\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u6bcf\u4e2a\u95ee\u9898\u7684\u72ec\u7279\u5c5e\u6027\uff0c\u5728\u6bcf\u4e2a\u6b65\u9aa4\u5c06\u6709\u6548\u8f93\u51fa\u6807\u8bb0\u4ece\u5b8c\u6574\u8bcd\u6c47\u8868\u9650\u5236\u5230\u5c11\u6570\u51e0\u4e2a\uff0c\u4ece\u800c\u51cf\u5c11\u89e3\u7a7a\u95f4\u590d\u6742\u5ea6\u3002\u8be5\u6846\u67b6\u8fd8\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\uff0c\u89e3\u91ca\u89e3\u7a7a\u95f4\u51cf\u5c11\u5982\u4f55\u5e26\u6765\u66f4\u9ad8\u6548\u7684\u5b66\u4e60\u3002", "result": "\u5728CW\u3001GLTL\u548cNavi\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGraFT\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u7aef\u5230\u7aef\u7ffb\u8bd1\u51c6\u786e\u7387\u5e73\u5747\u63d0\u53475.49%\uff0c\u57df\u5916\u7ffb\u8bd1\u51c6\u786e\u7387\u5e73\u5747\u63d0\u534714.06%\u3002", "conclusion": "GraFT\u6846\u67b6\u901a\u8fc7\u51cf\u5c11\u89e3\u7a7a\u95f4\u590d\u6742\u5ea6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u7136\u8bed\u8a00\u5230\u65f6\u5e8f\u903b\u8f91\u7ffb\u8bd1\u4e2d\u7684\u51c6\u786e\u6027\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2512.16832", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16832", "abs": "https://arxiv.org/abs/2512.16832", "authors": ["Aditya Yadavalli", "Tiago Pimentel", "Tamar I Regev", "Ethan Wilcox", "Alex Warstadt"], "title": "What Do Prosody and Text Convey? Characterizing How Meaningful Information is Distributed Across Multiple Channels", "comment": null, "summary": "Prosody -- the melody of speech -- conveys critical information often not captured by the words or text of a message. In this paper, we propose an information-theoretic approach to quantify how much information is expressed by prosody alone and not by text, and crucially, what that information is about. Our approach applies large speech and language models to estimate the mutual information between a particular dimension of an utterance's meaning (e.g., its emotion) and any of its communication channels (e.g., audio or text). We then use this approach to quantify how much information is conveyed by audio and text about sarcasm, emotion, and questionhood, using speech from television and podcasts. We find that for sarcasm and emotion the audio channel -- and by implication the prosodic channel -- transmits over an order of magnitude more information about these features than the text channel alone, at least when long-term context beyond the current sentence is unavailable. For questionhood, prosody provides comparatively less additional information. We conclude by outlining a program applying our approach to more dimensions of meaning, communication channels, and languages.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u4fe1\u606f\u8bba\u65b9\u6cd5\uff0c\u4f7f\u7528\u5927\u578b\u8bed\u97f3\u548c\u8bed\u8a00\u6a21\u578b\u91cf\u5316\u4ec5\u901a\u8fc7\u97f5\u5f8b\uff08\u800c\u975e\u6587\u672c\uff09\u4f20\u8fbe\u7684\u4fe1\u606f\u91cf\uff0c\u5e76\u5206\u6790\u8fd9\u4e9b\u4fe1\u606f\u7684\u5177\u4f53\u5185\u5bb9\uff0c\u53d1\u73b0\u5728\u8868\u8fbe\u8bbd\u523a\u548c\u60c5\u611f\u65f6\uff0c\u97f3\u9891\u901a\u9053\u6bd4\u6587\u672c\u901a\u9053\u4f20\u9012\u7684\u4fe1\u606f\u91cf\u9ad8\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "motivation": "\u97f5\u5f8b\u4f5c\u4e3a\u8bed\u97f3\u7684\u65cb\u5f8b\uff0c\u4f20\u8fbe\u4e86\u6587\u672c\u65e0\u6cd5\u6355\u6349\u7684\u5173\u952e\u4fe1\u606f\u3002\u7136\u800c\uff0c\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u65b9\u6cd5\u6765\u91cf\u5316\u4ec5\u901a\u8fc7\u97f5\u5f8b\u8868\u8fbe\u7684\u4fe1\u606f\u91cf\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u4fe1\u606f\u7684\u5177\u4f53\u5185\u5bb9\u3002\u8fd9\u963b\u788d\u4e86\u6211\u4eec\u5bf9\u8bed\u97f3\u4ea4\u6d41\u4e2d\u4e0d\u540c\u901a\u9053\u4fe1\u606f\u8d21\u732e\u7684\u7406\u89e3\u3002", "method": "\u91c7\u7528\u4fe1\u606f\u8bba\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u578b\u8bed\u97f3\u548c\u8bed\u8a00\u6a21\u578b\u4f30\u8ba1\u8bdd\u8bed\u7279\u5b9a\u610f\u4e49\u7ef4\u5ea6\uff08\u5982\u60c5\u611f\uff09\u4e0e\u4efb\u4f55\u901a\u4fe1\u901a\u9053\uff08\u5982\u97f3\u9891\u6216\u6587\u672c\uff09\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u3002\u901a\u8fc7\u4e92\u4fe1\u606f\u91cf\u5316\u97f3\u9891\u548c\u6587\u672c\u5728\u8868\u8fbe\u8bbd\u523a\u3001\u60c5\u611f\u548c\u7591\u95ee\u6027\u65b9\u9762\u7684\u4fe1\u606f\u91cf\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u8868\u8fbe\u8bbd\u523a\u548c\u60c5\u611f\u65f6\uff0c\u97f3\u9891\u901a\u9053\uff08\u5373\u97f5\u5f8b\u901a\u9053\uff09\u4f20\u8f93\u7684\u4fe1\u606f\u91cf\u6bd4\u6587\u672c\u901a\u9053\u9ad8\u4e00\u4e2a\u6570\u91cf\u7ea7\u4ee5\u4e0a\uff08\u7279\u522b\u662f\u5728\u7f3a\u4e4f\u5f53\u524d\u53e5\u5b50\u4e4b\u5916\u7684\u957f\u671f\u4e0a\u4e0b\u6587\u65f6\uff09\u3002\u5bf9\u4e8e\u7591\u95ee\u6027\uff0c\u97f5\u5f8b\u63d0\u4f9b\u7684\u989d\u5916\u4fe1\u606f\u76f8\u5bf9\u8f83\u5c11\u3002", "conclusion": "\u97f5\u5f8b\u5728\u4f20\u8fbe\u8bbd\u523a\u548c\u60c5\u611f\u65b9\u9762\u6bd4\u6587\u672c\u63d0\u4f9b\u663e\u8457\u66f4\u591a\u7684\u4fe1\u606f\uff0c\u5c24\u5176\u5728\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u65f6\u3002\u8be5\u65b9\u6cd5\u4e3a\u672a\u6765\u7814\u7a76\u66f4\u591a\u610f\u4e49\u7ef4\u5ea6\u3001\u901a\u4fe1\u901a\u9053\u548c\u8bed\u8a00\u7684\u4fe1\u606f\u8d21\u732e\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.16843", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16843", "abs": "https://arxiv.org/abs/2512.16843", "authors": ["Harsh Vardhan Bansal"], "title": "LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference", "comment": "Accepted and presented at 13th IEEE International Conference on Intelligent Systems and Embedded Design (ISED-2025)", "summary": "Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications", "AI": {"tldr": "LLMCache\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u5c42\u7ea7\u7f13\u5b58\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u7528\u8bed\u4e49\u76f8\u4f3c\u8f93\u5165\u5e8f\u5217\u7684\u4e2d\u95f4\u6fc0\u6d3b\u6765\u52a0\u901fTransformer\u63a8\u7406\uff0c\u5728BERT\u548cGPT-2\u4e0a\u5b9e\u73b0\u6700\u9ad83.1\u500d\u52a0\u901f\u4e14\u7cbe\u5ea6\u635f\u5931\u4f4e\u4e8e0.5%\u3002", "motivation": "Transformer\u6a21\u578b\u867d\u7136\u6027\u80fd\u4f18\u5f02\uff0c\u4f46\u63a8\u7406\u5ef6\u8fdf\u9ad8\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u548c\u5927\u89c4\u6a21\u90e8\u7f72\u3002\u73b0\u6709\u7684token\u7ea7KV\u7f13\u5b58\u673a\u5236\u5728\u9002\u7528\u8303\u56f4\u548c\u6548\u679c\u4e0a\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faLLMCache\u6846\u67b6\uff1a1) \u6a21\u578b\u65e0\u5173\uff0c\u652f\u6301\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u67b6\u6784\uff1b2) \u53ef\u5728\u4efb\u610fTransformer\u5c42\u8fdb\u884c\u7f13\u5b58\uff1b3) \u4f7f\u7528\u8f7b\u91cf\u7ea7\u6307\u7eb9\u673a\u5236\u5339\u914d\u8bed\u4e49\u76f8\u4f3c\u8f93\u5165\uff1b4) \u91c7\u7528\u81ea\u9002\u5e94\u6dd8\u6c70\u7b56\u7565\u7ba1\u7406\u7f13\u5b58\u9648\u65e7\u6027\u3002", "result": "\u5728SQuAD\u3001WikiText-103\u548cOpenBookQA\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5BERT\u548cGPT-2\uff0c\u63a8\u7406\u65f6\u95f4\u6700\u9ad8\u52a0\u901f3.1\u500d\uff0c\u7cbe\u5ea6\u635f\u5931\u4f4e\u4e8e0.5%\u3002", "conclusion": "LLMCache\u662f\u4e00\u4e2a\u5b9e\u7528\u4e14\u901a\u7528\u7684Transformer\u63a8\u7406\u4f18\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2512.16883", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16883", "abs": "https://arxiv.org/abs/2512.16883", "authors": ["Tzu-Han Lin", "Wei-Lin Chen", "Chen-An Li", "Hung-yi Lee", "Yun-Nung Chen", "Yu Meng"], "title": "AdaSearch: Balancing Parametric Knowledge and Search in Large Language Models via Reinforcement Learning", "comment": "Preprint. Code and artifacts will be uploaded to https://github.com/hank0316/AdaSearch", "summary": "Equipping large language models (LLMs) with search engines via reinforcement learning (RL) has emerged as an effective approach for building search agents. However, overreliance on search introduces unnecessary cost and risks exposure to noisy or malicious content, while relying solely on parametric knowledge risks hallucination. The central challenge is to develop agents that adaptively balance parametric knowledge with external search, invoking search only when necessary. Prior work mitigates search overuse by shaping rewards around the number of tool calls. However, these penalties require substantial reward engineering, provide ambiguous credit assignment, and can be exploited by agents that superficially reduce calls. Moreover, evaluating performance solely through call counts conflates necessary and unnecessary search, obscuring the measurement of true adaptive behavior. To address these limitations, we first quantify the self-knowledge awareness of existing search agents via an F1-based decision metric, revealing that methods such as Search-R1 often overlook readily available parametric knowledge. Motivated by these findings, we propose AdaSearch, a simple two-stage, outcome-driven RL framework that disentangles problem solving from the decision of whether to invoke search, and makes this decision process explicit and interpretable. This transparency is crucial for high-stakes domains such as finance and medical question answering, yet is largely neglected by prior approaches. Experiments across multiple model families and sizes demonstrate that AdaSearch substantially improves knowledge-boundary awareness, reduces unnecessary search calls, preserves strong task performance, and offers more transparent, interpretable decision behaviors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAdaSearch\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u5206\u79bb\u95ee\u9898\u89e3\u51b3\u4e0e\u641c\u7d22\u51b3\u7b56\uff0c\u63d0\u9ad8\u641c\u7d22\u4ee3\u7406\u7684\u77e5\u8bc6\u8fb9\u754c\u610f\u8bc6\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u641c\u7d22\u8c03\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u5e76\u589e\u5f3a\u51b3\u7b56\u900f\u660e\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u641c\u7d22\u4ee3\u7406\u5b58\u5728\u8fc7\u5ea6\u4f9d\u8d56\u641c\u7d22\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u4e0d\u5fc5\u8981\u7684\u6210\u672c\u548c\u66b4\u9732\u4e8e\u566a\u58f0/\u6076\u610f\u5185\u5bb9\u7684\u98ce\u9669\uff0c\u800c\u4ec5\u4f9d\u8d56\u53c2\u6570\u5316\u77e5\u8bc6\u53c8\u53ef\u80fd\u4ea7\u751f\u5e7b\u89c9\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u60e9\u7f5a\u5de5\u5177\u8c03\u7528\u6b21\u6570\u6765\u7f13\u89e3\u641c\u7d22\u8fc7\u5ea6\u4f7f\u7528\uff0c\u4f46\u8fd9\u9700\u8981\u5927\u91cf\u5956\u52b1\u5de5\u7a0b\u3001\u5b58\u5728\u4fe1\u7528\u5206\u914d\u6a21\u7cca\u6027\uff0c\u4e14\u53ef\u80fd\u88ab\u8868\u9762\u51cf\u5c11\u8c03\u7528\u7684\u4ee3\u7406\u5229\u7528\u3002", "method": "\u63d0\u51faAdaSearch\u6846\u67b6\uff1a1\uff09\u9996\u5148\u91cf\u5316\u73b0\u6709\u641c\u7d22\u4ee3\u7406\u7684\u81ea\u6211\u77e5\u8bc6\u610f\u8bc6\uff0c\u53d1\u73b0\u5982Search-R1\u7b49\u65b9\u6cd5\u7ecf\u5e38\u5ffd\u7565\u73b0\u6210\u7684\u53c2\u6570\u5316\u77e5\u8bc6\uff1b2\uff09\u91c7\u7528\u7b80\u5355\u7684\u4e24\u9636\u6bb5\u3001\u7ed3\u679c\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u95ee\u9898\u89e3\u51b3\u4e0e\u662f\u5426\u8c03\u7528\u641c\u7d22\u7684\u51b3\u7b56\u89e3\u8026\uff0c\u4f7f\u51b3\u7b56\u8fc7\u7a0b\u660e\u786e\u4e14\u53ef\u89e3\u91ca\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u7cfb\u5217\u548c\u89c4\u6a21\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAdaSearch\u663e\u8457\u63d0\u9ad8\u4e86\u77e5\u8bc6\u8fb9\u754c\u610f\u8bc6\uff0c\u51cf\u5c11\u4e86\u4e0d\u5fc5\u8981\u7684\u641c\u7d22\u8c03\u7528\uff0c\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u66f4\u900f\u660e\u3001\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u884c\u4e3a\u3002", "conclusion": "AdaSearch\u901a\u8fc7\u89e3\u8026\u95ee\u9898\u89e3\u51b3\u4e0e\u641c\u7d22\u51b3\u7b56\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u641c\u7d22\u4ee3\u7406\u8fc7\u5ea6\u4f9d\u8d56\u641c\u7d22\u7684\u95ee\u9898\uff0c\u5728\u91d1\u878d\u548c\u533b\u7597\u95ee\u7b54\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u7279\u522b\u6709\u4ef7\u503c\uff0c\u63d0\u4f9b\u4e86\u66f4\u900f\u660e\u548c\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u6846\u67b6\u3002"}}
{"id": "2512.16899", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16899", "abs": "https://arxiv.org/abs/2512.16899", "authors": ["Yushi Hu", "Reyhane Askari-Hemmat", "Melissa Hall", "Emily Dinan", "Luke Zettlemoyer", "Marjan Ghazvininejad"], "title": "Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image", "comment": "Code and data available at https://github.com/facebookresearch/MMRB2", "summary": "Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning (\"thinking-with-images\"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u5168\u9762\u7684\u591a\u6a21\u6001\u5956\u52b1\u6a21\u578b\u57fa\u51c6MMRB2\uff0c\u7528\u4e8e\u8bc4\u4f30\u56fe\u50cf-\u6587\u672c\u4ea4\u7ec7\u5e8f\u5217\u7684\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\uff0c\u5305\u542b\u56db\u4e2a\u5b50\u4efb\u52a1\u548c4000\u4e2a\u4e13\u5bb6\u6807\u6ce8\u7684\u504f\u597d\u5bf9\u3002", "motivation": "\u5956\u52b1\u6a21\u578b\u5bf9\u4e8e\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5bf9\u4e8e\u5904\u7406\u56fe\u50cf\u548c\u6587\u672c\u4ea4\u7ec7\u5e8f\u5217\u7684Omni\u6a21\u578b\u4ecd\u7f3a\u4e4f\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u5efa\u7acb\u5168\u9762\u7684\u591a\u6a21\u6001\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u521b\u5efaMMRB2\u57fa\u51c6\uff0c\u5305\u542b\u56db\u4e2a\u4efb\u52a1\uff1a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u3001\u56fe\u50cf\u7f16\u8f91\u3001\u4ea4\u7ec7\u751f\u6210\u548c\u591a\u6a21\u6001\u63a8\u7406\uff1b\u6536\u96c61000\u4e2a\u4e13\u5bb6\u6807\u6ce8\u7684\u504f\u597d\u5bf9/\u4efb\u52a1\uff0c\u6765\u81ea23\u4e2a\u6a21\u578b\u548c\u4ee3\u7406\u572821\u4e2a\u6e90\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff1b\u91c7\u7528\u96c6\u6210\u8fc7\u6ee4\u7b56\u7565\u786e\u4fdd\u504f\u597d\u5bf9\u5177\u6709\u5f3a\u4eba\u7c7b\u4e13\u5bb6\u5171\u8bc6\u3002", "result": "\u8bc4\u4f30\u73b0\u6709\u6a21\u578b\uff1aGemini 3 Pro\u8fbe\u523075-80%\u51c6\u786e\u7387\uff0cGPT-5\u548cGemini 2.5 Pro\u8fbe\u523066-75%\uff0c\u8d85\u8d8aGPT-4o\uff0859%\uff09\uff1b\u6700\u4f73\u5f00\u6e90\u6a21\u578bQwen3-VL-32B\u8fbe\u523064%\uff0c\u4e0eGemini 2.5 Flash\u76f8\u5f53\uff1b\u4eba\u7c7b\u51c6\u786e\u7387\u8d85\u8fc790%\uff1bMMRB2\u6027\u80fd\u4e0e\u4e0b\u6e38\u4efb\u52a1\u6210\u529f\u5f3a\u76f8\u5173\u3002", "conclusion": "MMRB2\u4e3a\u591a\u6a21\u6001\u5956\u52b1\u6a21\u578b\u63d0\u4f9b\u4e86\u9996\u4e2a\u5168\u9762\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u4e0e\u4eba\u7c7b\u6027\u80fd\u7684\u663e\u8457\u5dee\u8ddd\uff0c\u6307\u51fa\u4e86\u5956\u52b1\u6a21\u578b\u6539\u8fdb\u7684\u5173\u952e\u65b9\u5411\uff0c\u5bf9\u591a\u6a21\u6001AI\u53d1\u5c55\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2512.16902", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16902", "abs": "https://arxiv.org/abs/2512.16902", "authors": ["Eric Todd", "Jannik Brinkmann", "Rohit Gandikota", "David Bau"], "title": "In-Context Algebra", "comment": "28 pages, 18 figures. Code and data at https://algebra.baulab.info", "summary": "We investigate the mechanisms that arise when transformers are trained to solve arithmetic on sequences where tokens are variables whose meaning is determined only through their interactions. While prior work has found that transformers develop geometric embeddings that mirror algebraic structure, those previous findings emerge from settings where arithmetic-valued tokens have fixed meanings. We devise a new task in which the assignment of symbols to specific algebraic group elements varies from one sequence to another. Despite this challenging setup, transformers achieve near-perfect accuracy on the task and even generalize to unseen algebraic groups. We develop targeted data distributions to create causal tests of a set of hypothesized mechanisms, and we isolate three mechanisms models consistently learn: commutative copying where a dedicated head copies answers, identity element recognition that distinguishes identity-containing facts, and closure-based cancellation that tracks group membership to constrain valid answers. Complementary to the geometric representations found in fixed-symbol settings, our findings show that models develop symbolic reasoning mechanisms when trained to reason in-context with variables whose meanings are not fixed.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86transformer\u5728\u53d8\u91cf\u610f\u4e49\u4e0d\u56fa\u5b9a\u7684\u5e8f\u5217\u7b97\u672f\u4efb\u52a1\u4e2d\u5b66\u4e60\u5230\u7684\u63a8\u7406\u673a\u5236\uff0c\u53d1\u73b0\u4e86\u4e09\u79cd\u7b26\u53f7\u63a8\u7406\u673a\u5236\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u53d1\u73b0transformer\u4f1a\u5728\u56fa\u5b9a\u7b26\u53f7\u610f\u4e49\u7684\u7b97\u672f\u4efb\u52a1\u4e2d\u53d1\u5c55\u51e0\u4f55\u5d4c\u5165\u6765\u53cd\u6620\u4ee3\u6570\u7ed3\u6784\uff0c\u4f46\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u7b26\u53f7\u610f\u4e49\u5f80\u5f80\u662f\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5f53\u7b26\u53f7\u610f\u4e49\u968f\u5e8f\u5217\u53d8\u5316\u65f6\uff0ctransformer\u4f1a\u53d1\u5c55\u51fa\u4ec0\u4e48\u6837\u7684\u63a8\u7406\u673a\u5236\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b0\u4efb\u52a1\uff0c\u5176\u4e2d\u7b26\u53f7\u5230\u7279\u5b9a\u4ee3\u6570\u7fa4\u5143\u7d20\u7684\u5206\u914d\u968f\u5e8f\u5217\u53d8\u5316\u3002\u521b\u5efa\u6709\u9488\u5bf9\u6027\u7684\u6570\u636e\u5206\u5e03\u6765\u8fdb\u884c\u56e0\u679c\u6d4b\u8bd5\uff0c\u5e76\u5f00\u53d1\u4e86\u673a\u5236\u5206\u6790\u65b9\u6cd5\u6765\u8bc6\u522b\u6a21\u578b\u5b66\u4e60\u5230\u7684\u63a8\u7406\u7b56\u7565\u3002", "result": "transformer\u5728\u8be5\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u63a5\u8fd1\u5b8c\u7f8e\u7684\u51c6\u786e\u7387\uff0c\u751a\u81f3\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u4ee3\u6570\u7fa4\u3002\u7814\u7a76\u53d1\u73b0\u4e86\u4e09\u79cd\u6a21\u578b\u4e00\u81f4\u5b66\u4e60\u7684\u673a\u5236\uff1a\u4ea4\u6362\u590d\u5236\uff08\u4e13\u7528\u6ce8\u610f\u529b\u5934\u590d\u5236\u7b54\u6848\uff09\u3001\u5355\u4f4d\u5143\u8bc6\u522b\uff08\u533a\u5206\u5305\u542b\u5355\u4f4d\u5143\u7684\u4e8b\u5b9e\uff09\u548c\u57fa\u4e8e\u5c01\u95ed\u6027\u7684\u6d88\u53bb\uff08\u8ddf\u8e2a\u7fa4\u6210\u5458\u8eab\u4efd\u6765\u7ea6\u675f\u6709\u6548\u7b54\u6848\uff09\u3002", "conclusion": "\u4e0e\u56fa\u5b9a\u7b26\u53f7\u8bbe\u7f6e\u4e2d\u7684\u51e0\u4f55\u8868\u793a\u4e92\u8865\uff0c\u5f53\u8bad\u7ec3transformer\u5728\u53d8\u91cf\u610f\u4e49\u4e0d\u56fa\u5b9a\u7684\u4e0a\u4e0b\u6587\u4e2d\u8fdb\u884c\u63a8\u7406\u65f6\uff0c\u6a21\u578b\u4f1a\u53d1\u5c55\u51fa\u7b26\u53f7\u63a8\u7406\u673a\u5236\uff0c\u8fd9\u8868\u660etransformer\u80fd\u591f\u5b66\u4e60\u62bd\u8c61\u7684\u903b\u8f91\u63a8\u7406\u6a21\u5f0f\u3002"}}
{"id": "2512.16914", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16914", "abs": "https://arxiv.org/abs/2512.16914", "authors": ["Nikhil Prakash", "Donghao Ren", "Dominik Moritz", "Yannick Assogba"], "title": "Constructive Circuit Amplification: Improving Math Reasoning in LLMs via Targeted Sub-Network Updates", "comment": "18 pages, 3 figures", "summary": "Prior studies investigating the internal workings of LLMs have uncovered sparse subnetworks, often referred to as circuits, that are responsible for performing specific tasks. Additionally, it has been shown that model performance improvement through fine-tuning often results from the strengthening of existing circuits in the model. Taken together, these findings suggest the possibility of intervening directly on such circuits to make precise, task-targeted updates. Motivated by these findings, we propose a novel method called Constructive Circuit Amplification which identifies pivotal tokens from model reasoning traces as well as model components responsible for the desired task, and updates only those components. Applied to mathematical reasoning, it improves accuracy by up to +11.4% across multiple models while modifying as little as 1.59% of model components, with minimal impact on other abilities as measured by MMLU, TriviaQA, and TruthfulQA. These results demonstrate that targeted capabilities can be reliably enhanced by selectively updating a sparse set of model components.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6784\u9020\u6027\u7535\u8def\u653e\u5927\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u5173\u952etoken\u548c\u4efb\u52a1\u76f8\u5173\u7ec4\u4ef6\uff0c\u4ec5\u66f4\u65b0\u7a00\u758f\u7684\u6a21\u578b\u7ec4\u4ef6\u6765\u63d0\u5347\u7279\u5b9a\u4efb\u52a1\u6027\u80fd", "motivation": "\u5148\u524d\u7814\u7a76\u53d1\u73b0LLMs\u4e2d\u5b58\u5728\u8d1f\u8d23\u7279\u5b9a\u4efb\u52a1\u7684\u7a00\u758f\u5b50\u7f51\u7edc\uff08\u7535\u8def\uff09\uff0c\u4e14\u5fae\u8c03\u901a\u5e38\u901a\u8fc7\u589e\u5f3a\u73b0\u6709\u7535\u8def\u6765\u63d0\u5347\u6027\u80fd\u3002\u8fd9\u542f\u53d1\u4e86\u76f4\u63a5\u5e72\u9884\u8fd9\u4e9b\u7535\u8def\u8fdb\u884c\u7cbe\u786e\u3001\u4efb\u52a1\u9488\u5bf9\u6027\u66f4\u65b0\u7684\u53ef\u80fd\u6027\u3002", "method": "\u6784\u9020\u6027\u7535\u8def\u653e\u5927\u65b9\u6cd5\uff1a\u4ece\u6a21\u578b\u63a8\u7406\u8f68\u8ff9\u4e2d\u8bc6\u522b\u5173\u952etoken\uff0c\u627e\u51fa\u8d1f\u8d23\u76ee\u6807\u4efb\u52a1\u7684\u6a21\u578b\u7ec4\u4ef6\uff0c\u4ec5\u66f4\u65b0\u8fd9\u4e9b\u7ec4\u4ef6\u800c\u975e\u6574\u4e2a\u6a21\u578b\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5c06\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe+11.4%\uff0c\u540c\u65f6\u4ec5\u4fee\u65391.59%\u7684\u6a21\u578b\u7ec4\u4ef6\uff0c\u5bf9MMLU\u3001TriviaQA\u548cTruthfulQA\u7b49\u5176\u4ed6\u80fd\u529b\u5f71\u54cd\u6700\u5c0f\u3002", "conclusion": "\u901a\u8fc7\u9009\u62e9\u6027\u66f4\u65b0\u7a00\u758f\u7684\u6a21\u578b\u7ec4\u4ef6\u96c6\u5408\uff0c\u53ef\u4ee5\u53ef\u9760\u5730\u589e\u5f3a\u76ee\u6807\u80fd\u529b\uff0c\u8fd9\u4e3a\u7cbe\u786e\u3001\u9ad8\u6548\u7684\u6a21\u578b\u7f16\u8f91\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
