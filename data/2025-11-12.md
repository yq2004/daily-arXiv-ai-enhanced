<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 56]
- [cs.IR](#cs.IR) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Preliminary Study of RAG for Taiwanese Historical Archives](https://arxiv.org/abs/2511.07445)
*Claire Lin,Bo-Han Feng,Xuanjun Chen,Te-Lun Yang,Hung-yi Lee,Jyh-Shing Roger Jang*

Main category: cs.CL

TL;DR: 本研究首次将检索增强生成(RAG)应用于台湾历史档案，系统分析了查询特征和元数据整合策略对检索质量、答案生成及整体系统性能的影响。


<details>
  <summary>Details</summary>
Motivation: 虽然检索增强生成(RAG)在知识密集型任务中表现出色，但很少有研究将其应用于台湾历史档案领域。

Method: 构建RAG管道，应用于两个繁体中文历史数据集（热兰遮城和台湾省议会公报）及其对应的开放式查询集，系统研究查询特征和元数据整合策略的影响。

Result: 早期元数据整合能同时提升检索和答案准确性，但也揭示了RAG系统面临的持续挑战，包括生成过程中的幻觉问题以及处理时序或多跳历史查询的困难。

Conclusion: RAG在台湾历史档案应用中具有潜力，但需要解决幻觉问题和复杂历史查询处理等挑战。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising approach for knowledge-intensive tasks. However, few studies have examined RAG for Taiwanese Historical Archives. In this paper, we present an initial study of a RAG pipeline applied to two historical Traditional Chinese datasets, Fort Zeelandia and the Taiwan Provincial Council Gazette, along with their corresponding open-ended query sets. We systematically investigate the effects of query characteristics and metadata integration strategies on retrieval quality, answer generation, and the performance of the overall system. The results show that early-stage metadata integration enhances both retrieval and answer accuracy while also revealing persistent challenges for RAG systems, including hallucinations during generation and difficulties in handling temporal or multi-hop historical queries.

</details>


### [2] [Large Language Models for Scientific Idea Generation: A Creativity-Centered Survey](https://arxiv.org/abs/2511.07448)
*Fatemeh Shahhosseini,Arash Marioriyad,Ali Momen,Mahdieh Soleymani Baghshah,Mohammad Hossein Rohban,Shaghayegh Haghjooy Javanmard*

Main category: cs.CL

TL;DR: 本调查系统综述了LLM驱动的科学创意生成方法，将其分为五类：外部知识增强、基于提示的分布导向、推理时缩放、多智能体协作和参数级适应，并使用Boden创造力分类法和Rhodes 4P框架分析各方法的创意层次和来源。


<details>
  <summary>Details</summary>
Motivation: 科学创意生成是科学发现的核心，但LLM在科学创意生成中的创造力表现不一致且理解不足，需要系统梳理现有方法以推动LLM在科学发现中的可靠应用。

Method: 使用结构化综合方法，将现有LLM科学创意生成方法分为五类：外部知识增强、基于提示的分布导向、推理时缩放、多智能体协作和参数级适应，并运用Boden创造力分类法（组合性、探索性、变革性）和Rhodes 4P框架（人、过程、压力、产品）进行分析。

Result: 建立了LLM科学创意生成方法的分类体系，揭示了不同方法在平衡创造力与科学严谨性方面的特点，以及它们分别强调的创意层次（组合性、探索性、变革性）和创意来源（人、过程、压力、产品）。

Conclusion: 通过将方法论进展与创造力框架对齐，本调查阐明了该领域现状，并为LLM在科学发现中实现可靠、系统和变革性应用指明了关键方向。

Abstract: Scientific idea generation lies at the heart of scientific discovery and has driven human progress-whether by solving unsolved problems or proposing novel hypotheses to explain unknown phenomena. Unlike standard scientific reasoning or general creative generation, idea generation in science is a multi-objective and open-ended task, where the novelty of a contribution is as essential as its empirical soundness. Large language models (LLMs) have recently emerged as promising generators of scientific ideas, capable of producing coherent and factual outputs with surprising intuition and acceptable reasoning, yet their creative capacity remains inconsistent and poorly understood. This survey provides a structured synthesis of methods for LLM-driven scientific ideation, examining how different approaches balance creativity with scientific soundness. We categorize existing methods into five complementary families: External knowledge augmentation, Prompt-based distributional steering, Inference-time scaling, Multi-agent collaboration, and Parameter-level adaptation. To interpret their contributions, we employ two complementary frameworks: Boden's taxonomy of Combinatorial, Exploratory and Transformational creativity to characterize the level of ideas each family expected to generate, and Rhodes' 4Ps framework-Person, Process, Press, and Product-to locate the aspect or source of creativity that each method emphasizes. By aligning methodological advances with creativity frameworks, this survey clarifies the state of the field and outlines key directions toward reliable, systematic, and transformative applications of LLMs in scientific discovery.

</details>


### [3] [GRIP: In-Parameter Graph Reasoning through Fine-Tuning Large Language Models](https://arxiv.org/abs/2511.07457)
*Jiarui Feng,Donghong Cai,Yixin Chen,Muhan Zhang*

Main category: cs.CL

TL;DR: GRIP框架通过精心设计的微调任务，让LLM能够内化图中的复杂关系信息，并将这些知识存储在轻量级的LoRA参数中，使微调后的LLM能够在推理时无需访问原始图就能执行各种图相关任务。


<details>
  <summary>Details</summary>
Motivation: LLM在处理序列文本数据方面表现出色，但适应结构化数据（如知识图谱或网络数据）仍然具有挑战性。现有方法存在token开销大、需要大规模后训练和复杂对齐过程等问题，且由于模态对齐不佳导致结果不理想。

Method: 受LLM测试时适应的参数内知识注入启发，提出GRIP框架，通过精心设计的微调任务让LLM内化图中的复杂关系信息，并将知识存储在轻量级LoRA参数中。

Result: 在多个基准测试上的广泛实验验证了该方法的有效性和效率。

Conclusion: GRIP框架成功解决了LLM适应结构化数据的问题，通过参数内知识注入实现了高效的知识存储和推理，无需在推理时访问原始图结构。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in modeling sequential textual data and generalizing across diverse tasks. However, adapting LLMs to effectively handle structural data, such as knowledge graphs or web data, remains a challenging problem. Some approaches adopt complex strategies to convert graphs into text sequences, resulting in significant token overhead and rendering them impractical for large-scale graphs. Others introduce additional modules to encode graphs into fixed-size token representations for LLMs. However, these methods typically require large-scale post-training on graph-text corpus and complex alignment procedures, yet often yield sub-optimal results due to poor modality alignment. Inspired by in-parameter knowledge injection for test-time adaptation of LLMs, we propose GRIP, a novel framework that equips LLMs with the ability to internalize complex relational information from graphs through carefully designed fine-tuning tasks. This knowledge is efficiently stored within lightweight LoRA parameters, enabling the fine-tuned LLM to perform a wide range of graph-related tasks without requiring access to the original graph at inference time. Extensive experiments across multiple benchmarks validate the effectiveness and efficiency of our approach.

</details>


### [4] [It Takes Two: A Dual Stage Approach for Terminology-Aware Translation](https://arxiv.org/abs/2511.07461)
*Akshat Singh Jaswal*

Main category: cs.CL

TL;DR: DuTerm是一个两阶段术语约束机器翻译架构，结合术语感知NMT模型和基于提示的LLM后编辑，在多个语言对上实现高质量翻译。


<details>
  <summary>Details</summary>
Motivation: 解决术语约束机器翻译中严格约束执行与翻译质量之间的平衡问题，探索LLM在术语处理中的最佳应用方式。

Method: 两阶段架构：第一阶段使用在大型合成数据上微调的术语感知NMT模型；第二阶段使用基于提示的LLM进行后编辑，优化NMT输出并确保术语遵循。

Result: 在英德、英西、英俄WMT 2025术语共享任务语料上评估，显示LLM作为上下文驱动的修改器比严格约束执行产生更高质量的翻译。

Conclusion: LLM最适合作为上下文驱动的修改器而非生成器，在术语约束翻译中存在关键权衡：灵活、上下文驱动的术语处理优于严格约束执行。

Abstract: This paper introduces DuTerm, a novel two-stage architecture for terminology-constrained machine translation. Our system combines a terminology-aware NMT model, adapted via fine-tuning on large-scale synthetic data, with a prompt-based LLM for post-editing. The LLM stage refines NMT output and enforces terminology adherence. We evaluate DuTerm on English-to German, English-to-Spanish, and English-to-Russian with the WMT 2025 Terminology Shared Task corpus. We demonstrate that flexible, context-driven terminology handling by the LLM consistently yields higher quality translations than strict constraint enforcement. Our results highlight a critical trade-off, revealing that an LLM's work best for high-quality translation as context-driven mutators rather than generators.

</details>


### [5] [Motif 2 12.7B technical report](https://arxiv.org/abs/2511.07464)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Taehyun Kim,Eunhwan Park,Jeesoo Lee,Jeongdoo Lee,Junhyeok Lee,Wai Ting Cheung,Dahye Choi,Jaeheui Her,Jaeyeon Huh,Hanbin Jung,Changjin Kang,Beomgyu Kim,Minjae Kim,Taewhan Kim,Youngrok Kim,Hyukjin Kweon,Haesol Lee,Kungyu Lee,Dongpin Oh,Yeongjae Park,Bokki Ryu,Dongjoo Weon*

Main category: cs.CL

TL;DR: Motif-2-12.7B是一个开源基础模型，通过架构创新和系统级优化提升大语言模型效率，在有限计算预算下实现可扩展语言理解和稳健指令泛化。


<details>
  <summary>Details</summary>
Motivation: 在受限计算预算下实现可扩展的语言理解和稳健的指令泛化，通过架构创新和系统优化来提升大语言模型的效率。

Method: 基于Motif-2.6B，集成分组差分注意力(GDA)分离信号和噪声控制注意力路径；在5.5万亿token上预训练，使用课程驱动数据调度器；采用MuonClip优化器和自定义高性能内核；三阶段监督微调流程增强指令遵循、组合理解和语言精度。

Result: 在多样化基准测试中表现出竞争力，证明经过深思熟虑的架构扩展和优化训练设计可以与更大模型相媲美。

Conclusion: 通过架构创新和系统级优化，Motif-2-12.7B展示了在有限计算预算下实现高效大语言模型的可能性，其性能可与更大模型竞争。

Abstract: We introduce Motif-2-12.7B, a new open-weight foundation model that pushes the efficiency frontier of large language models by combining architectural innovation with system-level optimization. Designed for scalable language understanding and robust instruction generalization under constrained compute budgets, Motif-2-12.7B builds upon Motif-2.6B with the integration of Grouped Differential Attention (GDA), which improves representational efficiency by disentangling signal and noise-control attention pathways. The model is pre-trained on 5.5 trillion tokens spanning diverse linguistic, mathematical, scientific, and programming domains using a curriculum-driven data scheduler that gradually changes the data composition ratio. The training system leverages the MuonClip optimizer alongside custom high-performance kernels, including fused PolyNorm activations and the Parallel Muon algorithm, yielding significant throughput and memory efficiency gains in large-scale distributed environments. Post-training employs a three-stage supervised fine-tuning pipeline that successively enhances general instruction adherence, compositional understanding, and linguistic precision. Motif-2-12.7B demonstrates competitive performance across diverse benchmarks, showing that thoughtful architectural scaling and optimized training design can rival the capabilities of much larger models.

</details>


### [6] [Focusing on Language: Revealing and Exploiting Language Attention Heads in Multilingual Large Language Models](https://arxiv.org/abs/2511.07498)
*Xin Liu,Qiyang Song,Qihang Zhou,Haichao Du,Shaowen Xu,Wenbo Jiang,Weijuan Zhang,Xiaoqi Jia*

Main category: cs.CL

TL;DR: 本文研究了多头自注意力机制在大型语言模型多语言能力中的作用，提出了LAHIS方法来识别对多语言处理重要的注意力头，发现了语言特定和语言通用的注意力头，并开发了轻量级适配方法提升多语言性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的多语言能力日益重要，但多头自注意力机制在多语言处理中的具体作用尚未充分探索。理解注意力机制如何支持多语言能力对于提升模型性能和可解释性至关重要。

Method: 提出了语言注意力头重要性评分方法，通过单次前向和反向传播识别对多语言能力重要的注意力头。在Aya-23-8B、Llama-3.2-3B和Mistral-7B-v0.1模型上应用该方法，并开发了轻量级适配方法，仅需20个可调参数来调节注意力输出。

Result: 发现了语言特定和语言通用的注意力头。语言特定头能够实现跨语言注意力转移，引导模型朝向目标语言上下文，减轻非目标语言生成问题。轻量级适配方法将XQuAD准确率提升了2.4%。

Conclusion: 该研究从多头自注意力角度增强了大型语言模型的可解释性和多语言能力，为理解和支持多语言处理提供了新视角。

Abstract: Large language models (LLMs) increasingly support multilingual understanding and generation. Meanwhile, efforts to interpret their internal mechanisms have emerged, offering insights to enhance multilingual performance. While multi-head self-attention (MHA) has proven critical in many areas, its role in multilingual capabilities remains underexplored. In this work, we study the contribution of MHA in supporting multilingual processing in LLMs. We propose Language Attention Head Importance Scores (LAHIS), an effective and efficient method that identifies attention head importance for multilingual capabilities via a single forward and backward pass through the LLM. Applying LAHIS to Aya-23-8B, Llama-3.2-3B, and Mistral-7B-v0.1, we reveal the existence of both language-specific and language-general heads. Language-specific heads enable cross-lingual attention transfer to guide the model toward target language contexts and mitigate off-target language generation issue, contributing to addressing challenges in multilingual LLMs. We also introduce a lightweight adaptation that learns a soft head mask to modulate attention outputs over language heads, requiring only 20 tunable parameters to improve XQuAD accuracy. Overall, our work enhances both the interpretability and multilingual capabilities of LLMs from the perspective of MHA.

</details>


### [7] [LLMs vs. Traditional Sentiment Tools in Psychology: An Evaluation on Belgian-Dutch Narratives](https://arxiv.org/abs/2511.07641)
*Ratna Kandala,Katie Hoemann*

Main category: cs.CL

TL;DR: 荷兰语LLMs在弗拉芒语情感分析中表现不如传统方法，Pattern工具表现最佳，挑战了LLM在情感分析任务中的优越性假设。


<details>
  <summary>Details</summary>
Motivation: 理解日常语言中的情感细微差别对计算语言学和情感研究至关重要，需要评估LLMs在低资源语言变体（弗拉芒语）中的情感分析表现。

Method: 评估三个荷兰语特定LLMs（ChocoLlama-8B-Instruct、Reynaerde-7B-chat、GEITje-7B-ultra）与LIWC和Pattern工具在弗拉芒语情感价预测上的表现，使用约25000个自发文本响应数据集。

Result: 尽管有架构进步，荷兰语调优的LLMs表现不如传统方法，Pattern显示出优越性能。

Conclusion: 研究结果挑战了LLM在情感分析任务中的优越性假设，突显了在自发真实世界叙事中捕捉情感价的复杂性，强调需要为低资源语言变体开发文化和语言定制的评估框架。

Abstract: Understanding emotional nuances in everyday language is crucial for computational linguistics and emotion research. While traditional lexicon-based tools like LIWC and Pattern have served as foundational instruments, Large Language Models (LLMs) promise enhanced context understanding. We evaluated three Dutch-specific LLMs (ChocoLlama-8B-Instruct, Reynaerde-7B-chat, and GEITje-7B-ultra) against LIWC and Pattern for valence prediction in Flemish, a low-resource language variant. Our dataset comprised approximately 25000 spontaneous textual responses from 102 Dutch-speaking participants, each providing narratives about their current experiences with self-assessed valence ratings (-50 to +50). Surprisingly, despite architectural advancements, the Dutch-tuned LLMs underperformed compared to traditional methods, with Pattern showing superior performance. These findings challenge assumptions about LLM superiority in sentiment analysis tasks and highlight the complexity of capturing emotional valence in spontaneous, real-world narratives. Our results underscore the need for developing culturally and linguistically tailored evaluation frameworks for low-resource language variants, while questioning whether current LLM fine-tuning approaches adequately address the nuanced emotional expressions found in everyday language use.

</details>


### [8] [Stress Testing Factual Consistency Metrics for Long-Document Summarization](https://arxiv.org/abs/2511.07689)
*Zain Muhammad Mujahid,Dustin Wright,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 对6种广泛使用的无参考事实性评估指标在长文档摘要场景下的可靠性进行系统评估，发现这些短文档指标在语义等价摘要上产生不一致分数，对信息密集声明可靠性下降，且在长上下文条件下无法保持事实一致性。


<details>
  <summary>Details</summary>
Motivation: 评估抽象文本摘要的事实一致性仍然是一个重大挑战，特别是在长文档场景下，传统指标难以处理输入长度限制和长距离依赖问题。

Method: 通过7种事实保持性扰动（释义、简化、同义词替换、逻辑等价否定、词汇缩减、压缩和源文本插入）来测试指标鲁棒性，并分析其对检索上下文和声明信息密度的敏感性。在科幻、法律和科学三个长文档基准数据集上进行评估。

Result: 现有短文档指标在语义等价摘要上产生不一致分数，对信息密集声明的可靠性下降，扩展检索上下文在某些领域能提高稳定性，但没有指标能在长上下文条件下一致保持事实对齐。

Conclusion: 研究结果指出了改进事实性评估的具体方向，包括多跨度推理、上下文感知校准以及在意义保持变体上进行训练，以增强长文档摘要的鲁棒性。

Abstract: Evaluating the factual consistency of abstractive text summarization remains a significant challenge, particularly for long documents, where conventional metrics struggle with input length limitations and long-range dependencies. In this work, we systematically evaluate the reliability of six widely used reference-free factuality metrics, originally proposed for short-form summarization, in the long-document setting. We probe metric robustness through seven factuality-preserving perturbations applied to summaries, namely paraphrasing, simplification, synonym replacement, logically equivalent negations, vocabulary reduction, compression, and source text insertion, and further analyze their sensitivity to retrieval context and claim information density. Across three long-form benchmark datasets spanning science fiction, legal, and scientific domains, our results reveal that existing short-form metrics produce inconsistent scores for semantically equivalent summaries and exhibit declining reliability for information-dense claims whose content is semantically similar to many parts of the source document. While expanding the retrieval context improves stability in some domains, no metric consistently maintains factual alignment under long-context conditions. Finally, our results highlight concrete directions for improving factuality evaluation, including multi-span reasoning, context-aware calibration, and training on meaning-preserving variations to enhance robustness in long-form summarization. We release all code, perturbed data, and scripts required to reproduce our results at https://github.com/zainmujahid/metricEval-longSum.

</details>


### [9] [Critical Confabulation: Can LLMs Hallucinate for Social Good?](https://arxiv.org/abs/2511.07722)
*Peiqi Sui,Eamon Duede,Hoyt Long,Richard Jean So*

Main category: cs.CL

TL;DR: 论文提出'关键虚构'方法，利用LLM的幻觉能力填补历史档案中因社会政治不平等造成的空白，通过叙事填空任务验证LLMs能够生成受证据约束的替代叙事。


<details>
  <summary>Details</summary>
Motivation: LLMs虽然会产生幻觉，但经过精心约束的虚构可以具有社会价值，特别是用于填补因社会政治不平等导致的历史档案空白，重建历史中被忽视人物的叙事。

Method: 使用开放式叙事填空任务，让LLMs生成角色时间线中被掩盖的事件；评估经过数据污染审计的完全开放模型（OLMo-2系列）与未审计的开放权重和专有基线模型；设计多种提示词来引发受控且有用的幻觉。

Result: 验证了LLMs具备执行关键虚构的基础叙事理解能力，表明受控且明确指定的幻觉可以支持LLM在知识生产中的应用，同时不会损害历史准确性和忠实度。

Conclusion: 精心约束的LLM幻觉可以成为填补历史空白的有效工具，关键虚构方法为利用LLM进行知识生产提供了新途径，同时保持了历史准确性的平衡。

Abstract: LLMs hallucinate, yet some confabulations can have social affordances if carefully bounded. We propose critical confabulation (inspired by critical fabulation from literary and social theory), the use of LLM hallucinations to "fill-in-the-gap" for omissions in archives due to social and political inequality, and reconstruct divergent yet evidence-bound narratives for history's "hidden figures". We simulate these gaps with an open-ended narrative cloze task: asking LLMs to generate a masked event in a character-centric timeline sourced from a novel corpus of unpublished texts. We evaluate audited (for data contamination), fully-open models (the OLMo-2 family) and unaudited open-weight and proprietary baselines under a range of prompts designed to elicit controlled and useful hallucinations. Our findings validate LLMs' foundational narrative understanding capabilities to perform critical confabulation, and show how controlled and well-specified hallucinations can support LLM applications for knowledge production without collapsing speculation into a lack of historical accuracy and fidelity.

</details>


### [10] [Back to the Future: The Role of Past and Future Context Predictability in Incremental Language Production](https://arxiv.org/abs/2511.07752)
*Shiva Upadhye,Richard Futrell*

Main category: cs.CL

TL;DR: 该研究提出了一种新的信息论可预测性度量方法，整合了未来和过去上下文对单词可预测性的影响，并通过自然语言语料库分析揭示了前后文可预测性在词汇规划和句子规划中的作用机制。


<details>
  <summary>Details</summary>
Motivation: 虽然单词在给定过去上下文时的可预测性效应在语言产生和理解中已有较好理解，但自然语言产生研究中发现的后向可预测性效应（单词给定其未来上下文）仍缺乏深入理解，这可能与未来规划有关。

Method: 使用改进的度量和更强大的语言模型，在两个自然语言语料库研究中：1）重新审视经典可预测性对单词时长的影响；2）在生成框架内研究替代错误，独立建模词汇、上下文和交际因素对词汇选择的影响。

Result: 提出的概念驱动后向可预测性替代方法在两个研究中产生了质上相似的效果。通过细粒度替代错误分析，发现不同类型的错误暗示了说话者在词汇规划中如何优先处理形式、意义和基于上下文的信息。

Conclusion: 这些发现阐明了过去和未来上下文在说话者编码和选择词汇中的功能作用，为上下文可预测性效应和句子规划机制之间建立了桥梁。

Abstract: Contextual predictability shapes both the form and choice of words in online language production. The effects of the predictability of a word given its previous context are generally well-understood in both production and comprehension, but studies of naturalistic production have also revealed a poorly-understood backward predictability effect of a word given its future context, which may be related to future planning. Here, in two studies of naturalistic speech corpora, we investigate backward predictability effects using improved measures and more powerful language models, introducing a new principled and conceptually motivated information-theoretic predictability measure that integrates predictability from both the future and the past context. Our first study revisits classic predictability effects on word duration. Our second study investigates substitution errors within a generative framework that independently models the effects of lexical, contextual, and communicative factors on word choice, while predicting the actual words that surface as speech errors. We find that our proposed conceptually-motivated alternative to backward predictability yields qualitatively similar effects across both studies. Through a fine-grained analysis of substitution errors, we further show that different kinds of errors are suggestive of how speakers prioritize form, meaning, and context-based information during lexical planning. Together, these findings illuminate the functional roles of past and future context in how speakers encode and choose words, offering a bridge between contextual predictability effects and the mechanisms of sentence planning.

</details>


### [11] [Design, Results and Industry Implications of the World's First Insurance Large Language Model Evaluation Benchmark](https://arxiv.org/abs/2511.07794)
*Hua Zhou,Bing Ma,Yufei Zhang,Yi Zhao*

Main category: cs.CL

TL;DR: 本文介绍了CUFEInse v1.0保险领域专业评估基准的构建，包含5个核心维度、54个子指标和14,430个高质量问题，并对11个主流大语言模型进行了全面评估。


<details>
  <summary>Details</summary>
Motivation: 填补保险领域专业评估基准的空白，为学术界和产业界提供专业、系统、权威的评估工具，并为垂直领域大模型评估范式提供重要参考。

Method: 采用"量化导向、专家驱动、多重验证"原则，建立涵盖保险理论知识、行业理解、安全合规、智能体应用和逻辑严谨性的评估框架。

Result: 评估发现通用模型存在精算能力弱、合规适应性不足等瓶颈；高质量领域专用训练在保险垂直场景有优势但在业务适应性和合规性方面存在短板；准确识别了当前大模型在保险精算、核保理赔推理、合规营销文案等专业场景的共性瓶颈。

Conclusion: CUFEInse的建立填补了保险领域专业评估基准的空白，其构建理念和方法论为垂直领域大模型评估范式提供了重要参考，并为学术模型优化和工业模型选择提供了权威参考。展望了评估基准的未来迭代方向和保险大模型"领域适应+推理增强"的核心发展方向。

Abstract: This paper comprehensively elaborates on the construction methodology, multi-dimensional evaluation system, and underlying design philosophy of CUFEInse v1.0. Adhering to the principles of "quantitative-oriented, expert-driven, and multi-validation," the benchmark establishes an evaluation framework covering 5 core dimensions, 54 sub-indicators, and 14,430 high-quality questions, encompassing insurance theoretical knowledge, industry understanding, safety and compliance, intelligent agent application, and logical rigor. Based on this benchmark, a comprehensive evaluation was conducted on 11 mainstream large language models. The evaluation results reveal that general-purpose models suffer from common bottlenecks such as weak actuarial capabilities and inadequate compliance adaptation. High-quality domain-specific training demonstrates significant advantages in insurance vertical scenarios but exhibits shortcomings in business adaptation and compliance. The evaluation also accurately identifies the common bottlenecks of current large models in professional scenarios such as insurance actuarial, underwriting and claim settlement reasoning, and compliant marketing copywriting. The establishment of CUFEInse not only fills the gap in professional evaluation benchmarks for the insurance field, providing academia and industry with a professional, systematic, and authoritative evaluation tool, but also its construction concept and methodology offer important references for the evaluation paradigm of large models in vertical fields, serving as an authoritative reference for academic model optimization and industrial model selection. Finally, the paper looks forward to the future iteration direction of the evaluation benchmark and the core development direction of "domain adaptation + reasoning enhancement" for insurance large models.

</details>


### [12] [From Experience to Strategy: Empowering LLM Agents with Trainable Graph Memory](https://arxiv.org/abs/2511.07800)
*Siyu Xia,Zekun Xu,Jiajun Chai,Wentian Fan,Yan Song,Xiaohan Wang,Guojun Yin,Wei Lin,Haifeng Zhang,Jun Wang*

Main category: cs.CL

TL;DR: 论文提出了一种可训练的、基于图的记忆框架，通过强化学习优化元认知策略，提升LLM智能体在复杂任务中的推理能力和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体获取经验的方式存在局限：隐式记忆训练存在灾难性遗忘和可解释性差的问题，显式记忆提示缺乏适应性。需要一种更好的方式来利用先验经验指导当前决策。

Method: 提出了一个以智能体为中心、可训练的多层图记忆框架：1）将原始智能体轨迹抽象为状态机中的结构化决策路径；2）进一步提炼为高层、人类可解释的元认知策略；3）通过基于奖励的权重优化程序估计每个元认知的经验效用；4）通过元认知提示动态集成到LLM智能体训练循环中。

Result: 实验表明，可学习的图记忆框架具有强大的泛化能力，提高了LLM智能体的策略推理性能，并在强化学习训练中提供了持续收益。

Conclusion: 该记忆框架通过结构化的经验表示和适应性优化，有效增强了LLM智能体利用参数信息的能力，为提升自主任务解决能力提供了新思路。

Abstract: Large Language Models (LLMs) based agents have demonstrated remarkable potential in autonomous task-solving across complex, open-ended environments. A promising approach for improving the reasoning capabilities of LLM agents is to better utilize prior experiences in guiding current decisions. However, LLMs acquire experience either through implicit memory via training, which suffers from catastrophic forgetting and limited interpretability, or explicit memory via prompting, which lacks adaptability. In this paper, we introduce a novel agent-centric, trainable, multi-layered graph memory framework and evaluate how context memory enhances the ability of LLMs to utilize parametric information. The graph abstracts raw agent trajectories into structured decision paths in a state machine and further distills them into high-level, human-interpretable strategic meta-cognition. In order to make memory adaptable, we propose a reinforcement-based weight optimization procedure that estimates the empirical utility of each meta-cognition based on reward feedback from downstream tasks. These optimized strategies are then dynamically integrated into the LLM agent's training loop through meta-cognitive prompting. Empirically, the learnable graph memory delivers robust generalization, improves LLM agents' strategic reasoning performance, and provides consistent benefits during Reinforcement Learning (RL) training.

</details>


### [13] [AlignSurvey: A Comprehensive Benchmark for Human Preferences Alignment in Social Surveys](https://arxiv.org/abs/2511.07871)
*Chenxi Lin,Weikang Yuan,Zhuoren Jiang,Biao Huang,Ruitao Zhang,Jianan Ge,Yueqian Xu,Jianxing Yu*

Main category: cs.CL

TL;DR: AlignSurvey是首个系统复制和评估完整社会调查流程的基准，包含四个任务和专门评估指标，重点关注对齐保真度、一致性和公平性，并提供了多层级数据集和SurveyLM模型家族。


<details>
  <summary>Details</summary>
Motivation: 传统社会调查面临固定问题格式、高成本、有限适应性以及跨文化等效性等挑战，而现有使用LLM模拟调查的研究大多局限于结构化问题，忽视了完整调查流程，且由于训练数据偏差可能导致边缘群体代表性不足。

Method: 引入AlignSurvey基准，定义四个与关键调查阶段对齐的任务：社会角色建模、半结构化访谈建模、态度立场建模和调查响应建模。构建多层级数据集架构，包括社会基础语料库和完整流程调查数据集，并通过两阶段微调开源LLM获得SurveyLM模型家族。

Result: 开发了包含44K+访谈对话和400K+结构化调查记录的社会基础语料库，以及专家标注的AlignSurvey-Expert和两个全国代表性调查数据集。提供了专门评估指标来评估个体和群体层面的对齐保真度、一致性和公平性。

Conclusion: AlignSurvey为透明和负责任的社会科学研究提供了系统基准、数据集和工具，支持在人口多样性背景下评估LLM在社会调查中的表现，所有资源已在github和huggingface上开源。

Abstract: Understanding human attitudes, preferences, and behaviors through social surveys is essential for academic research and policymaking. Yet traditional surveys face persistent challenges, including fixed-question formats, high costs, limited adaptability, and difficulties ensuring cross-cultural equivalence. While recent studies explore large language models (LLMs) to simulate survey responses, most are limited to structured questions, overlook the entire survey process, and risks under-representing marginalized groups due to training data biases. We introduce AlignSurvey, the first benchmark that systematically replicates and evaluates the full social survey pipeline using LLMs. It defines four tasks aligned with key survey stages: social role modeling, semi-structured interview modeling, attitude stance modeling and survey response modeling. It also provides task-specific evaluation metrics to assess alignment fidelity, consistency, and fairness at both individual and group levels, with a focus on demographic diversity. To support AlignSurvey, we construct a multi-tiered dataset architecture: (i) the Social Foundation Corpus, a cross-national resource with 44K+ interview dialogues and 400K+ structured survey records; and (ii) a suite of Entire-Pipeline Survey Datasets, including the expert-annotated AlignSurvey-Expert (ASE) and two nationally representative surveys for cross-cultural evaluation. We release the SurveyLM family, obtained through two-stage fine-tuning of open-source LLMs, and offer reference models for evaluating domain-specific alignment. All datasets, models, and tools are available at github and huggingface to support transparent and socially responsible research.

</details>


### [14] [Planned Event Forecasting using Future Mentions and Related Entity Extraction in News Articles](https://arxiv.org/abs/2511.07879)
*Neelesh Kumar Shukla,Pranay Sanghvi*

Main category: cs.CL

TL;DR: 开发了一个基于新闻文章分析的社会动荡事件预测系统，使用主题建模和word2vec过滤相关新闻，NER识别实体，时间标准化处理未来日期，并提出相关实体提取方法来识别真正参与事件的实体。


<details>
  <summary>Details</summary>
Motivation: 在印度等民主国家，人们自由表达观点和需求可能导致抗议、集会等社会动荡事件，这些事件往往未经事先许可且具有破坏性。预测这些事件有助于行政官员采取必要行动。

Method: 使用主题建模和word2vec过滤相关新闻文章，应用命名实体识别方法识别人员、组织、地点和日期等实体，进行时间标准化将未来日期转换为标准格式，并提出相关实体提取方法来识别真正参与事件的实体。

Result: 开发了一个地理独立、通用的模型来识别过滤社会动荡事件的关键特征，能够从众多实体提及中提取真正相关的实体。

Conclusion: 该系统能够通过分析新闻文章中的公告来预测计划中的社会动荡事件，为行政官员提供预警信息，有助于更好地管理社会秩序。

Abstract: In democracies like India, people are free to express their views and demands. Sometimes this causes situations of civil unrest such as protests, rallies, and marches. These events may be disruptive in nature and are often held without prior permission from the competent authority. Forecasting these events helps administrative officials take necessary action. Usually, protests are announced well in advance to encourage large participation. Therefore, by analyzing such announcements in news articles, planned events can be forecasted beforehand. We developed such a system in this paper to forecast social unrest events using topic modeling and word2vec to filter relevant news articles, and Named Entity Recognition (NER) methods to identify entities such as people, organizations, locations, and dates. Time normalization is applied to convert future date mentions into a standard format. In this paper, we have developed a geographically independent, generalized model to identify key features for filtering civil unrest events. There could be many mentions of entities, but only a few may actually be involved in the event. This paper calls such entities Related Entities and proposes a method to extract them, referred to as Related Entity Extraction.

</details>


### [15] [Breaking the Adversarial Robustness-Performance Trade-off in Text Classification via Manifold Purification](https://arxiv.org/abs/2511.07888)
*Chenhao Dang,Jing Ma*

Main category: cs.CL

TL;DR: MC^2F是一个解决文本分类中对抗攻击鲁棒性问题的两模块系统，通过建模干净数据在编码器嵌入流形中的分布，既能提升对抗鲁棒性又能保持干净数据的性能。


<details>
  <summary>Details</summary>
Motivation: 文本分类中增强模型对抗攻击鲁棒性通常会降低在干净数据上的性能，这是一个持续存在的挑战。作者认为通过建模干净样本在编码器嵌入流形中的分布可以解决这个问题。

Method: 提出Manifold-Correcting Causal Flow (MC^2F)系统，包含两个模块：分层黎曼连续归一化流(SR-CNF)学习干净数据流形的密度，识别出分布外的嵌入；测地纯化解算器通过最短路径将对抗点投影回学习到的流形上，恢复干净、语义连贯的表示。

Result: 在三个数据集和多种对抗攻击上的广泛评估表明，MC^2F不仅在对抗鲁棒性上建立了新的最先进水平，而且完全保留了在干净数据上的性能，甚至带来了准确率的适度提升。

Conclusion: MC^2F方法成功解决了文本分类中对抗鲁棒性与干净数据性能之间的权衡问题，通过流形建模和校正实现了两方面的优化。

Abstract: A persistent challenge in text classification (TC) is that enhancing model robustness against adversarial attacks typically degrades performance on clean data. We argue that this challenge can be resolved by modeling the distribution of clean samples in the encoder embedding manifold. To this end, we propose the Manifold-Correcting Causal Flow (MC^2F), a two-module system that operates directly on sentence embeddings. A Stratified Riemannian Continuous Normalizing Flow (SR-CNF) learns the density of the clean data manifold. It identifies out-of-distribution embeddings, which are then corrected by a Geodesic Purification Solver. This solver projects adversarial points back onto the learned manifold via the shortest path, restoring a clean, semantically coherent representation. We conducted extensive evaluations on text classification (TC) across three datasets and multiple adversarial attacks. The results demonstrate that our method, MC^2F, not only establishes a new state-of-the-art in adversarial robustness but also fully preserves performance on clean data, even yielding modest gains in accuracy.

</details>


### [16] [Last Layer Logits to Logic: Empowering LLMs with Logic-Consistent Structured Knowledge Reasoning](https://arxiv.org/abs/2511.07910)
*Songze Li,Zhiqiang Liu,Zhaoyan Gong,Xiaoke Guo,Zhengke Gui,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: 提出了Logits-to-Logic框架，通过logits增强和过滤模块来纠正LLM在结构化知识推理中的逻辑缺陷，显著提升了逻辑一致性并在KGQA基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: LLM在非结构化文本预训练中表现出色，但在结构化知识推理任务中由于表示差异导致逻辑漂移问题，现有方法仅提供输入级指导无法从根本上解决输出中的逻辑不一致性。

Method: 提出Logits-to-Logic框架，针对自回归生成过程中的logits输出，包含logits增强和logits过滤两个核心模块来纠正逻辑缺陷。

Result: 大量实验表明该方法显著提升了LLM在结构化知识推理中的逻辑一致性，在多个KGQA基准测试中达到了最先进的性能。

Conclusion: Logits-to-Logic框架通过直接操作LLM的logits输出，有效解决了结构化知识推理中的逻辑漂移问题，为提升LLM逻辑一致性提供了新思路。

Abstract: Large Language Models (LLMs) achieve excellent performance in natural language reasoning tasks through pre-training on vast unstructured text, enabling them to understand the logic in natural language and generate logic-consistent responses. However, the representational differences between unstructured and structured knowledge make LLMs inherently struggle to maintain logic consistency, leading to \textit{Logic Drift} challenges in structured knowledge reasoning tasks such as Knowledge Graph Question Answering (KGQA). Existing methods address this limitation by designing complex workflows embedded in prompts to guide LLM reasoning. Nevertheless, these approaches only provide input-level guidance and fail to fundamentally address the \textit{Logic Drift} in LLM outputs. Additionally, their inflexible reasoning workflows cannot adapt to different tasks and knowledge graphs. To enhance LLMs' logic consistency in structured knowledge reasoning, we specifically target the logits output from the autoregressive generation process. We propose the \textit{Logits-to-Logic} framework, which incorporates logits strengthening and logits filtering as core modules to correct logical defects in LLM outputs. Extensive experiments show that our approach significantly improves LLMs' logic consistency in structured knowledge reasoning and achieves state-of-the-art performance on multiple KGQA benchmarks.

</details>


### [17] [Social Media for Mental Health: Data, Methods, and Findings](https://arxiv.org/abs/2511.07914)
*Nur Shazwani Kamarudin,Ghazaleh Beigi,Lydia Manikonda,Huan Liu*

Main category: cs.CL

TL;DR: 本章综述了利用社交媒体数据研究心理健康问题的前沿方法，重点分析了语言、视觉和情感指标，探讨了如何通过机器学习、自然语言处理等技术改善医疗实践和政策制定。


<details>
  <summary>Details</summary>
Motivation: 随着虚拟社区和论坛的普及，人们可以在不暴露身份的情况下自由交流，这为研究高度污名化的心理健康问题（如抑郁、焦虑、自杀念头）提供了新的数据来源。

Method: 采用文献综述方法，分析社交媒体数据中的语言、视觉和情感指标，运用机器学习、特征工程、自然语言处理等技术进行分类和研究。

Result: 研究表明社交媒体数据可以用于识别心理健康问题，为及时干预和支持提供可能，并能够影响政府和政策制定者。

Conclusion: 社交媒体数据为心理健康研究开辟了新途径，未来研究应继续探索如何更好地利用这些数据改善医疗实践和公共政策。

Abstract: There is an increasing number of virtual communities and forums available on the web. With social media, people can freely communicate and share their thoughts, ask personal questions, and seek peer-support, especially those with conditions that are highly stigmatized, without revealing personal identity. We study the state-of-the-art research methodologies and findings on mental health challenges like de- pression, anxiety, suicidal thoughts, from the pervasive use of social media data. We also discuss how these novel thinking and approaches can help to raise awareness of mental health issues in an unprecedented way. Specifically, this chapter describes linguistic, visual, and emotional indicators expressed in user disclosures. The main goal of this chapter is to show how this new source of data can be tapped to improve medical practice, provide timely support, and influence government or policymakers. In the context of social media for mental health issues, this chapter categorizes social media data used, introduces different deployed machine learning, feature engineering, natural language processing, and surveys methods and outlines directions for future research.

</details>


### [18] [Distinct Theta Synchrony across Speech Modes: Perceived, Spoken, Whispered, and Imagined](https://arxiv.org/abs/2511.07918)
*Jung-Sun Lee,Ha-Na Jo,Eunyeong Ko*

Main category: cs.CL

TL;DR: 本研究比较了不同语音模式（感知、外显、耳语和想象）中theta波段神经同步性的差异，发现外显和耳语语音在前颞叶区域表现出更广泛和更强的同步性，感知语音主要涉及后部和颞叶同步，而想象语音则显示出更局限但内部一致的额叶和辅助运动区域同步模式。


<details>
  <summary>Details</summary>
Motivation: 人类语音产生包含多种模式，每种模式反映不同的神经机制。theta波段同步性与语言处理、注意力控制和内部语音密切相关，但以往研究主要关注单一模式，缺乏对不同语音模式theta同步性的综合比较。

Method: 基于连接性指标分析不同语音模式中theta波段神经同步性的差异，重点关注区域间的变化。

Result: 外显和耳语语音表现出更广泛和更强的前颞叶同步性，反映主动运动-语音耦合；感知语音显示主导的后部和颞叶同步模式，与听觉感知和理解过程一致；想象语音呈现更空间局限但内部一致的同步模式，主要涉及额叶和辅助运动区域。

Conclusion: theta同步性的范围和空间分布在各种模式中存在显著差异，外显发音涉及广泛的皮层相互作用，耳语语音显示中等程度的参与，而感知主要依赖于颞顶网络。该研究揭示了语言感知和想象语音背后共享和不同的神经动力学。

Abstract: Human speech production encompasses multiple modes such as perceived, overt, whispered, and imagined, each reflecting distinct neural mechanisms. Among these, theta-band synchrony has been closely associated with language processing, attentional control, and inner speech. However, previous studies have largely focused on a single mode, such as overt speech, and have rarely conducted an integrated comparison of theta synchrony across different speech modes. In this study, we analyzed differences in theta-band neural synchrony across speech modes based on connectivity metrics, focusing on region-wise variations. The results revealed that overt and whispered speech exhibited broader and stronger frontotemporal synchrony, reflecting active motor-phonological coupling during overt articulation, whereas perceived speech showed dominant posterior and temporal synchrony patterns, consistent with auditory perception and comprehension processes. In contrast, imagined speech demonstrated a more spatially confined but internally coherent synchronization pattern, primarily involving frontal and supplementary motor regions. These findings indicate that the extent and spatial distribution of theta synchrony differ substantially across modes, with overt articulation engaging widespread cortical interactions, whispered speech showing intermediate engagement, and perception relying predominantly on temporoparietal networks. Therefore, this study aims to elucidate the differences in theta-band neural synchrony across various speech modes, thereby uncovering both the shared and distinct neural dynamics underlying language perception and imagined speech.

</details>


### [19] [Unified Work Embeddings: Contrastive Learning of a Bidirectional Multi-task Ranker](https://arxiv.org/abs/2511.07969)
*Matthias De Lange,Jens-Joris Decorte,Jeroen Van Hautte*

Main category: cs.CL

TL;DR: 论文提出了WorkBench评估套件和UWE模型，用于解决工作领域自然语言处理任务中的长尾分布、多标签和稀缺数据问题，通过统一框架实现零样本排序性能提升。


<details>
  <summary>Details</summary>
Motivation: 工作领域的自然语言处理任务面临真实世界复杂性，包括长尾分布、极端多标签和稀缺数据，而通用嵌入模型在该领域的表现尚不明确。

Method: 构建WorkBench评估套件包含6个工作相关任务，基于此开发UWE模型，使用双编码器架构和多对多InfoNCE目标，结合任务无关的软延迟交互。

Result: UWE在工作领域未见目标空间上实现零样本排序，支持低延迟推理，在宏观平均MAP和RP@10指标上显著优于通用嵌入模型。

Conclusion: WorkBench为多任务进展提供了统一评估基础，UWE通过利用训练数据结构实现了工作领域嵌入的有效泛化，展示了跨任务转移的潜力。

Abstract: Workforce transformation across diverse industries has driven an increased demand for specialized natural language processing capabilities. Nevertheless, tasks derived from work-related contexts inherently reflect real-world complexities, characterized by long-tailed distributions, extreme multi-label target spaces, and scarce data availability. The rise of generalist embedding models prompts the question of their performance in the work domain, especially as progress in the field has focused mainly on individual tasks. To this end, we introduce WorkBench, the first unified evaluation suite spanning six work-related tasks formulated explicitly as ranking problems, establishing a common ground for multi-task progress. Based on this benchmark, we find significant positive cross-task transfer, and use this insight to compose task-specific bipartite graphs from real-world data, synthetically enriched through grounding. This leads to Unified Work Embeddings (UWE), a task-agnostic bi-encoder that exploits our training-data structure with a many-to-many InfoNCE objective, and leverages token-level embeddings with task-agnostic soft late interaction. UWE demonstrates zero-shot ranking performance on unseen target spaces in the work domain, enables low-latency inference by caching the task target space embeddings, and shows significant gains in macro-averaged MAP and RP@10 over generalist embedding models.

</details>


### [20] [NOTAM-Evolve: A Knowledge-Guided Self-Evolving Optimization Framework with LLMs for NOTAM Interpretation](https://arxiv.org/abs/2511.07982)
*Maoqi Liu,Quan Fang,Yuhao Wu,Can Zhao,Yang Yang,Kaiquan Cai*

Main category: cs.CL

TL;DR: NOTAM-Evolve是一个自演进框架，通过结合知识图谱增强检索和闭环学习，使大语言模型能够自主掌握复杂的NOTAM（航行通告）解析任务，在结构化NOTAM解析上实现了30.4%的绝对准确率提升。


<details>
  <summary>Details</summary>
Motivation: NOTAMs的压缩和加密语言对人工和自动化处理都构成重大挑战，现有自动化系统通常仅限于浅层解析，无法提取操作决策所需的可操作情报。

Method: 提出NOTAM-Evolve自演进框架，结合知识图谱增强检索模块进行数据接地，并采用闭环学习过程，使LLM能够从自身输出中逐步改进，减少对大量人工标注推理轨迹的需求。

Result: 实验表明NOTAM-Evolve相比基础LLM实现了30.4%的绝对准确率提升，在结构化NOTAM解析任务上建立了新的最先进水平。

Conclusion: NOTAM-Evolve框架成功解决了NOTAM深度解析的双重推理挑战，通过自演进学习显著提升了自动化解析的准确性和实用性。

Abstract: Accurate interpretation of Notices to Airmen (NOTAMs) is critical for aviation safety, yet their condensed and cryptic language poses significant challenges to both manual and automated processing. Existing automated systems are typically limited to shallow parsing, failing to extract the actionable intelligence needed for operational decisions. We formalize the complete interpretation task as deep parsing, a dual-reasoning challenge requiring both dynamic knowledge grounding (linking the NOTAM to evolving real-world aeronautical data) and schema-based inference (applying static domain rules to deduce operational status). To tackle this challenge, we propose NOTAM-Evolve, a self-evolving framework that enables a large language model (LLM) to autonomously master complex NOTAM interpretation. Leveraging a knowledge graph-enhanced retrieval module for data grounding, the framework introduces a closed-loop learning process where the LLM progressively improves from its own outputs, minimizing the need for extensive human-annotated reasoning traces. In conjunction with this framework, we introduce a new benchmark dataset of 10,000 expert-annotated NOTAMs. Our experiments demonstrate that NOTAM-Evolve achieves a 30.4% absolute accuracy improvement over the base LLM, establishing a new state of the art on the task of structured NOTAM interpretation.

</details>


### [21] [State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?](https://arxiv.org/abs/2511.07989)
*Taja Kuzman Pungeršek,Peter Rupnik,Ivan Porupski,Vuk Dinić,Nikola Ljubešić*

Main category: cs.CL

TL;DR: LLMs在零样本设置下在南斯拉夫语言文本分类任务中表现优异，甚至超越微调的BERT类模型，但由于输出不可预测、推理速度慢和计算成本高等问题，微调BERT模型在大规模自动文本标注中仍更实用。


<details>
  <summary>Details</summary>
Motivation: 随着指令调优的解码器模型（LLMs）的兴起，文本分类领域逐渐转向零样本和少样本提示，但LLMs在文本分类特别是资源较少语言上的性能尚未充分探索。

Method: 评估当前语言模型在多个南斯拉夫语言文本分类任务上的表现，比较开源和闭源LLMs与微调BERT类模型在三个任务（议会演讲情感分类、新闻和议会演讲主题分类、网络文本体裁识别）上的性能。

Result: LLMs在零样本设置下表现出色，通常达到或超越微调BERT类模型的性能，且在南斯拉夫语言和英语中表现相当。

Conclusion: 尽管LLMs在零样本文本分类中表现优异，但由于输出不可预测性、推理速度慢和计算成本高等限制，微调BERT类模型在大规模自动文本标注中仍是更实用的选择。

Abstract: Until recently, fine-tuned BERT-like models provided state-of-the-art performance on text classification tasks. With the rise of instruction-tuned decoder-only models, commonly known as large language models (LLMs), the field has increasingly moved toward zero-shot and few-shot prompting. However, the performance of LLMs on text classification, particularly on less-resourced languages, remains under-explored. In this paper, we evaluate the performance of current language models on text classification tasks across several South Slavic languages. We compare openly available fine-tuned BERT-like models with a selection of open-source and closed-source LLMs across three tasks in three domains: sentiment classification in parliamentary speeches, topic classification in news articles and parliamentary speeches, and genre identification in web texts. Our results show that LLMs demonstrate strong zero-shot performance, often matching or surpassing fine-tuned BERT-like models. Moreover, when used in a zero-shot setup, LLMs perform comparably in South Slavic languages and English. However, we also point out key drawbacks of LLMs, including less predictable outputs, significantly slower inference, and higher computational costs. Due to these limitations, fine-tuned BERT-like models remain a more practical choice for large-scale automatic text annotation.

</details>


### [22] [Self-Correction Distillation for Structured Data Question Answering](https://arxiv.org/abs/2511.07998)
*Yushan Zhu,Wen Zhang,Long Jin,Mengshu Sun,Ling Zhong,Zhiqiang Liu,Juan Li,Lei Liang,Chong Long,Chao Deng,Junlan Feng*

Main category: cs.CL

TL;DR: 提出自校正蒸馏方法，通过错误提示机制和两阶段蒸馏策略，将大语言模型的查询生成和错误校正能力迁移到小语言模型，提升小模型在结构化数据问答中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有统一结构化问答框架在应用于小规模语言模型时面临挑战，因为小模型容易在生成结构化查询时出错，需要提升小模型的结构化数据问答能力。

Method: 自校正蒸馏方法，包含错误提示机制检测错误并提供定制化错误信息，以及两阶段蒸馏策略将大模型的查询生成和错误校正能力迁移到小模型。

Result: 在5个基准测试和3种结构化数据类型上的实验表明，该方法在小模型上取得最佳性能和优越泛化能力，接近GPT4在某些数据集上的表现，且配备错误提示机制的大模型在多数数据集上超越最先进结果。

Conclusion: 自校正蒸馏方法有效提升了小语言模型在结构化数据问答中的能力，同时错误提示机制也增强了大模型的性能表现。

Abstract: Structured data question answering (QA), including table QA, Knowledge Graph (KG) QA, and temporal KG QA, is a pivotal research area. Advances in large language models (LLMs) have driven significant progress in unified structural QA frameworks like TrustUQA. However, these frameworks face challenges when applied to small-scale LLMs since small-scale LLMs are prone to errors in generating structured queries. To improve the structured data QA ability of small-scale LLMs, we propose a self-correction distillation (SCD) method. In SCD, an error prompt mechanism (EPM) is designed to detect errors and provide customized error messages during inference, and a two-stage distillation strategy is designed to transfer large-scale LLMs' query-generation and error-correction capabilities to small-scale LLM. Experiments across 5 benchmarks with 3 structured data types demonstrate that our SCD achieves the best performance and superior generalization on small-scale LLM (8B) compared to other distillation methods, and closely approaches the performance of GPT4 on some datasets. Furthermore, large-scale LLMs equipped with EPM surpass the state-of-the-art results on most datasets.

</details>


### [23] [HyCoRA: Hyper-Contrastive Role-Adaptive Learning for Role-Playing](https://arxiv.org/abs/2511.08017)
*Shihao Yang,Zhicong Lu,Yong Yang,Bo Lv,Yang Shen,Nayu Liu*

Main category: cs.CL

TL;DR: HyCoRA是一个超对比角色自适应学习框架，通过平衡学习角色的独特和共享特征来提升多角色扮演能力，采用超半低秩适应结构和超对比学习机制。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么使用共享参数模块（忽略角色独特性），要么为每个角色分配独立模块（忽视角色间共性），无法平衡独特性和共享特征的学习。

Method: 提出超半低秩适应结构：一半是由轻量级超网络生成的角色特定模块，另一半是可训练的角色共享模块；并设计超对比学习机制帮助超网络区分角色独特特征。

Result: 在英文和中文基准测试上的广泛实验结果表明该框架的优越性，GPT-4评估和可视化分析验证了HyCoRA捕捉角色特征的能力。

Conclusion: HyCoRA框架有效平衡了角色独特性和共享特征的学习，显著提升了多角色扮演能力，并通过实验验证了其优越性。

Abstract: Multi-character role-playing aims to equip models with the capability to simulate diverse roles. Existing methods either use one shared parameterized module across all roles or assign a separate parameterized module to each role. However, the role-shared module may ignore distinct traits of each role, weakening personality learning, while the role-specific module may overlook shared traits across multiple roles, hindering commonality modeling. In this paper, we propose a novel HyCoRA: Hyper-Contrastive Role-Adaptive learning framework, which efficiently improves multi-character role-playing ability by balancing the learning of distinct and shared traits. Specifically, we propose a Hyper-Half Low-Rank Adaptation structure, where one half is a role-specific module generated by a lightweight hyper-network, and the other half is a trainable role-shared module. The role-specific module is devised to represent distinct persona signatures, while the role-shared module serves to capture common traits. Moreover, to better reflect distinct personalities across different roles, we design a hyper-contrastive learning mechanism to help the hyper-network distinguish their unique characteristics. Extensive experimental results on both English and Chinese available benchmarks demonstrate the superiority of our framework. Further GPT-4 evaluations and visual analyses also verify the capability of HyCoRA to capture role characteristics.

</details>


### [24] [BARD10: A New Benchmark Reveals Significance of Bangla Stop-Words in Authorship Attribution](https://arxiv.org/abs/2511.08085)
*Abdullah Muhammad Moosa,Nusrat Sultana,Mahdi Muhammad Moosa,Md. Miraiz Hossain*

Main category: cs.CL

TL;DR: 本研究介绍了新的孟加拉语作者识别数据集BARD10，并系统分析了停用词去除对古典和深度学习模型的影响，发现孟加拉语停用词是重要的风格指标，TF-IDF + SVM模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏平衡的孟加拉语作者识别基准数据集，且需要研究停用词在孟加拉语作者识别中的风格意义。

Method: 构建BARD10数据集（10位当代作者），使用四种分类器（SVM、Bangla BERT、XGBoost、MLP）在BARD10和BAAD16数据集上进行统一预处理和评估。

Result: TF-IDF + SVM在BAAD16上获得0.997宏F1分数，在BARD10上获得0.921，优于Bangla BERT；BARD10作者对停用词去除高度敏感，而BAAD16作者相对稳健。

Conclusion: 孟加拉语停用词是重要的风格指标；精细调校的机器学习模型在短文本限制下有效；BARD10连接了正式文学与当代网络对话，为未来长上下文或领域适应transformer提供了可复现基准。

Abstract: This research presents a comprehensive investigation into Bangla authorship attribution, introducing a new balanced benchmark corpus BARD10 (Bangla Authorship Recognition Dataset of 10 authors) and systematically analyzing the impact of stop-word removal across classical and deep learning models to uncover the stylistic significance of Bangla stop-words. BARD10 is a curated corpus of Bangla blog and opinion prose from ten contemporary authors, alongside the methodical assessment of four representative classifiers: SVM (Support Vector Machine), Bangla BERT (Bidirectional Encoder Representations from Transformers), XGBoost, and a MLP (Multilayer Perception), utilizing uniform preprocessing on both BARD10 and the benchmark corpora BAAD16 (Bangla Authorship Attribution Dataset of 16 authors). In all datasets, the classical TF-IDF + SVM baseline outperformed, attaining a macro-F1 score of 0.997 on BAAD16 and 0.921 on BARD10, while Bangla BERT lagged by as much as five points. This study reveals that BARD10 authors are highly sensitive to stop-word pruning, while BAAD16 authors remain comparatively robust highlighting genre-dependent reliance on stop-word signatures. Error analysis revealed that high frequency components transmit authorial signatures that are diminished or reduced by transformer models. Three insights are identified: Bangla stop-words serve as essential stylistic indicators; finely calibrated ML models prove effective within short-text limitations; and BARD10 connects formal literature with contemporary web dialogue, offering a reproducible benchmark for future long-context or domain-adapted transformers.

</details>


### [25] [Estranged Predictions: Measuring Semantic Category Disruption with Masked Language Modelling](https://arxiv.org/abs/2511.08109)
*Yuxuan Liu,Haim Dubossarsky,Ruth Ahnert*

Main category: cs.CL

TL;DR: 本研究通过掩码语言建模测量科幻小说中人类、动物和机器概念之间的渗透性，发现科幻小说比普通小说具有更高的概念边界模糊性，特别是机器概念表现出显著的跨类别替换和分散。


<details>
  <summary>Details</summary>
Motivation: 基于Darko Suvin的陌生化理论，研究旨在通过计算语言学方法量化科幻小说如何通过语言扰动来解构本体论范畴，探索科幻文学特有的概念边界模糊现象。

Method: 使用RoBERTa掩码语言模型生成词汇替代项，通过Gemini进行分类，采用保留率、替换率和熵三个指标来量化概念渗透性，对比科幻小说语料库(Gollancz SF Masterworks)和普通小说语料库(NovelTM)。

Result: 科幻小说显示出更高的概念渗透性，特别是机器概念出现显著的跨类别替换和分散，而人类概念保持语义连贯性并通常锚定替换层次结构，表明科幻小说在人类中心逻辑内进行了类型特定的重构。

Conclusion: 科幻小说中的陌生化可被视为语义规范的受控扰动，可通过概率建模检测；掩码语言模型在批判性使用时可作为解释性工具，揭示类型条件下的本体论假设，为计算文学研究提供了方法论贡献。

Abstract: This paper examines how science fiction destabilises ontological categories by measuring conceptual permeability across the terms human, animal, and machine using masked language modelling (MLM). Drawing on corpora of science fiction (Gollancz SF Masterworks) and general fiction (NovelTM), we operationalise Darko Suvin's theory of estrangement as computationally measurable deviation in token prediction, using RoBERTa to generate lexical substitutes for masked referents and classifying them via Gemini. We quantify conceptual slippage through three metrics: retention rate, replacement rate, and entropy, mapping the stability or disruption of category boundaries across genres. Our findings reveal that science fiction exhibits heightened conceptual permeability, particularly around machine referents, which show significant cross-category substitution and dispersion. Human terms, by contrast, maintain semantic coherence and often anchor substitutional hierarchies. These patterns suggest a genre-specific restructuring within anthropocentric logics. We argue that estrangement in science fiction operates as a controlled perturbation of semantic norms, detectable through probabilistic modelling, and that MLMs, when used critically, serve as interpretive instruments capable of surfacing genre-conditioned ontological assumptions. This study contributes to the methodological repertoire of computational literary studies and offers new insights into the linguistic infrastructure of science fiction.

</details>


### [26] [Multimodal LLMs Do Not Compose Skills Optimally Across Modalities](https://arxiv.org/abs/2511.08113)
*Paula Ontalvilla,Aitor Ormazabal,Gorka Azkune*

Main category: cs.CL

TL;DR: MLLMs在跨模态技能组合方面存在显著差距，即使通过思维链提示和微调策略改进，仍然无法有效组合已学技能解决新任务。


<details>
  <summary>Details</summary>
Motivation: 研究多模态大语言模型（MLLM）在跨模态技能组合方面的能力，即能否将已学技能组合起来解决新任务。

Method: 设计了三个评估任务，通过两种设置评估MLLMs：1）直接提示模型解决任务；2）使用两步级联推理强制技能组合。同时探索了思维链提示和特定微调策略来缓解技能组合差距。

Result: 所有评估的MLLMs都表现出显著的跨模态技能组合差距。思维链提示和微调策略虽然能提高性能，但仍存在明显的技能组合差距。

Conclusion: 需要更多研究来改进MLLMs中的跨模态技能组合能力。

Abstract: Skill composition is the ability to combine previously learned skills to solve new tasks. As neural networks acquire increasingly complex skills during their pretraining, it is not clear how successfully they can compose them. In this paper, we focus on Multimodal Large Language Models (MLLM), and study their ability to compose skills across modalities. To this end, we design three evaluation tasks which can be solved sequentially composing two modality-dependent skills, and evaluate several open MLLMs under two main settings: i) prompting the model to directly solve the task, and ii) using a two-step cascaded inference approach, which manually enforces the composition of the two skills for a given task. Even with these straightforward compositions, we find that all evaluated MLLMs exhibit a significant cross-modality skill composition gap. To mitigate the aforementioned gap, we explore two alternatives: i) use chain-of-thought prompting to explicitly instruct MLLMs for skill composition and ii) a specific fine-tuning recipe to promote skill composition. Although those strategies improve model performance, they still exhibit significant skill composition gaps, suggesting that more research is needed to improve cross-modal skill composition in MLLMs.

</details>


### [27] [Quantification and object perception in Multimodal Large Language Models deviate from human linguistic cognition](https://arxiv.org/abs/2511.08126)
*Raquel Montero,Natalia Moskvina,Paolo Morosi,Tamara Serrano,Elena Pagliarini,Evelina Leivada*

Main category: cs.CL

TL;DR: 本文研究了多模态大语言模型在量化表达方面的表现与人类的差异，重点关注量词排序、使用范围和原型性、以及人类近似数字系统偏差这三个跨语言特征。


<details>
  <summary>Details</summary>
Motivation: 量化表达是(M)多模态大语言模型特别困难的语言现象，但其性能差的确切原因尚不清楚，因为量化涉及逻辑、语用和数值领域。

Method: 通过分析三个跨语言的人类量化特征（量词排序、使用范围和原型性、人类近似数字系统偏差），研究这些特征在模型架构中的编码方式，以及与人类的差异。

Result: 发现人类与MLLMs在这些特征上存在明显差异，这些差异体现在各种任务中，反映了量化表达的体内与硅中表示的不同。

Conclusion: 这项工作为探讨MLLMs作为语义和语用代理的本质铺平了道路，跨语言视角可以阐明它们在不同语言中的能力是否稳健和稳定。

Abstract: Quantification has been proven to be a particularly difficult linguistic phenomenon for (Multimodal) Large Language Models (MLLMs). However, given that quantification interfaces with the logic, pragmatic, and numerical domains, the exact reasons for the poor performance are still unclear. This papers looks at three key features of human quantification shared cross-linguistically that have remained so far unexplored in the (M)LLM literature: the ordering of quantifiers into scales, the ranges of use and prototypicality, and the biases inherent in the human approximate number system. The aim is to determine how these features are encoded in the models' architecture, how they may differ from humans, and whether the results are affected by the type of model and language under investigation. We find that there are clear differences between humans and MLLMs with respect to these features across various tasks that tap into the representation of quantification in vivo vs. in silico. This work, thus, paves the way for addressing the nature of MLLMs as semantic and pragmatic agents, while the cross-linguistic lens can elucidate whether their abilities are robust and stable across different languages.

</details>


### [28] [Sentence-Anchored Gist Compression for Long-Context LLMs](https://arxiv.org/abs/2511.08128)
*Dmitrii Tarasov,Elizaveta Goncharova,Kuznetsov Andrey*

Main category: cs.CL

TL;DR: 本文研究了使用学习压缩令牌对大型语言模型进行上下文压缩的方法，可以在不显著降低性能的情况下实现2倍到8倍的压缩比。


<details>
  <summary>Details</summary>
Motivation: 减少处理长序列时的内存和计算需求，提高大型语言模型的效率。

Method: 通过微调预训练的大型语言模型，使其学习压缩上下文信息，使用压缩令牌来替代原始长序列。

Result: 在30亿参数的LLaMA模型实验中，该方法实现了与替代压缩技术相当的结果，同时获得了更高的压缩比。

Conclusion: 学习压缩令牌是一种有效的上下文压缩方法，能够在保持性能的同时显著减少计算和内存开销。

Abstract: This work investigates context compression for Large Language Models (LLMs) using learned compression tokens to reduce the memory and computational demands of processing long sequences. We demonstrate that pre-trained LLMs can be fine-tuned to compress their context by factors of 2x to 8x without significant performance degradation, as evaluated on both short-context and long-context benchmarks. Furthermore, in experiments on a 3-billion-parameter LLaMA model, our method achieves results on par with alternative compression techniques while attaining higher compression ratios.

</details>


### [29] [On the Interplay between Positional Encodings, Morphological Complexity, and Word Order Flexibility](https://arxiv.org/abs/2511.08139)
*Kushal Tatariya,Wessel Poelman,Miryam de Lhoneux*

Main category: cs.CL

TL;DR: 研究探讨了语言模型架构中的位置编码选择是否对与英语结构不同的语言性能有影响，通过分析形态复杂性与词序灵活性之间的权衡假设，在7种类型学多样语言上测试了绝对、相对和无位置编码的模型变体，发现位置编码与形态复杂性或词序灵活性之间没有明确的交互作用。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型架构主要基于英语设计，然后应用于其他语言，这可能导致对与英语结构不同的语言性能下降。研究旨在验证位置编码这一具体架构选择是否受到形态复杂性与词序灵活性权衡假设的影响。

Method: 在7种类型学多样的语言上预训练了具有绝对、相对和无位置编码的三种单语模型变体，并在四个下游任务上评估它们的性能。

Result: 与先前研究结果相反，没有观察到位置编码与形态复杂性或词序灵活性（通过各种代理指标衡量）之间存在明确的交互作用。

Conclusion: 研究结果表明，任务选择、语言选择和指标选择对于得出稳定结论至关重要，位置编码对语言建模的影响可能比先前认为的更复杂。

Abstract: Language model architectures are predominantly first created for English and subsequently applied to other languages. It is an open question whether this architectural bias leads to degraded performance for languages that are structurally different from English. We examine one specific architectural choice: positional encodings, through the lens of the trade-off hypothesis: the supposed interplay between morphological complexity and word order flexibility. This hypothesis posits a trade-off between the two: a more morphologically complex language can have a more flexible word order, and vice-versa. Positional encodings are a direct target to investigate the implications of this hypothesis in relation to language modelling. We pretrain monolingual model variants with absolute, relative, and no positional encodings for seven typologically diverse languages and evaluate them on four downstream tasks. Contrary to previous findings, we do not observe a clear interaction between position encodings and morphological complexity or word order flexibility, as measured by various proxies. Our results show that the choice of tasks, languages, and metrics are essential for drawing stable conclusions

</details>


### [30] [Relation as a Prior: A Novel Paradigm for LLM-based Document-level Relation Extraction](https://arxiv.org/abs/2511.08143)
*Qiankun Pi,Yepeng Sun,Jicang Lu,Qinlong Fan,Ningbo Huang,Shiyu Wang*

Main category: cs.CL

TL;DR: 提出RelPrior范式，通过将关系作为先验来改进基于LLM的文档级关系抽取，解决无关实体对噪声和预定义关系标签限制的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在文档级关系抽取中存在性能差距，主要由于'先抽取实体再预测关系'范式导致的两个问题：(1)大量无关实体对引入噪声干扰；(2)超出预定义集合的关系标签被误判为错误。

Method: RelPrior范式：利用二元关系作为先验来提取和确定实体是否相关，过滤无关实体对；利用预定义关系作为先验来匹配实体进行三元组抽取，避免直接预测关系。

Result: 在两个基准测试上的广泛实验表明，RelPrior实现了最先进的性能，超越了现有的基于LLM的方法。

Conclusion: RelPrior通过将关系作为先验的新范式，有效解决了LLM在文档级关系抽取中的关键挑战，显著提升了性能。

Abstract: Large Language Models (LLMs) have demonstrated their remarkable capabilities in document understanding. However, recent research reveals that LLMs still exhibit performance gaps in Document-level Relation Extraction (DocRE) as requiring fine-grained comprehension. The commonly adopted "extract entities then predict relations" paradigm in LLM-based methods leads to these gaps due to two main reasons: (1) Numerous unrelated entity pairs introduce noise and interfere with the relation prediction for truly related entity pairs. (2) Although LLMs have identified semantic associations between entities, relation labels beyond the predefined set are still treated as prediction errors. To address these challenges, we propose a novel Relation as a Prior (RelPrior) paradigm for LLM-based DocRE. For challenge (1), RelPrior utilizes binary relation as a prior to extract and determine whether two entities are correlated, thereby filtering out irrelevant entity pairs and reducing prediction noise. For challenge (2), RelPrior utilizes predefined relation as a prior to match entities for triples extraction instead of directly predicting relation. Thus, it avoids misjudgment caused by strict predefined relation labeling. Extensive experiments on two benchmarks demonstrate that RelPrior achieves state-of-the-art performance, surpassing existing LLM-based methods.

</details>


### [31] [Still Not There: Can LLMs Outperform Smaller Task-Specific Seq2Seq Models on the Poetry-to-Prose Conversion Task?](https://arxiv.org/abs/2511.08145)
*Kunal Kingkar Das,Manoj Balaji Jagadeeshan,Nallani Chakravartula Sahith,Jivnesh Sandhan,Pawan Goyal*

Main category: cs.CL

TL;DR: 本研究比较了指令微调LLM与专门任务模型在梵语诗歌转散文任务上的表现，发现专门微调的ByT5-Sanskrit模型显著优于所有LLM方法，同时提示策略在缺乏领域数据时提供了可行的替代方案。


<details>
  <summary>Details</summary>
Motivation: 验证LLM在低资源、形态丰富的梵语中是否仍能作为通用解决方案，特别是在具有挑战性的诗歌转散文任务上，该任务需要多步推理包括复合词分割、依赖关系解析和句法线性化。

Method: 对LLM应用指令微调和基于梵语语法及古典注释启发式的上下文学习模板，同时完全微调ByT5-Sanskrit序列到序列模型作为任务特定模型。

Result: 领域特定微调的ByT5-Sanskrit模型在所有指标上显著优于指令驱动的LLM方法，人类评估结果与Kendall's Tau分数高度相关。提示策略在缺乏领域数据时提供了可行替代方案。

Conclusion: 在低资源、形态丰富的语言如梵语中，专门任务模型仍优于通用LLM，特别是在需要复杂推理的任务上。提示策略可作为领域数据不可用时的有效替代方案。

Abstract: Large Language Models (LLMs) are increasingly treated as universal, general-purpose solutions across NLP tasks, particularly in English. But does this assumption hold for low-resource, morphologically rich languages such as Sanskrit? We address this question by comparing instruction-tuned and in-context-prompted LLMs with smaller task-specific encoder-decoder models on the Sanskrit poetry-to-prose conversion task. This task is intrinsically challenging: Sanskrit verse exhibits free word order combined with rigid metrical constraints, and its conversion to canonical prose (anvaya) requires multi-step reasoning involving compound segmentation, dependency resolution, and syntactic linearisation. This makes it an ideal testbed to evaluate whether LLMs can surpass specialised models. For LLMs, we apply instruction fine-tuning on general-purpose models and design in-context learning templates grounded in Paninian grammar and classical commentary heuristics. For task-specific modelling, we fully fine-tune a ByT5-Sanskrit Seq2Seq model. Our experiments show that domain-specific fine-tuning of ByT5-Sanskrit significantly outperforms all instruction-driven LLM approaches. Human evaluation strongly corroborates this result, with scores exhibiting high correlation with Kendall's Tau scores. Additionally, our prompting strategies provide an alternative to fine-tuning when domain-specific verse corpora are unavailable, and the task-specific Seq2Seq model demonstrates robust generalisation on out-of-domain evaluations.

</details>


### [32] [Do Syntactic Categories Help in Developmentally Motivated Curriculum Learning for Language Models?](https://arxiv.org/abs/2511.08199)
*Arzu Burcu Güven,Anna Rogers,Rob van der Goot*

Main category: cs.CL

TL;DR: 对BabyLM语料库和CHILDES中不同年龄组的句法特性进行分析，发现句法知识有助于解释模型在语言任务上的表现，某些课程学习方法对阅读任务有帮助，但主要性能提升来自使用可句法分类的数据子集而非完整嘈杂语料库。


<details>
  <summary>Details</summary>
Motivation: 研究儿童语言习得数据集的句法特性，探索如何利用句法知识来理解和改进模型在语言任务上的表现，特别是通过课程学习方法来模拟儿童语言发展过程。

Method: 分析BabyLM语料库和CHILDES数据集的句法特性，探索发展性课程学习和几种替代的认知启发式课程方法，比较使用完整语料库与句法可分类数据子集的性能差异。

Result: CHILDES数据集在年龄组间没有表现出强烈的句法区分，但关于训练数据的句法知识有助于解释模型在语言任务上的表现。某些课程学习方法对阅读任务有帮助，但主要性能提升来自使用句法可分类的数据子集。

Conclusion: 句法知识在解释模型性能方面具有价值，使用经过句法分类的数据子集比使用完整嘈杂语料库能带来更大的性能提升，而课程学习方法对某些特定任务（如阅读）有辅助作用。

Abstract: We examine the syntactic properties of BabyLM corpus, and age-groups within CHILDES. While we find that CHILDES does not exhibit strong syntactic differentiation by age, we show that the syntactic knowledge about the training data can be helpful in interpreting model performance on linguistic tasks. For curriculum learning, we explore developmental and several alternative cognitively inspired curriculum approaches. We find that some curricula help with reading tasks, but the main performance improvement come from using the subset of syntactically categorizable data, rather than the full noisy corpus.

</details>


### [33] [Encoder Fine-tuning with Stochastic Sampling Outperforms Open-weight GPT in Astronomy Knowledge Extraction](https://arxiv.org/abs/2511.08204)
*Shivam Rawat,Lucie Flek,Akbar Karimi*

Main category: cs.CL

TL;DR: 提出了一种基于SciBERT的多任务转换器系统，用于从天文学论文中提取望远镜引用、语义属性和仪器提及等关键信息。


<details>
  <summary>Details</summary>
Motivation: 天文学文献迅速扩张，需要自动化提取研究论文中的关键实体和上下文信息。

Method: 基于SciBERT模型构建多任务转换器系统，针对天文学语料进行微调，使用训练数据的随机采样段进行训练，在推理时对测试段进行多数投票。

Result: 尽管系统简单且实现成本低，但显著优于开源的GPT基线模型。

Conclusion: 该系统为天文学文献的知识提取提供了一种有效且高效的解决方案。

Abstract: Scientific literature in astronomy is rapidly expanding, making it increasingly important to automate the extraction of key entities and contextual information from research papers. In this paper, we present an encoder-based system for extracting knowledge from astronomy articles. Our objective is to develop models capable of classifying telescope references, detecting auxiliary semantic attributes, and recognizing instrument mentions from textual content. To this end, we implement a multi-task transformer-based system built upon the SciBERT model and fine-tuned for astronomy corpora classification. To carry out the fine-tuning, we stochastically sample segments from the training data and use majority voting over the test segments at inference time. Our system, despite its simplicity and low-cost implementation, significantly outperforms the open-weight GPT baseline.

</details>


### [34] [Benchmarking Educational LLMs with Analytics: A Case Study on Gender Bias in Feedback](https://arxiv.org/abs/2511.08225)
*Yishan Du,Conrad Borchers,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 本文提出了一个基于嵌入的基准测试框架，用于检测大型语言模型在形成性反馈中的性别偏见，发现即使是最先进的模型也存在对性别替换的不对称语义响应。


<details>
  <summary>Details</summary>
Motivation: 随着教师在教育实践中越来越多地使用GenAI，需要稳健的方法来为教学目的对大型语言模型进行基准测试，特别是检测其在形成性反馈中的偏见问题。

Method: 使用AES 2.0语料库中的600篇真实学生论文，构建了两种维度的受控反事实：通过词典替换实现隐式性别线索，以及通过提示中的作者背景实现显式性别线索。研究了6个代表性LLM，使用余弦和欧几里得距离量化响应差异，通过置换检验评估显著性，并使用降维可视化结构。

Result: 所有模型中，隐式操作对男性-女性反事实产生的语义偏移比女性-男性更大。只有GPT和Llama模型对显式性别线索敏感。定性分析显示一致的语用差异，如男性线索下更多自主支持性反馈，女性线索下更多控制性反馈。

Conclusion: 即使是先进的LLM也表现出对性别替换的不对称语义响应，表明它们提供的反馈存在持续的性别偏见。研究为教学GenAI的公平性审计提供了启示，并提出了学习分析中反事实评估的报告标准和确保公平反馈的实践指导。

Abstract: As teachers increasingly turn to GenAI in their educational practice, we need robust methods to benchmark large language models (LLMs) for pedagogical purposes. This article presents an embedding-based benchmarking framework to detect bias in LLMs in the context of formative feedback. Using 600 authentic student essays from the AES 2.0 corpus, we constructed controlled counterfactuals along two dimensions: (i) implicit cues via lexicon-based swaps of gendered terms within essays, and (ii) explicit cues via gendered author background in the prompt. We investigated six representative LLMs (i.e. GPT-5 mini, GPT-4o mini, DeepSeek-R1, DeepSeek-R1-Qwen, Gemini 2.5 Pro, Llama-3-8B). We first quantified the response divergence with cosine and Euclidean distances over sentence embeddings, then assessed significance via permutation tests, and finally, visualised structure using dimensionality reduction. In all models, implicit manipulations reliably induced larger semantic shifts for male-female counterfactuals than for female-male. Only the GPT and Llama models showed sensitivity to explicit gender cues. These findings show that even state-of-the-art LLMs exhibit asymmetric semantic responses to gender substitutions, suggesting persistent gender biases in feedback they provide learners. Qualitative analyses further revealed consistent linguistic differences (e.g., more autonomy-supportive feedback under male cues vs. more controlling feedback under female cues). We discuss implications for fairness auditing of pedagogical GenAI, propose reporting standards for counterfactual evaluation in learning analytics, and outline practical guidance for prompt design and deployment to safeguard equitable feedback.

</details>


### [35] [VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context](https://arxiv.org/abs/2511.08230)
*Heyang Liu,Ziyang Cheng,Yuhao Wang,Hongcheng Liu,Yiqi Li,Ronghua Wu,Qunshan Gu,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: 提出了VocalBench-zh，一个面向普通话的语音到语音评估套件，包含10个子集和超过1万个高质量实例，用于系统评估主流模型并揭示当前方法的挑战。


<details>
  <summary>Details</summary>
Motivation: 普通话作为全球使用最广泛的语言之一，虽然大多数模型都支持，但缺乏全面的语音到语音基准测试，阻碍了系统评估和公平模型比较。

Method: 开发了VocalBench-zh评估套件，包含10个精心设计的子集，涵盖12个面向用户的特征，包含超过1万个高质量实例。

Result: 对14个主流模型进行评估实验，揭示了当前方法的共同挑战，并强调了新一代语音交互系统需要的新见解。

Conclusion: VocalBench-zh填补了普通话语音到语音评估的空白，为开发者和用户提供了系统评估工具，推动了语音交互系统的发展。

Abstract: The development of multi-modal large language models (LLMs) leads to intelligent approaches capable of speech interactions. As one of the most widely spoken languages globally, Mandarin is supported by most models to enhance their applicability and reach. However, the scarcity of comprehensive speech-to-speech (S2S) benchmarks in Mandarin contexts impedes systematic evaluation for developers and hinders fair model comparison for users. In this work, we propose VocalBench-zh, an ability-level divided evaluation suite adapted to Mandarin context consisting of 10 well-crafted subsets and over 10K high-quality instances, covering 12 user-oriented characters. The evaluation experiment on 14 mainstream models reveals the common challenges for current routes, and highlights the need for new insights into next-generation speech interactive systems. The evaluation codes and datasets will be available at https://github.com/SJTU-OmniAgent/VocalBench-zh.

</details>


### [36] [Prompt Tuning for Natural Language to SQL with Embedding Fine-Tuning and RAG](https://arxiv.org/abs/2511.08245)
*Jisoo Jang,Tien-Cuong Bui,Yunjun Choi,Wen-Syan Li*

Main category: cs.CL

TL;DR: 提出了一种基于提示调优的错误纠正框架，用于自然语言到SQL的转换，通过诊断错误类型、识别原因并提供修复指令来改进SQL查询准确性。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言接口的广泛应用，需要高效准确地将自然语言查询转换为SQL表达式。现有方法在复杂场景下仍存在准确性不足的问题。

Method: 结合医学诊断过程的灵感，提出集成错误纠正机制的新框架，包括错误类型诊断、原因识别、修复指令生成和SQL修正。同时采用嵌入微调和检索增强生成技术利用外部知识库。

Result: 通过全面实验验证，该框架相比现有基线方法实现了12%的准确率提升。

Conclusion: 该框架在当代数据驱动环境中具有革新数据访问和处理方式的潜力，显著提高了NL-to-SQL转换的准确性。

Abstract: This paper introduces an Error Correction through Prompt Tuning for NL-to-SQL, leveraging the latest advancements in generative pre-training-based LLMs and RAG. Our work addresses the crucial need for efficient and accurate translation of natural language queries into SQL expressions in various settings with the growing use of natural language interfaces. We explore the evolution of NLIDBs from early rule-based systems to advanced neural network-driven approaches. Drawing inspiration from the medical diagnostic process, we propose a novel framework integrating an error correction mechanism that diagnoses error types, identifies their causes, provides fixing instructions, and applies these corrections to SQL queries. This approach is further enriched by embedding fine-tuning and RAG, which harnesses external knowledge bases for improved accuracy and transparency. Through comprehensive experiments, we demonstrate that our framework achieves a significant 12 percent accuracy improvement over existing baselines, highlighting its potential to revolutionize data access and handling in contemporary data-driven environments.

</details>


### [37] [ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech](https://arxiv.org/abs/2511.08247)
*Marios Koniaris,Argyro Tsipi,Panayiotis Tsanakas*

Main category: cs.CL

TL;DR: 提出了ParliaBench基准测试，用于评估议会演讲生成，结合计算指标和LLM评估，测量语言质量、语义连贯性和政治真实性三个维度。


<details>
  <summary>Details</summary>
Motivation: 议会演讲生成对大型语言模型提出了超出标准文本生成任务的特定挑战，需要政治真实性和意识形态一致性，而现有模型缺乏专门训练，评估方法也忽视政治维度。

Method: 构建英国议会演讲数据集，提出评估框架结合计算指标和LLM评估，引入两个新的基于嵌入的指标（政治光谱对齐和党派对齐），微调五个大型语言模型并生成28k演讲。

Result: 微调在大多数指标上产生统计显著改进，新提出的指标在政治维度上表现出强大的区分能力。

Conclusion: ParliaBench基准测试能够有效评估议会演讲生成的政治真实性，微调可显著提升模型性能，新指标为政治维度评估提供了有效工具。

Abstract: Parliamentary speech generation presents specific challenges for large language models beyond standard text generation tasks. Unlike general text generation, parliamentary speeches require not only linguistic quality but also political authenticity and ideological consistency. Current language models lack specialized training for parliamentary contexts, and existing evaluation methods focus on standard NLP metrics rather than political authenticity. To address this, we present ParliaBench, a benchmark for parliamentary speech generation. We constructed a dataset of speeches from UK Parliament to enable systematic model training. We introduce an evaluation framework combining computational metrics with LLM-as-a-judge assessments for measuring generation quality across three dimensions: linguistic quality, semantic coherence, and political authenticity. We propose two novel embedding-based metrics, Political Spectrum Alignment and Party Alignment, to quantify ideological positioning. We fine-tuned five large language models (LLMs), generated 28k speeches, and evaluated them using our framework, comparing baseline and fine-tuned models. Results show that fine-tuning produces statistically significant improvements across the majority of metrics and our novel metrics demonstrate strong discriminative power for political dimensions.

</details>


### [38] [Hierarchical structure understanding in complex tables with VLLMs: a benchmark and experiments](https://arxiv.org/abs/2511.08298)
*Luca Bindini,Simone Giovannini,Simone Marinai,Valeria Nardoni,Kimiya Noor Ali*

Main category: cs.CL

TL;DR: 该研究探索视觉大语言模型理解科学文章中表格结构的能力，特别是推断表格层次结构的能力，使用PubTables-1M数据集中的复杂层次表格作为基准测试。


<details>
  <summary>Details</summary>
Motivation: 研究VLLMs是否能在没有额外处理的情况下理解和解释科学文章中表格的层次结构，填补通用VLLMs在结构化数据理解方面的能力空白。

Method: 使用PubTables-1M数据集提取复杂层次表格作为基准，采用多种提示工程策略测试不同VLLMs，包括现成版本和微调版本，并与人类表现进行比较。

Result: 实验表明，未专门设计用于理解表格结构的通用VLLMs能够完成此任务，揭示了VLLMs处理复杂表格的潜力和局限性。

Conclusion: 这项研究为未来将结构化数据理解集成到通用VLLMs中提供了指导，展示了VLLMs在表格结构理解方面的基本能力。

Abstract: This work investigates the ability of Vision Large Language Models (VLLMs) to understand and interpret the structure of tables in scientific articles. Specifically, we explore whether VLLMs can infer the hierarchical structure of tables without additional processing. As a basis for our experiments we use the PubTables-1M dataset, a large-scale corpus of scientific tables. From this dataset, we extract a subset of tables that we introduce as Complex Hierarchical Tables (CHiTab): a benchmark collection of complex tables containing hierarchical headings. We adopt a series of prompt engineering strategies to probe the models' comprehension capabilities, experimenting with various prompt formats and writing styles. Multiple state-of-the-art open-weights VLLMs are evaluated on the benchmark first using their off-the-shelf versions and then fine-tuning some models on our task. We also measure the performance of humans to solve the task on a small set of tables comparing with performance of the evaluated VLLMs. The experiments support our intuition that generic VLLMs, not explicitly designed for understanding the structure of tables, can perform this task. This study provides insights into the potential and limitations of VLLMs to process complex tables and offers guidance for future work on integrating structured data understanding into general-purpose VLLMs.

</details>


### [39] [Automatic Paper Reviewing with Heterogeneous Graph Reasoning over LLM-Simulated Reviewer-Author Debates](https://arxiv.org/abs/2511.08317)
*Shuaimin Li,Liyang Fan,Yufang Lin,Zeyang Li,Xian Wei,Shiwen Ni,Hamid Alinejad-Rokny,Min Yang*

Main category: cs.CL

TL;DR: ReViewGraph是一个新颖的框架，通过LLM模拟多轮审稿人-作者辩论，构建异质图进行推理，以改进论文评审方法。


<details>
  <summary>Details</summary>
Motivation: 现有论文评审方法依赖浅层特征或LLM，容易出现幻觉、评分偏见和有限推理能力，且无法捕捉审稿人-作者交互中的复杂论证推理和协商动态。

Method: 通过LLM多智能体协作模拟审稿人-作者交流，提取多样化意见关系作为类型化边，构建异质交互图，应用图神经网络进行推理。

Result: 在三个数据集上的广泛实验表明，ReViewGraph优于强基线方法，平均相对改进达15.73%。

Conclusion: 建模详细的审稿人-作者辩论结构具有重要价值，ReViewGraph通过图推理方法能够捕捉细粒度的论证动态，实现更明智的评审决策。

Abstract: Existing paper review methods often rely on superficial manuscript features or directly on large language models (LLMs), which are prone to hallucinations, biased scoring, and limited reasoning capabilities. Moreover, these methods often fail to capture the complex argumentative reasoning and negotiation dynamics inherent in reviewer-author interactions. To address these limitations, we propose ReViewGraph (Reviewer-Author Debates Graph Reasoner), a novel framework that performs heterogeneous graph reasoning over LLM-simulated multi-round reviewer-author debates. In our approach, reviewer-author exchanges are simulated through LLM-based multi-agent collaboration. Diverse opinion relations (e.g., acceptance, rejection, clarification, and compromise) are then explicitly extracted and encoded as typed edges within a heterogeneous interaction graph. By applying graph neural networks to reason over these structured debate graphs, ReViewGraph captures fine-grained argumentative dynamics and enables more informed review decisions. Extensive experiments on three datasets demonstrate that ReViewGraph outperforms strong baselines with an average relative improvement of 15.73%, underscoring the value of modeling detailed reviewer-author debate structures.

</details>


### [40] [Adaptive Multi-Agent Response Refinement in Conversational Systems](https://arxiv.org/abs/2511.08319)
*Soyeong Jeong,Aparna Elangovan,Emine Yilmaz,Oleg Rokhlenko*

Main category: cs.CL

TL;DR: 提出了一种基于多智能体框架的LLM响应优化方法，通过动态协调专门负责事实性、个性化和连贯性的智能体来提升对话质量。


<details>
  <summary>Details</summary>
Motivation: LLMs在对话系统中生成类似人类的响应，但在考虑个性化和特定知识时存在不足，且用户难以检测错误并要求重新生成响应。

Method: 采用多智能体框架，每个智能体专门负责一个关键方面（事实性、个性化、连贯性），并引入动态通信策略自适应选择和协调相关智能体。

Result: 在具有挑战性的对话数据集上验证，该方法在涉及知识或用户个人资料的任务中显著优于相关基线。

Conclusion: 多智能体框架通过动态协调专门化智能体，有效提升了LLM在对话系统中的响应质量，特别是在事实性、个性化和连贯性方面。

Abstract: Large Language Models (LLMs) have demonstrated remarkable success in conversational systems by generating human-like responses. However, they can fall short, especially when required to account for personalization or specific knowledge. In real-life settings, it is impractical to rely on users to detect these errors and request a new response. One way to address this problem is to refine the response before returning it to the user. While existing approaches focus on refining responses within a single LLM, this method struggles to consider diverse aspects needed for effective conversations. In this work, we propose refining responses through a multi-agent framework, where each agent is assigned a specific role for each aspect. We focus on three key aspects crucial to conversational quality: factuality, personalization, and coherence. Each agent is responsible for reviewing and refining one of these aspects, and their feedback is then merged to improve the overall response. To enhance collaboration among them, we introduce a dynamic communication strategy. Instead of following a fixed sequence of agents, our approach adaptively selects and coordinates the most relevant agents based on the specific requirements of each query. We validate our framework on challenging conversational datasets, demonstrating that ours significantly outperforms relevant baselines, particularly in tasks involving knowledge or user's persona, or both.

</details>


### [41] [AgentPRM: Process Reward Models for LLM Agents via Step-Wise Promise and Progress](https://arxiv.org/abs/2511.08325)
*Zhiheng Xi,Chenyang Liao,Guanyu Li,Yajie Yang,Wenxiang Chen,Zhihao Zhang,Binghai Wang,Senjie Jin,Yuhao Zhou,Jian Guan,Wei Wu,Tao Ji,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: AgentPRM：一种为LLM智能体任务重新定义的过程奖励模型，通过捕捉序列决策的相互依赖性和对最终目标的贡献，实现更好的进度跟踪和探索-利用平衡，计算效率比基线高8倍以上。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM发展迅速，但在多轮决策任务（如网络购物、浏览器导航）中仍面临挑战。现有方法依赖复杂的提示工程或专家轨迹微调，本文探索构建过程奖励模型来评估每个决策并指导智能体决策过程。

Method: 提出AgentPRM模型，重新定义智能体任务的过程奖励，捕捉序列决策的相互依赖性和对最终目标的贡献。采用基于时序差分和广义优势估计的方法可扩展地获取训练数据。

Result: 在不同智能体任务上的广泛实验显示，AgentPRM比基线计算效率高8倍以上，在扩展测试时计算时表现出稳健的改进。

Conclusion: AgentPRM为LLM智能体提供了一种高效的过程奖励建模方法，能够显著提升多轮决策任务的性能，并为LLM智能体的强化学习提供了新的应用方向。

Abstract: Despite rapid development, large language models (LLMs) still encounter challenges in multi-turn decision-making tasks (i.e., agent tasks) like web shopping and browser navigation, which require making a sequence of intelligent decisions based on environmental feedback. Previous work for LLM agents typically relies on elaborate prompt engineering or fine-tuning with expert trajectories to improve performance. In this work, we take a different perspective: we explore constructing process reward models (PRMs) to evaluate each decision and guide the agent's decision-making process. Unlike LLM reasoning, where each step is scored based on correctness, actions in agent tasks do not have a clear-cut correctness. Instead, they should be evaluated based on their proximity to the goal and the progress they have made. Building on this insight, we propose a re-defined PRM for agent tasks, named AgentPRM, to capture both the interdependence between sequential decisions and their contribution to the final goal. This enables better progress tracking and exploration-exploitation balance. To scalably obtain labeled data for training AgentPRM, we employ a Temporal Difference-based (TD-based) estimation method combined with Generalized Advantage Estimation (GAE), which proves more sample-efficient than prior methods. Extensive experiments across different agentic tasks show that AgentPRM is over $8\times$ more compute-efficient than baselines, and it demonstrates robust improvement when scaling up test-time compute. Moreover, we perform detailed analyses to show how our method works and offer more insights, e.g., applying AgentPRM to the reinforcement learning of LLM agents.

</details>


### [42] [DPRM: A Dual Implicit Process Reward Model in Multi-Hop Question Answering](https://arxiv.org/abs/2511.08364)
*Xinyi Wang,Yiping Song,Zhiliang Tian,Bo Liu,Tingjin Luo,Minlie Huang*

Main category: cs.CL

TL;DR: 提出了DPRM模型，通过训练两个隐式过程奖励模型（CoT-PRM和KG-PRM）来处理多跳问答任务中的思维链和知识图谱推理，无需额外标注即可从结果信号中推导步骤级奖励，并通过一致性约束优化推理路径。


<details>
  <summary>Details</summary>
Motivation: 现有隐式过程奖励模型仅适用于纯文本场景，无法处理知识图谱的结构约束，也无法捕捉思维链与知识图谱路径之间的不一致性。

Method: 训练两个隐式PRM：CoT-PRM用于思维链推理，KG-PRM用于知识图谱推理（使用偏好对学习结构约束），并引入一致性约束使两者相互验证和优化。

Result: 在多个数据集上优于13个基线方法，Hit@1指标提升高达16.6%。

Conclusion: DPRM通过双隐式过程奖励模型有效解决了多跳问答任务中思维链和知识图谱推理的协调问题，显著提升了推理质量。

Abstract: In multi-hop question answering (MHQA) tasks, Chain of Thought (CoT) improves the quality of generation by guiding large language models (LLMs) through multi-step reasoning, and Knowledge Graphs (KGs) reduce hallucinations via semantic matching. Outcome Reward Models (ORMs) provide feedback after generating the final answers but fail to evaluate the process for multi-step reasoning. Traditional Process Reward Models (PRMs) evaluate the reasoning process but require costly human annotations or rollout generation. While implicit PRM is trained only with outcome signals and derives step rewards through reward parameterization without explicit annotations, it is more suitable for multi-step reasoning in MHQA tasks. However, existing implicit PRM has only been explored for plain text scenarios. When adapting to MHQA tasks, it cannot handle the graph structure constraints in KGs and capture the potential inconsistency between CoT and KG paths. To address these limitations, we propose the DPRM (Dual Implicit Process Reward Model). It trains two implicit PRMs for CoT and KG reasoning in MHQA tasks. Both PRMs, namely KG-PRM and CoT-PRM, derive step-level rewards from outcome signals via reward parameterization without additional explicit annotations. Among them, KG-PRM uses preference pairs to learn structural constraints from KGs. DPRM further introduces a consistency constraint between CoT and KG reasoning steps, making the two PRMs mutually verify and collaboratively optimize the reasoning paths. We also provide a theoretical demonstration of the derivation of process rewards. Experimental results show that our method outperforms 13 baselines on multiple datasets with up to 16.6% improvement on Hit@1.

</details>


### [43] [The Dynamic Articulatory Model DYNARTmo: Dynamic Movement Generation and Speech Gestures](https://arxiv.org/abs/2511.08372)
*Bernd J. Kröger*

Main category: cs.CL

TL;DR: 本文介绍了动态发音模型DYNARTmo的当前实现，该模型基于语音手势概念和相应手势得分生成连续的发音器运动。


<details>
  <summary>Details</summary>
Motivation: 为模拟从语言表征到发音-声学实现的层级控制提供神经生物学启发的计算框架。

Method: 使用语音手势概念和手势得分，通过手势库结构、手势协调以及转换为连续发音器轨迹来控制DYNARTmo声道模型。

Result: 开发了能够生成连续发音器运动的动态发音模型DYNARTmo。

Conclusion: DYNARTmo模型提供了一个计算框架，用于模拟语音产生的层级控制过程。

Abstract: This paper describes the current implementation of the dynamic articulatory model DYNARTmo, which generates continuous articulator movements based on the concept of speech gestures and a corresponding gesture score. The model provides a neurobiologically inspired computational framework for simulating the hierarchical control of speech production from linguistic representation to articulatory-acoustic realization. We present the structure of the gesture inventory, the coordination of gestures in the gesture score, and their translation into continuous articulator trajectories controlling the DYNARTmo vocal tract model.

</details>


### [44] [TurkEmbed: Turkish Embedding Model on NLI & STS Tasks](https://arxiv.org/abs/2511.08376)
*Özay Ezerceli,Gizem Gümüşçekiçci,Tuğba Erkoç,Berke Özenç*

Main category: cs.CL

TL;DR: TurkEmbed是土耳其语嵌入模型，在NLI和STS任务上超越现有模型，通过多样化数据集和matryoshka表示学习实现1-4%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有土耳其语嵌入模型依赖机器翻译数据集，限制了准确性和语义理解能力。

Method: 使用多样化数据集和高级训练技术，包括matryoshka表示学习，以适应资源受限环境并提供更快编码能力。

Result: 在土耳其STS-b-TR数据集上使用Pearson和Spearman相关性指标评估，显示语义相似性任务显著改进，在All-NLI-TR和STS-b-TR基准测试中超越当前最先进模型Emrecan。

Conclusion: TurkEmbed通过提供更细致的语言理解，有望增强土耳其NLP生态系统并促进下游应用的发展。

Abstract: This paper introduces TurkEmbed, a novel Turkish language embedding model designed to outperform existing models, particularly in Natural Language Inference (NLI) and Semantic Textual Similarity (STS) tasks. Current Turkish embedding models often rely on machine-translated datasets, potentially limiting their accuracy and semantic understanding. TurkEmbed utilizes a combination of diverse datasets and advanced training techniques, including matryoshka representation learning, to achieve more robust and accurate embeddings. This approach enables the model to adapt to various resource-constrained environments, offering faster encoding capabilities. Our evaluation on the Turkish STS-b-TR dataset, using Pearson and Spearman correlation metrics, demonstrates significant improvements in semantic similarity tasks. Furthermore, TurkEmbed surpasses the current state-of-the-art model, Emrecan, on All-NLI-TR and STS-b-TR benchmarks, achieving a 1-4\% improvement. TurkEmbed promises to enhance the Turkish NLP ecosystem by providing a more nuanced understanding of language and facilitating advancements in downstream applications.

</details>


### [45] [PCRLLM: Proof-Carrying Reasoning with Large Language Models under Stepwise Logical Constraints](https://arxiv.org/abs/2511.08392)
*Tangrui Li,Pei Wang,Hongzheng Wang Christian Hahm,Matteo Spatola,Justin Shi*

Main category: cs.CL

TL;DR: PCRLLM框架通过单步推理约束和显式规则说明，提升LLM的逻辑推理可靠性和可验证性，支持多LLM协作和基准数据生成。


<details>
  <summary>Details</summary>
Motivation: LLM在逻辑推理中缺乏明确的推理规则遵循，导致推理过程不可靠且难以验证，需要一种能约束推理过程并支持验证的框架。

Method: 提出Proof-Carrying Reasoning with LLMs (PCRLLM)框架，限制为单步推理，显式指定前提、规则和结论，支持目标逻辑验证和多LLM协作。

Result: PCRLLM能够提高推理过程的可靠性和可验证性，即使在黑盒设置下也能进行链级验证，并支持系统性的多LLM协作。

Conclusion: PCRLLM为LLM的逻辑推理提供了形式化约束和验证机制，结合自然语言表达和形式化严谨性，提升了推理的可靠性和可信度。

Abstract: Large Language Models (LLMs) often exhibit limited logical coherence, mapping premises to conclusions without adherence to explicit inference rules. We propose Proof-Carrying Reasoning with LLMs (PCRLLM), a framework that constrains reasoning to single-step inferences while preserving natural language formulations. Each output explicitly specifies premises, rules, and conclusions, thereby enabling verification against a target logic. This mechanism mitigates trustworthiness concerns by supporting chain-level validation even in black-box settings. Moreover, PCRLLM facilitates systematic multi-LLM collaboration, allowing intermediate steps to be compared and integrated under formal rules. Finally, we introduce a benchmark schema for generating large-scale step-level reasoning data, combining natural language expressiveness with formal rigor.

</details>


### [46] [Interaction Dynamics as a Reward Signal for LLMs](https://arxiv.org/abs/2511.08394)
*Sian Gooding,Edward Grefenstette*

Main category: cs.CL

TL;DR: TRACE提出了一种基于对话嵌入轨迹几何特性的新型奖励信号，仅使用交互动态就能达到与完整文本分析相当的准确性，且结合文本分析可获得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型对齐方法主要依赖文本内容奖励信号，忽视了交互动态这一丰富且互补的信号源。

Method: 引入TRACE方法，从对话嵌入轨迹的几何特性（称为'对话几何'）中提取奖励信号，训练仅基于结构信号的奖励模型。

Result: 仅使用交互动态的奖励模型达到68.20%的成对准确率，与完整文本分析的强大基线（70.04%）相当；结合文本分析的混合模型达到最高性能80.17%。

Conclusion: 在交互式设置中，代理的沟通方式与沟通内容同样重要，TRACE提供了一个隐私保护框架，既能对齐代理，又能作为诊断工具理解成功协作的交互模式。

Abstract: The alignment of Large Language Models (LLMs) for multi-turn conversations typically relies on reward signals derived from the content of the text. This approach, however, overlooks a rich, complementary source of signal: the dynamics of the interaction itself. This paper introduces TRACE (Trajectory-based Reward for Agent Collaboration Estimation), a novel reward signal derived from the geometric properties of a dialogue's embedding trajectory--a concept we term 'conversational geometry'. Our central finding is that a reward model trained only on these structural signals achieves a pairwise accuracy (68.20%) comparable to a powerful LLM baseline that analyzes the full transcript (70.04%). Furthermore, a hybrid model combining interaction dynamics with textual analysis achieves the highest performance (80.17%), demonstrating their complementary nature. This work provides strong evidence that for interactive settings, how an agent communicates is as powerful a predictor of success as what it says, offering a new, privacy-preserving framework that not only aligns agents but also serves as a diagnostic tool for understanding the distinct interaction patterns that drive successful collaboration.

</details>


### [47] [Bot Meets Shortcut: How Can LLMs Aid in Handling Unknown Invariance OOD Scenarios?](https://arxiv.org/abs/2511.08455)
*Shiyan Zheng,Herun Wan,Minnan Luo,Junhang Huang*

Main category: cs.CL

TL;DR: 该论文研究了社交机器人检测器在真实场景中的鲁棒性问题，重点关注文本特征中的捷径学习现象，并提出基于大语言模型的反事实数据增强缓解策略。


<details>
  <summary>Details</summary>
Motivation: 现有社交机器人检测器在基准测试中表现良好，但在多样化真实场景中鲁棒性有限，主要原因是真实标签不清晰和误导性线索多样。特别是捷径学习（模型依赖虚假相关性而非因果特征）的影响研究不足。

Method: 设计了一系列基于文本特征的捷径场景，构建用户标签与表面文本线索之间的虚假关联来评估模型鲁棒性。提出基于大语言模型的反事实数据增强缓解策略，从数据和模型两个角度在三个层面进行改进。

Result: 结果显示，无关特征分布的偏移显著降低了社交机器人检测器性能，基线模型平均相对准确率下降32%。提出的缓解策略在捷径场景下实现了平均相对性能提升56%。

Conclusion: 社交机器人检测器容易受到文本特征中捷径学习的影响，基于大语言模型的反事实数据增强策略能有效提升模型在捷径场景下的鲁棒性。

Abstract: While existing social bot detectors perform well on benchmarks, their robustness across diverse real-world scenarios remains limited due to unclear ground truth and varied misleading cues. In particular, the impact of shortcut learning, where models rely on spurious correlations instead of capturing causal task-relevant features, has received limited attention. To address this gap, we conduct an in-depth study to assess how detectors are influenced by potential shortcuts based on textual features, which are most susceptible to manipulation by social bots. We design a series of shortcut scenarios by constructing spurious associations between user labels and superficial textual cues to evaluate model robustness. Results show that shifts in irrelevant feature distributions significantly degrade social bot detector performance, with an average relative accuracy drop of 32\% in the baseline models. To tackle this challenge, we propose mitigation strategies based on large language models, leveraging counterfactual data augmentation. These methods mitigate the problem from data and model perspectives across three levels, including data distribution at both the individual user text and overall dataset levels, as well as the model's ability to extract causal information. Our strategies achieve an average relative performance improvement of 56\% under shortcut scenarios.

</details>


### [48] [SPEAR-MM: Selective Parameter Evaluation and Restoration via Model Merging for Efficient Financial LLM Adaptation](https://arxiv.org/abs/2511.08500)
*Berkcan Kapusuzoglu,Supriyo Chakraborty,Renkun Ni,Stephen Rawls,Sambit Sahu*

Main category: cs.CL

TL;DR: SPEAR-MM是一种通过模型合并选择性保留关键能力的方法，在金融领域适应中保持91.2%的通用推理能力，同时减少90%计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在适应金融领域时经常出现灾难性遗忘，丢失对客户交互和复杂金融分析至关重要的通用推理能力。

Method: 通过后验分析近似层面对外部基准的影响，然后通过球面插值合并选择性冻结或恢复Transformer层。

Result: 在LLaMA-3.1-8B上应用，SPEAR-MM保持91.2%的通用能力（标准持续预训练为69.7%），同时保持94%的领域适应收益。

Conclusion: 该方法提供可解释的权衡控制，对资源受限的金融机构至关重要，显著减少计算成本。

Abstract: Large language models (LLMs) adapted to financial domains often suffer from catastrophic forgetting of general reasoning capabilities essential for customer interactions and complex financial analysis. We introduce Selective Parameter Evaluation and Restoration via Model Merging (SPEAR-MM), a practical framework that preserves critical capabilities while enabling domain adaptation. Our method approximates layer-wise impact on external benchmarks through post-hoc analysis, then selectively freezes or restores transformer layers via spherical interpolation merging. Applied to LLaMA-3.1-8B for financial tasks, SPEAR-MM achieves 91.2% retention of general capabilities versus 69.7% for standard continual pretraining, while maintaining 94% of domain adaptation gains. The approach provides interpretable trade-off control and reduces computational costs by 90% crucial for resource-constrained financial institutions.

</details>


### [49] [Structured RAG for Answering Aggregative Questions](https://arxiv.org/abs/2511.08505)
*Omri Koshorek,Niv Granot,Aviv Alloni,Shahar Admati,Roee Hendel,Ido Weiss,Alan Arazi,Shay-Nitzan Cohen,Yonatan Belinkov*

Main category: cs.CL

TL;DR: S-RAG是一种专门针对聚合查询的检索增强生成方法，通过在语料库中构建结构化表示，并将自然语言查询转换为形式化查询，显著优于传统RAG系统和长上下文LLM。


<details>
  <summary>Details</summary>
Motivation: 当前RAG方法和数据集主要关注小范围相关文本的查询，无法有效处理需要从大量文档中收集信息并进行推理的聚合查询。

Method: 在数据摄入阶段构建语料库的结构化表示，在推理阶段将自然语言查询转换为对该表示的形式化查询。

Result: 在新建的HOTELS和WORLD CUP数据集以及公共基准测试中，S-RAG显著优于传统RAG系统和长上下文LLM。

Conclusion: S-RAG成功解决了聚合查询的挑战，为这一研究领域提供了新的方法和数据集。

Abstract: Retrieval-Augmented Generation (RAG) has become the dominant approach for answering questions over large corpora. However, current datasets and methods are highly focused on cases where only a small part of the corpus (usually a few paragraphs) is relevant per query, and fail to capture the rich world of aggregative queries. These require gathering information from a large set of documents and reasoning over them. To address this gap, we propose S-RAG, an approach specifically designed for such queries. At ingestion time, S-RAG constructs a structured representation of the corpus; at inference time, it translates natural-language queries into formal queries over said representation. To validate our approach and promote further research in this area, we introduce two new datasets of aggregative queries: HOTELS and WORLD CUP. Experiments with S-RAG on the newly introduced datasets, as well as on a public benchmark, demonstrate that it substantially outperforms both common RAG systems and long-context LLMs.

</details>


### [50] [Introducing A Bangla Sentence - Gloss Pair Dataset for Bangla Sign Language Translation and Research](https://arxiv.org/abs/2511.08507)
*Neelavro Saha,Rafi Shahriyar,Nafis Ashraf Roudra,Saadman Sakib,Annajiat Alim Rasel*

Main category: cs.CL

TL;DR: 本文介绍了Bangla-SGP数据集，这是一个包含1000个人工标注的句子-手势对和3000个合成生成对的孟加拉手语翻译数据集，并评估了多个transformer模型在手势翻译任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 孟加拉手语翻译是一个低资源NLP任务，缺乏大规模句子级翻译数据集，现有研究仅限于单词和字母级别检测。

Method: 创建了包含1000个高质量人工标注句子-手势对的Bangla-SGP数据集，通过基于规则的检索增强生成(RAG)管道使用句法和形态学规则生成了约3000个合成对，并微调了mBart50、Google mT5、GPT4.1-nano等transformer模型。

Result: 使用BLEU分数评估了句子到手势翻译性能，并在Bangla-SGP数据集和RWTH-PHOENIX-2014T基准上比较了模型的手势翻译一致性。

Conclusion: Bangla-SGP数据集填补了孟加拉手语句子级翻译数据集的空白，为低资源手语翻译任务提供了重要资源，并展示了transformer模型在该任务上的应用潜力。

Abstract: Bangla Sign Language (BdSL) translation represents a low-resource NLP task due to the lack of large-scale datasets that address sentence-level translation. Correspondingly, existing research in this field has been limited to word and alphabet level detection. In this work, we introduce Bangla-SGP, a novel parallel dataset consisting of 1,000 human-annotated sentence-gloss pairs which was augmented with around 3,000 synthetically generated pairs using syntactic and morphological rules through a rule-based Retrieval-Augmented Generation (RAG) pipeline. The gloss sequences of the spoken Bangla sentences are made up of individual glosses which are Bangla sign supported words and serve as an intermediate representation for a continuous sign. Our dataset consists of 1000 high quality Bangla sentences that are manually annotated into a gloss sequence by a professional signer. The augmentation process incorporates rule-based linguistic strategies and prompt engineering techniques that we have adopted by critically analyzing our human annotated sentence-gloss pairs and by working closely with our professional signer. Furthermore, we fine-tune several transformer-based models such as mBart50, Google mT5, GPT4.1-nano and evaluate their sentence-to-gloss translation performance using BLEU scores, based on these evaluation metrics we compare the model's gloss-translation consistency across our dataset and the RWTH-PHOENIX-2014T benchmark.

</details>


### [51] [AlphaResearch: Accelerating New Algorithm Discovery with Language Models](https://arxiv.org/abs/2511.08522)
*Zhaojian Yu,Kaiyue Feng,Yilun Zhao,Shilin He,Xiao-Ping Zhang,Arman Cohan*

Main category: cs.CL

TL;DR: AlphaResearch是一个自主研究代理，通过结合执行验证和模拟同行评审的双重研究环境，在开放式算法问题上发现新算法，在8个问题中获得2个胜率，并在"圆打包"问题上达到最佳已知性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂但易于验证的问题上取得显著进展，但在发现未知方面仍存在困难，需要开发能够自主发现新算法的研究代理。

Method: 构建双重研究环境（执行验证+模拟同行评审），通过迭代步骤：提出新想法→在双重环境中验证→优化研究提案；创建AlphaResearchComp基准，包含8个开放式算法问题。

Result: AlphaResearch在8个问题中获得2/8胜率，在"圆打包"问题上超越人类研究者和AlphaEvolve等基线，达到最佳已知性能；对6个失败案例进行了全面分析。

Conclusion: AlphaResearch展示了利用LLMs加速算法发现的可能性，为未来研究提供了有价值的见解。

Abstract: Large language models have made significant progress in complex but easy-to-verify problems, yet they still struggle with discovering the unknown. In this paper, we present \textbf{AlphaResearch}, an autonomous research agent designed to discover new algorithms on open-ended problems. To synergize the feasibility and innovation of the discovery process, we construct a novel dual research environment by combining the execution-based verify and simulated real-world peer review environment. AlphaResearch discovers new algorithm by iteratively running the following steps: (1) propose new ideas (2) verify the ideas in the dual research environment (3) optimize the research proposals for better performance. To promote a transparent evaluation process, we construct \textbf{AlphaResearchComp}, a new evaluation benchmark that includes an eight open-ended algorithmic problems competition, with each problem carefully curated and verified through executable pipelines, objective metrics, and reproducibility checks. AlphaResearch gets a 2/8 win rate in head-to-head comparison with human researchers, demonstrate the possibility of accelerating algorithm discovery with LLMs. Notably, the algorithm discovered by AlphaResearch on the \emph{``packing circles''} problem achieves the best-of-known performance, surpassing the results of human researchers and strong baselines from recent work (e.g., AlphaEvolve). Additionally, we conduct a comprehensive analysis of the remaining challenges of the 6/8 failure cases, providing valuable insights for future research.

</details>


### [52] [Investigating CoT Monitorability in Large Reasoning Models](https://arxiv.org/abs/2511.08525)
*Shu Yang,Junchao Wu,Xilin Gou,Xuansheng Wu,Derek Wong,Ninhao Liu,Di Wang*

Main category: cs.CL

TL;DR: 本文首次系统研究了CoT可监控性的挑战与潜力，围绕LRMs在CoT中真实表达决策因素的程度以及基于CoT的监控器可靠检测错误行为的程度展开，提出了新的监控范式MoME。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过详细的推理轨迹为AI安全创造了新机会——CoT可监控性，但面临两个关键挑战：模型不总是真实表达其内部决策过程，监控器可能过于敏感或不够敏感且容易被欺骗。

Method: 围绕两个核心视角展开研究：(i) 言语化程度：LRMs在CoT中真实表达决策因素的程度；(ii) 监控可靠性：基于CoT的监控器可靠检测错误行为的程度。通过数学、科学和伦理领域的实证证据和相关分析，研究不同CoT干预方法对监控效果的影响，并提出MoME新范式。

Result: 提供了言语化质量、监控可靠性和LLM性能之间的实证证据和相关分析，研究了不同CoT干预方法对监控有效性的影响，并提出了MoME新范式。

Conclusion: CoT可监控性在AI安全中具有重要潜力，但需要解决言语化真实性和监控可靠性两大挑战。提出的MoME范式为通过CoT监控模型错误行为提供了新的结构化方法。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks by engaging in extended reasoning before producing final answers. Beyond improving abilities, these detailed reasoning traces also create a new opportunity for AI safety, CoT Monitorability: monitoring potential model misbehavior, such as the use of shortcuts or sycophancy, through their chain-of-thought (CoT) during decision-making. However, two key fundamental challenges arise when attempting to build more effective monitors through CoT analysis. First, as prior research on CoT faithfulness has pointed out, models do not always truthfully represent their internal decision-making in the generated reasoning. Second, monitors themselves may be either overly sensitive or insufficiently sensitive, and can potentially be deceived by models' long, elaborate reasoning traces. In this paper, we present the first systematic investigation of the challenges and potential of CoT monitorability. Motivated by two fundamental challenges we mentioned before, we structure our study around two central perspectives: (i) verbalization: to what extent do LRMs faithfully verbalize the true factors guiding their decisions in the CoT, and (ii) monitor reliability: to what extent can misbehavior be reliably detected by a CoT-based monitor? Specifically, we provide empirical evidence and correlation analyses between verbalization quality, monitor reliability, and LLM performance across mathematical, scientific, and ethical domains. Then we further investigate how different CoT intervention methods, designed to improve reasoning efficiency or performance, will affect monitoring effectiveness. Finally, we propose MoME, a new paradigm in which LLMs monitor other models' misbehavior through their CoT and provide structured judgments along with supporting evidence.

</details>


### [53] [From Semantic Roles to Opinion Roles: SRL Data Extraction for Multi-Task and Transfer Learning in Low-Resource ORL](https://arxiv.org/abs/2511.08537)
*Amirmohammad Omidi Galdiani,Sepehr Rezaei Melal,Mohammad Norasteh,Arash Yousefi Jordehi,Seyed Abolghasem Mirroshandel*

Main category: cs.CL

TL;DR: 构建了一个从OntoNotes 5.0 WSJ语料库中提取的高质量语义角色标注数据集，并将其适配用于意见角色标注任务，包含97,169个谓词-论元实例。


<details>
  <summary>Details</summary>
Motivation: 为研究人员提供一个可重用的资源，利用语义角色标注来增强意见角色标注，特别是在低资源意见挖掘场景中。

Method: 基于PropBank标注框架，实现可复现的提取流程，包括对齐谓词-论元结构与表层文本、将句法树指针转换为连贯跨度，并进行严格清洗以确保语义保真度。

Result: 成功构建了包含97,169个谓词-论元实例的数据集，明确定义了Agent、Predicate和Patient角色，并将其映射到意见角色标注的Holder、Expression和Target模式。

Conclusion: 这项工作为研究人员提供了利用语义角色标注增强意见角色标注的可重用资源，特别适用于低资源意见挖掘场景。

Abstract: This report presents a detailed methodology for constructing a high-quality Semantic Role Labeling (SRL) dataset from the Wall Street Journal (WSJ) portion of the OntoNotes 5.0 corpus and adapting it for Opinion Role Labeling (ORL) tasks. Leveraging the PropBank annotation framework, we implement a reproducible extraction pipeline that aligns predicate-argument structures with surface text, converts syntactic tree pointers to coherent spans, and applies rigorous cleaning to ensure semantic fidelity. The resulting dataset comprises 97,169 predicate-argument instances with clearly defined Agent (ARG0), Predicate (REL), and Patient (ARG1) roles, mapped to ORL's Holder, Expression, and Target schema. We provide a detailed account of our extraction algorithms, discontinuous argument handling, annotation corrections, and statistical analysis of the resulting dataset. This work offers a reusable resource for researchers aiming to leverage SRL for enhancing ORL, especially in low-resource opinion mining scenarios.

</details>


### [54] [Moral Susceptibility and Robustness under Persona Role-Play in Large Language Models](https://arxiv.org/abs/2511.08565)
*Davi Bastos Costa,Felippe Alves,Renato Vicente*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在角色扮演情境下的道德响应，通过道德基础问卷量化了道德易感性和道德鲁棒性，发现模型家族对鲁棒性影响最大，而模型大小对易感性有显著影响。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地在社交环境中应用，需要分析它们如何表达和改变道德判断，特别是在角色扮演情境下。

Method: 使用道德基础问卷(MFQ)作为基准，通过角色扮演提示让LLMs扮演特定角色，量化道德易感性和道德鲁棒性这两个属性。

Result: Claude家族在道德鲁棒性方面表现最佳，其次是Gemini和GPT-4；模型大小对道德易感性有明确影响，较大模型更易受影响；鲁棒性和易感性呈正相关。

Conclusion: 角色扮演条件显著影响大型语言模型的道德行为，为理解LLMs在社交环境中的道德表现提供了系统性视角。

Abstract: Large language models (LLMs) increasingly operate in social contexts, motivating analysis of how they express and shift moral judgments. In this work, we investigate the moral response of LLMs to persona role-play, prompting a LLM to assume a specific character. Using the Moral Foundations Questionnaire (MFQ), we introduce a benchmark that quantifies two properties: moral susceptibility and moral robustness, defined from the variability of MFQ scores across and within personas, respectively. We find that, for moral robustness, model family accounts for most of the variance, while model size shows no systematic effect. The Claude family is, by a significant margin, the most robust, followed by Gemini and GPT-4 models, with other families exhibiting lower robustness. In contrast, moral susceptibility exhibits a mild family effect but a clear within-family size effect, with larger variants being more susceptible. Moreover, robustness and susceptibility are positively correlated, an association that is more pronounced at the family level. Additionally, we present moral foundation profiles for models without persona role-play and for personas averaged across models. Together, these analyses provide a systematic view of how persona conditioning shapes moral behavior in large language models.

</details>


### [55] [Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models](https://arxiv.org/abs/2511.08577)
*Tianyu Fu,Yichen You,Zekai Chen,Guohao Dai,Huazhong Yang,Yu Wang*

Main category: cs.CL

TL;DR: TaH是一种动态潜在思考方法，仅在困难标记处进行深度迭代，通过轻量级决策器触发潜在迭代，使用LoRA模块优化硬标记预测，并引入双重因果注意力机制实现跨迭代信息流，显著提升LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有循环transformer方法对所有标记进行固定次数的额外迭代，但存在潜在过度思考现象：原本正确的简单标记预测在额外迭代中可能被错误修正。

Method: 提出Think-at-Hard方法：1）轻量级神经决策器识别可能错误的硬标记；2）仅在硬标记处触发潜在迭代；3）使用LoRA模块将LLM目标从通用下一标记预测转向硬标记精炼；4）引入双重因果注意力机制扩展注意力到迭代深度维度。

Result: 在五个挑战性基准测试中，TaH显著提升LLM推理性能：相比对所有标记迭代两次的基线，准确率提升8.1-11.3%，同时免除94%标记的第二次迭代；相比单次迭代Qwen3模型，准确率提升4.0-5.0%；当允许LoRA和决策器增加少于3%参数时，增益分别增加到8.5-12.6%和5.3-5.4%。

Conclusion: TaH通过动态选择性地在困难标记处进行深度迭代，有效解决了潜在过度思考问题，在保持相同参数量的同时显著提升了LLM的推理能力，为参数受限环境下的LLM优化提供了有效解决方案。

Abstract: Improving reasoning capabilities of Large Language Models (LLMs), especially under parameter constraints, is crucial for real-world applications. Prior work proposes recurrent transformers, which allocate a fixed number of extra iterations per token to improve generation quality. After the first, standard forward pass, instead of verbalization, last-layer hidden states are fed back as inputs for additional iterations to refine token predictions. Yet we identify a latent overthinking phenomenon: easy token predictions that are already correct after the first pass are sometimes revised into errors in additional iterations. To address this, we propose Think-at-Hard (TaH), a dynamic latent thinking method that iterates deeper only at hard tokens. It employs a lightweight neural decider to trigger latent iterations only at tokens that are likely incorrect after the standard forward pass. During latent iterations, Low-Rank Adaptation (LoRA) modules shift the LLM objective from general next-token prediction to focused hard-token refinement. We further introduce a duo-causal attention mechanism that extends attention from the token sequence dimension to an additional iteration depth dimension. This enables cross-iteration information flow while maintaining full sequential parallelism. Experiments show that TaH boosts LLM reasoning performance across five challenging benchmarks while maintaining the same parameter count. Compared with baselines that iterate twice for all output tokens, TaH delivers 8.1-11.3% accuracy gains while exempting 94% of tokens from the second iteration. Against strong single-iteration Qwen3 models finetuned with the same data, it also delivers 4.0-5.0% accuracy gains. When allowing less than 3% additional parameters from LoRA and the iteration decider, the gains increase to 8.5-12.6% and 5.3-5.4%, respectively. Our code is available at https://github.com/thu-nics/TaH.

</details>


### [56] [Training Language Models to Explain Their Own Computations](https://arxiv.org/abs/2511.08579)
*Belinda Z. Li,Zifan Carl Guo,Vincent Huang,Jacob Steinhardt,Jacob Andreas*

Main category: cs.CL

TL;DR: 语言模型可以通过微调学习生成关于自身内部计算的自然语言描述，包括特征编码信息、激活因果结构和输入标记影响，且自我解释效果优于其他模型解释。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型是否能够利用对自身内部计算的特殊访问权限，产生新的解释技术来忠实描述其行为。

Method: 使用现有可解释性技术作为真实标签，微调语言模型生成三类自然语言描述：特征编码信息、内部激活因果结构、输入标记对输出的影响。

Result: 仅用数万个解释示例训练后，解释模型对新查询展现出非平凡泛化能力，自我解释效果优于其他模型解释，即使其他模型能力更强。

Conclusion: 语言模型能够可靠地解释自身内部计算，这种解释方法可作为现有可解释性方法的可扩展补充。

Abstract: Can language models (LMs) learn to faithfully describe their internal computations? Are they better able to describe themselves than other models? We study the extent to which LMs' privileged access to their own internals can be leveraged to produce new techniques for explaining their behavior. Using existing interpretability techniques as a source of ground truth, we fine-tune LMs to generate natural language descriptions of (1) the information encoded by LM features, (2) the causal structure of LMs' internal activations, and (3) the influence of specific input tokens on LM outputs. When trained with only tens of thousands of example explanations, explainer models exhibit non-trivial generalization to new queries. This generalization appears partly attributable to explainer models' privileged access to their own internals: using a model to explain its own computations generally works better than using a *different* model to explain its computations (even if the other model is significantly more capable). Our results suggest not only that LMs can learn to reliably explain their internal computations, but that such explanations offer a scalable complement to existing interpretability methods.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [57] [BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives](https://arxiv.org/abs/2511.08029)
*Aarush Sinha,Pavan Kumar S,Roshan Balaji,Nirav Pravinbhai Bhatt*

Main category: cs.IR

TL;DR: BiCA方法利用PubMed文章的引用链接生成高质量的困难负样本，通过微调GTE_small和GTE_Base模型，在生物医学领域实现了零样本密集检索的显著提升。


<details>
  <summary>Details</summary>
Motivation: 在生物医学和科学领域，由于源文档与困难负样本之间难以区分，传统的困难负样本挖掘方法面临挑战。引用文档天然与源文档具有上下文相关性但不是重复内容，因此适合作为困难负样本。

Method: 提出BiCA方法，利用20,000篇PubMed文章的引用链接进行困难负样本挖掘，通过引用感知的负样本改进领域特定的密集检索器，对GTE_small和GTE_Base模型进行微调。

Result: 在BEIR数据集上的零样本密集检索中，使用nDCG@10指标在领域内和领域外任务上均观察到持续改进，在LoTTE的长尾主题上使用Success@5指标优于基线方法。

Conclusion: 利用文档链接结构生成高信息量的负样本具有巨大潜力，能够以最少的微调实现最先进的性能，展示了高效领域适应的可行路径。

Abstract: Hard negatives are essential for training effective retrieval models. Hard-negative mining typically relies on ranking documents using cross-encoders or static embedding models based on similarity metrics such as cosine distance. Hard negative mining becomes challenging for biomedical and scientific domains due to the difficulty in distinguishing between source and hard negative documents. However, referenced documents naturally share contextual relevance with the source document but are not duplicates, making them well-suited as hard negatives. In this work, we propose BiCA: Biomedical Dense Retrieval with Citation-Aware Hard Negatives, an approach for hard-negative mining by utilizing citation links in 20,000 PubMed articles for improving a domain-specific small dense retriever. We fine-tune the GTE_small and GTE_Base models using these citation-informed negatives and observe consistent improvements in zero-shot dense retrieval using nDCG@10 for both in-domain and out-of-domain tasks on BEIR and outperform baselines on long-tailed topics in LoTTE using Success@5. Our findings highlight the potential of leveraging document link structure to generate highly informative negatives, enabling state-of-the-art performance with minimal fine-tuning and demonstrating a path towards highly data-efficient domain adaptation.

</details>


### [58] [LLaDA-Rec: Discrete Diffusion for Parallel Semantic ID Generation in Generative Recommendation](https://arxiv.org/abs/2511.06254)
*Teng Shi,Chenglei Shen,Weijie Yu,Shen Nie,Chongxuan Li,Xiao Zhang,Ming He,Yan Han,Jun Xu*

Main category: cs.IR

TL;DR: LLaDA-Rec是一个基于离散扩散的生成式推荐框架，通过并行语义ID生成解决传统自回归模型的单向约束和错误累积问题。


<details>
  <summary>Details</summary>
Motivation: 现有自回归生成推荐模型存在两个内在限制：(1) 单向约束，因果注意力机制限制每个标记只能关注其前驱标记，阻碍全局语义建模；(2) 错误累积，固定的从左到右生成顺序导致早期标记的预测错误传播到后续标记。

Method: 提出离散扩散框架，包含三个关键设计：(1) 并行标记化方案，生成适用于双向建模的语义ID；(2) 用户历史级别和下一项目级别的双重掩码机制，捕获项目间序列依赖和项目内语义关系；(3) 自适应顺序的离散扩散解码策略，解决标准波束搜索与扩散生成的不兼容性。

Result: 在三个真实世界数据集上的实验表明，LLaDA-Rec在ID基推荐器和最先进的生成推荐器上都取得了更好的性能。

Conclusion: LLaDA-Rec将离散扩散确立为生成式推荐的新范式，通过双向注意力和自适应生成顺序更有效地建模项目间和项目内依赖关系，并缓解错误累积问题。

Abstract: Generative recommendation represents each item as a semantic ID, i.e., a sequence of discrete tokens, and generates the next item through autoregressive decoding. While effective, existing autoregressive models face two intrinsic limitations: (1) unidirectional constraints, where causal attention restricts each token to attend only to its predecessors, hindering global semantic modeling; and (2) error accumulation, where the fixed left-to-right generation order causes prediction errors in early tokens to propagate to the predictions of subsequent token. To address these issues, we propose LLaDA-Rec, a discrete diffusion framework that reformulates recommendation as parallel semantic ID generation. By combining bidirectional attention with the adaptive generation order, the approach models inter-item and intra-item dependencies more effectively and alleviates error accumulation. Specifically, our approach comprises three key designs: (1) a parallel tokenization scheme that produces semantic IDs for bidirectional modeling, addressing the mismatch between residual quantization and bidirectional architectures; (2) two masking mechanisms at the user-history and next-item levels to capture both inter-item sequential dependencies and intra-item semantic relationships; and (3) an adapted beam search strategy for adaptive-order discrete diffusion decoding, resolving the incompatibility of standard beam search with diffusion-based generation. Experiments on three real-world datasets show that LLaDA-Rec consistently outperforms both ID-based and state-of-the-art generative recommenders, establishing discrete diffusion as a new paradigm for generative recommendation.

</details>
