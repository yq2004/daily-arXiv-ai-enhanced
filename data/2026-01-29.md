<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 69]
- [cs.IR](#cs.IR) [Total: 12]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [From Intuition to Expertise: Rubric-Based Cognitive Calibration for Human Detection of LLM-Generated Korean Text](https://arxiv.org/abs/2601.19913)
*Shinwoo Park,Yo-Sub Han*

Main category: cs.CL

TL;DR: 研究通过结构化校准提升专家识别韩语文本是否为AI生成的能力，开发了LREAD评分标准，使语言学专业学生的识别准确率从60%提升到100%。


<details>
  <summary>Details</summary>
Motivation: 区分人类撰写的韩语文本与流畅的LLM输出对即使是受过语言学训练的读者也很困难，他们可能过度依赖表面的语言规范性。需要研究是否可以将专家检测视为可学习的技能，并通过结构化校准来改进。

Method: 开发了LREAD评分标准，基于韩国国家写作标准并针对微观语言特征（如标点可选性、空格行为、语域转换）。采用三阶段纵向盲测协议：第一阶段测量直觉检测，第二阶段强制基于标准的评分并需要明确理由，第三阶段评估在保留的小学作文上的领域专注掌握能力。

Result: 经过校准，多数学员投票准确率从60%提升到100%，组间一致性显著提高（Fleiss' kappa从-0.09提升到0.82）。相比最先进的LLM检测器，经过校准的人类更依赖语言特定的微观诊断特征，这些特征未被粗糙的语篇先验很好捕捉。

Conclusion: 基于评分标准的专家判断可以作为非英语环境下自动检测器的可解释补充工具。研究发布了完整的评分标准和校准检测特征分类体系。

Abstract: Distinguishing human-written Korean text from fluent LLM outputs remains difficult even for linguistically trained readers, who can over-trust surface well-formedness. We study whether expert detection can be treated as a learnable skill and improved through structured calibration. We introduce LREAD, a rubric derived from national Korean writing standards and adapted to target micro-level artifacts (e.g., punctuation optionality, spacing behavior, and register shifts). In a three-phase longitudinal blind protocol with Korean linguistics majors, Phase 1 measures intuition-only detection, Phase 2 enforces criterion-level scoring with explicit justifications, and Phase 3 evaluates domain-focused mastery on held-out elementary essays. Across phases, majority-vote accuracy increases from 60% to 100%, accompanied by stronger inter-annotator agreement (Fleiss' kappa: -0.09 --> 0.82). Compared to state-of-the-art LLM detectors, calibrated humans rely more on language-specific micro-diagnostics that are not well captured by coarse discourse priors. Our findings suggest that rubric-scaffolded expert judgment can serve as an interpretable complement to automated detectors for non-English settings, and we release the full rubric and a taxonomy of calibrated detection signatures.

</details>


### [2] [Simulating Complex Multi-Turn Tool Calling Interactions in Stateless Execution Environments](https://arxiv.org/abs/2601.19914)
*Maxwell Crouse,Ibrahim Abdelaziz,Kshitij Fadnis,Siva Sankalp Patel,Kinjal Basu,Chulaka Gunasekara,Sadhana Kumaravel,Asim Munawar,Pavan Kapanipathi*

Main category: cs.CL

TL;DR: 提出DiGiT-TC方法，生成无需状态执行环境的工具调用对话数据，解决现实场景中状态环境不可用的数据生成问题。


<details>
  <summary>Details</summary>
Motivation: 现有合成工具调用对话数据生成方法通常假设存在状态执行环境，但现实中许多场景（如企业安全环境、多源工具规范）无法提供状态环境，这限制了现有方法的适用性。

Method: 提出DiGiT-TC方法，采用新颖的生成模式，在用户请求中隐式表示某些工具调用，从而生成具有状态环境搜索特征的工具调用对话数据。

Result: 在标准工具调用基准测试中验证了该方法，即使在有状态问题设置中，也能带来显著的性能提升。

Conclusion: DiGiT-TC方法填补了现实世界工具使用场景中合成数据生成的空白，无需依赖状态执行环境即可生成高质量的工具调用对话数据。

Abstract: Synthetic data has proven itself to be a valuable resource for tuning smaller, cost-effective language models to handle the complexities of multi-turn tool calling conversations. While many frameworks and systems for producing synthetic multi-turn tool calling data have been proposed, prior works have frequently assumed that any tool calling interactions will take place in an execution environment that maintains state. When such an environment is available, this is advantageous as it allows for the validity of an interaction to be determined by whether or not the state of the execution environment matches to some prespecified objective. Unfortunately, this does not hold in many real-world tool use settings, e.g., in enterprise settings where data security is of the utmost importance or in cases where tool specifications are synthesized from multiple sources. In this work, we address this gap by introducing a data generation method, DiGiT-TC, that is designed to produce tool calling conversations that have the characteristics of conversations generated through search in a stateful environment. The key to our technique lies in a novel generation pattern that allows our approach to implicitly represent certain tool calls in the user request. We validate our approach on standard tool calling benchmarks and demonstrate that, even in stateful problem settings, our approach results in strong performance gains.

</details>


### [3] [Modeling Next-Token Prediction as Left-Nested Intuitionistic Implication](https://arxiv.org/abs/2601.19915)
*Paul Tarau*

Main category: cs.CL

TL;DR: Arrow Language Model：基于直觉主义逻辑解释的神经架构，将前缀编码为左嵌套蕴含链，通过非交换组合保持顺序，将下一个词预测视为肯定前件推理。


<details>
  <summary>Details</summary>
Motivation: 提出基于逻辑推导的神经架构替代方案，旨在超越传统的基于注意力的Transformer模型，探索从直觉主义逻辑视角重新解释序列预测任务。

Method: 1. 将前缀编码为左嵌套蕴含链，通过非交换组合保持顺序
2. 将下一个词预测对应为肯定前件推理
3. 序列处理视为Curry-Howard对应下的构造性证明扩展
4. 使用Prolog专用定理证明器验证神经模型的基本属性
5. 提出实用的低秩神经实现

Result: 1. 证明了与乘法RNN等价的神经架构自然地从证明论解释中产生
2. 验证了交换与非交换序列、单词与多词预测选择之间的关系
3. 建立了模型相对于Transformer和状态空间模型的定位

Conclusion: Arrow Language Model为基于逻辑的神经架构推导提供了新视角，展示了从直觉主义逻辑到实际神经实现的可行路径，为Transformer替代方案开辟了新方向。

Abstract: We introduce the \emph{Arrow Language Model}, a neural architecture derived from an intuitionistic-logic interpretation of next-token prediction. Instead of representing tokens as additive embeddings mixed by attention, we encode a prefix as a \emph{left-nested implication chain} whose structure preserves order through non-commutative composition. Next-token prediction corresponds to \emph{modus ponens}, and sequence processing becomes constructive proof extension under the Curry--Howard correspondence. Our Prolog-based specialized theorem provers validate fundamental properties of the neural models, among which relations between commutative vs. non-commutative sequencing and single-token vs. multi-token prediction choices. We show that a neural architecture equivalent to multiplicative RNNs arises naturally from a proof-theoretic interpretation of next-token prediction as nested intuitionistic implication, we present a practical low-rank neural realization and position the model relative to Transformers and state-space models.
  Keywords: logic-based derivation of neural architectures, intuitionistic implicational logic, token-as-operator neural models, state-space models, alternatives to transformer-based foundational models.

</details>


### [4] [PaperAudit-Bench: Benchmarking Error Detection in Research Papers for Critical Automated Peer Review](https://arxiv.org/abs/2601.19916)
*Songjun Tu,Yiwen Ma,Jiahao Lin,Qichao Zhang,Xiangyuan Lan,Junfeng. Li,Nan Xu,Linjing Li,Dongbin Zhao*

Main category: cs.CL

TL;DR: PaperAudit-Bench：用于评估LLM在长上下文论文评审中检测细微错误能力的基准，包含数据集和自动化评审框架，能提升评审的严格性和区分度。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型生成的同行评审虽然流畅，但在处理分散在论文各处的细微实质性问题时缺乏足够的批判性严谨度，需要更好的评估工具。

Method: 提出了PaperAudit-Bench，包含两个组件：(1) PaperAudit-Dataset：包含可在单个章节识别和需要跨章节推理的错误数据集，用于长上下文设置下的受控评估；(2) PaperAudit-Review：自动化评审框架，将结构化错误检测与基于证据的评审生成相结合。

Result: 实验显示不同模型和检测深度下错误可检测性差异很大，表明长上下文设置下识别此类错误的难度。相比基线方法，整合显式错误检测能产生更严格、更具区分度的评估。数据集还支持通过SFT和RL训练轻量级LLM检测器，能以更低计算成本实现有效错误检测。

Conclusion: PaperAudit-Bench为评估和改进LLM在学术论文评审中的批判性分析能力提供了有效工具，通过结构化错误检测机制显著提升了自动化评审的质量和实用性。

Abstract: Large language models can generate fluent peer reviews, yet their assessments often lack sufficient critical rigor when substantive issues are subtle and distributed across a paper. In this paper, we introduce PaperAudit-Bench, which consists of two components: (1) PaperAudit-Dataset, an error dataset covering both errors identifiable within individual sections and those requiring cross-section reasoning, designed for controlled evaluation under long-context settings; and (2) PaperAudit-Review, an automated review framework that integrates structured error detection with evidence-aware review generation to support critical assessment. Experiments on PaperAudit-Bench reveal large variability in error detectability across models and detection depths, highlighting the difficulty of identifying such errors under long-context settings. Relative to representative automated reviewing baselines, incorporating explicit error detection into the review workflow produces systematically stricter and more discriminative evaluations, demonstrating its suitability for peer review. Finally, we show that the dataset supports training lightweight LLM detectors via SFT and RL, enabling effective error detection at reduced computational cost.

</details>


### [5] [PILOT: Planning via Internalized Latent Optimization Trajectories for Large Language Models](https://arxiv.org/abs/2601.19917)
*Haoyu Zheng,Yun Zhu,Yuqian Yuan,Bo Yuan,Wenqiao Zhang,Siliang Tang,Jun Xiao*

Main category: cs.CL

TL;DR: PILOT框架通过轻量级超网络生成查询条件化的潜在指导向量，将大模型的战略监督内化到小模型中，无需修改主干权重，显著提升多步推理能力。


<details>
  <summary>Details</summary>
Motivation: 紧凑型LLM在长时域任务中缺乏制定全局策略的能力，导致错误传播。虽然可以通过教师模型的外部指导解锁潜在推理能力，但运行时依赖外部指导存在延迟和可用性问题。

Method: 提出PILOT框架：使用轻量级超网络合成查询条件化的潜在指导向量，该向量作为内部引导机制，将模型表示引导到最优推理路径，而不修改主干权重。

Result: 在数学和编程基准测试中，PILOT有效稳定了推理轨迹，显著优于强基线（如MATH500上提升8.9%），且推理延迟可忽略。

Conclusion: PILOT提供了一种非侵入式方法，将大模型的战略监督内化到小模型中，解决了紧凑LLM在多步推理中的策略规划问题，具有实际应用价值。

Abstract: Strategic planning is critical for multi-step reasoning, yet compact Large Language Models (LLMs) often lack the capacity to formulate global strategies, leading to error propagation in long-horizon tasks. Our analysis reveals that LLMs possess latent reasoning capabilities that can be unlocked when conditioned on explicit plans from a teacher model; however, runtime reliance on external guidance is often impractical due to latency and availability constraints. To bridge this gap, we propose PILOT (Planning via Internalized Latent Optimization Trajectories), a non-invasive framework designed to internalize the strategic oversight of large models into intrinsic Latent Guidance. Instead of altering backbone weights, PILOT employs a lightweight Hyper-Network to synthesize a query-conditioned Latent Guidance vector. This vector acts as an internal steering mechanism, guiding the model's representations toward optimal reasoning paths. Extensive experiments on mathematical and coding benchmarks demonstrate that PILOT effectively stabilizes reasoning trajectories, consistently outperforming strong baselines (e.g., +8.9% on MATH500) with negligible inference latency.

</details>


### [6] [Lowest Span Confidence: A Zero-Shot Metric for Efficient and Black-Box Hallucination Detection in LLMs](https://arxiv.org/abs/2601.19918)
*Yitong Qiao,Licheng Pan,Yu Mi,Lei Liu,Yue Shen,Fei Sun,Zhixuan Chu*

Main category: cs.CL

TL;DR: 提出LSC（最低跨度置信度）零样本度量方法，仅需单次前向传播和输出概率即可检测LLM幻觉，在资源受限条件下优于现有基线


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法需要昂贵的密集采样策略或白盒LLM状态，这在常见的API场景中不可用或效率低下，需要一种在最小资源假设下有效的检测方法

Method: 提出LSC度量，通过滑动窗口机制评估语义连贯跨度的联合似然，识别不同长度n-gram中最低边际置信度区域，捕捉与事实不一致相关的局部不确定性模式

Result: 在多个SOTA LLM和多样化基准测试中，LSC一致优于现有零样本基线，在资源受限条件下仍能提供强大的检测性能

Conclusion: LSC是一种高效、零样本的幻觉检测方法，仅需最小资源假设，通过分析局部不确定性模式有效识别事实不一致，为实际部署提供了实用解决方案

Abstract: Hallucinations in Large Language Models (LLMs), i.e., the tendency to generate plausible but non-factual content, pose a significant challenge for their reliable deployment in high-stakes environments. However, existing hallucination detection methods generally operate under unrealistic assumptions, i.e., either requiring expensive intensive sampling strategies for consistency checks or white-box LLM states, which are unavailable or inefficient in common API-based scenarios. To this end, we propose a novel efficient zero-shot metric called Lowest Span Confidence (LSC) for hallucination detection under minimal resource assumptions, only requiring a single forward with output probabilities. Concretely, LSC evaluates the joint likelihood of semantically coherent spans via a sliding window mechanism. By identifying regions of lowest marginal confidence across variable-length n-grams, LSC could well capture local uncertainty patterns strongly correlated with factual inconsistency. Importantly, LSC can mitigate the dilution effect of perplexity and the noise sensitivity of minimum token probability, offering a more robust estimate of factual uncertainty. Extensive experiments across multiple state-of-the-art (SOTA) LLMs and diverse benchmarks show that LSC consistently outperforms existing zero-shot baselines, delivering strong detection performance even under resource-constrained conditions.

</details>


### [7] [FastWhisper: Adaptive Self-knowledge Distillation for Real-time Automatic Speech Recognition](https://arxiv.org/abs/2601.19919)
*Junseok Lee,Nahoon Kim,Sangyong Lee,Chang-Jae Chun*

Main category: cs.CL

TL;DR: 本文提出自适应自知识蒸馏（ASKD）方法，通过动态降低对教师模型的依赖来提高学生模型的泛化能力，并将其应用于Whisper模型的压缩，创建了更小更快的FastWhisper模型。


<details>
  <summary>Details</summary>
Motivation: 传统的知识蒸馏方法中，学生模型可能会继承教师模型的缺点，导致泛化能力下降。为了解决这个问题，需要一种能够减少对教师模型依赖、提升学生模型自我训练能力的方法。

Method: 提出自适应自知识蒸馏（ASKD）方法：1）动态调整对教师模型的依赖程度；2）采用自知识蒸馏技术提升学生模型的泛化能力；3）将ASKD应用于Whisper模型，蒸馏出更小的FastWhisper变体。

Result: FastWhisper在词错误率（WER）上比教师模型Whisper降低了1.07%，同时推理速度提高了5倍，实现了更好的性能和更快的计算效率。

Conclusion: ASKD方法能有效缓解传统知识蒸馏中学生模型继承教师模型缺点的问题，通过提升自我训练能力改善了模型泛化性能，并在语音识别任务中验证了其有效性。

Abstract: Knowledge distillation is one of the most effective methods for model compression. Previous studies have focused on the student model effectively training the predictive distribution of the teacher model. However, during training, the student model may inherit the shortcomings of the teacher model, which can lead to a decline in generalization capacity. To mitigate this issue, we propose adaptive self-knowledge distillation (ASKD), which dynamically reduces the dependence of the teacher model to improve the self-training capacity, and performs the self-knowledge distillation method to improve the generalization capacity of the student model. We further distill the Whisper model into a smaller variant, called FastWhisper. In our post-training setting, FastWhisper achieved a word error rate of 1.07% lower than the teacher model Whisper, and its relative inference time was 5 times faster.

</details>


### [8] [Demystifying Multi-Agent Debate: The Role of Confidence and Diversity](https://arxiv.org/abs/2601.19921)
*Xiaochen Zhu,Caiqi Zhang,Yizhou Chi,Tom Stafford,Nigel Collier,Andreas Vlachos*

Main category: cs.CL

TL;DR: 多智能体辩论通过引入多样性初始化和置信度调制更新机制，显著提升LLM推理性能，超越传统多数投票方法


<details>
  <summary>Details</summary>
Motivation: 传统多智能体辩论（MAD）虽然计算成本高，但性能往往不如简单多数投票。研究发现，在智能体同质化和均匀信念更新的情况下，辩论无法可靠改善结果。受人类审议和集体决策研究启发，需要解决两个关键缺失机制

Method: 提出两种轻量级干预：1) 多样性感知初始化，选择更多样化的候选答案池，增加辩论开始时正确假设存在的可能性；2) 置信度调制辩论协议，智能体表达校准后的置信度，并基于他人置信度调整更新

Result: 理论分析表明，多样性感知初始化提高了MAD成功的先验概率而不改变底层更新动态，置信度调制更新使辩论能系统性向正确假设漂移。在六个推理导向的QA基准测试中，该方法持续超越传统MAD和多数投票

Conclusion: 研究将人类审议与基于LLM的辩论联系起来，证明简单而原则性的修改能显著提升辩论效果，为多智能体系统设计提供了新思路

Abstract: Multi-agent debate (MAD) is widely used to improve large language model (LLM) performance through test-time scaling, yet recent work shows that vanilla MAD often underperforms simple majority vote despite higher computational cost. Studies show that, under homogeneous agents and uniform belief updates, debate preserves expected correctness and therefore cannot reliably improve outcomes. Drawing on findings from human deliberation and collective decision-making, we identify two key mechanisms missing from vanilla MAD: (i) diversity of initial viewpoints and (ii) explicit, calibrated confidence communication. We propose two lightweight interventions. First, a diversity-aware initialisation that selects a more diverse pool of candidate answers, increasing the likelihood that a correct hypothesis is present at the start of debate. Second, a confidence-modulated debate protocol in which agents express calibrated confidence and condition their updates on others' confidence. We show theoretically that diversity-aware initialisation improves the prior probability of MAD success without changing the underlying update dynamics, while confidence-modulated updates enable debate to systematically drift to the correct hypothesis. Empirically, across six reasoning-oriented QA benchmarks, our methods consistently outperform vanilla MAD and majority vote. Our results connect human deliberation with LLM-based debate and demonstrate that simple, principled modifications can substantially enhance debate effectiveness.

</details>


### [9] [HEART: A Unified Benchmark for Assessing Humans and LLMs in Emotional Support Dialogue](https://arxiv.org/abs/2601.19922)
*Laya Iyer,Kriti Aggarwal,Sanmi Koyejo,Gail Heyman,Desmond C. Ong,Subhabrata Mukherjee*

Main category: cs.CL

TL;DR: HEART框架首次将人类与LLM放在多轮情感支持对话中进行直接比较，揭示前沿模型在某些共情维度接近或超越人类平均水平，但人类在适应性重构、张力命名和细微语气转换方面仍具优势。


<details>
  <summary>Details</summary>
Motivation: 尽管语言模型快速发展，但缺乏清晰方法来评估它们在人际交往领域（如情绪阅读、语调调整、处理抵抗/沮丧/痛苦时刻）的能力如何与人类相比。当前研究缺乏在相同多轮情感支持对话中直接比较人类与LLM的框架。

Method: 提出HEART框架：1) 为每个对话历史配对人类和模型响应；2) 通过盲审人类评分者和LLM-as-judge评估器集合进行评估；3) 使用基于人际沟通科学的五维度评估标准：人类对齐、共情响应、调适、共鸣和任务遵循。

Result: 1) 多个前沿模型在感知共情和一致性方面接近或超越人类平均水平；2) 人类在适应性重构、张力命名和细微语气转换（尤其在对抗性轮次）方面保持优势；3) 人类与LLM-as-judge偏好在大约80%成对比较中一致，匹配人类间一致性水平；4) 评估理由强调相似的HEART维度，表明评估标准正在趋同。

Conclusion: HEART通过将人类和模型置于平等地位，将支持性对话重新定义为独立于一般推理或语言流畅性的能力维度，为理解模型生成支持在何处与人类社交判断一致/分歧，以及情感对话能力如何随模型规模扩展提供了统一实证基础。

Abstract: Supportive conversation depends on skills that go beyond language fluency, including reading emotions, adjusting tone, and navigating moments of resistance, frustration, or distress. Despite rapid progress in language models, we still lack a clear way to understand how their abilities in these interpersonal domains compare to those of humans. We introduce HEART, the first-ever framework that directly compares humans and LLMs on the same multi-turn emotional-support conversations. For each dialogue history, we pair human and model responses and evaluate them through blinded human raters and an ensemble of LLM-as-judge evaluators. All assessments follow a rubric grounded in interpersonal communication science across five dimensions: Human Alignment, Empathic Responsiveness, Attunement, Resonance, and Task-Following. HEART uncovers striking behavioral patterns. Several frontier models approach or surpass the average human responses in perceived empathy and consistency. At the same time, humans maintain advantages in adaptive reframing, tension-naming, and nuanced tone shifts, particularly in adversarial turns. Human and LLM-as-judge preferences align on about 80 percent of pairwise comparisons, matching inter-human agreement, and their written rationales emphasize similar HEART dimensions. This pattern suggests an emerging convergence in the criteria used to assess supportive quality. By placing humans and models on equal footing, HEART reframes supportive dialogue as a distinct capability axis, separable from general reasoning or linguistic fluency. It provides a unified empirical foundation for understanding where model-generated support aligns with human social judgment, where it diverges, and how affective conversational competence scales with model size.

</details>


### [10] [Table-BiEval: A Self-Supervised, Dual-Track Framework for Decoupling Structure and Content in LLM Evaluation](https://arxiv.org/abs/2601.19923)
*Boxiang Zhao,Qince Li,Zhonghao Wang,Zelin Cao,Yi Wang,Peng Cheng,Bo Lin*

Main category: cs.CL

TL;DR: Table-BiEval：一种基于自监督评估框架的新方法，用于定量评估LLM将自然语言转换为结构化格式的能力，通过解耦结构和内容来测量语义保真度。


<details>
  <summary>Details</summary>
Motivation: 随着LLM发展为自主代理，需要能够准确将自然语言转换为工具调用所需的结构化格式，并将复杂表格信息转换为机器可读规范。但当前评估缺乏有效方法来衡量这种结构保真度，传统文本指标无法检测代码类输出中的语义漂移。

Method: 提出Table-BiEval方法，基于无人工干预的自监督评估框架，利用确定性中间表示，计算内容语义准确性和归一化树编辑距离，从而解耦结构和内容评估。

Result: 对15个最先进的LLM进行实证评估，涵盖层次结构和平面表格两个拓扑维度。结果显示显著变异性，中型模型在结构效率上能意外超越大型模型，深度递归嵌套仍然是当前架构的普遍瓶颈。

Conclusion: Table-BiEval提供了一种有效、无人工的评估框架，能够量化LLM在结构化转换任务中的性能，揭示了当前模型在结构保真度方面的局限性，特别是深度递归处理能力不足。

Abstract: As Large Language Models (LLMs) evolve into autonomous agents, the capability to faithfully translate natural language into rigorous structured formats-essential for tool invocation-and to convert complex tabular information into machine-readable specifications has become paramount. However, current evaluations lack effective methodologies to measure this structural fidelity without costly human intervention, as traditional text metrics fail to detect semantic drift in code-like outputs. This paper proposes Table-BiEval, a novel approach based on a human-free, self-supervised evaluation framework, to assess LLMs performance quantitatively. By leveraging deterministic Intermediate Representations, our framework calculates Content Semantic Accuracy and Normalized Tree Edit Distance to decouple structure from content. Also, it empirically evaluates 15 state-of-the-art LLMs across dual topological dimensions-hierarchical structures and flat tables. The results reveal substantial variability, highlighting that mid-sized models can surprisingly outperform larger counterparts in structural efficiency and confirming that deep recursive nesting remains a universal bottleneck for current architectures.

</details>


### [11] [OPT-Engine: Benchmarking the Limits of LLMs in Optimization Modeling via Complexity Scaling](https://arxiv.org/abs/2601.19924)
*Yitian Chen,Cheng Cheng,Yinan Sun,Zi Ling,Dongdong Ge*

Main category: cs.CL

TL;DR: OPT-ENGINE是一个评估大语言模型优化建模能力的可扩展基准框架，包含10个标准优化任务，研究发现工具集成推理比纯文本推理更具鲁棒性，约束自动化是主要性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在优化建模方面取得了显著进展，但其在复杂现实任务中的自动化建模和问题解决能力边界仍不清楚，需要更系统性的评估框架来理解这些能力限制。

Method: 提出了OPT-ENGINE基准框架，包含10个标准优化任务（5个线性规划和5个混合整数规划），通过控制难度级别来评估LLMs在优化建模中的推理能力，特别关注分布外泛化和性能瓶颈分析。

Result: 实证研究表明：1）随着任务复杂性增加，集成外部求解器的工具推理方法表现出显著更高的鲁棒性，而纯文本推理存在性能上限；2）约束的自动化建模是当前LLMs的主要性能瓶颈。

Conclusion: 该研究为开发下一代面向高级优化的LLMs提供了可操作的指导，强调了工具集成推理的重要性，并指出了约束自动化建模是需要重点突破的关键瓶颈。

Abstract: Large Language Models (LLMs) have demonstrated impressive progress in optimization modeling, fostering a rapid expansion of new methodologies and evaluation benchmarks. However, the boundaries of their capabilities in automated formulation and problem solving remain poorly understood, particularly when extending to complex, real-world tasks. To bridge this gap, we propose OPT-ENGINE, an extensible benchmark framework designed to evaluate LLMs on optimization modeling with controllable and scalable difficulty levels. OPT-ENGINE spans 10 canonical tasks across operations research, with five Linear Programming and five Mixed-Integer Programming. Utilizing OPT-ENGINE, we conduct an extensive study of LLMs' reasoning capabilities, addressing two critical questions: 1.) Do LLMs' performance remain robust when generalizing to out-of-distribution optimization tasks that scale in complexity beyond current benchmark levels? and 2.) At what stage, from problem interpretation to solution generation, do current LLMs encounter the most significant bottlenecks? Our empirical results yield two key insights: first, tool-integrated reasoning with external solvers exhibits significantly higher robustness as task complexity escalates, while pure-text reasoning reaches a ceiling; second, the automated formulation of constraints constitutes the primary performance bottleneck. These findings provide actionable guidance for developing next-generation LLMs for advanced optimization. Our code is publicly available at \textcolor{blue}{https://github.com/Cardinal-Operations/OPTEngine}.

</details>


### [12] [Evaluating Large Language Models for Abstract Evaluation Tasks: An Empirical Study](https://arxiv.org/abs/2601.19925)
*Yinuo Liu,Emre Sezgin,Eric A. Youngstrom*

Main category: cs.CL

TL;DR: 本研究评估了ChatGPT-5、Gemini-3-Pro和Claude-Sonnet-4.5三种大语言模型在学术摘要评审中的一致性和可靠性，发现LLMs在客观标准上与人类评审员有中等一致性，但在主观维度表现较弱。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型能够处理请求和生成文本，但其在评估复杂学术内容方面的可行性仍需进一步研究。本研究旨在探索LLMs在协助科学评审方面的潜力。

Method: 使用160个本地会议摘要，由人类评审员和三种LLM（ChatGPT-5、Gemini-3-Pro、Claude-Sonnet-4.5）按照同一评分标准进行评分。通过组内相关系数（ICCs）分析LLMs之间以及AI-人类的一致性，并使用Bland-Altman图可视化一致性模式和系统偏差。

Result: LLMs之间达成良好到优秀的一致性（ICCs：0.59-0.87）。ChatGPT和Claude在整体质量和内容特定标准上与人类评审员达成中等一致性（ICCs约0.45-0.60），在主观维度上一致性较低（ICCs：0.23-0.38）。Gemini在一半标准上表现一般，在影响力和适用性上无可靠性。三种LLMs与人类平均综合分数的差异可接受或可忽略。

Conclusion: LLMs能够批量处理摘要，在整体质量和客观标准上与人类专家达成中等一致性。通过适当的流程架构，它们可以一致地应用于大量摘要评审。但在主观维度上的较弱表现表明，AI应作为评估的补充工具，而人类专业知识仍然至关重要。

Abstract: Introduction: Large language models (LLMs) can process requests and generate texts, but their feasibility for assessing complex academic content needs further investigation. To explore LLM's potential in assisting scientific review, this study examined ChatGPT-5, Gemini-3-Pro, and Claude-Sonnet-4.5's consistency and reliability in evaluating abstracts compared to one another and to human reviewers. Methods: 160 abstracts from a local conference were graded by human reviewers and three LLMs using one rubric. Composite score distributions across three LLMs and fourteen reviewers were examined. Inter-rater reliability was calculated using intraclass correlation coefficients (ICCs) for within-AI reliability and AI-human concordance. Bland-Altman plots were examined for visual agreement patterns and systematic bias. Results: LLMs achieved good-to-excellent agreement with each other (ICCs: 0.59-0.87). ChatGPT and Claude reached moderate agreement with human reviewers on overall quality and content-specific criteria, with ICCs ~.45-.60 for composite, impression, clarity, objective, and results. They exhibited fair agreement on subjective dimensions, with ICC ranging from 0.23-0.38 for impact, engagement, and applicability. Gemini showed fair agreement on half criteria and no reliability on impact and applicability. Three LLMs showed acceptable or negligible mean difference (ChatGPT=0.24, Gemini=0.42, Claude=-0.02) from the human mean composite scores. Discussion: LLMs could process abstracts in batches with moderate agreement with human experts on overall quality and objective criteria. With appropriate process architecture, they can apply a rubric consistently across volumes of abstracts exceeding feasibility for a human rater. The weaker performance on subjective dimensions indicates that AI should serve a complementary role in evaluation, while human expertise remains essential.

</details>


### [13] [The Grammar of Transformers: A Systematic Review of Interpretability Research on Syntactic Knowledge in Language Models](https://arxiv.org/abs/2601.19926)
*Nora Graichen,Iria de-Dios-Flores,Gemma Boleda*

Main category: cs.CL

TL;DR: 本文对337篇评估Transformer语言模型句法能力的文章进行了系统综述，分析了1,015个模型结果，发现当前研究过度集中于英语、BERT模型和简单句法现象，模型在形式句法现象上表现良好，但在句法-语义接口现象上表现不稳定。


<details>
  <summary>Details</summary>
Motivation: 当前对Transformer语言模型句法能力的研究缺乏系统性评估，研究分布不平衡，过度集中于特定语言、模型和简单现象，需要全面了解模型的实际句法能力及其局限性。

Method: 对337篇相关文章进行系统综述，分析1,015个模型结果，涵盖多种句法现象和可解释性方法，评估模型在不同类型句法任务上的表现。

Result: 研究发现：1）研究过度集中于英语（单一语言）、BERT（单一模型）和词性标注、一致性等简单现象；2）模型在形式句法现象上表现良好；3）在句法-语义接口现象（如约束关系、填充语-空位依赖）上表现不稳定且较弱。

Conclusion: 未来研究需要：1）报告完整数据；2）改进理论建构与方法的一致性；3）增加机制性方法的使用；4）拓宽语言和语言现象的实证范围，实现更全面均衡的评估。

Abstract: We present a systematic review of 337 articles evaluating the syntactic abilities of Transformer-based language models, reporting on 1,015 model results from a range of syntactic phenomena and interpretability methods. Our analysis shows that the state of the art presents a healthy variety of methods and data, but an over-focus on a single language (English), a single model (BERT), and phenomena that are easy to get at (like part of speech and agreement). Results also suggest that TLMs capture these form-oriented phenomena well, but show more variable and weaker performance on phenomena at the syntax-semantics interface, like binding or filler-gap dependencies. We provide recommendations for future work, in particular reporting complete data, better aligning theoretical constructs and methods across studies, increasing the use of mechanistic methods, and broadening the empirical scope regarding languages and linguistic phenomena.

</details>


### [14] [Attribution Techniques for Mitigating Hallucinated Information in RAG Systems: A Survey](https://arxiv.org/abs/2601.19927)
*Yuqing Zhao,Ziyao Liu,Yongsen Zheng,Kwok-Yan Lam*

Main category: cs.CL

TL;DR: 该论文综述了基于归因的技术在检索增强生成（RAG）系统中缓解幻觉问题的应用，提出了RAG幻觉的分类法、统一的归因技术流程，并进行了系统比较。


<details>
  <summary>Details</summary>
Motivation: LLM-based QA系统虽然表现出色，但存在幻觉问题。RAG框架通过引入外部参考来增强响应，但也因检索器和生成器之间的复杂交互引入了新的幻觉形式。现有研究缺乏对这些基于归因技术的统一流程、清晰分类和系统比较。

Method: 1) 提出RAG系统中幻觉类型的分类法；2) 构建归因技术的统一流程；3) 根据目标幻觉类型回顾相关技术；4) 讨论各种技术的优缺点并提供实用指南。

Result: 建立了RAG幻觉的系统分类框架，统一了归因技术流程，为不同幻觉类型提供了针对性解决方案的比较分析，并提供了实践指导。

Conclusion: 该综述填补了RAG系统中基于归因技术研究的空白，为未来研究和实际应用提供了系统框架和实用指南，有助于更有效地缓解LLM幻觉问题。

Abstract: Large Language Models (LLMs)-based question answering (QA) systems play a critical role in modern AI, demonstrating strong performance across various tasks. However, LLM-generated responses often suffer from hallucinations, unfaithful statements lacking reliable references. Retrieval-Augmented Generation (RAG) frameworks enhance LLM responses by incorporating external references but also introduce new forms of hallucination due to complex interactions between the retriever and generator. To address these challenges, researchers have explored attribution-based techniques that ensure responses are verifiably supported by retrieved content. Despite progress, a unified pipeline for these techniques, along with a clear taxonomy and systematic comparison of their strengths and weaknesses, remains lacking. A well-defined taxonomy is essential for identifying specific failure modes within RAG systems, while comparative analysis helps practitioners choose appropriate solutions based on hallucination types and application context. This survey investigates how attribution-based techniques are used within RAG systems to mitigate hallucinations and addresses the gap by: (i) outlining a taxonomy of hallucination types in RAG systems, (ii) presenting a unified pipeline for attribution techniques, (iii) reviewing techniques based on the hallucinations they target, and (iv) discussing strengths and weaknesses with practical guidelines. This work offers insights for future research and practical use of attribution techniques in RAG systems.

</details>


### [15] [Towards a Mechanistic Understanding of Large Reasoning Models: A Survey of Training, Inference, and Failures](https://arxiv.org/abs/2601.19928)
*Yi Hu,Jiaqi Gu,Ruxin Wang,Zijun Yao,Hao Peng,Xiaobao Wu,Jianhui Chen,Muhan Zhang,Liangming Pan*

Main category: cs.CL

TL;DR: 关于大型推理模型（LRMs）机制理解的全面综述，涵盖训练动态、推理机制和意外行为三个核心维度，旨在弥合黑盒性能与机制透明度之间的差距。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习推动的大型推理模型在性能上取得了显著成就，但理解这些模型内部工作机制的研究同样重要，这有助于提升模型的透明度和可靠性。

Method: 本文采用综述研究方法，对大型推理模型的机制理解文献进行系统性整理和分析，主要从三个维度组织研究成果：训练动态、推理机制和意外行为。

Result: 通过综合分析现有研究成果，揭示了大型推理模型在训练过程中的动态变化、推理机制的工作原理以及可能出现的意外行为模式。

Conclusion: 该综述为大型推理模型的机制理解研究提供了系统性框架，并指出了未来研究方向，包括应用可解释性、改进方法论和建立统一理论框架等挑战。

Abstract: Reinforcement learning (RL) has catalyzed the emergence of Large Reasoning Models (LRMs) that have pushed reasoning capabilities to new heights. While their performance has garnered significant excitement, exploring the internal mechanisms driving these behaviors has become an equally critical research frontier. This paper provides a comprehensive survey of the mechanistic understanding of LRMs, organizing recent findings into three core dimensions: 1) training dynamics, 2) reasoning mechanisms, and 3) unintended behaviors. By synthesizing these insights, we aim to bridge the gap between black-box performance and mechanistic transparency. Finally, we discuss under-explored challenges to outline a roadmap for future mechanistic studies, including the need for applied interpretability, improved methodologies, and a unified theoretical framework.

</details>


### [16] [Stingy Context: 18:1 Hierarchical Code Compression for LLM Auto-Coding](https://arxiv.org/abs/2601.19929)
*David Linus Ostby*

Main category: cs.CL

TL;DR: Stingy Context提出了一种基于层次树压缩的方案，能在自动编码任务中实现18:1的LLM上下文压缩比，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在处理大型代码库时面临上下文窗口有限的问题，导致处理长代码时出现"迷失在中间"效应，影响任务完成质量。

Method: 采用TREEFRAG分解技术，构建层次树结构压缩代码，将239k tokens的代码库压缩至11k tokens，同时保持任务保真度。

Result: 在12个前沿模型上测试40个真实世界问题，达到94-97%的成功率，优于平面压缩方法，有效缓解迷失在中间效应。

Conclusion: Stingy Context提供了一种高效的大规模代码库压缩方案，能在保持任务性能的同时大幅降低LLM上下文需求。

Abstract: We introduce Stingy Context, a hierarchical tree-based compression scheme achieving 18:1 reduction in LLM context for auto-coding tasks. Using our TREEFRAG exploit decomposition, we reduce a real source code base of 239k tokens to 11k tokens while preserving task fidelity. Empirical results across 12 Frontier models show 94 to 97% success on 40 real-world issues at low cost, outperforming flat methods and mitigating lost-in-the-middle effects.

</details>


### [17] [SDUs DAISY: A Benchmark for Danish Culture](https://arxiv.org/abs/2601.19930)
*Jacob Nielsen,Stine L. Beltoft,Peter Schneider-Kamp,Lukas Galke Poech*

Main category: cs.CL

TL;DR: Daisy是一个基于丹麦文化经典2006的新文化基准数据集，包含741个封闭式问答对，涵盖从公元前1300年到当代的丹麦文化遗产


<details>
  <summary>Details</summary>
Motivation: 需要为丹麦文化创建一个基于文化遗产的基准测试，以评估对丹麦文化深入知识的理解，而不仅仅是主流信息

Method: 从丹麦文化经典2006中选取文化作品，查询对应的维基百科页面，使用语言模型生成随机问题，然后进行人工审核和修正

Result: 创建了包含741个封闭式问答对的Daisy数据集，涵盖130个文化作品，时间跨度从公元前1300年到当代，问题类型包括核心和边缘知识

Conclusion: Daisy为评估对丹麦文化遗产的理解提供了一个全面且经过人工验证的基准，有助于推动文化知识评估的研究

Abstract: We introduce a new benchmark for Danish culture via cultural heritage, Daisy, based on the curated topics from the Danish Culture Canon 2006. For each artifact in the culture canon, we query the corresponding Wikipedia page and have a language model generate random questions. This yields a sampling strategy within each work, with a mix of central of peripheral questions for each work, not only knowledge of mainstream information, but also in-depth cornerstones defining the heritage of Danish Culture, defined by the Canon committee. Each question-answer pair is humanly approved or corrected in the final dataset consisting of 741 close-ended question answer pairs covering topics, from 1300 BC. archaeological findings, 1700 century poems and musicals pieces to contemporary pop music and Danish design and architecture.

</details>


### [18] [CascadeMind at SemEval-2026 Task 4: A Hybrid Neuro-Symbolic Cascade for Narrative Similarity](https://arxiv.org/abs/2601.19931)
*Sebastien Kawada,Dylan Holyoak*

Main category: cs.CL

TL;DR: 该论文提出了一个混合神经符号系统用于叙事故事相似性任务，通过神经自一致性投票与多尺度叙事分析集成相结合，在不确定情况下使用符号方法作为决胜机制。


<details>
  <summary>Details</summary>
Motivation: 解决叙事故事相似性判断中的真正模糊情况，当神经网络投票产生完美平局时，需要更可靠的决策机制来处理这种不确定性。

Method: 1. 神经组件：使用大语言模型进行多轮并行投票，采用绝对多数阈值决策；2. 符号组件：当投票出现完美平局时，激活多尺度叙事分析集成，结合词汇重叠、语义嵌入、故事语法结构、事件链对齐和叙事张力曲线五种信号；3. 级联架构：先尝试神经决策，不确定时再使用符号方法。

Result: 在开发集上达到81%的准确率，证明在真正模糊的叙事比较中，选择性地将决策推迟给符号方法可以增强神经预测。

Conclusion: 混合神经符号系统通过级联决策策略有效处理叙事相似性判断中的不确定性，展示了符号方法在增强神经网络对模糊案例处理能力方面的价值。

Abstract: We present a hybrid neuro-symbolic system for the SemEval-2026 Task 4 on Narrative Story Similarity. Our approach combines neural self-consistency voting with a novel Multi-Scale Narrative Analysis Ensemble that operates as a symbolic tiebreaker. The neural network component uses a large language model with multiple parallel votes, applying a supermajority threshold for confident decisions and escalating uncertain cases to additional voting rounds. When votes result in a perfect tie, a symbolic ensemble combining five narrative similarity signals (lexical overlap, semantic embeddings, story grammar structure, event chain alignment, and narrative tension curves) provides the final decision. Our cascade architecture achieves 81% accuracy on the development set, demonstrating that selective deferral to symbolic methods can enhance neural predictions on genuinely ambiguous narrative comparisons.

</details>


### [19] ["Newspaper Eat" Means "Not Tasty": A Taxonomy and Benchmark for Coded Languages in Real-World Chinese Online Reviews](https://arxiv.org/abs/2601.19932)
*Ruyuan Wan,Changye Li,Ting-Hao 'Kenneth' Huang*

Main category: cs.CL

TL;DR: CodedLang：一个包含7,744个中文谷歌地图评论的数据集，其中900条带有编码语言的标注，用于研究语言模型处理编码语言的能力。


<details>
  <summary>Details</summary>
Motivation: 编码语言是人类交流的重要组成部分，但当前语言模型处理编码语言的能力较差，且缺乏真实世界数据集和清晰分类法限制了研究进展。

Method: 构建CodedLang数据集，开发包含七类编码策略的分类法（如语音、字形、跨语言替换），在编码语言检测、分类和评论评分预测任务上评估语言模型，并进行语音分析。

Result: 即使是强大的语言模型也难以识别和理解编码语言，许多编码表达依赖于基于发音的策略，语音分析揭示了编码与解码形式的关系。

Conclusion: 编码语言是现实世界NLP系统面临的重要且未被充分探索的挑战，需要更多研究来改进模型处理这种语言现象的能力。

Abstract: Coded language is an important part of human communication. It refers to cases where users intentionally encode meaning so that the surface text differs from the intended meaning and must be decoded to be understood. Current language models handle coded language poorly. Progress has been limited by the lack of real-world datasets and clear taxonomies. This paper introduces CodedLang, a dataset of 7,744 Chinese Google Maps reviews, including 900 reviews with span-level annotations of coded language. We developed a seven-class taxonomy that captures common encoding strategies, including phonetic, orthographic, and cross-lingual substitutions. We benchmarked language models on coded language detection, classification, and review rating prediction. Results show that even strong models can fail to identify or understand coded language. Because many coded expressions rely on pronunciation-based strategies, we further conducted a phonetic analysis of coded and decoded forms. Together, our results highlight coded language as an important and underexplored challenge for real-world NLP systems.

</details>


### [20] [Text-to-State Mapping for Non-Resolution Reasoning: The Contradiction-Preservation Principle](https://arxiv.org/abs/2601.19933)
*Kei Saito*

Main category: cs.CL

TL;DR: 本文提出了将自然语言映射到非分辨率推理框架的形式化方法，使用文本到状态映射函数φ将语言输入转换为叠加状态，保持语义模糊性而非强制过早的解释坍缩。


<details>
  <summary>Details</summary>
Motivation: 非分辨率推理框架虽然建立了保持模糊性的计算架构，但自然语言如何映射到这些数学结构的问题仍未解决。需要构建从原始文本到形式化状态空间的算法桥梁。

Method: 引入文本到状态映射函数φ，形式化矛盾保持原则，要求真正模糊的表达式在状态表示中保持非零熵，并利用现有大型语言模型作为解释生成器开发提取协议。

Result: 在涵盖词汇、结构和语用模糊性的68个测试句子上进行实证验证，模糊输入的映射实现平均香农熵H(S)=1.087比特，而基线单解释方法产生H(S)=0.000。

Conclusion: 该框架提供了从原始文本到NRR算子作用的形式化状态空间之间的缺失算法桥梁，实现了语言模型推理中的架构坍缩延迟。

Abstract: Non-Resolution Reasoning (NRR) provides a formal framework for maintaining semantic ambiguity rather than forcing premature interpretation collapse. While the foundational architecture establishes state spaces and operators for ambiguity-preserving computation, the critical question of how natural language maps to these mathematical structures remains open. This paper introduces the text-to-state mapping function φ that transforms linguistic input into superposition states within the NRR framework. We formalize the Contradiction-Preservation Principle, which requires that genuinely ambiguous expressions maintain non-zero entropy in their state representations, and develop extraction protocols using existing Large Language Models as interpretation generators. Empirical validation across 68 test sentences spanning lexical, structural, and pragmatic ambiguity demonstrates that our mapping achieves mean Shannon entropy H(S) = 1.087 bits for ambiguous inputs while baseline single-interpretation approaches yield H(S) = 0.000. The framework provides the missing algorithmic bridge between raw text and the formal state spaces on which NRR operators act, enabling architectural collapse deferment in language model inference.

</details>


### [21] [Quantifying non deterministic drift in large language models](https://arxiv.org/abs/2601.19934)
*Claire Nicholson*

Main category: cs.CL

TL;DR: 本文通过重复运行实验，量化了大型语言模型在相同提示下输出变异性的基准行为漂移，发现即使在温度0.0时也存在非确定性，并揭示了模型大小、部署方式和提示类型对漂移模式的影响。


<details>
  <summary>Details</summary>
Motivation: 在LLMs实际应用中，即使温度和解码参数固定，相同提示并不总是产生相同输出。现有研究缺乏对基准行为漂移的系统性量化，本文旨在建立无稳定化技术下的参考基准，为未来漂移缓解方法评估提供依据。

Method: 对gpt-4o-mini和llama3.1-8b两个公开模型进行重复运行实验，涵盖五种提示类别，使用精确重复、扰动输入和重用模式，在温度0.0和0.7下测量。采用唯一输出比例、词汇相似度和词数统计三种指标量化漂移。

Result: 结果显示即使在温度0.0时也存在非确定性，模型大小、部署方式和提示类型对漂移模式有显著影响。gpt-4o-mini和llama3.1-8b表现出不同的变异模式，部署类型（云端vs本地）也影响输出稳定性。

Conclusion: 本研究建立了LLMs基准行为漂移的系统性经验基线，为评估未来漂移缓解和控制方法提供了参考点。同时指出了词汇指标的局限性，并强调了新兴语义方法的重要性。

Abstract: Large language models (LLMs) are widely used for tasks ranging from summarisation to decision support. In practice, identical prompts do not always produce identical outputs, even when temperature and other decoding parameters are fixed. In this work, we conduct repeated-run experiments to empirically quantify baseline behavioural drift, defined as output variability observed when the same prompt is issued multiple times under operator-free conditions. We evaluate two publicly accessible models, gpt-4o-mini and llama3.1-8b, across five prompt categories using exact repeats, perturbed inputs, and reuse modes at temperatures of 0.0 and 0.7. Drift is measured using unique output fractions, lexical similarity, and word count statistics, enabling direct comparison across models, prompting modes, and deployment types. The results show that nondeterminism persists even at temperature 0.0, with distinct variability patterns by model size, deployment, and prompt type. We situate these findings within existing work on concept drift, behavioural drift, and infrastructure-induced nondeterminism, discuss the limitations of lexical metrics, and highlight emerging semantic approaches. By establishing a systematic empirical baseline in the absence of stabilisation techniques, this study provides a reference point for evaluating future drift mitigation and control methods.

</details>


### [22] [Mem2ActBench: A Benchmark for Evaluating Long-Term Memory Utilization in Task-Oriented Autonomous Agents](https://arxiv.org/abs/2601.19935)
*Yiting Shen,Kun Li,Wei Zhou,Songlin Hu*

Main category: cs.CL

TL;DR: Mem2ActBench 是一个新的基准测试，用于评估LLM智能体能否主动利用长期记忆来执行基于工具的任务，包括选择适当工具和参数基础，填补了现有基准主要测试被动记忆检索的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要测试智能体在响应明确问题时被动检索孤立事实的能力，但未能评估更关键的能力——主动应用记忆来执行任务。特别是在持久助手使用场景中，用户会在长时间、中断的交互中提及相同话题，期望先前建立的偏好和任务状态能够被隐式应用。

Method: 1. 构建Mem2ActBench基准：通过自动化流水线合并异构数据源（ToolACE、BFCL、Oasst1），通过一致性建模解决冲突，合成2,029个会话，平均每个会话包含12个用户-助手-工具轮次。
2. 从这些记忆链中，采用反向生成方法产生400个工具使用任务。
3. 通过人工评估确认91.3%的任务是强记忆依赖的。
4. 在七个记忆框架上进行实验评估。

Result: 1. 成功构建了包含2,029个会话的Mem2ActBench数据集，平均每个会话包含12个用户-助手-工具轮次。
2. 生成了400个工具使用任务，其中91.3%被人工评估确认为强记忆依赖。
3. 在七个记忆框架上的实验表明，当前系统在主动利用记忆进行参数基础方面仍然不足。

Conclusion: Mem2ActBench填补了评估LLM智能体主动应用长期记忆执行工具型任务的基准空白。实验结果表明，现有系统在主动利用记忆进行参数基础方面存在不足，凸显了需要更有效的方法来评估和改进任务执行中的记忆应用能力。

Abstract: Large Language Model (LLM)-based agents are increasingly deployed for complex, tool-based tasks where long-term memory is critical to driving actions. Existing benchmarks, however, primarily test a angent's ability to passively retrieve isolated facts in response to explicit questions. They fail to evaluate the more crucial capability of actively applying memory to execute tasks. To address this gap, we introduce \textsc{Mem2ActBench}, a benchmark for evaluating whether agents can proactively leverage long-term memory to execute tool-based actions by selecting appropriate tools and grounding their parameters. The benchmark simulates persistent assistant usage, where users mention the same topic across long, interrupted interactions and expect previously established preferences and task states to be implicitly applied. We build the dataset with an automated pipeline that merges heterogeneous sources (ToolACE, BFCL, Oasst1), resolves conflicts via consistency modeling, and synthesizes 2,029 sessions with 12 user--assistant--tool turns on average. From these memory chains, a reverse-generation method produces 400 tool-use tasks, with human evaluation confirming 91.3\% are strongly memory-dependent. Experiments on seven memory frameworks show that current systems remain inadequate at actively utilizing memory for parameter grounding, highlighting the need for more effective approaches to evaluate and improve memory application in task execution.

</details>


### [23] [Benchmarking von ASR-Modellen im deutschen medizinischen Kontext: Eine Leistungsanalyse anhand von Anamnesegesprächen](https://arxiv.org/abs/2601.19945)
*Thomas Schuster,Julius Trögele,Nico Döring,Robin Krüger,Matthieu Hoffmann,Holger Friedrich*

Main category: cs.CL

TL;DR: 该研究评估了29种德语医疗对话ASR模型，发现性能差异显著，最佳模型WER低于3%，但医疗术语和方言识别仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 德语医疗场景缺乏专门的ASR评估基准，特别是考虑方言影响的医疗对话识别，需要填补这一空白。

Method: 构建模拟医患对话数据集，评估29个ASR模型（包括Whisper、Voxtral、Wav2Vec2等开源模型和AssemblyAI、Deepgram等商业API），使用WER、CER、BLEU三种指标。

Result: 模型性能差异显著：最佳系统WER可低于3%，但其他模型在医疗术语和方言变体上错误率较高，显示医疗领域方言识别仍是技术挑战。

Conclusion: 德语医疗ASR已取得良好进展，但医疗术语和方言识别仍需改进，未来需进行语义质量分析以进一步提升实用价值。

Abstract: Automatic Speech Recognition (ASR) offers significant potential to reduce the workload of medical personnel, for example, through the automation of documentation tasks. While numerous benchmarks exist for the English language, specific evaluations for the German-speaking medical context are still lacking, particularly regarding the inclusion of dialects. In this article, we present a curated dataset of simulated doctor-patient conversations and evaluate a total of 29 different ASR models. The test field encompasses both open-weights models from the Whisper, Voxtral, and Wav2Vec2 families as well as commercial state-of-the-art APIs (AssemblyAI, Deepgram). For evaluation, we utilize three different metrics (WER, CER, BLEU) and provide an outlook on qualitative semantic analysis. The results demonstrate significant performance differences between the models: while the best systems already achieve very good Word Error Rates (WER) of partly below 3%, the error rates of other models, especially concerning medical terminology or dialect-influenced variations, are considerably higher.

</details>


### [24] [On the Effectiveness of LLM-Specific Fine-Tuning for Detecting AI-Generated Text](https://arxiv.org/abs/2601.20006)
*Michał Gromadzki,Anna Wróblewska,Agnieszka Kaliska*

Main category: cs.CL

TL;DR: 论文提出基于大规模语料库和新型训练策略的AI生成文本检测方法，通过构建人类文本和AI生成文本的亿级语料库，开发多种检测模型，在包含21个大语言模型的基准测试中达到99.6%的token级准确率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型生成文本越来越接近人类写作，在教育、出版和数字安全等领域面临真实性验证的挑战，检测AI生成文本已成为关键的技术和伦理问题。

Method: 构建了10亿token的人类文本语料库（涵盖多种体裁）和19亿token的AI生成文本语料库（通过提示多种LLM在不同领域生成）。开发多种检测模型，提出两种新颖的训练范式：按单个LLM微调和按LLM家族微调。

Result: 在覆盖21个大语言模型的1亿token基准测试中，最佳微调检测器达到99.6%的token级准确率，显著优于现有开源基线方法。

Conclusion: 研究表明基于大规模语料库和专门训练策略的AI文本检测方法具有极高准确性，为解决AI生成文本验证问题提供了有效技术方案。

Abstract: The rapid progress of large language models has enabled the generation of text that closely resembles human writing, creating challenges for authenticity verification in education, publishing, and digital security. Detecting AI-generated text has therefore become a crucial technical and ethical issue. This paper presents a comprehensive study of AI-generated text detection based on large-scale corpora and novel training strategies. We introduce a 1-billion-token corpus of human-authored texts spanning multiple genres and a 1.9-billion-token corpus of AI-generated texts produced by prompting a variety of LLMs across diverse domains. Using these resources, we develop and evaluate numerous detection models and propose two novel training paradigms: Per LLM and Per LLM family fine-tuning. Across a 100-million-token benchmark covering 21 large language models, our best fine-tuned detector achieves up to $99.6\%$ token-level accuracy, substantially outperforming existing open-source baselines.

</details>


### [25] [LinguaMap: Which Layers of LLMs Speak Your Language and How to Tune Them?](https://arxiv.org/abs/2601.20009)
*J. Ben Tamo,Daniel Carlander-Reuterfelt,Jonathan Rubin,Dezhi Hong,Mingxian Wang,Oleg Poliannikov*

Main category: cs.CL

TL;DR: 该研究通过分析大型语言模型在非英语任务中的语言控制问题，发现两种关键失败模式，并提出基于层定位的选择性微调方法，仅微调最后3-5%的参数即可实现98%以上的语言一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管有多语言预训练，大型语言模型在非英语任务中仍面临语言控制问题——无法按照预期语言进行响应。研究者识别出两种关键失败模式：多语言转移瓶颈（正确语言但错误任务响应）和语言一致性瓶颈（正确任务响应但错误语言）。

Method: 1. 设计四场景评估协议，覆盖MMLU、MGSM和XQuAD基准测试；2. 扩展logit lens分析，逐层追踪语言概率并计算隐藏状态的跨语言语义相似性；3. 基于分析结果，提出选择性微调方法，仅微调负责语言控制的最后几层。

Result: 1. 揭示了语言模型的三阶段内部结构：早期层将输入对齐到共享语义空间，中间层执行任务推理，后期层驱动语言特定生成；2. 在Qwen-3-32B和Bloom-7.1B上，仅微调3-5%的参数即可实现超过98%的语言一致性，且不牺牲任务准确性；3. 该方法的性能与全范围微调几乎相同，但计算资源消耗显著减少。

Conclusion: 该研究首次利用语言控制的层定位实现高效的多语言适应，通过选择性微调最后几层即可显著提升语言一致性，为大型语言模型的多语言优化提供了计算高效的解决方案。

Abstract: Despite multilingual pretraining, large language models often struggle with non-English tasks, particularly in language control, the ability to respond in the intended language. We identify and characterize two key failure modes: the multilingual transfer bottleneck (correct language, incorrect task response) and the language consistency bottleneck (correct task response, wrong language). To systematically surface these issues, we design a four-scenario evaluation protocol spanning MMLU, MGSM, and XQuAD benchmarks. To probe these issues with interpretability, we extend logit lens analysis to track language probabilities layer by layer and compute cross-lingual semantic similarity of hidden states. The results reveal a three-phase internal structure: early layers align inputs into a shared semantic space, middle layers perform task reasoning, and late layers drive language-specific generation. Guided by these insights, we introduce selective fine-tuning of only the final layers responsible for language control. On Qwen-3-32B and Bloom-7.1B, this method achieves over 98 percent language consistency across six languages while fine-tuning only 3-5 percent of parameters, without sacrificing task accuracy. Importantly, this result is nearly identical to that of full-scope fine-tuning (for example, above 98 percent language consistency for both methods across all prompt scenarios) but uses a fraction of the computational resources. To the best of our knowledge, this is the first approach to leverage layer-localization of language control for efficient multilingual adaptation.

</details>


### [26] [Semantic Uncertainty Quantification of Hallucinations in LLMs: A Quantum Tensor Network Based Method](https://arxiv.org/abs/2601.20026)
*Pragatheeswaran Vipulanandan,Kamal Premaratne,Dilip Sarkar*

Main category: cs.CL

TL;DR: 该论文提出了一种基于量子张量网络的不确定性量化框架，用于检测大语言模型的幻觉，通过语义等价聚类和熵最大化策略来识别不可靠输出。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然生成能力强，但容易产生"捏造"内容，输出结果即使在同一提示下也任意变化且不可靠，需要一种原则性和可解释的幻觉检测方法。

Method: 采用量子张量网络管道，提出量子物理启发的token序列概率不确定性量化框架，基于语义等价进行聚类，并引入熵最大化策略来优先选择高确定性、语义连贯的输出。

Result: 在TriviaQA、NQ、SVAMP和SQuAD数据集上进行了116个实验，涵盖Mistral-7B、LLaMA-2/3等多种架构，在AUROC和AURAC指标上均优于现有基线方法，且在不同生成长度和量化级别下保持稳健。

Conclusion: 该量子启发的不确定性量化框架为幻觉检测提供了原则性和可解释的方案，能够在资源受限部署中保持可靠性，为需要人工监督的情况提供了实用指导。

Abstract: Large language models (LLMs) exhibit strong generative capabilities but remain vulnerable to confabulations, fluent yet unreliable outputs that vary arbitrarily even under identical prompts. Leveraging a quantum tensor network based pipeline, we propose a quantum physics inspired uncertainty quantification framework that accounts for aleatoric uncertainty in token sequence probability for semantic equivalence based clustering of LLM generations. This offers a principled and interpretable scheme for hallucination detection. We further introduce an entropy maximization strategy that prioritizes high certainty, semantically coherent outputs and highlights entropy regions where LLM decisions are likely to be unreliable, offering practical guidelines for when human oversight is warranted. We evaluate the robustness of our scheme under different generation lengths and quantization levels, dimensions overlooked in prior studies, demonstrating that our approach remains reliable even in resource constrained deployments. A total of 116 experiments on TriviaQA, NQ, SVAMP, and SQuAD across multiple architectures including Mistral-7B, Mistral-7B-instruct, Falcon-rw-1b, LLaMA-3.2-1b, LLaMA-2-13b-chat, LLaMA-2-7b-chat, LLaMA-2-13b, and LLaMA-2-7b show consistent improvements in AUROC and AURAC over state of the art baselines.

</details>


### [27] [TAIGR: Towards Modeling Influencer Content on Social Media via Structured, Pragmatic Inference](https://arxiv.org/abs/2601.20032)
*Nishanth Sridhar Nakshatri,Eylon Caplan,Rajkumar Pujari,Dan Goldwasser*

Main category: cs.CL

TL;DR: 提出TAIGR框架，通过三阶段结构化分析健康影响者话语，以解决传统基于声明的验证方法无法捕捉语用意义的问题。


<details>
  <summary>Details</summary>
Motivation: 健康影响者在塑造公众信念方面作用日益重要，但其内容通常通过对话叙事和修辞策略传达，而非明确的事实声明。传统的基于声明的验证方法难以捕捉影响者话语的语用意义。

Method: 提出TAIGR框架，包含三个阶段：1)识别核心影响者建议（takeaway）；2)构建捕捉影响者建议论证的论证图；3)使用因子图进行概率推理以验证建议。

Result: 在健康影响者视频转录本的内容验证任务中评估TAIGR，显示准确验证需要建模话语的语用和论证结构，而非将转录本视为平面声明集合。

Conclusion: TAIGR框架能有效分析影响者话语的结构化论证，比传统声明验证方法更好地捕捉话语的语用意义，为内容验证提供了新方法。

Abstract: Health influencers play a growing role in shaping public beliefs, yet their content is often conveyed through conversational narratives and rhetorical strategies rather than explicit factual claims. As a result, claim-centric verification methods struggle to capture the pragmatic meaning of influencer discourse. In this paper, we propose TAIGR (Takeaway Argumentation Inference with Grounded References), a structured framework designed to analyze influencer discourse, which operates in three stages: (1) identifying the core influencer recommendation--takeaway; (2) constructing an argumentation graph that captures influencer justification for the takeaway; (3) performing factor graph-based probabilistic inference to validate the takeaway. We evaluate TAIGR on a content validation task over influencer video transcripts on health, showing that accurate validation requires modeling the discourse's pragmatic and argumentative structure rather than treating transcripts as flat collections of claims.

</details>


### [28] [VERGE: Formal Refinement and Guidance Engine for Verifiable LLM Reasoning](https://arxiv.org/abs/2601.20055)
*Vikash Singh,Darion Cassel,Nathaniel Weir,Nick Feng,Sam Bayless*

Main category: cs.CL

TL;DR: VERGE是一个神经符号框架，通过结合LLM和SMT求解器进行验证引导的迭代精炼，提高LLM输出的逻辑正确性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型具备语法流畅性，但在高风险领域中确保其逻辑正确性仍然是一个基本挑战。需要解决LLM在逻辑推理方面的可靠性问题。

Method: 框架将LLM输出分解为原子声明，自动形式化为一阶逻辑，并使用自动定理证明验证逻辑一致性。包含三个创新：(1)通过形式语义等价检查实现多模型共识，(2)语义路由将不同类型声明导向适当验证策略，(3)通过最小修正子集精确定位逻辑错误。

Result: 使用GPT-OSS-120B模型，VERGE在一组推理基准测试中相比单次方法在收敛时平均性能提升18.7%。

Conclusion: 这种混合方法在可能时提供形式保证，在其他情况下提供共识验证，推进可信AI的发展。

Abstract: Despite the syntactic fluency of Large Language Models (LLMs), ensuring their logical correctness in high-stakes domains remains a fundamental challenge. We present a neurosymbolic framework that combines LLMs with SMT solvers to produce verification-guided answers through iterative refinement. Our approach decomposes LLM outputs into atomic claims, autoformalizes them into first-order logic, and verifies their logical consistency using automated theorem proving. We introduce three key innovations: (1) multi-model consensus via formal semantic equivalence checking to ensure logic-level alignment between candidates, eliminating the syntactic bias of surface-form metrics, (2) semantic routing that directs different claim types to appropriate verification strategies: symbolic solvers for logical claims and LLM ensembles for commonsense reasoning, and (3) precise logical error localization via Minimal Correction Subsets (MCS), which pinpoint the exact subset of claims to revise, transforming binary failure signals into actionable feedback. Our framework classifies claims by their logical status and aggregates multiple verification signals into a unified score with variance-based penalty. The system iteratively refines answers using structured feedback until acceptance criteria are met or convergence is achieved. This hybrid approach delivers formal guarantees where possible and consensus verification elsewhere, advancing trustworthy AI. With the GPT-OSS-120B model, VERGE demonstrates an average performance uplift of 18.7% at convergence across a set of reasoning benchmarks compared to single-pass approaches.

</details>


### [29] [Counterfactual Cultural Cues Reduce Medical QA Accuracy in LLMs: Identifier vs Context Effects](https://arxiv.org/abs/2601.20102)
*Amirhossein Haji Mohammad Rezaei,Zahra Shakeri*

Main category: cs.CL

TL;DR: 医疗AI在遇到非决定性文化信息时不应改变临床诊断，但当前模型在文化提示下诊断准确性显著下降，特别是标识符和上下文同时出现时


<details>
  <summary>Details</summary>
Motivation: 构建可持续和公平的医疗保健需要医学语言模型在面对非决定性文化信息时保持临床诊断的正确性，避免因文化偏见影响诊断准确性

Method: 创建反事实基准测试，将150个MedQA测试项目扩展为1650个变体，插入三种文化相关元素（标识符、上下文线索或其组合），涵盖三个文化群体，并加入长度匹配的中性对照，由临床医生验证所有变体中正确答案保持不变

Result: 文化线索显著影响所有模型的准确性，当标识符和上下文同时出现时准确率下降最大（下降3-7个百分点），中性编辑产生较小非系统性变化。超过一半基于文化的解释最终得出错误答案，文化参考推理与诊断失败相关

Conclusion: 当前医疗语言模型在面对文化信息时表现出系统性诊断偏差，需要进一步评估和缓解文化引起的诊断错误，研究发布了提示和增强数据以支持相关工作

Abstract: Engineering sustainable and equitable healthcare requires medical language models that do not change clinically correct diagnoses when presented with non-decisive cultural information. We introduce a counterfactual benchmark that expands 150 MedQA test items into 1650 variants by inserting culture-related (i) identifier tokens, (ii) contextual cues, or (iii) their combination for three groups (Indigenous Canadian, Middle-Eastern Muslim, Southeast Asian), plus a length-matched neutral control, where a clinician verified that the gold answer remains invariant in all variants. We evaluate GPT-5.2, Llama-3.1-8B, DeepSeek-R1, and MedGemma (4B/27B) under option-only and brief-explanation prompting. Across models, cultural cues significantly affect accuracy (Cochran's Q, $p<10^-14$), with the largest degradation when identifier and context co-occur (up to 3-7 percentage points under option-only prompting), while neutral edits produce smaller, non-systematic changes. A human-validated rubric ($κ=0.76$) applied via an LLM-as-judge shows that more than half of culturally grounded explanations end in an incorrect answer, linking culture-referential reasoning to diagnostic failure. We release prompts and augmentations to support evaluation and mitigation of culturally induced diagnostic errors.

</details>


### [30] [FFE-Hallu:Hallucinations in Fixed Figurative Expressions:Benchmark of Idioms and Proverbs in the Persian Language](https://arxiv.org/abs/2601.20105)
*Faezeh Hosseini,Mohammadali Yousefzadeh,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: 首个针对波斯语固定比喻表达（FFEs）幻觉的全面基准测试FFEHallu，揭示LLMs在比喻语言理解和文化基础方面的系统性缺陷。


<details>
  <summary>Details</summary>
Motivation: 固定比喻表达（如习语和谚语）对大型语言模型构成持续挑战，这些表达具有文化基础、非组合性和固定性，容易导致比喻幻觉。目前缺乏针对波斯语等代表不足语言的全面评估基准。

Method: 构建FFEHallu基准，包含600个精心策划的实例，涵盖三个互补任务：1）从意义生成FFE；2）检测四个受控构建类别中的虚构FFE；3）从英语到波斯语的FFE翻译。评估六个最先进的多语言LLM。

Result: 模型在比喻能力和文化基础方面存在系统性弱点。虽然GPT4.1在拒绝虚构FFE和检索真实表达方面表现相对较强，但大多数模型难以可靠区分真实表达与高质量虚构，在跨语言翻译中经常产生幻觉。

Conclusion: 当前LLM在处理比喻语言方面存在重大差距，需要有针对性的基准来评估和减轻比喻幻觉问题。

Abstract: Figurative language, particularly fixed figurative expressions (FFEs) such as idioms and proverbs, poses persistent challenges for large language models (LLMs). Unlike literal phrases, FFEs are culturally grounded, largely non-compositional, and conventionally fixed, making them especially vulnerable to figurative hallucination. We define figurative hallucination as the generation or endorsement of expressions that sound idiomatic and plausible but do not exist as authentic figurative expressions in the target language. We introduce FFEHallu, the first comprehensive benchmark for evaluating figurative hallucination in LLMs, with a focus on Persian, a linguistically rich yet underrepresented language. FFEHallu consists of 600 carefully curated instances spanning three complementary tasks: (i) FFE generation from meaning, (ii) detection of fabricated FFEs across four controlled construction categories, and (iii) FFE to FFE translation from English to Persian. Evaluating six state of the art multilingual LLMs, we find systematic weaknesses in figurative competence and cultural grounding. While models such as GPT4.1 demonstrate relatively strong performance in rejecting fabricated FFEs and retrieving authentic ones, most models struggle to reliably distinguish real expressions from high quality fabrications and frequently hallucinate during cross lingual translation. These findings reveal substantial gaps in current LLMs handling of figurative language and underscore the need for targeted benchmarks to assess and mitigate figurative hallucination.

</details>


### [31] [Rewarding Intellectual Humility Learning When Not To Answer In Large Language Models](https://arxiv.org/abs/2601.20126)
*Abha Jha,Akanksha Mahajan,Ashwath Vaithinathan Aravindan,Praveen Saravanan,Sai Sailaja Policharla,Sonal Chaturbhuj Gehlot*

Main category: cs.CL

TL;DR: 该研究提出了一种基于可验证奖励的强化学习（RLVR）训练范式，通过奖励模型在不确定时选择"我不知道"来减少幻觉，在多项选择题任务上取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常产生幻觉或不可验证的内容，在事实性领域降低了其可靠性。需要一种训练范式来明确奖励模型在不确定时选择"我不知道"以促进智力谦逊。

Method: 使用可验证奖励的强化学习（RLVR），采用三元奖励结构（-1，r_abs，1），其中r_abs是弃权奖励值。在MedMCQA和Hendrycks Math基准上对Granite-3.3-2B-Instruct和Qwen-3-4B-Instruct进行微调和评估，研究不同弃权奖励结构的影响，并探索将RLVR与监督微调策略结合的效果。

Result: 适度的弃权奖励（r_abs ≈ -0.25到0.3）能持续减少错误回答，同时不会严重降低多项选择题的准确率。较大模型对弃权激励表现出更强的鲁棒性。在开放式问答任务中，由于探索不足存在局限性，但可以通过监督弃权训练部分缓解。

Conclusion: 可验证奖励设计是一种可行且灵活的实际方法，可用于缓解语言模型中的幻觉问题。研究提供了可复现的弃权训练框架代码。

Abstract: Large Language Models (LLMs) often produce hallucinated or unverifiable content, undermining their reliability in factual domains. This work investigates Reinforcement Learning with Verifiable Rewards (RLVR) as a training paradigm that explicitly rewards abstention ("I don't know") alongside correctness to promote intellectual humility. We fine-tune and evaluate Granite-3.3-2B-Instruct and Qwen-3-4B-Instruct on the MedMCQA and Hendrycks Math benchmarks using a ternary reward structure ($-1$, r_abs, 1) under varying abstention reward structures. We further study the effect of combining RLVR with supervised fine-tuning strategies that teach abstention prior to reinforcement learning. Our results show that moderate abstention rewards (r_abs $\approx -0.25$ to 0.3) consistently reduce incorrect responses without severe accuracy degradation on multiple-choice tasks, with larger models exhibiting greater robustness to abstention incentives. On open-ended question answering, we observe limitations due to insufficient exploration, which can be partially mitigated through supervised abstention training. Overall, these findings demonstrate the feasibility and flexibility of verifiable reward design as a practical approach for hallucination mitigation in language models. Reproducible code for our abstention training framework is available here https://github.com/Mystic-Slice/rl-abstention.

</details>


### [32] [BengaliSent140: A Large-Scale Bengali Binary Sentiment Dataset for Hate and Non-Hate Speech Classification](https://arxiv.org/abs/2601.20129)
*Akif Islam,Sujan Kumar Roy,Md. Ekramul Hamid*

Main category: cs.CL

TL;DR: 本文介绍了BengaliSent140，一个大规模孟加拉语二元情感数据集，通过整合7个现有数据集构建而成，包含139,792个文本样本，为深度学习模型训练提供了更全面的语言覆盖。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语情感分析研究受到大规模多样化标注数据集稀缺的限制。现有数据集规模有限或局限于单一领域（如社交媒体评论），不足以训练需要大量异构数据的现代深度学习模型。

Method: 通过整合7个现有孟加拉语文本数据集构建统一语料库，系统地将异构标注方案统一为二元情感分类（Not Hate=0, Hate=1），确保跨数据源的一致性。

Result: 创建了包含139,792个独特文本样本的BengaliSent140数据集，其中68,548个仇恨类和71,244个非仇恨类实例，形成了相对平衡的类别分布。该数据集提供了比现有孟加拉语情感数据集更广泛的语言和上下文覆盖。

Conclusion: BengaliSent140为训练和基准测试深度学习模型提供了坚实基础，通过整合多源多领域数据，解决了孟加拉语情感分析中数据稀缺的问题。数据集已公开提供，并报告了基线实验结果以证明其实用性。

Abstract: Sentiment analysis for the Bengali language has attracted increasing research interest in recent years. However, progress remains constrained by the scarcity of large-scale and diverse annotated datasets. Although several Bengali sentiment and hate speech datasets are publicly available, most are limited in size or confined to a single domain, such as social media comments. Consequently, these resources are often insufficient for training modern deep learning based models, which require large volumes of heterogeneous data to learn robust and generalizable representations. In this work, we introduce BengaliSent140, a large-scale Bengali binary sentiment dataset constructed by consolidating seven existing Bengali text datasets into a unified corpus. To ensure consistency across sources, heterogeneous annotation schemes are systematically harmonized into a binary sentiment formulation with two classes: Not Hate (0) and Hate (1). The resulting dataset comprises 139,792 unique text samples, including 68,548 hate and 71,244 not-hate instances, yielding a relatively balanced class distribution. By integrating data from multiple sources and domains, BengaliSent140 offers broader linguistic and contextual coverage than existing Bengali sentiment datasets and provides a strong foundation for training and benchmarking deep learning models. Baseline experimental results are also reported to demonstrate the practical usability of the dataset. The dataset is publicly available at https://www.kaggle.com/datasets/akifislam/bengalisent140/

</details>


### [33] [Mind the Shift: Using Delta SSL Embeddings to Enhance Child ASR](https://arxiv.org/abs/2601.20142)
*Zilai Wang,Natarajan Balaji Shankar,Kaiyuan Zhang,Zihan Wang,Abeer Alwan*

Main category: cs.CL

TL;DR: 该论文提出使用delta SSL嵌入（微调模型与预训练模型嵌入之间的差异）来提升儿童语音识别性能，通过融合不同SSL模型的特征实现了在MyST儿童语料库上的SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 儿童自动语音识别面临数据有限和预训练领域不匹配的挑战。微调SSL模型会导致表征空间偏移，而delta嵌入可能编码任务特定信息，可与微调特征互补。

Method: 提出delta SSL嵌入的概念，定义为微调模型嵌入与预训练模型嵌入之间的差异。评估了在MyST儿童语料库上使用不同SSL模型（HuBERT、W2V2、WavLM）的多种融合策略。

Result: delta嵌入融合显著提升了性能：使用WavLM与delta HuBERT嵌入融合获得10%相对WER降低，与delta W2V2融合获得4.4%降低。WavLM与delta W2V2嵌入融合达到9.64 WER，在MyST语料库上创造了SSL模型的新SOTA。

Conclusion: delta嵌入是有效的，特征融合是推进儿童ASR的有前景方向。研究表明delta SSL嵌入编码了任务特定信息，能够补充微调特征，显著提升儿童语音识别性能。

Abstract: Self-supervised learning (SSL) models have achieved impressive results across many speech tasks, yet child automatic speech recognition (ASR) remains challenging due to limited data and pretraining domain mismatch. Fine-tuning SSL models on child speech induces shifts in the representation space. We hypothesize that delta SSL embeddings, defined as the differences between embeddings from a finetuned model and those from its pretrained counterpart, encode task-specific information that complements finetuned features from another SSL model. We evaluate multiple fusion strategies on the MyST childrens corpus using different models. Results show that delta embedding fusion with WavLM yields up to a 10 percent relative WER reduction for HuBERT and a 4.4 percent reduction for W2V2, compared to finetuned embedding fusion. Notably, fusing WavLM with delta W2V2 embeddings achieves a WER of 9.64, setting a new state of the art among SSL models on the MyST corpus. These findings demonstrate the effectiveness of delta embeddings and highlight feature fusion as a promising direction for advancing child ASR.

</details>


### [34] [Trajectory2Task: Training Robust Tool-Calling Agents with Synthesized Yet Verifiable Data for Complex User Intents](https://arxiv.org/abs/2601.20144)
*Ziyi Wang,Yuxuan Lu,Yimeng Zhang,Jing Huang,Jiri Gesi,Xianfeng Tang,Chen Luo,Yisi Sang,Hanqing Lu,Manling Li,Dakuo Wang*

Main category: cs.CL

TL;DR: Trajectory2Task：用于研究现实场景中工具调用智能体的可验证数据生成管道，涵盖模糊意图、变化意图和不可行意图三种用户场景。


<details>
  <summary>Details</summary>
Motivation: 当前工具调用智能体的研究大多集中在理想化设置中，而现实应用中的用户请求往往具有模糊性、随时间变化或受政策约束不可行等问题，且缺乏覆盖这些复杂交互模式的训练和评估数据。

Method: 提出Trajectory2Task管道：首先通过多轮探索生成有效的工具调用轨迹，然后将这些轨迹转化为用户面向的任务，并进行可控的意图适应，最终生成可验证的任务以支持闭环评估和训练。

Result: 在生成的复杂用户场景任务上对7个最先进的LLM进行基准测试，观察到频繁的失败；使用任务执行中获得的成功轨迹对轻量级LLM进行微调，发现在所有三种条件下都有持续改进，并且在未见过的工具使用领域有更好的泛化能力。

Conclusion: Trajectory2Task能够生成用于研究现实工具调用场景的可验证数据，通过基于成功轨迹的微调可以显著提升LLM的工具调用能力，表明该方法有助于开发更强的通用工具调用能力。

Abstract: Tool-calling agents are increasingly deployed in real-world customer-facing workflows. Yet most studies on tool-calling agents focus on idealized settings with general, fixed, and well-specified tasks. In real-world applications, user requests are often (1) ambiguous, (2) changing over time, or (3) infeasible due to policy constraints, and training and evaluation data that cover these diverse, complex interaction patterns remain under-represented. To bridge the gap, we present Trajectory2Task, a verifiable data generation pipeline for studying tool use at scale under three realistic user scenarios: ambiguous intent, changing intent, and infeasible intents. The pipeline first conducts multi-turn exploration to produce valid tool-call trajectories. It then converts these trajectories into user-facing tasks with controlled intent adaptations. This process yields verifiable task that support closed-loop evaluation and training. We benchmark seven state-of-the-art LLMs on the generated complex user scenario tasks and observe frequent failures. Finally, using successful trajectories obtained from task rollouts, we fine-tune lightweight LLMs and find consistent improvements across all three conditions, along with better generalization to unseen tool-use domains, indicating stronger general tool-calling ability.

</details>


### [35] [Me-Agent: A Personalized Mobile Agent with Two-Level User Habit Learning for Enhanced Interaction](https://arxiv.org/abs/2601.20162)
*Shuoxin Wang,Chang Liu,Gowen Loo,Lifan Zheng,Kaiwen Wei,Xinyi Zeng,Jingyuan Zhang,Yu Tian*

Main category: cs.CL

TL;DR: Me-Agent是一个可学习、可记忆的个性化移动智能体，通过两级用户习惯学习机制解决LLM智能体缺乏个性化的问题，在个性化基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM移动智能体虽然性能显著提升，但仅遵循显式用户指令而忽视个性化需求，存在三个主要局限：1) 无法解释模糊指令；2) 缺乏从用户交互历史中学习；3) 无法处理个性化指令。

Method: 提出Me-Agent，采用两级用户习惯学习方法：在提示层面，设计基于个人奖励模型的用户偏好学习策略；在记忆层面，设计分层偏好记忆，存储用户长期记忆和应用特定记忆。

Result: 在User FingerTip基准测试（包含大量日常生活模糊指令）和通用基准测试中，Me-Agent在个性化方面达到最先进性能，同时保持有竞争力的指令执行性能。

Conclusion: Me-Agent通过有效的个性化学习机制成功解决了LLM移动智能体的个性化不足问题，证明了其在处理模糊指令和个性化需求方面的有效性。

Abstract: Large Language Model (LLM)-based mobile agents have made significant performance advancements. However, these agents often follow explicit user instructions while overlooking personalized needs, leading to significant limitations for real users, particularly without personalized context: (1) inability to interpret ambiguous instructions, (2) lack of learning from user interaction history, and (3) failure to handle personalized instructions. To alleviate the above challenges, we propose Me-Agent, a learnable and memorable personalized mobile agent. Specifically, Me-Agent incorporates a two-level user habit learning approach. At the prompt level, we design a user preference learning strategy enhanced with a Personal Reward Model to improve personalization performance. At the memory level, we design a Hierarchical Preference Memory, which stores users' long-term memory and app-specific memory in different level memory. To validate the personalization capabilities of mobile agents, we introduce User FingerTip, a new benchmark featuring numerous ambiguous instructions for daily life. Extensive experiments on User FingerTip and general benchmarks demonstrate that Me-Agent achieves state-of-the-art performance in personalization while maintaining competitive instruction execution performance.

</details>


### [36] [Improving X-Codec-2.0 for Multi-Lingual Speech: 25 Hz Latent Rate and 24 kHz Sampling](https://arxiv.org/abs/2601.20185)
*Husein Zolkepli*

Main category: cs.CL

TL;DR: 通过简单的修改（增加池化和增大解码器跳数），将X-Codec-2.0的潜在率从50Hz降至25Hz，同时将输出采样率从16kHz提升至24kHz，显著提升了效率和感知质量。


<details>
  <summary>Details</summary>
Motivation: 虽然X-Codec-2.0在神经音频压缩和多语言语音建模中表现出色，但其50Hz潜在率和16kHz采样率的配置限制了时间效率和音频保真度。

Method: 引入额外的池化操作并增大解码器跳数，在不改变核心架构的情况下，将潜在率从50Hz降低到25Hz，同时将输出采样率从16kHz提高到24kHz。

Result: 在多语言Common Voice 17测试集上，相比原始X-Codec-2.0基线，UTMOSv2评估显示MOS提升了0.29，并在所有25Hz运行的编解码器中达到了最佳性能。

Conclusion: 通过简单的架构修改，可以显著提升音频编解码器的效率和感知质量，同时保持多语言语音建模的强性能。

Abstract: X-Codec-2.0 has shown strong performance in neural audio compression and multilingual speech modeling, operating at a 50 Hz latent rate and a 16 kHz sampling rate using frozen HuBERT features. While effective, this configuration limits temporal efficiency and audio fidelity. In this work, we explore a simple and effective modification by introducing additional pooling and increasing the decoder hop size. This reduces the latent rate from 50 Hz to 25 Hz and simultaneously raises the output sampling rate from 16 kHz to 24 kHz, improving efficiency and perceptual quality without altering the core architecture. Evaluated on the multilingual Common Voice 17 test set, the proposed configuration achieves a 0.29 MOS improvement over the original X-Codec-2.0 baseline based on UTMOSv2, and attains the best reported performance among all codecs operating at 25 Hz. The source code, checkpoints, and generation comparisons are released at \href{https://huggingface.co/Scicom-intl/xcodec2-25TPS-24k}{https://huggingface.co/Scicom-intl/xcodec2-25TPS-24k}.

</details>


### [37] [Unit-Based Agent for Semi-Cascaded Full-Duplex Dialogue Systems](https://arxiv.org/abs/2601.20230)
*Haoyuan Yu,Yuxuan Chen,Minjie Cai*

Main category: cs.CL

TL;DR: 基于多模态大语言模型的半级联全双工对话系统，通过分解对话为最小对话单元实现独立处理和智能转换


<details>
  <summary>Details</summary>
Motivation: 全双工语音交互对于自然的人机交互至关重要，需要系统能够同时处理说话和倾听，实现流畅自然的对话体验

Method: 提出将复杂对话分解为最小对话单元的框架，围绕多模态大语言模型构建半级联全双工对话系统，辅以语音活动检测和文本转语音合成模块，以即插即用方式运行

Result: 在HumDial数据集上的实验验证了框架有效性，在Human-like Spoken Dialogue Systems Challenge（Track 2: Full-Duplex Interaction）测试集上排名第二

Conclusion: 该框架为全双工对话系统提供了一种有效的解决方案，能够实现自然流畅的人机语音交互，代码已开源供社区使用

Abstract: Full-duplex voice interaction is crucial for natural human computer interaction. We present a framework that decomposes complex dialogue into minimal conversational units, enabling the system to process each unit independently and predict when to transit to the next. This framework is instantiated as a semi-cascaded full-duplex dialogue system built around a multimodal large language model, supported by auxiliary modules such as voice activity detection (VAD) and text-to-speech (TTS) synthesis. The resulting system operates in a train-free, plug-and-play manner. Experiments on the HumDial dataset demonstrate the effectiveness of our framework, which ranks second among all teams on the test set of the Human-like Spoken Dialogue Systems Challenge (Track 2: Full-Duplex Interaction). Code is available at the GitHub repository https://github.com/yu-haoyuan/fd-badcat.

</details>


### [38] [Automated Benchmark Generation from Domain Guidelines Informed by Bloom's Taxonomy](https://arxiv.org/abs/2601.20253)
*Si Chen,Le Huy Khiem,Annalisa Szymanski,Ronald Metoyer,Ting Hua,Nitesh V. Chawla*

Main category: cs.CL

TL;DR: 提出了一个基于布鲁姆分类法的自动基准生成框架，将专家实践转化为违反式场景，并扩展为自动评分的多项选择题和多轮对话，用于评估LLM在实践领域中的情境化推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM基准大多依赖预先存在的人类考试数据集，这些数据集在实践领域（如教学、营养学、护理）往往不可用。实践领域的知识是程序性的、基于专业判断的，需要评估模型超越事实记忆的情境化推理能力。

Method: 1. 基于布鲁姆分类法，将专家实践指南转化为隐含违反式场景；2. 将这些场景扩展为自动评分的多项选择题和多轮对话；3. 覆盖四个认知层次（记忆、理解、应用、分析）；4. 应用于教学、营养学和护理三个实践领域。

Result: 发现了模型与人类推理的差异：LLM有时在高级推理（分析）上表现相对更好，但在低级项目（记忆）上失败更频繁。生成了大规模、心理测量学知情的基准，能够揭示这些非直观的模型行为。

Conclusion: 该框架能够生成确定性的、可复现的、可扩展的评估基准，支持在真实世界环境中评估情境化推理能力，为实践领域的LLM评估提供了新方法。

Abstract: Open-ended question answering (QA) evaluates a model's ability to perform contextualized reasoning beyond factual recall. This challenge is especially acute in practice-based domains, where knowledge is procedural and grounded in professional judgment, while most existing LLM benchmarks depend on pre-existing human exam datasets that are often unavailable in such settings. We introduce a framework for automated benchmark generation from expert-authored guidelines informed by Bloom's Taxonomy. It converts expert practices into implicit violation-based scenarios and expands them into auto-graded multiple-choice questions (MCQs) and multi-turn dialogues across four cognitive levels, enabling deterministic, reproducible, and scalable evaluation. Applied to three applied domains: teaching, dietetics, and caregiving, we find differences between model and human-like reasoning: LLMs sometimes perform relatively better on higher-order reasoning (Analyze) but fail more frequently on lower-level items (Remember). We produce large-scale, psychometrically informed benchmarks that surface these non-intuitive model behaviors and enable evaluation of contextualized reasoning in real-world settings.

</details>


### [39] [SoftHateBench: Evaluating Moderation Models Against Reasoning-Driven, Policy-Compliant Hostility](https://arxiv.org/abs/2601.20256)
*Xuanyu Su,Diana Inkpen,Nathalie Japkowicz*

Main category: cs.CL

TL;DR: 该论文介绍了SoftHateBench，一个用于评估内容审核系统检测"软仇恨言论"能力的生成式基准。软仇恨言论指表面上合理但通过推理和框架引导受众责怪或排斥目标群体的言论。


<details>
  <summary>Details</summary>
Motivation: 当前社交媒体内容审核系统主要针对表面上的毒性内容（硬仇恨言论）进行优化，但无法有效检测推理驱动的软仇恨言论。现有基准无法系统性地衡量这种差距，需要专门工具来评估系统对微妙、推理式仇恨言论的检测能力。

Method: 研究整合了议题论证模型（AMT）和关联理论（RT）的统一框架：AMT提供将明确仇恨立场重写为看似中立讨论的论证结构，RT确保生成的AMT链保持逻辑连贯性。基于此构建了包含7个社会文化领域、28个目标群体的4,745个软仇恨实例的基准。

Result: 评估显示，从硬仇恨到软仇恨层级的检测性能普遍下降：基于编码器的检测器、通用大语言模型和安全模型在检测明确敌意时表现良好，但当相同立场通过微妙、推理式语言表达时，这些系统经常失效。

Conclusion: 当前内容审核系统对软仇恨言论的检测存在明显不足，需要开发更复杂的、能够理解推理和论证结构的检测方法。SoftHateBench为系统评估和改进提供了重要工具，强调了解决推理驱动仇恨言论检测的重要性。

Abstract: Online hate on social media ranges from overt slurs and threats (\emph{hard hate speech}) to \emph{soft hate speech}: discourse that appears reasonable on the surface but uses framing and value-based arguments to steer audiences toward blaming or excluding a target group. We hypothesize that current moderation systems, largely optimized for surface toxicity cues, are not robust to this reasoning-driven hostility, yet existing benchmarks do not measure this gap systematically. We introduce \textbf{\textsc{SoftHateBench}}, a generative benchmark that produces soft-hate variants while preserving the underlying hostile standpoint. To generate soft hate, we integrate the \emph{Argumentum Model of Topics} (AMT) and \emph{Relevance Theory} (RT) in a unified framework: AMT provides the backbone argument structure for rewriting an explicit hateful standpoint into a seemingly neutral discussion while preserving the stance, and RT guides generation to keep the AMT chain logically coherent. The benchmark spans \textbf{7} sociocultural domains and \textbf{28} target groups, comprising \textbf{4,745} soft-hate instances. Evaluations across encoder-based detectors, general-purpose LLMs, and safety models show a consistent drop from hard to soft tiers: systems that detect explicit hostility often fail when the same stance is conveyed through subtle, reasoning-based language. \textcolor{red}{\textbf{Disclaimer.} Contains offensive examples used solely for research.}

</details>


### [40] [RusLICA: A Russian-Language Platform for Automated Linguistic Inquiry and Category Analysis](https://arxiv.org/abs/2601.20275)
*Elina Sigdel,Anastasia Panfilova*

Main category: cs.CL

TL;DR: 该研究提出了针对俄语的心理语言学特征分析方法，基于LIWC方法论但专门为俄语构建词典，整合了96个分析类别。


<details>
  <summary>Details</summary>
Motivation: 现有的LIWC工具主要针对英语，需要为俄语开发专门的文本心理语言学特征分析工具，考虑俄语的语法和文化特性。

Method: 基于多个词典学资源、语义词典和语料库构建俄语专用词典，将词元映射到42个心理语言学类别，整合句法、形态、词汇、统计特征和预训练语言模型预测结果。

Result: 开发了包含96个分析类别的俄语LIWC方法，实现了RusLICA网络服务分析器，专门针对俄语语言特性。

Conclusion: 该方法为俄语文本的心理语言学分析提供了专门工具，克服了直接翻译英语词典的局限性，考虑了俄语的语言特异性。

Abstract: Defining psycholinguistic characteristics in written texts is a task gaining increasing attention from researchers. One of the most widely used tools in the current field is Linguistic Inquiry and Word Count (LIWC) that originally was developed to analyze English texts and translated into multiple languages. Our approach offers the adaptation of LIWC methodology for the Russian language, considering its grammatical and cultural specificities. The suggested approach comprises 96 categories, integrating syntactic, morphological, lexical, general statistical features, and results of predictions obtained using pre-trained language models (LMs) for text analysis. Rather than applying direct translation to existing thesauri, we built the dictionary specifically for the Russian language based on the content from several lexicographic resources, semantic dictionaries and corpora. The paper describes the process of mapping lemmas to 42 psycholinguistic categories and the implementation of the analyzer as part of RusLICA web service.

</details>


### [41] [Beyond the Needle's Illusion: Decoupled Evaluation of Evidence Access and Use under Semantic Interference at 326M-Token Scale](https://arxiv.org/abs/2601.20276)
*Tianwei Lin,Zuyi Zhou,Xinda Zhao,Chenke Wang,Xiaohong Li,Yu Chen,Chuanrui Hu,Jian Pei,Yafeng Deng*

Main category: cs.CL

TL;DR: 提出了EverMemBench-S（EMB-S）基准测试，用于评估长上下文LLM代理在对抗性环境下的证据访问能力，发现语义干扰是主要瓶颈而非上下文长度。


<details>
  <summary>Details</summary>
Motivation: 现有的Needle-in-a-Haystack（NIAH）评估主要测量良性跨度定位，但现实世界中的长上下文LLM代理需要在包含语义干扰的大规模环境中准确访问和使用证据。需要更严格的评估方法来测试模型在对抗性条件下的表现。

Method: 1. 构建基于326M令牌MemoryBank的EMB-S基准测试，包含经过碰撞测试的近误硬负例和跨文档的金证据集；2. 提出解耦诊断协议，分别报告证据访问（文档ID定位）和端到端QA质量；3. 在从64K上下文到326M令牌的参考语料库阶梯上评估系统表现。

Result: 在良性NIAH中表现饱和的系统在语义干扰下的证据访问能力急剧下降。结果表明，语义辨别能力而非上下文长度本身，是大规模长上下文记忆的主要瓶颈。

Conclusion: 语义干扰是长上下文LLM代理在实际应用中面临的主要挑战，仅依赖上下文长度扩展不足以解决大规模记忆问题。需要开发更好的语义辨别能力来提升长上下文环境下的证据访问准确性。

Abstract: Long-context LLM agents must access the right evidence from large environments and use it faithfully. However, the popular Needle-in-a-Haystack (NIAH) evaluation mostly measures benign span localization. The needle is near-unique, and the haystack is largely irrelevant. We introduce EverMemBench-S (EMB-S), an adversarial NIAH-style benchmark built on a 326M-token MemoryBank. While the full MemoryBank spans 326M tokens for retrieval-based (RAG) evaluation, we evaluate native long-context models only at scales that fit within each model's context window (up to 1M tokens in this work) to ensure a fair comparison. EMB-S pairs queries with collision-tested near-miss hard negatives and gold evidence sets spanning one or more documents, validated via human screening and LLM verification. We also propose a decoupled diagnostic protocol that reports evidence access (document-ID localization) separately from end-to-end QA quality under full-context prompting. This enables consistent diagnosis for both native long-context prompting and retrieval pipelines. Across a reference-corpus ladder from domain-isolated 64K contexts to a globally shared 326M-token environment, we observe a clear reality gap. Systems that saturate benign NIAH degrade sharply in evidence access under semantic interference. These results indicate that semantic discrimination, not context length alone, is the dominant bottleneck for long-context memory at scale.

</details>


### [42] [MiLorE-SSL: Scaling Multilingual Capabilities in Self-Supervised Models without Forgetting](https://arxiv.org/abs/2601.20300)
*Jing Xu,Minglin Wu,Xueyuan Chen,Xixin Wu,Helen Meng*

Main category: cs.CL

TL;DR: MiLorE-SSL是一个轻量级框架，结合LoRA和软MoE机制，用于高效的多语言持续训练，仅需2.14%可训练参数就能在新增语言上取得良好性能并减少遗忘。


<details>
  <summary>Details</summary>
Motivation: 当前多语言自监督学习模型受限于预训练时的语言范围，重新训练新语言计算成本高，而顺序训练容易导致灾难性遗忘，需要高效的多语言持续学习方法。

Method: 提出MiLorE-SSL框架，结合LoRA模块（提供高效低秩适应）和软混合专家机制（促进跨语言专家共享，减少跨语言干扰），并使用有限回放数据缓解遗忘问题。

Result: 在ML-SUPERB基准测试中，MiLorE-SSL在新语言上表现强劲，同时提升了现有语言能力，仅需2.14%的可训练参数。

Conclusion: MiLorE-SSL为多语言自监督学习的持续训练提供了一种高效轻量级解决方案，能够有效整合新语言同时减轻遗忘问题。

Abstract: Self-supervised learning (SSL) has greatly advanced speech representation learning, but multilingual SSL models remain constrained to languages encountered during pretraining. Retraining from scratch to incorporate new languages is computationally expensive, while sequential training without migitation strategies often leads to catastrophic forgetting. To address this, we propose MiLorE-SSL, a lightweight framework that combines LoRA modules with a soft mixture-of-experts (MoE) mechanism for efficient continual multilingual training. LoRA provides efficient low-rank adaptation, while soft MoE promotes flexible expert sharing across languages, reducing cross-lingual interference. To further mitigate forgetting, we introduce limited replay data from existing languages, avoiding reliance on large historical corpora. Experiments on ML-SUPERB demonstrate that MiLorE-SSL achieves strong performance in new languages and improves the ability in existing ones with only 2.14% trainable parameters.

</details>


### [43] [SAPO: Self-Adaptive Process Optimization Makes Small Reasoners Stronger](https://arxiv.org/abs/2601.20312)
*Kaiyuan Chen,Guangmin Zheng,Jin Wang,Xiaobing Zhou,Xuejie Zhang*

Main category: cs.CL

TL;DR: SAPO方法通过自适应过程监督信号减少推理器-验证器差距，提升小语言模型在数学和代码任务上的表现，优于现有自进化方法。


<details>
  <summary>Details</summary>
Motivation: 现有自进化方法忽视了细粒度推理步骤的影响，导致推理器-验证器差距；蒙特卡洛过程监督的计算效率低下进一步加剧了缩小这一差距的困难。

Method: 提出SAPO（Self-Adaptive Process Optimization）方法，基于错误相关负波（ERN）启发，通过自适应地引入过程监督信号，主动最小化推理器-验证器差距，而非依赖低效的蒙特卡洛估计。

Result: 在数学和代码两类挑战性任务上，SAPO方法优于大多数现有自进化方法。此外，为研究SAPO对验证器性能的影响，引入了两个新的过程奖励模型基准。

Conclusion: SAPO通过自适应过程优化有效缩小了推理器-验证器差距，提升了小语言模型的自我改进能力，在复杂推理任务中表现出色。

Abstract: Existing self-evolution methods overlook the influence of fine-grained reasoning steps, which leads to the reasoner-verifier gap. The computational inefficiency of Monte Carlo (MC) process supervision further exacerbates the difficulty in mitigating the gap. Motivated by the Error-Related Negativity (ERN), which the reasoner can localize error following incorrect decisions, guiding rapid adjustments, we propose a Self-Adaptive Process Optimization (SAPO) method for self-improvement in Small Language Models (SLMs). SAPO adaptively and efficiently introduces process supervision signals by actively minimizing the reasoner-verifier gap rather than relying on inefficient MC estimations. Extensive experiments demonstrate that the proposed method outperforms most existing self-evolution methods on two challenging task types: mathematics and code. Additionally, to further investigate SAPO's impact on verifier performance, this work introduces two new benchmarks for process reward models in both mathematical and coding tasks.

</details>


### [44] [Beyond Speedup -- Utilizing KV Cache for Sampling and Reasoning](https://arxiv.org/abs/2601.20326)
*Zeyu Xing,Xing Li,Hui-Ling Zhen,Mingxuan Yuan,Sinno Jialin Pan*

Main category: cs.CL

TL;DR: 利用KV缓存作为轻量级表示，避免重新计算或存储完整隐藏状态，在链式嵌入和快慢思维切换两个应用中表现出色，大幅减少token生成同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: KV缓存通常仅用于加速自回归解码，但其中编码的上下文信息可以被下游任务免费复用。研究旨在探索KV缓存作为轻量级表示的潜力，避免重新计算或存储完整隐藏状态的计算开销。

Method: 将KV缓存视为轻量级表示，研究其在两个关键应用中的表现：1) 链式嵌入（Chain-of-Embedding），2) 快慢思维切换（Fast/Slow Thinking Switching）。在Llama-3.1-8B-Instruct、Qwen2-7B-Instruct、Qwen3-8B和DeepSeek-R1-Distil-Qwen-14B等模型上进行实验验证。

Result: 1) 在链式嵌入应用中，KV衍生表示在Llama-3.1-8B-Instruct和Qwen2-7B-Instruct上达到竞争性或更优性能；2) 在快慢思维切换中，KV表示使得Qwen3-8B和DeepSeek-R1-Distil-Qwen-14B能够实现自适应推理，减少高达5.7倍的token生成，同时保持最小准确率损失。

Conclusion: KV缓存可作为免费、有效的采样和推理基础，为LLM推理中的表示复用开辟了新方向。虽然不如专用嵌入强大，但KV衍生表示在特定应用中已足够有效。

Abstract: KV caches, typically used only to speed up autoregressive decoding, encode contextual information that can be reused for downstream tasks at no extra cost. We propose treating the KV cache as a lightweight representation, eliminating the need to recompute or store full hidden states. Despite being weaker than dedicated embeddings, KV-derived representations are shown to be sufficient for two key applications: \textbf{(i) Chain-of-Embedding}, where they achieve competitive or superior performance on Llama-3.1-8B-Instruct and Qwen2-7B-Instruct; and \textbf{(ii) Fast/Slow Thinking Switching}, where they enable adaptive reasoning on Qwen3-8B and DeepSeek-R1-Distil-Qwen-14B, reducing token generation by up to $5.7\times$ with minimal accuracy loss. Our findings establish KV caches as a free, effective substrate for sampling and reasoning, opening new directions for representation reuse in LLM inference. Code: https://github.com/cmd2001/ICLR2026_KV-Embedding.

</details>


### [45] [CE-RM: A Pointwise Generative Reward Model Optimized via Two-Stage Rollout and Unified Criteria](https://arxiv.org/abs/2601.20327)
*Xinyu Hu,Yancheng He,Weixun Wang,Tao Feng,Li Lin,Jiashun Liu,Wenbo Su,Bo Zheng,Xiaojun Wan*

Main category: cs.CL

TL;DR: CE-RM-4B是一个基于两阶段rollout方法和统一查询标准的生成式奖励模型，仅使用5.7K高质量数据就在多个基准测试中表现优异，尤其在RL实践中能提供更有效的改进。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-as-a-Judge范式在基准测试中表现优秀，但在实际强化学习实践中存在明显差距，主要因为现有研究存在成对评估主导和评估标准优化不足等限制。

Method: 提出CE-RM-4B生成式奖励模型，采用专门的两阶段rollout训练方法，并采用统一的基于查询的评估标准，仅使用约5.7K从开源偏好数据集中精选的高质量数据。

Result: CE-RM-4B在多样的奖励模型基准测试中表现优异，特别是在Best-of-N场景中，并在下游RL实践中提供更有效的改进。

Conclusion: 通过解决现有LLM-as-a-Judge范式的局限性，CE-RM-4B成功缩小了基准测试性能与实际RL实践效果之间的差距，证明了点式生成奖励模型的有效性。

Abstract: Automatic evaluation is crucial yet challenging for open-ended natural language generation, especially when rule-based metrics are infeasible. Compared with traditional methods, the recent LLM-as-a-Judge paradigms enable better and more flexible evaluation, and show promise as generative reward models for reinforcement learning. However, prior work has revealed a notable gap between their seemingly impressive benchmark performance and actual effectiveness in RL practice. We attribute this issue to some limitations in existing studies, including the dominance of pairwise evaluation and inadequate optimization of evaluation criteria. Therefore, we propose CE-RM-4B, a pointwise generative reward model trained with a dedicated two-stage rollout method, and adopting unified query-based criteria. Using only about 5.7K high-quality data curated from the open-source preference dataset, our CE-RM-4B achieves superior performance on diverse reward model benchmarks, especially in Best-of-N scenarios, and delivers more effective improvements in downstream RL practice.

</details>


### [46] [PsychePass: Calibrating LLM Therapeutic Competence via Trajectory-Anchored Tournaments](https://arxiv.org/abs/2601.20330)
*Zhuang Chen,Dazhen Wan,Zhangkai Zheng,Guanqun Bi,Xiyao Xiao,Binghang Li,Minlie Huang*

Main category: cs.CL

TL;DR: PsychePass是一个通过轨迹锚定锦标赛来评估大语言模型心理治疗能力的框架，解决了当前评估方法的无锚定缺陷和不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 当前评估大语言模型在心理健康咨询中能力的范式存在无锚定缺陷，导致两种不稳定性：过程漂移（无引导的客户模拟偏离咨询目标）和标准漂移（静态点式评分缺乏可靠判断的稳定性）。

Method: 提出PsychePass框架，通过轨迹锚定锦标赛来校准LLM的治疗能力。首先在模拟中锚定交互轨迹，让客户精确控制咨询过程以探测多方面能力；然后通过高效的瑞士系统锦标赛锚定对战轨迹，利用动态配对对战产生稳健的Elo评分。

Result: 广泛实验验证了PsychePass的有效性，并显示其与人类专家判断有很强的一致性。此外，锦标赛轨迹可以转化为可信的奖励信号，用于基于策略的强化学习来提升LLM的性能。

Conclusion: PsychePass为评估和提升大语言模型在心理治疗领域的能力提供了一个系统、稳定且可扩展的框架，解决了当前评估方法的局限性。

Abstract: While large language models show promise in mental healthcare, evaluating their therapeutic competence remains challenging due to the unstructured and longitudinal nature of counseling. We argue that current evaluation paradigms suffer from an unanchored defect, leading to two forms of instability: process drift, where unsteered client simulation wanders away from specific counseling goals, and standard drift, where static pointwise scoring lacks the stability for reliable judgment. To address this, we introduce Ps, a unified framework that calibrates the therapeutic competence of LLMs via trajectory-anchored tournaments. We first anchor the interaction trajectory in simulation, where clients precisely control the fluid consultation process to probe multifaceted capabilities. We then anchor the battle trajectory in judgments through an efficient Swiss-system tournament, utilizing dynamic pairwise battles to yield robust Elo ratings. Beyond ranking, we demonstrate that tournament trajectories can be transformed into credible reward signals, enabling on-policy reinforcement learning to enhance LLMs' performance. Extensive experiments validate the effectiveness of PsychePass and its strong consistency with human expert judgments.

</details>


### [47] [MobileBench-OL: A Comprehensive Chinese Benchmark for Evaluating Mobile GUI Agents in Real-World Environment](https://arxiv.org/abs/2601.20335)
*Qinzhuo Wu,Zhizhuo Yang,Hanhao Li,Pengzhi Gao,Wei Liu,Jian Luan*

Main category: cs.CL

TL;DR: MobileBench-OL：面向移动GUI代理的在线评测基准，包含1080个任务，评估任务执行、复杂推理和噪声鲁棒性，填补现有基准与真实环境间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有移动GUI代理评测基准存在局限性：在线基准过于关注任务指令跟随能力，忽视推理和探索能力；未考虑真实移动环境中的随机噪声；导致基准与真实环境存在差距。

Method: 提出MobileBench-OL在线基准，包含1080个任务，源自80个中国应用；通过5个子集设置多个评估维度；提供带重置机制的自动评估框架，支持稳定可重复的真实世界评测。

Result: 评估12个领先GUI代理显示，它们在满足真实世界需求方面仍有显著改进空间；人工评估进一步证实MobileBench-OL能可靠测量代理在真实环境中的性能。

Conclusion: MobileBench-OL填补了移动GUI代理评测基准的空白，通过综合评估任务执行、复杂推理和噪声鲁棒性，提供了更贴近真实环境的评测标准，数据与代码将在接受后开源。

Abstract: Recent advances in mobile Graphical User Interface (GUI) agents highlight the growing need for comprehensive evaluation benchmarks. While new online benchmarks offer more realistic testing than offline ones, they tend to focus on the agents' task instruction-following ability while neglecting their reasoning and exploration ability. Moreover, these benchmarks do not consider the random noise in real-world mobile environments. This leads to a gap between benchmarks and real-world environments. To addressing these limitations, we propose MobileBench-OL, an online benchmark with 1080 tasks from 80 Chinese apps. It measures task execution, complex reasoning, and noise robustness of agents by including 5 subsets, which set multiple evaluation dimensions. We also provide an auto-eval framework with a reset mechanism, enabling stable and repeatable real-world benchmarking. Evaluating 12 leading GUI agents on MobileBench-OL shows significant room for improvement to meet real-world requirements. Human evaluation further confirms that MobileBench-OL can reliably measure the performance of leading GUI agents in real environments. Our data and code will be released upon acceptance.

</details>


### [48] [Improving Diffusion Language Model Decoding through Joint Search in Generation Order and Token Space](https://arxiv.org/abs/2601.20339)
*Yangyi Shen,Tianjian Feng,Jiaqi Han,Wen Wang,Tianlang Chen,Chunhua Shen,Jure Leskovec,Stefano Ermon*

Main category: cs.CL

TL;DR: 本文提出了Order-Token Search方法，通过联合搜索生成顺序和token值来探索扩散语言模型的解码轨迹空间，在数学推理和编程基准测试中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前扩散语言模型的解码方法通常只遵循单一轨迹，限制了在轨迹空间中的探索能力。为了充分利用DLMs的顺序无关生成特性，需要开发能够探索多种可能解码轨迹的方法。

Method: 提出Order-Token Search方法，核心是构建一个似然估计器来评分去噪操作，实现稳定的剪枝和高效的多样化轨迹探索，联合搜索生成顺序和token值。

Result: 在GSM8K、MATH500、Countdown和HumanEval等数学推理和编程基准测试中，Order-Token Search持续优于基线方法，分别获得3.1%、3.8%、7.9%和6.8%的绝对提升，与经过后训练的diffu-GRPO d1-LLaDA模型表现相当或更好。

Conclusion: 联合搜索是推进扩散语言模型解码能力的关键组成部分，Order-Token Search为DLMs提供了有效的轨迹空间探索方法。

Abstract: Diffusion Language Models (DLMs) offer order-agnostic generation that can explore many possible decoding trajectories. However, current decoding methods commit to a single trajectory, limiting exploration in trajectory space. We introduce Order-Token Search to explore this space through jointly searching over generation order and token values. Its core is a likelihood estimator that scores denoising actions, enabling stable pruning and efficient exploration of diverse trajectories. Across mathematical reasoning and coding benchmarks, Order-Token Search consistently outperforms baselines on GSM8K, MATH500, Countdown, and HumanEval (3.1%, 3.8%, 7.9%, and 6.8% absolute over backbone), matching or surpassing diffu-GRPO post-trained d1-LLaDA. Our work establishes joint search as a key component for advancing decoding in DLMs.

</details>


### [49] [Beyond Accuracy: A Cognitive Load Framework for Mapping the Capability Boundaries of Tool-use Agents](https://arxiv.org/abs/2601.20412)
*Qihao Wang,Yue Hu,Mingzhe Lu,Jiayue Wu,Yanbing Liu,Yuanmin Tang*

Main category: cs.CL

TL;DR: 论文提出了基于认知负荷理论的框架，通过解构任务复杂度（内在负荷和外在负荷）来诊断LLM使用工具的能力边界，并构建了可参数化调整认知负荷的基准测试ToolLoad-Bench。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试主要报告最终准确率，只能显示模型能做什么，但无法揭示认知瓶颈和真实能力边界。需要从简单的性能评分转向诊断工具，以理解LLM使用外部工具时的能力限制。

Method: 基于认知负荷理论，将任务复杂度分解为两个可量化组件：1）内在负荷：解决方案路径的固有结构复杂度，使用新颖的工具交互图形式化；2）外在负荷：由模糊任务呈现引起的难度。构建ToolLoad-Bench基准，这是首个可参数化调整认知负荷的基准。

Result: 评估显示随着认知负荷增加，模型性能出现明显的悬崖式下降，能够精确映射每个模型的能力边界。验证了框架预测与实证结果高度校准，建立了理解智能体极限的原则性方法。

Conclusion: 该框架为理解LLM使用工具的能力边界提供了诊断工具，从性能评分转向认知瓶颈分析，为构建更高效系统奠定了实践基础。

Abstract: The ability of Large Language Models (LLMs) to use external tools unlocks powerful real-world interactions, making rigorous evaluation essential. However, current benchmarks primarily report final accuracy, revealing what models can do but obscuring the cognitive bottlenecks that define their true capability boundaries. To move from simple performance scoring to a diagnostic tool, we introduce a framework grounded in Cognitive Load Theory. Our framework deconstructs task complexity into two quantifiable components: Intrinsic Load, the inherent structural complexity of the solution path, formalized with a novel Tool Interaction Graph; and Extraneous Load, the difficulty arising from ambiguous task presentation. To enable controlled experiments, we construct ToolLoad-Bench, the first benchmark with parametrically adjustable cognitive load. Our evaluation reveals distinct performance cliffs as cognitive load increases, allowing us to precisely map each model's capability boundary. We validate that our framework's predictions are highly calibrated with empirical results, establishing a principled methodology for understanding an agent's limits and a practical foundation for building more efficient systems.

</details>


### [50] [SpeechMapper: Speech-to-text Embedding Projector for LLMs](https://arxiv.org/abs/2601.20417)
*Biswesh Mohapatra,Marcely Zanon Boito,Ioan Calapodescu*

Main category: cs.CL

TL;DR: SpeechMapper提出了一种高效且可扩展的语音-LLM集成方法，通过预训练语音块和简短指令微调，在减少计算成本和避免过拟合的同时，实现了与当前最佳语音LLM相当甚至更优的性能。


<details>
  <summary>Details</summary>
Motivation: 当前语音LLM通常将语音基础模型通过投影层连接到LLM，并在语音指令数据上训练所有组件。这种方法计算成本高，容易对任务和提示过拟合，需要更高效、更通用的解决方案。

Method: 1. 首先在不连接LLM的情况下，在廉价硬件上预训练语音块；2. 然后通过简短的1K步指令微调阶段，将预训练块高效地连接到目标LLM；3. 提出任务无关和任务特定的指令微调策略，包括基于ASR的自适应策略。

Result: 在语音翻译和口语问答任务上的实验表明：1. 在任务无关设置下，SpeechMapper与IWSLT25最佳指令跟随语音LLM性能相当，尽管从未在这些任务上训练过；2. 在任务特定设置下，SpeechMapper在多个数据集上超越该模型，尽管使用更少的数据和计算资源。

Conclusion: SpeechMapper提供了一种实用且可扩展的方法，能够在不进行大规模指令微调的情况下，实现高效、通用的语音-LLM集成，有效解决了计算成本高和过拟合问题。

Abstract: Current speech LLMs bridge speech foundation models to LLMs using projection layers, training all of these components on speech instruction data. This strategy is computationally intensive and susceptible to task and prompt overfitting. We present SpeechMapper, a cost-efficient speech-to-LLM-embedding training approach that mitigates overfitting, enabling more robust and generalizable models. Our model is first pretrained without the LLM on inexpensive hardware, and then efficiently attached to the target LLM via a brief 1K-step instruction tuning (IT) stage. Through experiments on speech translation and spoken question answering, we demonstrate the versatility of SpeechMapper's pretrained block, presenting results for both task-agnostic IT, an ASR-based adaptation strategy that does not train in the target task, and task-specific IT. In task-agnostic settings, Speechmapper rivals the best instruction-following speech LLM from IWSLT25, despite never being trained on these tasks, while in task-specific settings, it outperforms this model across many datasets, despite requiring less data and compute. Overall, SpeechMapper offers a practical and scalable approach for efficient, generalizable speech-LLM integration without large-scale IT.

</details>


### [51] [Hopes and Fears -- Emotion Distribution in the Topic Landscape of Finnish Parliamentary Speech 2000-2020](https://arxiv.org/abs/2601.20424)
*Anna Ristilä,Otto Tarkka,Veronika Laippala,Kimmo Elo*

Main category: cs.CL

TL;DR: 该研究通过分析芬兰议会2000-2020年演讲，发现不同议题的情绪表达存在差异，并证实议会演讲整体趋向积极


<details>
  <summary>Details</summary>
Motivation: 现有研究通常将议会话语视为同质整体，忽视了议题特定的情绪模式。虽然人们对议会中最具情感色彩的议题有直觉判断，但缺乏对不同议题情绪表达的实证研究

Method: 使用情感分析模型，从共时和历时两个角度分析芬兰议会2000-2020年间演讲的议题情绪表达

Result: 研究结果证实了议会演讲整体积极性的增强趋势，并揭示了议会辩论中议题特定的情绪表达模式

Conclusion: 该研究填补了议会话语中议题情绪表达研究的空白，为理解议会辩论的情感动态提供了新视角

Abstract: Existing research often treats parliamentary discourse as a homogeneous whole, overlooking topic-specific patterns. Parliamentary speeches address a wide range of topics, some of which evoke stronger emotions than others. While everyone has intuitive assumptions about what the most emotive topics in a parliament may be, there has been little research into the emotions typically linked to different topics. This paper strives to fill this gap by examining emotion expression among the topics of parliamentary speeches delivered in Eduskunta, the Finnish Parliament, between 2000 and 2020. An emotion analysis model is used to investigate emotion expression in topics, from both synchronic and diachronic perspectives. The results strengthen evidence of increasing positivity in parliamentary speech and provide further insights into topic-specific emotion expression within parliamentary debate.

</details>


### [52] [PEARL: Plan Exploration and Adaptive Reinforcement Learning for Multihop Tool Use](https://arxiv.org/abs/2601.20439)
*Qihao Wang,Mingzhe Lu,Jiayue Wu,Yue Hu,Yanbing Liu*

Main category: cs.CL

TL;DR: PEARL框架通过两阶段方法（离线探索和在线强化学习）显著提升LLM在复杂工具调用中的规划和执行能力，在ToolHop基准上达到56.5%的最新成功率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在外部工具调用方面潜力巨大，但在复杂的多轮工具调用中面临显著挑战：规划能力弱、工具幻觉、参数生成错误以及交互鲁棒性差。

Method: 提出PEARL框架，采用两阶段方法：1）离线阶段：代理探索工具以学习有效使用模式和失败条件；2）在线强化学习阶段：通过群组相对策略优化（GRPO）训练专用规划器，并设计精心构造的奖励函数为规划质量提供明确信号。

Result: 在ToolHop和T-Eval基准测试中，PEARL显著优于现有方法，在ToolHop上达到56.5%的最新成功率，同时保持较低调用错误率。

Conclusion: PEARL框架在解决工具使用的复杂规划挑战方面取得了关键进展，有助于开发更鲁棒可靠的基于LLM的智能体。

Abstract: Large Language Models show great potential with external tools, but face significant challenges in complex, multi-turn tool invocation. They often exhibit weak planning, tool hallucination, erroneous parameter generation, and struggle with robust interaction. To tackle these issues, we present PEARL, a novel framework to enhance LLM planning and execution for sophisticated tool use. PEARL adopts a two-stage approach: an offline phase where the agent explores tools to learn valid usage patterns and failure conditions, and an online reinforcement learning phase. In the online phase, a dedicated Planner is trained via group Relative Policy Optimization (GRPO) with a carefully designed reward function that provides distinct signals for planning quality. Experiments on the ToolHop and T-Eval benchmarks show PEARL significantly outperforms existing methods, achieving a new state-of-the-art success rate of \textbf{56.5\%} on ToolHop while maintaining a low invocation error rate. Our work marks a key advance in addressing the complex planning challenges of tool use, contributing to the development of more robust and reliable LLM-based agents.

</details>


### [53] [MuVaC: AVariational Causal Framework for Multimodal Sarcasm Understanding in Dialogues](https://arxiv.org/abs/2601.20451)
*Diandian Guo,Fangfang Yuan,Cong Cao,Xixun Lin,Chuan Zhou,Hao Peng,Yanan Cao,Yanbing Liu*

Main category: cs.CL

TL;DR: MuVaC是一个基于变分因果推理的多模态讽刺分析框架，联合优化讽刺检测和解释生成任务，通过模拟人类认知机制实现更可靠的多模态讽刺理解。


<details>
  <summary>Details</summary>
Motivation: 社交媒体中多模态对话的讽刺现象普遍存在，理解其真实意图至关重要。现有研究要么只关注多模态讽刺检测(MSD)，要么只关注多模态讽刺解释(MuSE)，即使有尝试整合这两个任务的研究也忽视了它们内在的因果依赖关系。

Method: 提出MuVaC变分因果推理框架：1) 从结构因果模型角度建模MSD和MuSE，建立变分因果路径定义联合优化目标；2) 设计对齐-融合方法整合多模态特征，为讽刺检测和解释生成提供鲁棒的融合表示；3) 通过确保检测结果与解释之间的一致性来增强推理可信度。

Result: 实验结果表明MuVaC在公开数据集上表现出优越性能，为理解多模态讽刺提供了新的视角。

Conclusion: MuVaC通过模拟人类认知机制，联合优化多模态讽刺检测和解释生成，有效利用任务间的因果依赖关系，实现了更鲁棒的多模态特征学习和更可靠的讽刺理解。

Abstract: The prevalence of sarcasm in multimodal dialogues on the social platforms presents a crucial yet challenging task for understanding the true intent behind online content. Comprehensive sarcasm analysis requires two key aspects: Multimodal Sarcasm Detection (MSD) and Multimodal Sarcasm Explanation (MuSE). Intuitively, the act of detection is the result of the reasoning process that explains the sarcasm. Current research predominantly focuses on addressing either MSD or MuSE as a single task. Even though some recent work has attempted to integrate these tasks, their inherent causal dependency is often overlooked. To bridge this gap, we propose MuVaC, a variational causal inference framework that mimics human cognitive mechanisms for understanding sarcasm, enabling robust multimodal feature learning to jointly optimize MSD and MuSE. Specifically, we first model MSD and MuSE from the perspective of structural causal models, establishing variational causal pathways to define the objectives for joint optimization. Next, we design an alignment-then-fusion approach to integrate multimodal features, providing robust fusion representations for sarcasm detection and explanation generation. Finally, we enhance the reasoning trustworthiness by ensuring consistency between detection results and explanations. Experimental results demonstrate the superiority of MuVaC in public datasets, offering a new perspective for understanding multimodal sarcasm.

</details>


### [54] [BMAM: Brain-inspired Multi-Agent Memory Framework](https://arxiv.org/abs/2601.20465)
*Yang Li,Jiaxiang Liu,Yusong Wang,Yujie Wu,Mingkun Xu*

Main category: cs.CL

TL;DR: BMAM是一种受大脑启发的多智能体记忆架构，通过功能特化的记忆子系统解决长期交互中智能体记忆退化和行为不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 基于语言模型的智能体在长期交互中面临记忆退化和行为不一致的挑战，作者称之为"灵魂侵蚀"问题。现有记忆系统通常采用单一非结构化存储，难以保持时间基础信息和跨会话行为一致性。

Method: BMAM将智能体记忆建模为一组功能特化的子系统，包括情景记忆、语义记忆、显著性感知记忆和控制导向记忆，这些组件在互补的时间尺度上运作。系统沿明确的时间线组织情景记忆，并通过融合多种互补信号来检索证据。

Result: 在LoCoMo基准测试中，BMAM在标准长期评估设置下达到78.45%的准确率。消融分析证实，受海马体启发的情景记忆子系统在时间推理中起着关键作用。

Conclusion: BMAM通过模拟认知记忆系统的功能专业化架构，有效解决了语言模型智能体在长期交互中的记忆退化问题，为构建具有一致性和时间感知能力的智能体提供了可行的记忆框架。

Abstract: Language-model-based agents operating over extended interaction horizons face persistent challenges in preserving temporally grounded information and maintaining behavioral consistency across sessions, a failure mode we term soul erosion. We present BMAM (Brain-inspired Multi-Agent Memory), a general-purpose memory architecture that models agent memory as a set of functionally specialized subsystems rather than a single unstructured store. Inspired by cognitive memory systems, BMAM decomposes memory into episodic, semantic, salience-aware, and control-oriented components that operate at complementary time scales. To support long-horizon reasoning, BMAM organizes episodic memories along explicit timelines and retrieves evidence by fusing multiple complementary signals. Experiments on the LoCoMo benchmark show that BMAM achieves 78.45 percent accuracy under the standard long-horizon evaluation setting, and ablation analyses confirm that the hippocampus-inspired episodic memory subsystem plays a critical role in temporal reasoning.

</details>


### [55] [Can We Improve Educational Diagram Generation with In-Context Examples? Not if a Hallucination Spoils the Bunch](https://arxiv.org/abs/2601.20476)
*Evanfiya Logacheva,Arto Hellas,Tsvetomila Mihaylova,Juha Sorva,Ava Heinonen,Juho Leinonen*

Main category: cs.CL

TL;DR: 该研究提出了一种基于修辞结构理论(RST)的图表代码生成方法，通过上下文示例提高图表生成质量，减少AI幻觉，但LLM的随机性导致生成质量不稳定。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在计算教育中广泛应用，但生成材料的质量引发教育者和学生的担忧。现有图表生成方法存在与用户期望不一致的问题，AI幻觉现象影响生成可靠性。

Method: 提出基于修辞结构理论(RST)的图表代码生成方法，使用上下文示例来对齐模型输出与用户期望。评估采用计算机科学教育者评估150个LLM生成的图表，考察逻辑组织、连接性、布局美观性和AI幻觉等维度。

Result: 该方法降低了事实幻觉率，提高了图表对上下文的忠实度。但LLM的随机性导致生成质量不稳定。分析发现：文本上下文越复杂，幻觉率越高；LLM经常无法检测自身输出中的错误。

Conclusion: 基于RST的上下文示例方法能改善图表生成质量，减少AI幻觉，但LLM的随机性限制了可靠性。研究还建立了评估数据集，可用于自动化图表评估。

Abstract: Generative artificial intelligence (AI) has found a widespread use in computing education; at the same time, quality of generated materials raises concerns among educators and students. This study addresses this issue by introducing a novel method for diagram code generation with in-context examples based on the Rhetorical Structure Theory (RST), which aims to improve diagram generation by aligning models' output with user expectations. Our approach is evaluated by computer science educators, who assessed 150 diagrams generated with large language models (LLMs) for logical organization, connectivity, layout aesthetic, and AI hallucination. The assessment dataset is additionally investigated for its utility in automated diagram evaluation. The preliminary results suggest that our method decreases the rate of factual hallucination and improves diagram faithfulness to provided context; however, due to LLMs' stochasticity, the quality of the generated diagrams varies. Additionally, we present an in-depth analysis and discussion on the connection between AI hallucination and the quality of generated diagrams, which reveals that text contexts of higher complexity lead to higher rates of hallucination and LLMs often fail to detect mistakes in their output.

</details>


### [56] [Beyond Divergent Creativity: A Human-Based Evaluation of Creativity in Large Language Models](https://arxiv.org/abs/2601.20546)
*Kumiko Nakajima,Jan Zuiderveld,Sandro Pezzelle*

Main category: cs.CL

TL;DR: 论文提出条件性发散关联任务（CDAT），在人类创造力理论基础上评估大语言模型的创造力，发现先进模型更注重适当性而非新颖性。


<details>
  <summary>Details</summary>
Motivation: 现有评估大语言模型创造力的方法（如发散关联任务DAT）缺乏对人类创造力理论的坚实基础，只关注新颖性而忽略了适当性这一核心创造力要素，导致评估结果难以解释。

Method: 提出条件性发散关联任务（CDAT），在评估新颖性的同时考虑上下文适当性，更好地分离噪音与创造力。评估了多种最先进的大语言模型，并与两个无创造能力的基线进行比较。

Result: DAT评估下，大语言模型的得分低于无创造能力的基线，说明DAT对模型评估无效。CDAT评估显示，较小模型家族通常最具创造力，而先进模型家族在较低新颖性水平下更注重适当性。训练和对齐过程可能使模型在创造力边界上向适当性偏移。

Conclusion: 基于新颖性和适当性组合的创造力理论，CDAT比DAT能更好地评估大语言模型的创造力。训练和对齐过程可能使先进模型在创造力方面做出权衡，更倾向于生成适当但不够新颖的内容。

Abstract: Large language models (LLMs) are increasingly used in verbal creative tasks. However, previous assessments of the creative capabilities of LLMs remain weakly grounded in human creativity theory and are thus hard to interpret. The widely used Divergent Association Task (DAT) focuses on novelty, ignoring appropriateness, a core component of creativity. We evaluate a range of state-of-the-art LLMs on DAT and show that their scores on the task are lower than those of two baselines that do not possess any creative abilities, undermining its validity for model evaluation. Grounded in human creativity theory, which defines creativity as the combination of novelty and appropriateness, we introduce Conditional Divergent Association Task (CDAT). CDAT evaluates novelty conditional on contextual appropriateness, separating noise from creativity better than DAT, while remaining simple and objective. Under CDAT, smaller model families often show the most creativity, whereas advanced families favor appropriateness at lower novelty. We hypothesize that training and alignment likely shift models along this frontier, making outputs more appropriate but less creative. We release the dataset and code.

</details>


### [57] [Single-Nodal Spontaneous Symmetry Breaking in NLP Models](https://arxiv.org/abs/2601.20582)
*Shalom Rosner,Ronit D. Gross,Ella Koresh,Ido Kanter*

Main category: cs.CL

TL;DR: 该论文将统计力学中的自发对称性破缺概念引入自然语言处理模型，发现BERT等模型在预训练和微调过程中会出现类似现象，即使在确定性动态和有限架构下也会发生，且这一现象可追溯到单个注意力头甚至单个节点层面。


<details>
  <summary>Details</summary>
Motivation: 受到统计力学中自发对称性破缺现象的启发，研究者想要探索这种物理现象是否也存在于自然语言处理模型中。统计力学中，系统在热力学极限下发生相变时，虽然哈密顿量保持反演对称性，但低温自由能却表现出降低的对称性。研究者希望验证类似现象是否在NLP模型的训练过程中出现。

Method: 研究者分析了BERT-6架构在Wikipedia数据集上的预训练过程，以及在FewRel分类任务上的微调过程。通过研究注意力头和单个节点的行为，观察对称性破缺现象。使用凸包分析来上界节点功能，并研究了节点数量增加时学习能力的交叉变化，这种变化受到随机猜测减少和节点协同增强之间的权衡影响。

Result: 研究发现NLP模型在预训练和微调过程中确实会出现自发对称性破缺现象，即使在确定性动态和有限架构下也会发生。这一现象可追溯到单个注意力头层面，甚至可缩小到节点子集和单个节点层面。节点在预训练后获得学习有限标记的能力，在微调后获得学习特定分类任务标签的能力。节点数量增加时会出现学习能力的交叉变化，节点协同作用超过个体能力之和。与自旋玻璃系统不同，该框架中每个节点功能都明确贡献于全局网络任务。

Conclusion: 该研究成功地将统计力学中的自发对称性破缺概念应用于自然语言处理模型分析，揭示了NLP模型训练过程中出现的类似物理现象。这一发现为理解神经网络的学习机制提供了新的理论视角，表明即使在有限架构和确定性训练过程中，对称性破缺现象依然存在，且可追溯到模型的微观组件层面。

Abstract: Spontaneous symmetry breaking in statistical mechanics primarily occurs during phase transitions at the thermodynamic limit where the Hamiltonian preserves inversion symmetry, yet the low-temperature free energy exhibits reduced symmetry. Herein, we demonstrate the emergence of spontaneous symmetry breaking in natural language processing (NLP) models during both pre-training and fine-tuning, even under deterministic dynamics and within a finite training architecture. This phenomenon occurs at the level of individual attention heads and is scaled-down to its small subset of nodes and also valid at a single-nodal level, where nodes acquire the capacity to learn a limited set of tokens after pre-training or labels after fine-tuning for a specific classification task. As the number of nodes increases, a crossover in learning ability occurs, governed by the tradeoff between a decrease following random-guess among increased possible outputs, and enhancement following nodal cooperation, which exceeds the sum of individual nodal capabilities. In contrast to spin-glass systems, where a microscopic state of frozen spins cannot be directly linked to the free-energy minimization goal, each nodal function in this framework contributes explicitly to the global network task and can be upper-bounded using convex hull analysis. Results are demonstrated using BERT-6 architecture pre-trained on Wikipedia dataset and fine-tuned on the FewRel classification task.

</details>


### [58] [A Computational Approach to Language Contact -- A Case Study of Persian](https://arxiv.org/abs/2601.20592)
*Ali Basirat,Danial Namazifard,Navid Baradaran Hemmati*

Main category: cs.CL

TL;DR: 研究波斯语单语模型中间表征中的语言接触痕迹，发现句法信息对历史接触不敏感，而形态特征受语言特定结构强烈影响。


<details>
  <summary>Details</summary>
Motivation: 探究单语语言模型中间表征中是否保留语言接触的结构痕迹，特别是以波斯语这种历史接触丰富的语言为研究对象。

Method: 使用波斯语训练的模型，暴露于与波斯语有不同接触程度和类型的语言，量化中间表征中的语言信息量，评估不同形态句法特征在模型组件中的分布。

Result: 普遍句法信息对历史接触不敏感，而格和性等形态特征受语言特定结构强烈影响，表明单语模型中的接触效应具有选择性和结构约束性。

Conclusion: 单语语言模型中的语言接触效应不是全局性的，而是选择性地影响特定类型的语言特征，特别是形态特征而非句法特征。

Abstract: We investigate structural traces of language contact in the intermediate representations of a monolingual language model. Focusing on Persian (Farsi) as a historically contact-rich language, we probe the representations of a Persian-trained model when exposed to languages with varying degrees and types of contact with Persian. Our methodology quantifies the amount of linguistic information encoded in intermediate representations and assesses how this information is distributed across model components for different morphosyntactic features. The results show that universal syntactic information is largely insensitive to historical contact, whereas morphological features such as Case and Gender are strongly shaped by language-specific structure, suggesting that contact effects in monolingual language models are selective and structurally constrained.

</details>


### [59] [AgentIF-OneDay: A Task-level Instruction-Following Benchmark for General AI Agents in Daily Scenarios](https://arxiv.org/abs/2601.20613)
*Kaiyuan Chen,Qimin Wu,Taiyu Hou,Tianhao Tang,Xueyu Hu,Yuchen Hou,Bikun Li,Chengming Qian,Guoyin Wang,Haolin Chen,Haotong Tian,Haoye Zhang,Haoyu Bian,Hongbing Pan,Hongkang Zhang,Hongyi Zhou,Jiaqi Cai,Jiewu Rao,Jiyuan Ren,Keduan Huang,Lucia Zhu Huang,Mingyu Yuan,Naixu Guo,Qicheng Tang,Qinyan Zhang,Shuai Chen,Siheng Chen,Ting Ting Li,Xiaoxing Guo,Yaocheng Zuo,Yaoqi Guo,Yinan Wang,Yinzhou Yu,Yize Wang,Yuan Jiang,Yuan Tian,Yuanshuo Zhang,Yuxuan Liu,Yvette Yan Zeng,Zenyu Shan,Zihan Yin,Xiaobo Hu,Yang Liu,Yixin Ren,Yuan Gong*

Main category: cs.CL

TL;DR: 提出了AgentIF-OneDay基准，评估AI代理在多样化日常任务中的表现，包括工作流执行、隐含指令理解和迭代优化，发现基于API的代理产品和基于RL的ChatGPT代理表现领先。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理评估过于关注任务难度提升，而忽视了覆盖普通用户日常工作、生活和学习所需的多样化任务类型，导致大众对AI代理能力的认知有限。

Method: 提出AgentIF-OneDay基准，包含104个任务和767个评分点，涵盖三个用户中心类别：开放工作流执行、隐含指令理解和迭代优化。采用实例级评分标准和改进的评估流程，将基于LLM的验证与人工判断相结合。

Result: 评估了四种主流通用AI代理，发现基于API构建的代理产品和基于强化学习的ChatGPT代理同时处于第一梯队。领先的LLM API和开源模型已经内化了代理能力，使AI应用团队能够开发前沿的代理产品。

Conclusion: AgentIF-OneDay基准能够有效评估AI代理在多样化日常任务中的表现，揭示了当前代理技术的现状和发展趋势，为AI应用开发提供了重要参考。

Abstract: The capacity of AI agents to effectively handle tasks of increasing duration and complexity continues to grow, demonstrating exceptional performance in coding, deep research, and complex problem-solving evaluations. However, in daily scenarios, the perception of these advanced AI capabilities among general users remains limited. We argue that current evaluations prioritize increasing task difficulty without sufficiently addressing the diversity of agentic tasks necessary to cover the daily work, life, and learning activities of a broad demographic. To address this, we propose AgentIF-OneDay, aimed at determining whether general users can utilize natural language instructions and AI agents to complete a diverse array of daily tasks. These tasks require not only solving problems through dialogue but also understanding various attachment types and delivering tangible file-based results. The benchmark is structured around three user-centric categories: Open Workflow Execution, which assesses adherence to explicit and complex workflows; Latent Instruction, which requires agents to infer implicit instructions from attachments; and Iterative Refinement, which involves modifying or expanding upon ongoing work. We employ instance-level rubrics and a refined evaluation pipeline that aligns LLM-based verification with human judgment, achieving an 80.1% agreement rate using Gemini-3-Pro. AgentIF-OneDay comprises 104 tasks covering 767 scoring points. We benchmarked four leading general AI agents and found that agent products built based on APIs and ChatGPT agents based on agent RL remain in the first tier simultaneously. Leading LLM APIs and open-source models have internalized agentic capabilities, enabling AI application teams to develop cutting-edge Agent products.

</details>


### [60] [P2S: Probabilistic Process Supervision for General-Domain Reasoning Question Answering](https://arxiv.org/abs/2601.20649)
*Wenlin Zhong,Chengyuan Liu,Yiquan Wu,Bovin Tan,Changlong Sun,Yi Wang,Xiaozhong Liu,Kun Kuang*

Main category: cs.CL

TL;DR: P2S（概率过程监督）是一种自监督强化学习框架，通过计算路径忠实度奖励为推理过程的每一步提供细粒度监督，无需额外奖励模型或人工标注。


<details>
  <summary>Details</summary>
Motivation: 现有基于结果的强化学习方法（如RLPR）虽然利用最终答案生成概率作为奖励信号，但忽视了推理过程本身的逐步监督，存在奖励稀疏性问题。

Method: 提出概率过程监督（P2S）框架：1）在强化学习过程中合成并过滤高质量参考推理链（gold-CoT）；2）计算路径忠实度奖励（PFR），基于当前推理前缀生成gold-CoT后缀的条件概率来评估每个推理步骤；3）PFR可与任何基于结果的奖励灵活结合。

Result: 在阅读理解和医学问答基准测试上的广泛实验表明，P2S显著优于强基线方法。

Conclusion: P2S通过提供细粒度的过程奖励直接解决了奖励稀疏性问题，能够有效监督推理过程而无需额外奖励模型或人工标注，在一般领域推理任务中表现出色。

Abstract: While reinforcement learning with verifiable rewards (RLVR) has advanced LLM reasoning in structured domains like mathematics and programming, its application to general-domain reasoning tasks remains challenging due to the absence of verifiable reward signals. To this end, methods like Reinforcement Learning with Reference Probability Reward (RLPR) have emerged, leveraging the probability of generating the final answer as a reward signal. However, these outcome-focused approaches neglect crucial step-by-step supervision of the reasoning process itself. To address this gap, we introduce Probabilistic Process Supervision (P2S), a novel self-supervision framework that provides fine-grained process rewards without requiring a separate reward model or human-annotated reasoning steps. During reinforcement learning, P2S synthesizes and filters a high-quality reference reasoning chain (gold-CoT). The core of our method is to calculate a Path Faithfulness Reward (PFR) for each reasoning step, which is derived from the conditional probability of generating the gold-CoT's suffix, given the model's current reasoning prefix. Crucially, this PFR can be flexibly integrated with any outcome-based reward, directly tackling the reward sparsity problem by providing dense guidance. Extensive experiments on reading comprehension and medical Question Answering benchmarks show that P2S significantly outperforms strong baselines.

</details>


### [61] [A Dialectic Pipeline for Improving LLM Robustness](https://arxiv.org/abs/2601.20659)
*Sara Candussio*

Main category: cs.CL

TL;DR: 提出一种自对话辩证流水线，通过自我反思和修正错误答案来减少LLM幻觉，保持泛化能力的同时提升输出质量。


<details>
  <summary>Details</summary>
Motivation: 当前减少LLM幻觉的方法如领域微调或训练专用验证器需要大量计算资源，且局限于特定知识领域，限制了大规模应用。需要一种既能保持模型泛化能力又能提升输出质量的方法。

Method: 提出辩证流水线方法，通过自对话让LLM反思和修正初步错误答案。在oracle-RAG设置中为所有流水线阶段提供相关上下文，并研究上下文摘要和过滤的影响。在不同数据集和模型家族上测试不同流水线设置。

Result: 辩证流水线显著优于标准模型回答，且一致性地比仅使用思维链提示获得更高性能。

Conclusion: 自对话辩证流水线是一种有效减少LLM幻觉的方法，能在保持模型泛化能力的同时显著提升输出质量，且优于传统思维链方法。

Abstract: Assessing ways in which Language Models can reduce their hallucinations and improve the outputs' quality is crucial to ensure their large-scale use.
  However, methods such as fine-tuning on domain-specific data or the training of a separate \textit{ad hoc} verifier require demanding computational resources (not feasible for many user applications) and constrain the models to specific fields of knowledge.
  In this thesis, we propose a dialectic pipeline that preserves LLMs' generalization abilities while improving the quality of its answer via self-dialogue, enabling it to reflect upon and correct tentative wrong answers.
  We experimented with different pipeline settings, testing our proposed method on different datasets and on different families of models. All the pipeline stages are enriched with the relevant context (in an oracle-RAG setting) and a study on the impact of its summarization or its filtering is conducted.
  We find that our proposed dialectic pipeline is able to outperform by significative margins the standard model answers and that it consistently achieves higher performances than Chain-of-Thought only prompting.

</details>


### [62] [Harnessing Large Language Models for Precision Querying and Retrieval-Augmented Knowledge Extraction in Clinical Data Science](https://arxiv.org/abs/2601.20674)
*Juan Jose Rubio Jan,Jack Wu,Julia Ive*

Main category: cs.CL

TL;DR: 本研究探索LLMs在EHR数据科学任务中的应用，包括结构化数据查询和非结构化文本信息提取，提出自动评估框架并在MIMIC III数据集上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在电子健康记录数据科学任务中的实际应用潜力，特别是针对结构化数据查询和非结构化临床文本信息提取这两项基础任务，以评估LLMs在临床工作流程中的支持能力。

Method: 1) 针对结构化数据查询任务，使用Python/Pandas等编程语言；2) 针对非结构化临床文本信息提取，采用检索增强生成(RAG)管道；3) 提出灵活的自动评估框架，生成针对每个数据集或任务特性的合成问答对；4) 在MIMIC III数据集的精选子集上进行实验（四个结构化表和一个临床记录类型）；5) 混合使用本地托管和API基础的LLMs；6) 结合精确匹配指标、语义相似度和人工判断进行评估。

Result: 研究发现LLMs在支持精确查询和准确信息提取方面具有潜力，能够准确与大型结构化数据集交互进行数据分析，并在RAG支持下可靠地从自由文本健康记录中提取语义正确的信息。

Conclusion: LLMs在临床工作流程中支持精确查询和准确信息提取方面展现出显著潜力，为电子健康记录数据科学任务提供了有效的技术支持。

Abstract: This study applies Large Language Models (LLMs) to two foundational Electronic Health Record (EHR) data science tasks: structured data querying (using programmatic languages, Python/Pandas) and information extraction from unstructured clinical text via a Retrieval Augmented Generation (RAG) pipeline. We test the ability of LLMs to interact accurately with large structured datasets for analytics and the reliability of LLMs in extracting semantically correct information from free text health records when supported by RAG. To this end, we presented a flexible evaluation framework that automatically generates synthetic question and answer pairs tailored to the characteristics of each dataset or task. Experiments were conducted on a curated subset of MIMIC III, (four structured tables and one clinical note type), using a mix of locally hosted and API-based LLMs. Evaluation combined exact-match metrics, semantic similarity, and human judgment. Our findings demonstrate the potential of LLMs to support precise querying and accurate information extraction in clinical workflows.

</details>


### [63] [Efficient Multimodal Planning Agent for Visual Question-Answering](https://arxiv.org/abs/2601.20676)
*Zhuo Chen,Xinyu Geng,Xinyu Wang,Yong Jiang,Zhen Zhang,Pengjun Xie,Kewei Tu*

Main category: cs.CL

TL;DR: 提出一种训练多模态规划代理的方法，动态分解mRAG流水线来解决VQA任务，在保持性能的同时大幅提升效率


<details>
  <summary>Details</summary>
Motivation: 现有的多模态检索增强生成（mRAG）方法在处理VQA任务时通常采用多阶段流水线，存在固有依赖关系，导致效率低下。需要一种方法在保持VQA任务性能的同时解决效率限制问题。

Method: 训练一个多模态规划代理，动态分解mRAG流水线，智能判断每个mRAG步骤的必要性，优化效率与效果的权衡

Result: 代理能减少冗余计算，相比现有方法减少超过60%的搜索时间，降低昂贵工具调用。在六个不同数据集上平均表现优于所有基线方法，包括深度研究代理和精心设计的基于提示的方法

Conclusion: 提出的多模态规划代理方法有效解决了mRAG流水线效率问题，在保持VQA任务性能的同时显著提升了计算效率，为知识密集型VQA任务提供了更高效的解决方案

Abstract: Visual Question-Answering (VQA) is a challenging multimodal task that requires integrating visual and textual information to generate accurate responses. While multimodal Retrieval-Augmented Generation (mRAG) has shown promise in enhancing VQA systems by providing more evidence on both image and text sides, the default procedure that addresses VQA queries, especially the knowledge-intensive ones, often relies on multi-stage pipelines of mRAG with inherent dependencies. To mitigate the inefficiency limitations while maintaining VQA task performance, this paper proposes a method that trains a multimodal planning agent, dynamically decomposing the mRAG pipeline to solve the VQA task. Our method optimizes the trade-off between efficiency and effectiveness by training the agent to intelligently determine the necessity of each mRAG step. In our experiments, the agent can help reduce redundant computations, cutting search time by over 60\% compared to existing methods and decreasing costly tool calls. Meanwhile, experiments demonstrate that our method outperforms all baselines, including a Deep Research agent and a carefully designed prompt-based method, on average over six various datasets. Code will be released.

</details>


### [64] [ShieldedCode: Learning Robust Representations for Virtual Machine Protected Code](https://arxiv.org/abs/2601.20679)
*Mingqiao Mo,Yunlong Tan,Hao Zhang,Heng Zhang,Yangfan He*

Main category: cs.CL

TL;DR: ShieldedCode是首个保护感知框架，通过分层依赖建模和对比学习优化，学习虚拟机器保护代码的鲁棒表示，显著提升代码生成和相似性检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统虚拟机保护(VMP)依赖僵化的基于规则的转换，设计成本高且易受自动化分析攻击，而大型语言模型在代码生成方面的潜力尚未在软件保护领域得到充分利用。

Method: 1) 构建大规模源代码与标准化VM实现的配对数据集；2) 引入指令内、指令前和指令间三个层次的分层依赖建模；3) 联合优化语言建模与功能感知和保护感知的对比目标；4) 提出保护效果优化任务量化不同VM变体的韧性；5) 采用两阶段持续预训练和微调流程。

Result: 在L0 VM代码生成上达到26.95% Pass@1，优于GPT-4o的22.58%；在二进制相似性检测Recall@1上比jTrans等SOTA方法提升10%；显著提高跨不同保护级别的鲁棒性。

Conclusion: 该框架首次实现了对VMP保护代码的学习型表示，为基于学习的软件防御开辟了新研究方向，展示了LLM在软件保护领域的巨大潜力。

Abstract: Large language models (LLMs) have achieved remarkable progress in code generation, yet their potential for software protection remains largely untapped. Reverse engineering continues to threaten software security, while traditional virtual machine protection (VMP) relies on rigid, rule-based transformations that are costly to design and vulnerable to automated analysis. In this work, we present the first protection-aware framework that learns robust representations of VMP-protected code. Our approach builds large-scale paired datasets of source code and normalized VM implementations, and introduces hierarchical dependency modeling at intra-, preceding-, and inter-instruction levels. We jointly optimize language modeling with functionality-aware and protection-aware contrastive objectives to capture both semantic equivalence and protection strength. To further assess resilience, we propose a protection effectiveness optimization task that quantifies and ranks different VM variants derived from the same source. Coupled with a two-stage continual pre-training and fine-tuning pipeline, our method enables models to generate, compare, and reason over protected code. Extensive experiments show that our framework significantly improves robustness across diverse protection levels, opening a new research direction for learning-based software defense. In this work, we present ShieldedCode, the first protection-aware framework that learns robust representations of VMP-protected code. Our method achieves 26.95% Pass@1 on L0 VM code generation compared to 22.58% for GPT-4o., and improves binary similarity detection Recall@1 by 10% over state of art methods like jTrans.

</details>


### [65] [Online Density-Based Clustering for Real-Time Narrative Evolution Monitorin](https://arxiv.org/abs/2601.20680)
*Ostap Vykhopen,Viktoria Skorik,Maxim Tereschenko,Veronika Solopova*

Main category: cs.CL

TL;DR: 本研究评估用在线聚类算法替代HDBSCAN来处理社交媒体流数据，解决传统批量聚类在实时叙事监控中的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 社交媒体叙事监控系统面临实时处理流数据的可扩展性挑战。传统批量聚类算法如HDBSCAN需要完全重新训练每个时间窗口，导致内存限制、计算效率低下，且无法适应实时演变的叙事。

Method: 采用三层架构系统（数据收集、建模、仪表板生成），评估多种在线聚类算法。提出平衡传统聚类指标（轮廓系数、戴维斯-布尔丁指数）与叙事指标（叙事独特性、偶然性和方差）的评估标准。使用乌克兰信息空间历史数据集进行滑动窗口模拟。

Result: 研究评估了在线聚类算法在集群质量保持、计算效率、内存占用和现有工作流集成兼容性方面的表现。通过历史数据模拟比较了算法在实际操作环境中的权衡。

Conclusion: 该研究填补了面向批处理的主题建模框架与社交媒体监控流式特性之间的关键空白，对计算社会科学、危机信息学和叙事监控系统具有重要意义。

Abstract: Automated narrative intelligence systems for social media monitoring face significant scalability challenges when processing continuous data streams using traditional batch clustering algorithms. We investigate the replacement of HDBSCAN (offline clustering) with online (streaming/incremental) clustering methods in a production narrative report generation pipeline. The proposed system employs a three-stage architecture (data collection, modeling, dashboard generation) that processes thousands of multilingual social media documents daily. While HDBSCAN excels at discovering hierarchical density-based clusters and handling noise, its batch-only nature necessitates complete retraining for each time window, resulting in memory constraints, computational inefficiency, and inability to adapt to evolving narratives in real-time. This work evaluates a bunch of online clustering algorithms across dimensions of cluster quality preservation, computational efficiency, memory footprint, and integration compatibility with existing workflows. We propose evaluation criteria that balance traditional clustering metrics (Silhouette Coefficient, Davies-Bouldin Index) with narrative metrics (narrative distinctness, contingency and variance). Our methodology includes sliding-window simulations on historical datasets from Ukraine information space, enabling comparative analysis of algorithmic trade-offs in realistic operational contexts. This research addresses a critical gap between batch-oriented topic modeling frameworks and the streaming nature of social media monitoring, with implications for computational social science, crisis informatics, and narrative surveillance systems.

</details>


### [66] [AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts](https://arxiv.org/abs/2601.20730)
*Shicheng Fang,Yuxin Wang,XiaoRan Liu,Jiahao Lu,Chuanyuan Tan,Xinchi Chen,Yining Zheng. Xuanjing Huang,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文提出了AgentLongBench基准，通过横向思维谜题模拟环境交互来评估LLM代理在动态长上下文中的能力，发现现有代理在动态信息合成方面存在显著弱点。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理需要管理扩展的动态上下文，但现有基准大多是静态的，依赖于被动检索任务，无法模拟代理与环境交互的复杂性（如非线性推理和迭代反馈）。

Method: 引入AgentLongBench基准，基于横向思维谜题通过模拟环境推演来评估代理，生成知识密集和知识无关场景下的严格交互轨迹，测试了32K到4M tokens范围内的最先进模型和记忆系统。

Result: 实验显示代理在静态检索方面表现良好，但在动态信息合成方面存在严重困难，特别是在需要解决查询所需的最少token数量时表现不佳。大规模工具响应中的高信息密度比长轮对话中的内存碎片化带来更大挑战。

Conclusion: LLM代理在处理动态、交互式长上下文时存在显著缺陷，特别是当需要从密集信息中动态合成答案时。解决查询所需的最少token数量是导致性能下降的关键因素，这对未来代理系统的设计具有重要意义。

Abstract: The evolution of Large Language Models (LLMs) into autonomous agents necessitates the management of extensive, dynamic contexts. Current benchmarks, however, remain largely static, relying on passive retrieval tasks that fail to simulate the complexities of agent-environment interaction, such as non-linear reasoning and iterative feedback. To address this, we introduce \textbf{AgentLongBench}, which evaluates agents through simulated environment rollouts based on Lateral Thinking Puzzles. This framework generates rigorous interaction trajectories across knowledge-intensive and knowledge-free scenarios. Experiments with state-of-the-art models and memory systems (32K to 4M tokens) expose a critical weakness: while adept at static retrieval, agents struggle with the dynamic information synthesis essential for workflows. Our analysis indicates that this degradation is driven by the minimum number of tokens required to resolve a query. This factor explains why the high information density inherent in massive tool responses poses a significantly greater challenge than the memory fragmentation typical of long-turn dialogues.

</details>


### [67] [QueerGen: How LLMs Reflect Societal Norms on Gender and Sexuality in Sentence Completion Tasks](https://arxiv.org/abs/2601.20731)
*Mae Sosto,Delfina Sol Martinez Pandiani,Laura Hollink*

Main category: cs.CL

TL;DR: 研究发现大型语言模型会复制社会规范（特别是异性恋-顺性别规范），并在文本生成中产生可测量的偏见。掩码语言模型对同性恋标记对象产生最不利的情感、更高的毒性和更负面的评价，而自回归语言模型部分缓解了这些模式。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究大型语言模型如何复制社会规范（特别是异性恋-顺性别规范），以及这些规范如何转化为文本生成中的可测量偏见。研究者关注性别或性取向信息是否影响LLM对不同类别对象的响应。

Method: 研究方法包括：1) 将研究对象分为三类：同性恋标记、非同性恋标记和"未标记"的规范化类别；2) 将表征不平衡操作化为四个维度的可测量差异：情感、评价、毒性和预测多样性；3) 通过英语句子补全任务分析不同LLM的响应模式。

Result: 研究结果显示：1) 掩码语言模型对同性恋标记对象产生最不利的情感、更高的毒性和更负面的评价；2) 自回归语言模型部分缓解了这些偏见模式；3) 闭源自回归语言模型倾向于对未标记对象产生更有害的输出；4) 偏见的形式和程度强烈依赖于具体模型特征。

Conclusion: 研究结论表明：大型语言模型确实复制了规范化的社会假设，但偏见的形式和程度强烈依赖于具体模型特征。这些模型可能重新分配而非消除表征伤害，说明技术特性会影响偏见的表达方式，但不会完全消除偏见。

Abstract: This paper examines how Large Language Models (LLMs) reproduce societal norms, particularly heterocisnormativity, and how these norms translate into measurable biases in their text generations. We investigate whether explicit information about a subject's gender or sexuality influences LLM responses across three subject categories: queer-marked, non-queer-marked, and the normalized "unmarked" category. Representational imbalances are operationalized as measurable differences in English sentence completions across four dimensions: sentiment, regard, toxicity, and prediction diversity. Our findings show that Masked Language Models (MLMs) produce the least favorable sentiment, higher toxicity, and more negative regard for queer-marked subjects. Autoregressive Language Models (ARLMs) partially mitigate these patterns, while closed-access ARLMs tend to produce more harmful outputs for unmarked subjects. Results suggest that LLMs reproduce normative social assumptions, though the form and degree of bias depend strongly on specific model characteristics, which may redistribute, but not eliminate, representational harms.

</details>


### [68] [Like a Therapist, But Not: Reddit Narratives of AI in Mental Health Contexts](https://arxiv.org/abs/2601.20747)
*Elham Aghakhani,Rezvaneh Rezapour*

Main category: cs.CL

TL;DR: 研究通过分析Reddit上5,126个心理健康相关帖子，探讨用户如何评估和与用于情感支持的AI系统互动，发现用户参与主要受叙述结果、信任和响应质量影响，而非单纯情感联结。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地被用于情感支持和心理健康相关互动，但在日常使用中人们对这些系统的评估和关系建立方式尚不明确，需要了解在敏感现实场景中用户如何解释语言技术。

Method: 基于技术接受模型和治疗联盟理论构建理论指导的标注框架，采用混合LLM-人工标注管道，对5,126个Reddit帖子进行大规模分析，评估评价性语言、采纳相关态度和关系对齐。

Result: 用户参与主要受叙述结果、信任和响应质量影响；积极情绪与任务和目标对齐最强相关；以陪伴为导向的使用更常涉及联盟错配和依赖、症状加剧等风险报告。

Conclusion: 研究表明理论建构可以在大规模话语分析中操作化，强调研究用户在敏感现实环境中如何解释语言技术的重要性，为AI在心理健康领域的负责任设计提供洞见。

Abstract: Large language models (LLMs) are increasingly used for emotional support and mental health-related interactions outside clinical settings, yet little is known about how people evaluate and relate to these systems in everyday use. We analyze 5,126 Reddit posts from 47 mental health communities describing experiential or exploratory use of AI for emotional support or therapy. Grounded in the Technology Acceptance Model and therapeutic alliance theory, we develop a theory-informed annotation framework and apply a hybrid LLM-human pipeline to analyze evaluative language, adoption-related attitudes, and relational alignment at scale. Our results show that engagement is shaped primarily by narrated outcomes, trust, and response quality, rather than emotional bond alone. Positive sentiment is most strongly associated with task and goal alignment, while companionship-oriented use more often involves misaligned alliances and reported risks such as dependence and symptom escalation. Overall, this work demonstrates how theory-grounded constructs can be operationalized in large-scale discourse analysis and highlights the importance of studying how users interpret language technologies in sensitive, real-world contexts.

</details>


### [69] [Persona Prompting as a Lens on LLM Social Reasoning](https://arxiv.org/abs/2601.20757)
*Jing Yang,Moritz Hechtbauer,Elisabeth Khalilov,Evelyn Luise Brinkmann,Vera Schmitt,Nils Feldhus*

Main category: cs.CL

TL;DR: 研究探讨了在仇恨言论检测等社会敏感任务中，人格提示（Persona Prompting）对LLM生成理由质量的影响，发现人格提示虽能改善分类但会降低理由质量，且无法真正对齐真实人口统计群体或减轻模型偏见。


<details>
  <summary>Details</summary>
Motivation: 在社会敏感任务（如仇恨言论检测）中，LLM生成解释的质量对用户信任和模型对齐至关重要。人格提示被广泛用于引导模型进行用户特定生成，但其对模型理由生成的影响尚未充分探索。

Method: 研究通过模拟不同人口统计人格来条件化LLM，使用带词级理由标注的数据集，测量与不同人口统计群体人类标注的一致性，并评估人格提示对模型偏见和人类对齐的影响。在三个LLM上进行了评估。

Result: 研究发现三个关键结果：1）人格提示改善了最主观任务（仇恨言论）的分类性能，但降低了理由质量；2）模拟人格无法与真实世界人口统计群体对齐，高人格间一致性表明模型难以被显著引导；3）无论是否使用人格提示，模型都表现出一致的人口统计偏见和过度标记内容为有害的强烈倾向。

Conclusion: 人格提示在社会敏感任务中虽然能改善分类，但通常以牺牲理由质量为代价，且无法缓解潜在的偏见，因此在应用时需要谨慎权衡。

Abstract: For socially sensitive tasks like hate speech detection, the quality of explanations from Large Language Models (LLMs) is crucial for factors like user trust and model alignment. While Persona prompting (PP) is increasingly used as a way to steer model towards user-specific generation, its effect on model rationales remains underexplored. We investigate how LLM-generated rationales vary when conditioned on different simulated demographic personas. Using datasets annotated with word-level rationales, we measure agreement with human annotations from different demographic groups, and assess the impact of PP on model bias and human alignment. Our evaluation across three LLMs results reveals three key findings: (1) PP improving classification on the most subjective task (hate speech) but degrading rationale quality. (2) Simulated personas fail to align with their real-world demographic counterparts, and high inter-persona agreement shows models are resistant to significant steering. (3) Models exhibit consistent demographic biases and a strong tendency to over-flag content as harmful, regardless of PP. Our findings reveal a critical trade-off: while PP can improve classification in socially-sensitive tasks, it often comes at the cost of rationale quality and fails to mitigate underlying biases, urging caution in its application.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [70] [LLaTTE: Scaling Laws for Multi-Stage Sequence Modeling in Large-Scale Ads Recommendation](https://arxiv.org/abs/2601.20083)
*Lee Xiong,Zhirong Chen,Rahul Mayuranath,Shangran Qiu,Arda Ozdemir,Lu Li,Yang Hu,Dave Li,Jingtao Ren,Howard Cheng,Fabian Souto Herrera,Ahmed Agiza,Baruch Epshtein,Anuj Aggarwal,Julia Ulziisaikhan,Chao Wang,Dinesh Ramasamy,Parshva Doshi,Sri Reddy,Arnold Overwijk*

Main category: cs.IR

TL;DR: LLaTTE：用于广告推荐的Transformer架构，发现推荐系统中的序列建模遵循类似LLM的幂律缩放规律，并提出两阶段架构解决延迟约束问题。


<details>
  <summary>Details</summary>
Motivation: 工业推荐系统面临严格的延迟约束，同时需要利用深层长上下文模型的能力。如何在保证低延迟的同时实现模型规模的持续扩展是核心挑战。

Method: 提出LLaTTE（LLM-Style Latent Transformers for Temporal Events）架构，包含两阶段设计：将大型长上下文模型的重计算卸载到异步上游用户模型，下游排序任务使用轻量级模型。

Result: 发现推荐系统序列建模遵循类似LLM的幂律缩放规律；语义特征是缩放的前提条件；上游改进可预测地传递到下游排序任务；在Meta部署实现Facebook Feed和Reels 4.3%转化率提升。

Conclusion: LLaTTE为工业推荐系统提供了一种实用的缩放蓝图，通过两阶段架构在严格延迟约束下实现模型规模的持续扩展，证明了推荐系统中存在可预测的缩放规律。

Abstract: We present LLaTTE (LLM-Style Latent Transformers for Temporal Events), a scalable transformer architecture for production ads recommendation. Through systematic experiments, we demonstrate that sequence modeling in recommendation systems follows predictable power-law scaling similar to LLMs. Crucially, we find that semantic features bend the scaling curve: they are a prerequisite for scaling, enabling the model to effectively utilize the capacity of deeper and longer architectures. To realize the benefits of continued scaling under strict latency constraints, we introduce a two-stage architecture that offloads the heavy computation of large, long-context models to an asynchronous upstream user model. We demonstrate that upstream improvements transfer predictably to downstream ranking tasks. Deployed as the largest user model at Meta, this multi-stage framework drives a 4.3\% conversion uplift on Facebook Feed and Reels with minimal serving overhead, establishing a practical blueprint for harnessing scaling laws in industrial recommender systems.

</details>


### [71] [IMRNNs: An Efficient Method for Interpretable Dense Retrieval via Embedding Modulation](https://arxiv.org/abs/2601.20084)
*Yash Saxena,Ankur Padia,Kalpa Gunaratna,Manas Gaur*

Main category: cs.IR

TL;DR: IMRNNNs通过轻量级适配器框架增强稠密检索器，实现查询和文档之间的动态双向调制，既提升可解释性又改进检索效果。


<details>
  <summary>Details</summary>
Motivation: 黑盒稠密检索器在RAG中缺乏可解释性，查询和文档的语义交互关系不透明，现有方法如重排序计算成本高且无法揭示底层语义对齐。

Method: 提出IMRNNs框架，使用两个独立适配器：一个基于当前查询条件化文档嵌入，另一个利用初始检索文档的语料级反馈优化查询嵌入，实现动态迭代调制。

Result: 在七个基准数据集上，IMRNNs相比SOTA基线平均提升6.35% nDCG、7.14%召回率和7.04% MRR，同时增强检索可解释性。

Conclusion: 可解释性驱动的调制机制不仅能解释RAG系统中的检索行为，还能显著提升检索效果，为稠密检索器提供了轻量级可解释增强方案。

Abstract: Interpretability in black-box dense retrievers remains a central challenge in Retrieval-Augmented Generation (RAG). Understanding how queries and documents semantically interact is critical for diagnosing retrieval behavior and improving model design. However, existing dense retrievers rely on static embeddings for both queries and documents, which obscures this bidirectional relationship. Post-hoc approaches such as re-rankers are computationally expensive, add inference latency, and still fail to reveal the underlying semantic alignment. To address these limitations, we propose Interpretable Modular Retrieval Neural Networks (IMRNNs), a lightweight framework that augments any dense retriever with dynamic, bidirectional modulation at inference time. IMRNNs employ two independent adapters: one conditions document embeddings on the current query, while the other refines the query embedding using corpus-level feedback from initially retrieved documents. This iterative modulation process enables the model to adapt representations dynamically and expose interpretable semantic dependencies between queries and documents. Empirically, IMRNNs not only enhance interpretability but also improve retrieval effectiveness. Across seven benchmark datasets, applying our method to standard dense retrievers yields average gains of +6.35% nDCG, +7.14% recall, and +7.04% MRR over state-of-the-art baselines. These results demonstrate that incorporating interpretability-driven modulation can both explain and enhance retrieval in RAG systems.

</details>


### [72] [Taxonomy of the Retrieval System Framework: Pitfalls and Paradigms](https://arxiv.org/abs/2601.20131)
*Deep Shah,Sanket Badhe,Nehal Kathrotia*

Main category: cs.IR

TL;DR: 本文提出了一个用于设计嵌入检索系统的四层框架，通过分析表示层、粒度层、编排层和鲁棒性层的设计选择，帮助从业者在效率与效果之间找到最优平衡。


<details>
  <summary>Details</summary>
Motivation: 设计嵌入检索系统需要在效率与效果之间进行复杂的权衡，当前缺乏系统性的框架来指导这些设计决策。本文旨在通过结构化分析系统设计栈的各个层面，为从业者提供全面的优化框架。

Method: 采用垂直遍历系统设计栈的方法，将检索系统设计分为四个层次：1) 表示层：分析损失函数和架构（Bi-encoder和Cross-encoder）如何定义语义相关性和几何投影；2) 粒度层：评估原子和分层分块等分割策略如何缓解长文档中的信息瓶颈；3) 编排层：讨论超越单向量范式的方法，包括分层检索、代理分解和多阶段重排序管道；4) 鲁棒性层：识别针对领域泛化失败、词汇盲点和时间漂移导致的检索质量静默退化的架构缓解措施。

Result: 通过分类这些限制和设计选择，本文为现代神经搜索系统中优化效率-效果边界提供了一个全面的框架，帮助从业者系统性地设计和优化嵌入检索系统。

Conclusion: 本文提出的四层框架为嵌入检索系统设计提供了系统化的方法论，通过结构化分析不同层次的设计决策，使从业者能够在效率与效果之间做出明智的权衡，从而优化神经搜索系统的整体性能。

Abstract: Designing an embedding retrieval system requires navigating a complex design space of conflicting trade-offs between efficiency and effectiveness. This work structures these decisions as a vertical traversal of the system design stack. We begin with the Representation Layer by examining how loss functions and architectures, specifically Bi-encoders and Cross-encoders, define semantic relevance and geometric projection. Next, we analyze the Granularity Layer and evaluate how segmentation strategies like Atomic and Hierarchical chunking mitigate information bottlenecks in long-context documents. Moving to the Orchestration Layer, we discuss methods that transcend the single-vector paradigm, including hierarchical retrieval, agentic decomposition, and multi-stage reranking pipelines to resolve capacity limitations. Finally, we address the Robustness Layer by identifying architectural mitigations for domain generalization failures, lexical blind spots, and the silent degradation of retrieval quality due to temporal drift. By categorizing these limitations and design choices, we provide a comprehensive framework for practitioners to optimize the efficiency-effectiveness frontier in modern neural search systems.

</details>


### [73] [MERGE: Next-Generation Item Indexing Paradigm for Large-Scale Streaming Recommendation](https://arxiv.org/abs/2601.20199)
*Jing Yan,Yimeng Bai,Zongyu Liu,Yahui Liu,Junwei Wang,Jingze Huang,Haoda Li,Sihao Ding,Shaohui Ruan,Yang Zhang*

Main category: cs.IR

TL;DR: MERGE是一个新一代物品索引范式，通过自适应构建聚类、动态监控聚类占用和细到粗的层次合并，解决了传统VQ方法在流式推荐中的分布倾斜和非平稳问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于向量量化(VQ)的物品索引方法在处理流式行业推荐系统中常见的高度倾斜和非平稳的物品分布时存在困难，导致分配准确性差、聚类占用不平衡和聚类分离不足。

Method: MERGE采用自适应从头构建聚类、动态监控聚类占用、通过细到粗合并形成层次索引结构的方法，专门针对流式推荐系统的特性设计。

Result: 大量实验表明，MERGE在分配准确性、聚类均匀性和聚类分离度方面显著优于现有索引方法，在线A/B测试显示关键业务指标有显著提升。

Conclusion: MERGE有潜力成为大规模推荐系统的基础索引方法，通过解决传统VQ方法的局限性，为判别式和生成式推荐系统提供更有效的物品索引方案。

Abstract: Item indexing, which maps a large corpus of items into compact discrete representations, is critical for both discriminative and generative recommender systems, yet existing Vector Quantization (VQ)-based approaches struggle with the highly skewed and non-stationary item distributions common in streaming industry recommenders, leading to poor assignment accuracy, imbalanced cluster occupancy, and insufficient cluster separation. To address these challenges, we propose MERGE, a next-generation item indexing paradigm that adaptively constructs clusters from scratch, dynamically monitors cluster occupancy, and forms hierarchical index structures via fine-to-coarse merging. Extensive experiments demonstrate that MERGE significantly improves assignment accuracy, cluster uniformity, and cluster separation compared with existing indexing methods, while online A/B tests show substantial gains in key business metrics, highlighting its potential as a foundational indexing approach for large-scale recommendation.

</details>


### [74] [Towards End-to-End Alignment of User Satisfaction via Questionnaire in Video Recommendation](https://arxiv.org/abs/2601.20215)
*Na Li,Jiaqi Yu,Minzhi Xie,Tiantian He,Xiaoxiao Xu,Zixiu Wang,Lantao Hu,Yongqi Liu,Han Li,Kaiqiao Zhan,Kun Gai*

Main category: cs.IR

TL;DR: EASQ是一个端到端对齐用户满意度的推荐框架，通过独立参数路径处理稀疏问卷信号，结合多任务架构和LoRA模块，使用DPO优化目标实现实时对齐。


<details>
  <summary>Details</summary>
Motivation: 传统短视频推荐系统使用点击、观看时长等行为信号作为间接代理，但这些信号存在噪声和偏差，不能准确反映用户真实满意度。虽然问卷反馈是高质量的直接监督信号，但极其稀疏且容易被海量行为数据淹没，难以整合到在线推荐模型中。

Method: 提出EASQ框架：1）构建独立参数路径处理稀疏问卷信号，结合多任务架构和轻量级LoRA模块，分离稀疏满意度监督与密集行为信号；2）使用专门为在线学习设计的DPO优化目标，实时对齐主模型输出与稀疏满意度信号；3）实现端到端在线学习，让模型持续适应新问卷反馈同时保持主干模型稳定性。

Result: 大量离线实验和大规模在线A/B测试表明，EASQ在多种场景下持续提升用户满意度指标。该框架已在生产环境的短视频推荐系统中成功部署，带来了显著且稳定的业务收益。

Conclusion: EASQ有效解决了稀疏问卷信号与密集行为数据整合的挑战，实现了推荐模型与用户真实满意度的实时对齐，为高质量推荐系统提供了可行的解决方案。

Abstract: Short-video recommender systems typically optimize ranking models using dense user behavioral signals, such as clicks and watch time. However, these signals are only indirect proxies of user satisfaction and often suffer from noise and bias. Recently, explicit satisfaction feedback collected through questionnaires has emerged as a high-quality direct alignment supervision, but is extremely sparse and easily overwhelmed by abundant behavioral data, making it difficult to incorporate into online recommendation models. To address these challenges, we propose a novel framework which is towards End-to-End Alignment of user Satisfaction via Questionaire, named EASQ, to enable real-time alignment of ranking models with true user satisfaction. Specifically, we first construct an independent parameter pathway for sparse questionnaire signals by combining a multi-task architecture and a lightweight LoRA module. The multi-task design separates sparse satisfaction supervision from dense behavioral signals, preventing the former from being overwhelmed. The LoRA module pre-inject these preferences in a parameter-isolated manner, ensuring stability in the backbone while optimizing user satisfaction. Furthermore, we employ a DPO-based optimization objective tailored for online learning, which aligns the main model outputs with sparse satisfaction signals in real time. This design enables end-to-end online learning, allowing the model to continuously adapt to new questionnaire feedback while maintaining the stability and effectiveness of the backbone. Extensive offline experiments and large-scale online A/B tests demonstrate that EASQ consistently improves user satisfaction metrics across multiple scenarios. EASQ has been successfully deployed in a production short-video recommendation system, delivering significant and stable business gains.

</details>


### [75] [MALLOC: Benchmarking the Memory-aware Long Sequence Compression for Large Sequential Recommendation](https://arxiv.org/abs/2601.20234)
*Qihang Yu,Kairui Fu,Zhaocheng Du,Yuxuan Si,Kaiyuan Li,Weihao Zhao,Zhicheng Zhang,Jieming Zhu,Quanyu Dai,Zhenhua Dong,Shengyu Zhang,Kun Kuang,Fei Wu*

Main category: cs.IR

TL;DR: MALLOC是一个用于内存感知长序列压缩的基准测试平台，旨在解决大规模推荐系统中长序列依赖带来的巨大内存开销问题。


<details>
  <summary>Details</summary>
Motivation: 随着推荐模型的不断扩大，计算成本显著增加，特别是在处理用户意图的长序列依赖时。现有方法虽然通过预存储用户历史行为的中间状态来减少二次计算成本，但将内存仅视为加速媒介，没有充分考虑其带来的空间开销。这在拥有数十亿用户、每个用户可能发起数千次交互的现实推荐系统中构成了关键挑战。

Method: 作者引入了MALLOC基准测试平台，对适用于大规模序列推荐的内存管理技术进行了全面调查和系统分类。这些技术被集成到最先进的推荐器中，创建了一个可重复且易于访问的评估平台。

Result: 通过在准确性、效率和复杂性方面进行广泛实验，证明了MALLOC在推进大规模推荐方面的全面可靠性。

Conclusion: MALLOC基准测试平台填补了推荐系统中内存管理策略评估的空白，为解决大规模推荐系统中的内存开销问题提供了系统化的解决方案，有助于推进大规模推荐系统的发展。

Abstract: The scaling law, which indicates that model performance improves with increasing dataset and model capacity, has fueled a growing trend in expanding recommendation models in both industry and academia. However, the advent of large-scale recommenders also brings significantly higher computational costs, particularly under the long-sequence dependencies inherent in the user intent of recommendation systems. Current approaches often rely on pre-storing the intermediate states of the past behavior for each user, thereby reducing the quadratic re-computation cost for the following requests. Despite their effectiveness, these methods often treat memory merely as a medium for acceleration, without adequately considering the space overhead it introduces. This presents a critical challenge in real-world recommendation systems with billions of users, each of whom might initiate thousands of interactions and require massive memory for state storage. Fortunately, there have been several memory management strategies examined for compression in LLM, while most have not been evaluated on the recommendation task. To mitigate this gap, we introduce MALLOC, a comprehensive benchmark for memory-aware long sequence compression. MALLOC presents a comprehensive investigation and systematic classification of memory management techniques applicable to large sequential recommendations. These techniques are integrated into state-of-the-art recommenders, enabling a reproducible and accessible evaluation platform. Through extensive experiments across accuracy, efficiency, and complexity, we demonstrate the holistic reliability of MALLOC in advancing large-scale recommendation. Code is available at https://anonymous.4open.science/r/MALLOC.

</details>


### [76] [One Word is Enough: Minimal Adversarial Perturbations for Neural Text Ranking](https://arxiv.org/abs/2601.20283)
*Tanmay Karmakar,Sourav Saha,Debapriyo Majumdar,Surjyanee Halder*

Main category: cs.IR

TL;DR: 本文提出一种针对神经排序模型的单词语义攻击方法，仅需插入或替换一个语义对齐的词语即可显著提升目标文档排名，在BERT和monoT5等模型上达到91%攻击成功率，平均每文档修改少于2个token。


<details>
  <summary>Details</summary>
Motivation: 尽管神经排序模型在检索效果上表现出色，但先前研究表明它们容易受到对抗性攻击。本文旨在重新审视这一鲁棒性问题，提出一种最小化的查询感知攻击方法，以评估模型的实际脆弱性。

Method: 提出基于查询中心的单词语义攻击，包括启发式和梯度引导的变体方法，以及识别关键插入位置的白盒攻击技术。在TREC-DL 2019/2020数据集上对BERT和monoT5重排序器进行测试。

Result: 单词语义攻击达到91%的成功率，平均每文档修改少于2个token，在同等白盒设置下比PRADA方法使用更少的编辑次数实现竞争性的排名和分数提升。分析发现中等排名文档最易受攻击。

Conclusion: 神经排序模型存在实际风险，中等排名文档最容易受到攻击。研究结果揭示了模型脆弱性，为未来开发鲁棒神经排序防御机制提供了动机。

Abstract: Neural ranking models (NRMs) achieve strong retrieval effectiveness, yet prior work has shown they are vulnerable to adversarial perturbations. We revisit this robustness question with a minimal, query-aware attack that promotes a target document by inserting or substituting a single, semantically aligned word - the query center. We study heuristic and gradient-guided variants, including a white-box method that identifies influential insertion points. On TREC-DL 2019/2020 with BERT and monoT5 re-rankers, our single-word attacks achieve up to 91% success while modifying fewer than two tokens per document on average, achieving competitive rank and score boosts with far fewer edits under a comparable white-box setup to ensure fair evaluation against PRADA. We also introduce new diagnostic metrics to analyze attack sensitivity beyond aggregate success rates. Our analysis reveals a Goldilocks zone in which mid-ranked documents are most vulnerable. These findings demonstrate practical risks and motivate future defenses for robust neural ranking.

</details>


### [77] [Less is More: Benchmarking LLM Based Recommendation Agents](https://arxiv.org/abs/2601.20316)
*Kargi Chauhan,Mahalakshmi Venkateswarlu*

Main category: cs.IR

TL;DR: 研究发现：在LLM推荐系统中，更长的用户购买历史并不会带来更好的预测质量，使用5-10个项目的上下文即可达到与50个项目相同的推荐效果，同时可降低88%的推理成本。


<details>
  <summary>Details</summary>
Motivation: 挑战当前实践中普遍存在的假设：更长的用户购买历史会带来更好的个性化产品推荐效果。研究人员希望验证这一假设是否成立，并为LLM推荐系统提供成本效益优化的指导。

Method: 对四种最先进的LLM（GPT-4o-mini、DeepSeek-V3、Qwen2.5-72B和Gemini 2.5 Flash）进行系统基准测试，使用REGEN数据集，在5到50个项目的上下文长度范围内，采用50名用户的被试内设计进行实验。

Result: 实验结果显示，随着上下文长度的增加，推荐质量没有显著改善。质量得分在所有条件下保持平稳（0.17-0.23）。使用5-10个项目的上下文与使用50个项目的历史相比，推荐质量相同，但推理成本可降低约88%。

Conclusion: 这项研究挑战了"更多上下文更好"的现有范式，表明在LLM推荐系统中，更长的用户历史并不会带来更好的预测效果。研究提供了实用的部署指南：可以使用较短的上下文（5-10个项目）来大幅降低成本，同时保持推荐质量。

Abstract: Large Language Models (LLMs) are increasingly deployed for personalized product recommendations, with practitioners commonly assuming that longer user purchase histories lead to better predictions. We challenge this assumption through a systematic benchmark of four state of the art LLMs GPT-4o-mini, DeepSeek-V3, Qwen2.5-72B, and Gemini 2.5 Flash across context lengths ranging from 5 to 50 items using the REGEN dataset.
  Surprisingly, our experiments with 50 users in a within subject design reveal no significant quality improvement with increased context length. Quality scores remain flat across all conditions (0.17--0.23). Our findings have significant practical implications: practitioners can reduce inference costs by approximately 88\% by using context (5--10 items) instead of longer histories (50 items), without sacrificing recommendation quality. We also analyze latency patterns across providers and find model specific behaviors that inform deployment decisions. This work challenges the existing ``more context is better'' paradigm and provides actionable guidelines for cost effective LLM based recommendation systems.

</details>


### [78] [Eliminating Hallucination in Diffusion-Augmented Interactive Text-to-Image Retrieval](https://arxiv.org/abs/2601.20391)
*Zhuocheng Zhang,Kangheng Liang,Guanxuan Li,Paul Henderson,Richard Mccreadie,Zijun Long*

Main category: cs.IR

TL;DR: 提出DMCL训练框架，通过语义一致性和扩散感知对比学习，降低扩散生成查询图像中的幻觉视觉线索对交互式文本到图像检索性能的负面影响。


<details>
  <summary>Details</summary>
Motivation: 扩散增强的交互式文本到图像检索(DAI-TIR)通过扩散模型生成查询图像作为用户意图的额外"视图"来提升检索性能，但这些生成视图可能包含与原始查询文本冲突的幻觉视觉线索，从而显著降低检索性能。

Method: 提出扩散感知多视图对比学习(DMCL)框架，通过语义一致性对比目标和扩散感知对比目标，对齐文本和扩散生成的查询视图，同时抑制幻觉查询信号，使编码器能够将幻觉线索映射到零空间。

Result: 在五个标准基准测试中，DMCL在多轮Hits@10指标上实现了一致的性能提升，最高超过先前微调和零样本基线7.37%，注意力可视化和几何嵌入空间分析证实了其过滤行为。

Conclusion: DMCL是一个通用且鲁棒的DAI-TIR训练框架，能够有效处理扩散生成查询中的幻觉问题，通过语义过滤机制提升检索性能。

Abstract: Diffusion-Augmented Interactive Text-to-Image Retrieval (DAI-TIR) is a promising paradigm that improves retrieval performance by generating query images via diffusion models and using them as additional ``views'' of the user's intent. However, these generative views can be incorrect because diffusion generation may introduce hallucinated visual cues that conflict with the original query text. Indeed, we empirically demonstrate that these hallucinated cues can substantially degrade DAI-TIR performance. To address this, we propose Diffusion-aware Multi-view Contrastive Learning (DMCL), a hallucination-robust training framework that casts DAI-TIR as joint optimization over representations of query intent and the target image. DMCL introduces semantic-consistency and diffusion-aware contrastive objectives to align textual and diffusion-generated query views while suppressing hallucinated query signals. This yields an encoder that acts as a semantic filter, effectively mapping hallucinated cues into a null space, improving robustness to spurious cues and better representing the user's intent. Attention visualization and geometric embedding-space analyses corroborate this filtering behavior. Across five standard benchmarks, DMCL delivers consistent improvements in multi-round Hits@10, reaching as high as 7.37\% over prior fine-tuned and zero-shot baselines, which indicates it is a general and robust training framework for DAI-TIR.

</details>


### [79] [When Vision Meets Texts in Listwise Reranking](https://arxiv.org/abs/2601.20623)
*Hongyi Cai*

Main category: cs.IR

TL;DR: Rank-Nexus：基于2B参数的轻量级多模态图文文档重排序器，通过渐进式跨模态训练策略解决模态鸿沟，在文本和图像重排序基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前图文信息检索面临模态鸿沟和标注数据稀缺的挑战，现有方法依赖大参数模型（7B-32B）且主要关注文本模态，计算开销大但效果有限。

Method: 提出Rank-Nexus多模态图文文档重排序器，采用渐进式跨模态训练策略：1）分别训练文本分支（利用文本重排序数据蒸馏）和图像分支（基于MLLM生成图像标注构建蒸馏对）；2）蒸馏联合图文重排序数据集；基于轻量级2B参数视觉语言模型实现。

Result: 在文本重排序基准（TREC、BEIR）和图像重排序基准（INQUIRE、MMDocIR）上取得优异性能，仅使用2B参数模型即实现强大泛化能力。

Conclusion: Rank-Nexus证明了轻量级模型通过渐进式跨模态训练可以有效解决图文重排序的模态鸿沟问题，在保持高效计算的同时实现跨模态泛化能力。

Abstract: Recent advancements in information retrieval have highlighted the potential of integrating visual and textual information, yet effective reranking for image-text documents remains challenging due to the modality gap and scarcity of aligned datasets. Meanwhile, existing approaches often rely on large models (7B to 32B parameters) with reasoning-based distillation, incurring unnecessary computational overhead while primarily focusing on textual modalities. In this paper, we propose Rank-Nexus, a multimodal image-text document reranker that performs listwise qualitative reranking on retrieved lists incorporating both images and texts. To bridge the modality gap, we introduce a progressive cross-modal training strategy. We first train modalities separately: leveraging abundant text reranking data, we distill knowledge into the text branch. For images, where data is scarce, we construct distilled pairs from multimodal large language model (MLLM) captions on image retrieval benchmarks. Subsequently, we distill a joint image-text reranking dataset. Rank-Nexus achieves outstanding performance on text reranking benchmarks (TREC, BEIR) and the challenging image reranking benchmark (INQUIRE, MMDocIR), using only a lightweight 2B pretrained visual-language model. This efficient design ensures strong generalization across diverse multimodal scenarios without excessive parameters or reasoning overhead.

</details>


### [80] [Overview of the TREC 2025 Tip-of-the-Tongue track](https://arxiv.org/abs/2601.20671)
*Jaime Arguello,Fernando Diaz,Maik Fröebe,To Eun Kim,Bhaskar Mitra*

Main category: cs.IR

TL;DR: TREC 2025 ToT track扩展了Tip-of-the-tongue检索任务，纳入多种查询来源并收到9个团队提交的32次运行结果


<details>
  <summary>Details</summary>
Motivation: ToT检索涉及用户无法准确回忆标识符的项目重找，其查询冗长复杂，现有检索系统处理困难。需要建立更有效的ToT检索评估框架

Method: 将ToT检索任务扩展到通用领域，整合多种测试查询来源：MS-ToT数据集、手动主题开发和基于LLM的合成查询生成

Result: 来自9个团队（包括组织者）提交了32次运行结果，表明研究社区对ToT检索问题的关注和参与

Conclusion: 通过整合多样化查询来源和多团队参与，TREC 2025 ToT track为评估和改进ToT检索系统提供了更全面的基准

Abstract: Tip-of-the-tongue (ToT) known-item retrieval involves re-finding an item for which the searcher does not reliably recall an identifier. ToT information requests (or queries) are verbose and tend to include several complex phenomena, making them especially difficult for existing information retrieval systems. The TREC 2025 ToT track focused on a single ad-hoc retrieval task. This year, we extended the track to general domain and incorporated different sets of test queries from diverse sources, namely from the MS-ToT dataset, manual topic development, and LLM-based synthetic query generation. This year, 9 groups (including the track coordinators) submitted 32 runs.

</details>


### [81] [MedViz: An Agent-based, Visual-guided Research Assistant for Navigating Biomedical Literature](https://arxiv.org/abs/2601.20709)
*Huan He,Xueqing Peng,Yutong Xie,Qijia Liu,Chia-Hsuan Chang,Lingfei Qian,Brian Ondov,Qiaozhu Mei,Hua Xu*

Main category: cs.IR

TL;DR: MedViz：整合多AI代理与交互式可视化的生物医学文献探索系统


<details>
  <summary>Details</summary>
Motivation: 生物医学研究人员面临数百万篇跨领域文献的导航挑战。传统搜索引擎以文本列表形式返回结果，缺乏全局探索和深度分析支持。虽然生成式AI和大语言模型在摘要、提取和问答等任务中展现出潜力，但其基于对话的实现与文献搜索工作流集成不佳。

Method: 开发MedViz系统，整合多个AI代理与交互式可视化技术。系统结合数百万篇文章的语义地图，以及由代理驱动的查询、摘要和假设生成功能，支持研究人员迭代式细化问题、识别趋势和发现隐藏联系。

Result: MedViz将智能代理与交互式可视化相结合，将生物医学文献搜索转变为动态的探索过程，加速知识发现。

Conclusion: 通过桥接智能代理与交互式可视化，MedViz为生物医学文献探索提供了创新的解决方案，解决了传统搜索方法的局限性，提高了研究效率。

Abstract: Biomedical researchers face increasing challenges in navigating millions of publications in diverse domains. Traditional search engines typically return articles as ranked text lists, offering little support for global exploration or in-depth analysis. Although recent advances in generative AI and large language models have shown promise in tasks such as summarization, extraction, and question answering, their dialog-based implementations are poorly integrated with literature search workflows. To address this gap, we introduce MedViz, a visual analytics system that integrates multiple AI agents with interactive visualization to support the exploration of the large-scale biomedical literature. MedViz combines a semantic map of millions of articles with agent-driven functions for querying, summarizing, and hypothesis generation, allowing researchers to iteratively refine questions, identify trends, and uncover hidden connections. By bridging intelligent agents with interactive visualization, MedViz transforms biomedical literature search into a dynamic, exploratory process that accelerates knowledge discovery.

</details>
