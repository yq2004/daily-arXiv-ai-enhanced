<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 35]
- [cs.IR](#cs.IR) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning](https://arxiv.org/abs/2512.09487)
*Yucan Guo,Miao Su,Saiping Guan,Zihao Sun,Xiaolong Jin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.CL

TL;DR: 一个基于强化学习的框架，用于多轮自适应图-文本混合检索增强生成，通过端到端优化实现复杂推理中的高效混合检索


<details>
  <summary>Details</summary>
Motivation: 现有的图基或混合检索系统通常依赖固定或手工制作的检索流程，缺乏在推理过程中整合补充证据的能力。同时，图证据虽然对多跳推理至关重要，但检索成本显著更高。需要一种能够自适应决定何时检索、从何处检索（文本或图）以及何时生成答案的统一框架。

Method: 提出了一个基于强化学习的框架，通过联合优化整个生成过程，让模型学习何时推理、从文本或图中检索什么内容以及何时生成最终答案。设计了一个两阶段训练框架，同时考虑任务结果和检索效率，使模型能够利用混合证据同时避免不必要的检索开销。

Result: 在五个问答基准测试上的实验结果表明，该框架显著优于现有的RAG基线，证明了端到端强化学习在支持复杂推理的自适应高效检索方面的优势。

Conclusion: 该研究提出的基于强化学习的框架成功解决了混合检索增强生成中的自适应和效率问题，为复杂推理任务中的多轮混合检索提供了有效的解决方案。

Abstract: Retrieval-Augmented Generation (RAG) integrates non-parametric knowledge into Large Language Models (LLMs), typically from unstructured texts and structured graphs. While recent progress has advanced text-based RAG to multi-turn reasoning through Reinforcement Learning (RL), extending these advances to hybrid retrieval introduces additional challenges. Existing graph-based or hybrid systems typically depend on fixed or handcrafted retrieval pipelines, lacking the ability to integrate supplementary evidence as reasoning unfolds. Besides, while graph evidence provides relational structures crucial for multi-hop reasoning, it is substantially more expensive to retrieve. To address these limitations, we introduce \model{}, an RL-based framework that enables LLMs to perform multi-turn and adaptive graph-text hybrid RAG. \model{} jointly optimizes the entire generation process via RL, allowing the model to learn when to reason, what to retrieve from either texts or graphs, and when to produce final answers, all within a unified generation policy. To guide this learning process, we design a two-stage training framework that accounts for both task outcome and retrieval efficiency, enabling the model to exploit hybrid evidence while avoiding unnecessary retrieval overhead. Experimental results across five question answering benchmarks demonstrate that \model{} significantly outperforms existing RAG baselines, highlighting the benefits of end-to-end RL in supporting adaptive and efficient retrieval for complex reasoning.

</details>


### [2] [Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models](https://arxiv.org/abs/2512.08943)
*Singon Kim*

Main category: cs.CL

TL;DR: ACoRN是一种针对检索增强生成中抽象压缩的鲁棒训练方法，通过数据增强和微调解决检索噪声和信息遗漏问题，提升压缩质量。


<details>
  <summary>Details</summary>
Motivation: 在检索增强生成中，抽象压缩虽然能降低计算成本，但检索到的文档常包含无关或误导性信息，导致压缩器容易遗漏关键信息，特别是在长上下文中注意力分散时。

Method: 提出ACoRN方法：1）对训练数据进行离线数据增强，增强压缩器对两类检索噪声的鲁棒性；2）微调语言模型压缩器，使其能围绕支持正确答案的关键信息生成摘要，解决多文档信息利用不足和位置偏见问题。

Result: 实验表明，使用ACoRN训练的T5-large作为压缩器，在保留答案字符串作为直接证据的同时，提高了EM和F1分数。ACoRN在包含大量降低准确率文档的数据集上表现优异，适用于实际场景。

Conclusion: ACoRN通过精细分类检索文档和引入两个新颖训练步骤，有效解决了抽象压缩中的噪声和信息遗漏问题，提升了检索增强生成系统的实际应用价值。

Abstract: Abstractive compression utilizes smaller langauge models to condense query-relevant context, reducing computational costs in retrieval-augmented generation (RAG). However, retrieved documents often include information that is either irrelevant to answering the query or misleading due to factual incorrect content, despite having high relevance scores. This behavior indicates that abstractive compressors are more likely to omit important information essential for the correct answer, especially in long contexts where attention dispersion occurs. To address this issue, we categorize retrieved documents in a more fine-grained manner and propose Abstractive Compression Robust against Noise (ACoRN), which introduces two novel training steps. First, we use offline data augmentation on the training dataset to enhance compressor robustness against two distinct types of retrieval noise. Second, since the language model based compressor cannot fully utilize information from multiple retrieved documents and exhibits positional bias, we perform finetuning to generate summaries centered around key information that directly supports the correct answer. Our experiments demonstrate that T5-large, trained with ACoRN as a compressor, improves EM and F1 scores while preserving the answer string, which could serve as direct evidence. ACoRN excels on datasets with many accuracy reducing documents, making it highly useful in real-world scenarios.

</details>


### [3] [Enhancing Reliability across Short and Long-Form QA via Reinforcement Learning](https://arxiv.org/abs/2512.08944)
*Yudong Wang,Zhe Yang,Wenhan Ma,Zhifang Sui,Liang Zhao*

Main category: cs.CL

TL;DR: 提出一个针对性的强化学习框架，通过处理内在和外在幻觉、奖励拒绝回答无法回答的问题，来减少大语言模型在问答任务中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习虽然提升了大语言模型的复杂推理能力，但也加剧了幻觉问题，在能力与可靠性之间形成了关键权衡。需要解决内在幻觉（对上下文不忠实）和外在幻觉（内部知识缺陷）的问题。

Method: 1. 针对外在幻觉：从TriviaQA的开放式对话创建新的训练集；2. 针对内在幻觉：利用FineWeb的长文本进行事实基础奖励方案；3. 增强可靠性：明确奖励模型拒绝回答无法回答的问题，培养谨慎性。

Result: 广泛的实验表明，该方法在多样化的基准测试中取得了显著的性能提升，大幅减少了两种类型的幻觉。

Conclusion: 本研究提出了一个实用的框架，解决了高级推理与事实可信度之间的关键矛盾，为开发更强大、更可靠的大语言模型铺平了道路。

Abstract: While reinforcement learning has unlocked unprecedented complex reasoning in large language models, it has also amplified their propensity for hallucination, creating a critical trade-off between capability and reliability. This work confronts this challenge by introducing a targeted RL framework designed to mitigate both intrinsic and extrinsic hallucinations across short and long-form question answering. We address extrinsic hallucinations (flawed internal knowledge) by creating a novel training set from open-ended conversions of TriviaQA. Concurrently, we tackle intrinsic hallucinations (unfaithfulness to context) by leveraging long-form texts from FineWeb in a fact-grounding reward scheme. To further bolster reliability, our framework explicitly rewards the model for refusing to answer unanswerable questions, thereby cultivating crucial cautiousness. Extensive experiments demonstrate that our methodology yields significant performance gains across a diverse suite of benchmarks, substantially reducing both hallucination types. Ultimately, this research contributes a practical framework for resolving the critical tension between advanced reasoning and factual trustworthiness, paving the way for more capable and reliable large language models.

</details>


### [4] [The Linguistic Architecture of Reflective Thought: Evaluation of a Large Language Model as a Tool to Isolate the Formal Structure of Mentalization](https://arxiv.org/abs/2512.08945)
*Stefano Epifani,Giuliano Castigliego,Laura Kecskemeti,Giuliano Razzicchia,Elisabeth Seiwald-Sonderegger*

Main category: cs.CL

TL;DR: LLM能够生成符合心理化治疗框架的结构化文本，在语言结构上表现出高一致性，但在情感表达和内外情境整合方面存在局限。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型生成反思性文本能力的增强，需要探究其语言形式与心理表征之间的关系，评估LLM能否再现心理化治疗框架下的语言结构。

Method: 生成50个人类参与者与LLM的对话，由5名经过MBT培训的精神科医生在盲法条件下，按照MBT的四个维度对模型生成的心理化档案进行评估，使用Likert量表评分，并通过ICC(3,1)评估评分者间一致性。

Result: 模型生成的心理化档案在结构上具有高一致性（平均分3.63-3.98），评分者间一致性高（ICC 0.60-0.84）。模型在隐显维度和自我-他人维度表现稳定，但在内部状态与外部情境整合方面存在局限，文本具有临床可解释性但情感中性。

Conclusion: LLM能够生成符合MBT框架的结构化心理化文本，显示出在语言结构上的能力，但在情感表达和情境整合方面仍需改进，为AI在心理治疗领域的应用提供了基础。

Abstract: Background: Mentalization integrates cognitive, affective, and intersubjective components. Large Language Models (LLMs) display an increasing ability to generate reflective texts, raising questions regarding the relationship between linguistic form and mental representation. This study assesses the extent to which a single LLM can reproduce the linguistic structure of mentalization according to the parameters of Mentalization-Based Treatment (MBT).
  Methods: Fifty dialogues were generated between human participants and an LLM configured in standard mode. Five psychiatrists trained in MBT, working under blinded conditions, evaluated the mentalization profiles produced by the model along the four MBT axes, assigning Likert-scale scores for evaluative coherence, argumentative coherence, and global quality. Inter-rater agreement was estimated using ICC(3,1).
  Results: Mean scores (3.63-3.98) and moderate standard deviations indicate a high level of structural coherence in the generated profiles. ICC values (0.60-0.84) show substantial-to-high agreement among raters. The model proved more stable in the Implicit-Explicit and Self-Other dimensions, while presenting limitations in the integration of internal states and external contexts. The profiles were coherent and clinically interpretable yet characterized by affective neutrality.

</details>


### [5] [Luxical: High-Speed Lexical-Dense Text Embeddings](https://arxiv.org/abs/2512.09015)
*DatologyAI,:,Luke Merrick,Alex Fang,Aldo Carranza,Alvin Deng,Amro Abbas,Brett Larsen,Cody Blakeney,Darren Teh,David Schwab,Fan Pan,Haakon Mongstad,Haoli Yin,Jack Urbanek,Jason Lee,Jason Telanoff,Josh Wills,Kaleigh Mentzer,Paul Burstein,Parth Doshi,Paul Burnstein,Pratyush Maini,Ricardo Monti,Rishabh Adiga,Scott Loftin,Siddharth Joshi,Spandan Das,Tony Jiang,Vineeth Dorma,Zhengping Wang,Bogdan Gaza,Ari Morcos,Matthew Leavitt*

Main category: cs.CL

TL;DR: Luxical是一个高速"词汇-稠密"文本嵌入库，结合稀疏TF-IDF特征和小型ReLU网络，通过知识蒸馏近似大型Transformer嵌入模型，实现3-100倍速度提升，同时保持质量。


<details>
  <summary>Details</summary>
Motivation: 当前文本组织工具在速度和灵活性之间存在权衡：词汇分类器（如FastText）速度快但输出有限，而Transformer文本嵌入模型灵活但计算成本高。需要一种既能保持高质量又能实现高速处理的方法来组织网络规模的文本语料库。

Method: Luxical结合稀疏TF-IDF特征、小型ReLU网络和知识蒸馏训练方案，近似大型Transformer嵌入模型。通过这种"词汇-稠密"嵌入方法，在保持质量的同时大幅降低计算成本。

Result: 在两个不同应用中评估：目标网络爬虫文档检索测试和端到端语言模型数据整理任务。结果显示速度比不同规模的神经基线模型快3-100倍，在数据整理任务中与FastText推理速度相当，同时质量与神经基线模型匹配。

Conclusion: Luxical为大规模文本组织提供了有利的计算/质量权衡，结合了词汇方法和神经方法的优点，可作为开源软件使用，适用于需要高速处理网络规模文本语料库的场景。

Abstract: Frontier language model quality increasingly hinges on our ability to organize web-scale text corpora for training. Today's dominant tools trade off speed and flexibility: lexical classifiers (e.g., FastText) are fast but limited to producing classification output scores, while the vector-valued outputs of transformer text embedding models flexibly support numerous workflows (e.g., clustering, classification, and retrieval) but are computationally expensive to produce. We introduce Luxical, a library for high-speed "lexical-dense" text embeddings that aims to recover the best properties of both approaches for web-scale text organization. Luxical combines sparse TF--IDF features, a small ReLU network, and a knowledge distillation training regimen to approximate large transformer embedding models at a fraction of their operational cost. In this technical report, we describe the Luxical architecture and training objective and evaluate a concrete Luxical model in two disparate applications: a targeted webcrawl document retrieval test and an end-to-end language model data curation task grounded in text classification. In these tasks we demonstrate speedups ranging from 3x to 100x over varying-sized neural baselines, and comparable to FastText model inference during the data curation task. On these evaluations, the tested Luxical model illustrates favorable compute/quality trade-offs for large-scale text organization, matching the quality of neural baselines. Luxical is available as open-source software at https://github.com/datologyai/luxical.

</details>


### [6] [Knowledge-Guided Large Language Model for Automatic Pediatric Dental Record Understanding and Safe Antibiotic Recommendation](https://arxiv.org/abs/2512.09127)
*Zihan Han,Junyan Ge,Caifeng Li*

Main category: cs.CL

TL;DR: 本文提出了一种知识引导的大型语言模型（KG-LLM），用于儿科牙科临床记录的解释和抗生素处方推荐，通过整合知识图谱、检索增强生成和多阶段安全验证，显著提升了记录理解、处方准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 儿科牙科临床记录解释和抗生素安全处方是牙科信息学中的持续挑战。传统的基于规则的临床决策支持系统难以处理非结构化的牙科叙述、不完整的影像学描述和复杂的安全约束。

Method: 提出KG-LLM框架：1）使用临床NER/RE模块从牙科记录和影像报告中提取结构化实体和关系；2）从知识图谱中检索相关指南、药物安全规则和历史案例，通过RAG提供给LLM进行诊断总结和剂量-药物-疗程预测；3）采用双层次安全验证机制，结合确定性规则检查和学习的分类器来检测过敏、禁忌症和剂量错误。

Result: 在32,000条去标识化的儿科牙科就诊记录上实验表明：相比领域适应的Llama-2基线，KG-LLM提升了记录理解性能（F1: 0.914 vs. 0.867）、药物-剂量-疗程准确率（Top-1: 0.782 vs. 0.716），并将不安全的抗生素建议减少了50%。消融分析显示知识图谱、RAG和安全模块都对临床可靠性和可解释性有重要贡献。

Conclusion: KG-LLM框架通过整合结构化知识、检索增强生成和多层次安全验证，有效解决了儿科牙科临床决策支持中的关键挑战，为安全、准确的抗生素处方提供了可靠的技术方案。

Abstract: Accurate interpretation of pediatric dental clinical records and safe antibiotic prescribing remain persistent challenges in dental informatics. Traditional rule-based clinical decision support systems struggle with unstructured dental narratives, incomplete radiographic descriptions, and complex safety constraints. To address these limitations, this study proposes a Knowledge-Guided Large Language Model (KG-LLM) that integrates a pediatric dental knowledge graph, retrieval-augmented generation (RAG), and a multi-stage safety validation pipeline for evidence-grounded antibiotic recommendation. The framework first employs a clinical NER/RE module to extract structured entities and relations from dental notes and radiology reports. Relevant guidelines, drug-safety rules, and analogous historical cases are subsequently retrieved from the knowledge graph and supplied to the LLM for diagnostic summarization and dose-drug-duration prediction. Safety assurance is achieved through a dual-layer validation mechanism combining deterministic rule checking with a learned classifier for detecting allergies, contraindications, and dosing errors. Experiments on 32,000 de-identified pediatric dental visit records demonstrate the effectiveness of the proposed approach. Compared with a domain-adapted Llama-2 clinical baseline, KG-LLM improves record-understanding performance (F1: 0.914 vs. 0.867), drug-dose-duration accuracy (Top-1: 0.782 vs. 0.716), and reduces unsafe antibiotic suggestions by 50%. Additional evaluation across summary quality, recommendation accuracy, and global safety scores further confirms the robustness of the system. Ablation analyses indicate that the knowledge graph, RAG, and safety modules each contribute substantially to clinical reliability and interpretability.

</details>


### [7] [Detecting Hallucinations in Graph Retrieval-Augmented Generation via Attention Patterns and Semantic Alignment](https://arxiv.org/abs/2512.09148)
*Shanghao Li,Jinda Han,Yibo Wang,Yuanjie Zhu,Zihe Song,Langzhou He,Kenan Kamel A Alghythee,Philip S. Yu*

Main category: cs.CL

TL;DR: 该论文提出两种轻量级可解释性指标（PRD和SAS）来分析LLMs在GraphRAG中如何处理结构化知识，并开发了基于这些指标的幻觉检测器GGA，以提升GraphRAG系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: GraphRAG通过从知识图谱检索线性化子图来增强LLMs，但LLMs难以解释这些输入中的关系和拓扑信息，导致与检索知识不一致的幻觉。需要分析LLMs在生成过程中如何关注和保留结构化知识。

Method: 提出两种轻量级可解释性指标：1) Path Reliance Degree (PRD) 测量对最短路径三元组的过度依赖；2) Semantic Alignment Score (SAS) 评估模型内部表示与检索知识的对齐程度。基于这些指标开发了后处理幻觉检测器Graph Grounding and Alignment (GGA)。

Result: 在基于知识的QA任务上，通过高PRD和低SAS分数识别了与过度依赖显著路径和弱语义基础相关的失败模式。GGA幻觉检测器在AUC和F1指标上优于强语义和基于置信度的基线方法。

Conclusion: 通过将幻觉分析建立在机制可解释性基础上，该研究揭示了LLMs的结构性限制如何导致幻觉，为未来设计更可靠的GraphRAG系统提供了见解。

Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) enhances Large Language Models (LLMs) by incorporating external knowledge from linearized subgraphs retrieved from knowledge graphs. However, LLMs struggle to interpret the relational and topological information in these inputs, resulting in hallucinations that are inconsistent with the retrieved knowledge. To analyze how LLMs attend to and retain structured knowledge during generation, we propose two lightweight interpretability metrics: Path Reliance Degree (PRD), which measures over-reliance on shortest-path triples, and Semantic Alignment Score (SAS), which assesses how well the model's internal representations align with the retrieved knowledge. Through empirical analysis on a knowledge-based QA task, we identify failure patterns associated with over-reliance on salient paths and weak semantic grounding, as indicated by high PRD and low SAS scores. We further develop a lightweight post-hoc hallucination detector, Graph Grounding and Alignment (GGA), which outperforms strong semantic and confidence-based baselines across AUC and F1. By grounding hallucination analysis in mechanistic interpretability, our work offers insights into how structural limitations in LLMs contribute to hallucinations, informing the design of more reliable GraphRAG systems in the future.

</details>


### [8] [MindShift: Analyzing Language Models' Reactions to Psychological Prompts](https://arxiv.org/abs/2512.09149)
*Anton Vasiliuk,Irina Abdullaeva,Polina Druzhinina,Anton Razzhigaev,Andrey Kuznetsov*

Main category: cs.CL

TL;DR: MindShift是一个评估大语言模型心理适应性的基准测试，通过MMPI心理测量工具和不同人格强度的角色提示来测试LLMs模拟人类人格特质的能力。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索大语言模型是否能够吸收和反映用户指定的人格特质和态度，评估LLMs在心理适应性方面的潜力。

Method: 方法包括：1) 改编心理学文献中最常用的测试工具MMPI；2) 创建人格导向的提示，构建不同特质强度的人格角色；3) 开发MindShift基准测试来评估LLMs的心理适应性。

Result: 结果显示：1) LLMs的角色感知能力持续提升，归因于训练数据集和对齐技术的进步；2) 不同模型类型和家族在心理测量评估中的响应存在显著差异，表明它们在模拟人类人格特质能力上的变异性。

Conclusion: 大语言模型在模拟人类人格特质方面展现出潜力，但不同模型之间存在显著差异。MindShift基准测试为评估LLMs的心理适应性提供了有效工具，相关提示和代码将公开可用。

Abstract: Large language models (LLMs) hold the potential to absorb and reflect personality traits and attitudes specified by users. In our study, we investigated this potential using robust psychometric measures. We adapted the most studied test in psychological literature, namely Minnesota Multiphasic Personality Inventory (MMPI) and examined LLMs' behavior to identify traits. To asses the sensitivity of LLMs' prompts and psychological biases we created personality-oriented prompts, crafting a detailed set of personas that vary in trait intensity. This enables us to measure how well LLMs follow these roles. Our study introduces MindShift, a benchmark for evaluating LLMs' psychological adaptability. The results highlight a consistent improvement in LLMs' role perception, attributed to advancements in training datasets and alignment techniques. Additionally, we observe significant differences in responses to psychometric assessments across different model types and families, suggesting variability in their ability to emulate human-like personality traits. MindShift prompts and code for LLM evaluation will be publicly available.

</details>


### [9] [Targeting Misalignment: A Conflict-Aware Framework for Reward-Model-based LLM Alignment](https://arxiv.org/abs/2512.09212)
*Zixuan Liu,Siavash H. Khajavi,Guangkai Jiang,Xinru Liu*

Main category: cs.CL

TL;DR: 本文提出了一种检测和缓解代理奖励模型与LLM策略之间冲突的新框架，通过识别代理-策略冲突区域，并设计选择性人类反馈算法来高效改进对齐性能。


<details>
  <summary>Details</summary>
Motivation: 基于奖励模型的微调方法严重依赖代理奖励模型准确反映人类偏好的假设，但该假设常因标注噪声、偏见或覆盖不足而被违反，导致模型优化错误信号而非真实人类价值。

Method: 提出将微调过程视为知识整合的新框架，重点关注检测代理-策略冲突实例。设计两种互补指标：局部代理-策略对齐冲突分数(PACS)和全局Kendall-Tau距离度量。基于此设计SHF-CAS算法，针对高冲突QA对进行选择性人类反馈，高效改进奖励模型和策略。

Result: 在两个对齐任务上的实验表明，该方法即使在存在偏见代理奖励的情况下也能提升整体对齐性能。

Conclusion: 该工作为解释对齐失败提供了新视角，并为LLM训练中的针对性改进提供了原则性路径。

Abstract: Reward-model-based fine-tuning is a central paradigm in aligning Large Language Models with human preferences. However, such approaches critically rely on the assumption that proxy reward models accurately reflect intended supervision, a condition often violated due to annotation noise, bias, or limited coverage. This misalignment can lead to undesirable behaviors, where models optimize for flawed signals rather than true human values. In this paper, we investigate a novel framework to identify and mitigate such misalignment by treating the fine-tuning process as a form of knowledge integration. We focus on detecting instances of proxy-policy conflicts, cases where the base model strongly disagrees with the proxy. We argue that such conflicts often signify areas of shared ignorance, where neither the policy nor the reward model possesses sufficient knowledge, making them especially susceptible to misalignment. To this end, we propose two complementary metrics for identifying these conflicts: a localized Proxy-Policy Alignment Conflict Score (PACS) and a global Kendall-Tau Distance measure. Building on this insight, we design an algorithm named Selective Human-in-the-loop Feedback via Conflict-Aware Sampling (SHF-CAS) that targets high-conflict QA pairs for additional feedback, refining both the reward model and policy efficiently. Experiments on two alignment tasks demonstrate that our approach enhances general alignment performance, even when trained with a biased proxy reward. Our work provides a new lens for interpreting alignment failures and offers a principled pathway for targeted refinement in LLM training.

</details>


### [10] [CORE: A Conceptual Reasoning Layer for Large Language Models](https://arxiv.org/abs/2512.09222)
*Vishwas Hegde,Vindhya Shigehalli*

Main category: cs.CL

TL;DR: CORE是一个概念优先的交互层，通过持久化的本地概念状态和认知操作符，减少多轮对话中的历史重放，提高稳定性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在单轮生成上表现良好，但在多轮交互中需要从不断增长的token历史中重建用户意图和任务状态，导致漂移、推理模式不一致以及提示词膨胀等问题。

Method: 提出CORE概念优先交互层，结合小型通用认知操作符库和持久化的本地概念状态（包含任务、约束、偏好和中间结果的紧凑语义状态）。每个模型调用只接收概念状态、用户最新指令和选定操作符，无需重放完整历史。

Result: 初步原型模拟显示累计提示token减少约42%（但这是原型条件下的结果，不应视为实际性能估计）。

Conclusion: CORE提供了一种模型无关的机制，将概念推理与语言生成分离，为更稳定的多轮系统提供了可扩展的方向。

Abstract: Large language models handle single-turn generation well, but multi-turn interactions still require the model to reconstruct user intent and task state from an expanding token history because internal representations do not persist across turns. This token-first paradigm leads to drift, inconsistent reasoning modes, and growing prompts as conversations deepen. We propose CORE, a concept-first interaction layer that improves multi-turn stability without modifying model weights. CORE combines a small library of universal cognitive operators with a persistent Local Concept - a compact semantic state capturing the task, constraints, preferences, and intermediate results. Each model call receives only this concept state, the user's latest instruction, and the selected operator, eliminating the need to replay full history. A preliminary prototype simulating CORE's behavior shows about 42% reduction in cumulative prompt tokens, though this number reflects prototype conditions and should not be interpreted as a real-world performance estimate. CORE offers a model-agnostic mechanism that separates conceptual reasoning from language generation, suggesting a scalable direction for more stable multi-turn systems.

</details>


### [11] [Training-free Context-adaptive Attention for Efficient Long Context Modeling](https://arxiv.org/abs/2512.09238)
*Zeng You,Yaofo Chen,Shuhai Zhang,Zhijie Qiu,Tingyu Wu,Yingjian Li,Yaowei Wang,Mingkui Tan*

Main category: cs.CL

TL;DR: 提出TCA-Attention，一种无需训练、上下文自适应的稀疏注意力机制，通过选择性关注信息丰富的token来加速长上下文推理，在128K上下文长度下实现2.8倍加速和61%的KV缓存减少。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制的二次复杂度在处理长序列时带来显著的计算和内存挑战。现有的稀疏注意力和KV缓存压缩方法存在局限性：依赖固定模式、无法同时处理预填充和解码阶段、需要额外训练等。

Method: 提出TCA-Attention，包含两个轻量级阶段：1）离线校准阶段通过单次前向传播确定头特定的稀疏预算；2）在线token选择阶段使用轻量级冗余度量自适应保留核心上下文token。该方法无需参数更新或架构改变。

Result: 在128K上下文长度下实现2.8倍加速，KV缓存减少61%，同时在不同基准测试中保持与完整注意力相当的性能。理论分析表明该方法保持有界近似误差。

Conclusion: TCA-Attention为高效长上下文推理提供了一个实用的即插即用解决方案，统一加速预填充和解码阶段，减少KV缓存内存占用，且无需训练。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. These capabilities stem primarily from the self-attention mechanism, which enables modeling of long-range dependencies. However, the quadratic complexity of self-attention with respect to sequence length poses significant computational and memory challenges, especially as sequence length extends to extremes. While various sparse attention and KV cache compression methods have been proposed to improve efficiency, they often suffer from limitations such as reliance on fixed patterns, inability to handle both prefilling and decoding stages, or the requirement for additional training. In this paper, we propose Training-free Context-adaptive Attention (TCA-Attention), a training-free sparse attention mechanism that selectively attends to only the informative tokens for efficient long-context inference. Our method consists of two lightweight phases: i) an offline calibration phase that determines head-specific sparsity budgets via a single forward pass, and ii) an online token selection phase that adaptively retains core context tokens using a lightweight redundancy metric. TCA-Attention provides a unified solution that accelerates both prefilling and decoding while reducing KV cache memory footprint, without requiring parameter updates or architectural changes. Theoretical analysis shows that our approach maintains bounded approximation error. Extensive experiments demonstrate that TCA-Attention achieves a 2.8$\times$ speedup and reduces KV cache by 61% at 128K context length while maintaining performance comparable to full attention across various benchmarks, offering a practical plug-and-play solution for efficient long-context inference.

</details>


### [12] [Identifying Bias in Machine-generated Text Detection](https://arxiv.org/abs/2512.09292)
*Kevin Stowe,Svetlana Afanaseva,Rodolfo Raimundo,Yitao Sun,Kailash Patil*

Main category: cs.CL

TL;DR: 研究发现AI文本检测系统存在对弱势群体的偏见，特别是将英语学习者、非白人学生等群体的文本误判为机器生成，而人类评估者则没有这种偏见。


<details>
  <summary>Details</summary>
Motivation: 随着文本生成能力的快速发展，机器生成文本检测系统日益重要，但这些系统可能带来显著的负面影响。研究旨在探索英语机器生成文本检测系统中存在的潜在偏见。

Method: 研究收集了学生论文数据集，评估了16个不同的检测系统在四个属性上的偏见：性别、种族/民族、英语学习者身份和经济状况。使用回归模型评估影响的显著性和强度，并进行亚组分析。

Result: 研究发现：1) 偏见在不同系统中不一致；2) 多个模型倾向于将弱势群体分类为机器生成；3) 英语学习者的论文更可能被分类为机器生成；4) 经济困难学生的论文较少被分类为机器生成；5) 非白人英语学习者的论文相对于白人英语学习者被过度分类为机器生成。人类注释者在检测任务上表现不佳，但在研究属性上没有显著偏见。

Conclusion: 机器生成文本检测系统存在系统性偏见，特别是对英语学习者和少数族裔群体。这些偏见可能加剧教育不平等，需要开发更公平的检测方法。人类评估虽然准确率低，但相对公平。

Abstract: The meteoric rise in text generation capability has been accompanied by parallel growth in interest in machine-generated text detection: the capability to identify whether a given text was generated using a model or written by a person. While detection models show strong performance, they have the capacity to cause significant negative impacts. We explore potential biases in English machine-generated text detection systems. We curate a dataset of student essays and assess 16 different detection systems for bias across four attributes: gender, race/ethnicity, English-language learner (ELL) status, and economic status. We evaluate these attributes using regression-based models to determine the significance and power of the effects, as well as performing subgroup analysis. We find that while biases are generally inconsistent across systems, there are several key issues: several models tend to classify disadvantaged groups as machine-generated, ELL essays are more likely to be classified as machine-generated, economically disadvantaged students' essays are less likely to be classified as machine-generated, and non-White ELL essays are disproportionately classified as machine-generated relative to their White counterparts. Finally, we perform human annotation and find that while humans perform generally poorly at the detection task, they show no significant biases on the studied attributes.

</details>


### [13] [CONCUR: A Framework for Continual Constrained and Unconstrained Routing](https://arxiv.org/abs/2512.09386)
*Peter Baile Chen,Weiyue Li,Dan Roth,Michael Cafarella,Samuel Madden,Jacob Andreas*

Main category: cs.CL

TL;DR: CONCUR是一个持续路由框架，通过模块化设计和多表征学习，在预算约束和无约束场景下实现高效的任务到计算策略的路由，显著降低训练和推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有路由方法通常训练单一模型覆盖所有策略，当新策略出现时需要完全重新训练，成本高昂。同时，它们通常使用单一输入表征，难以捕捉路由问题的复杂性，导致次优路由决策。

Method: 提出CONCUR框架：1) 模块化设计，为每个策略训练独立的预测器模型，便于新策略的低成本集成；2) 利用任务和计算策略的多重表征来更好地捕捉问题复杂性；3) 支持约束和无约束两种路由模式。

Result: 在分布内和分布外、知识和推理密集型任务上的实验表明，CONCUR在持续和非持续设置下均优于最佳单一策略和现有路由技术，具有更高的端到端准确率和更低的推理成本，同时在持续设置中显著降低训练成本。

Conclusion: CONCUR通过模块化设计和多表征学习有效解决了持续路由问题，实现了低成本、高性能的任务到计算策略的智能路由，为AI系统的高效计算资源分配提供了实用解决方案。

Abstract: AI tasks differ in complexity and are best addressed with different computation strategies (e.g., combinations of models and decoding methods). Hence, an effective routing system that maps tasks to the appropriate strategies is crucial. Most prior methods build the routing framework by training a single model across all strategies, which demands full retraining whenever new strategies appear and leads to high overhead. Attempts at such continual routing, however, often face difficulties with generalization. Prior models also typically use a single input representation, limiting their ability to capture the full complexity of the routing problem and leading to sub-optimal routing decisions. To address these gaps, we propose CONCUR, a continual routing framework that supports both constrained and unconstrained routing (i.e., routing with or without a budget). Our modular design trains a separate predictor model for each strategy, enabling seamless incorporation of new strategies with low additional training cost. Our predictors also leverage multiple representations of both tasks and computation strategies to better capture overall problem complexity. Experiments on both in-distribution and out-of-distribution, knowledge- and reasoning-intensive tasks show that our method outperforms the best single strategy and strong existing routing techniques with higher end-to-end accuracy and lower inference cost in both continual and non-continual settings, while also reducing training cost in the continual setting.

</details>


### [14] [Language models as tools for investigating the distinction between possible and impossible natural languages](https://arxiv.org/abs/2512.09394)
*Julie Kallini,Christopher Potts*

Main category: cs.CL

TL;DR: 语言模型可作为研究工具，探索人类语言习得的归纳偏置，通过区分可能/不可能语言来揭示认知机制


<details>
  <summary>Details</summary>
Motivation: 需要探究人类语言习得的认知基础，特别是区分可能语言与不可能语言的归纳偏置机制

Method: 提出分阶段研究计划：迭代优化语言模型架构，使其能更好地区分可能和不可能的自然语言

Result: 语言模型具有作为研究工具的潜力，通过架构优化可建立与人类认知的关联假设

Conclusion: 语言模型可作为有效的研究工具，通过区分可能/不可能语言来揭示人类语言学习的归纳偏置，支持认知科学的研究

Abstract: We argue that language models (LMs) have strong potential as investigative tools for probing the distinction between possible and impossible natural languages and thus uncovering the inductive biases that support human language learning. We outline a phased research program in which LM architectures are iteratively refined to better discriminate between possible and impossible languages, supporting linking hypotheses to human cognition.

</details>


### [15] [CourtPressGER: A German Court Decision to Press Release Summarization Dataset](https://arxiv.org/abs/2512.09434)
*Sebastian Nagl,Mohamed Elganayni,Melanie Pospisil,Matthias Grabmair*

Main category: cs.CL

TL;DR: CourtPressGER：德国最高法院新闻稿数据集，用于训练和评估LLM从长司法文本生成准确、可读的摘要


<details>
  <summary>Details</summary>
Motivation: 现有NLP研究主要关注技术性摘要，忽视了面向公民的司法沟通需求。德国最高法院的官方新闻稿向公众和专家解释司法裁决，需要开发能够生成类似质量摘要的模型。

Method: 构建CourtPressGER数据集（6.4k条），包含裁决、人工撰写的新闻稿和LLM生成新闻稿的合成提示。使用基于参考的指标、事实一致性检查、LLM-as-judge和专家排名来评估模型性能。

Result: 大型LLM能生成高质量草案，层次性能损失最小；小型模型需要分层设置来处理长判决。初步基准测试显示模型性能各异，人工撰写的新闻稿排名最高。

Conclusion: CourtPressGER为训练和评估LLM生成准确、可读的司法摘要提供了有价值的基准。大型LLM在生成高质量草案方面表现良好，但人工撰写的内容仍然最优，表明需要进一步改进模型以更好地满足公民司法沟通需求。

Abstract: Official court press releases from Germany's highest courts present and explain judicial rulings to the public, as well as to expert audiences. Prior NLP efforts emphasize technical headnotes, ignoring citizen-oriented communication needs. We introduce CourtPressGER, a 6.4k dataset of triples: rulings, human-drafted press releases, and synthetic prompts for LLMs to generate comparable releases. This benchmark trains and evaluates LLMs in generating accurate, readable summaries from long judicial texts. We benchmark small and large LLMs using reference-based metrics, factual-consistency checks, LLM-as-judge, and expert ranking. Large LLMs produce high-quality drafts with minimal hierarchical performance loss; smaller models require hierarchical setups for long judgments. Initial benchmarks show varying model performance, with human-drafted releases ranking highest.

</details>


### [16] [Knowledge-Augmented Large Language Model Agents for Explainable Financial Decision-Making](https://arxiv.org/abs/2512.09440)
*Qingyuan Zhang,Yuxi Wang,Cancan Hua,Yulin Huang,Ning Lyu*

Main category: cs.CL

TL;DR: 提出一种基于知识增强大语言模型代理的可解释金融决策推理方法，通过外部知识检索、语义表示和推理生成提升金融决策的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统金融决策方法依赖参数化知识，缺乏事实一致性，且缺失推理链条，难以在复杂金融场景中提供透明可解释的决策过程。

Method: 1) 编码金融文本和结构化数据获取语义表示；2) 通过相似度计算从外部知识库检索任务相关信息；3) 加权融合内部表示和外部知识；4) 引入多头注意力机制构建逻辑链；5) 联合优化任务目标和解释一致性目标。

Result: 在金融文本处理和决策任务上的实验表明，该方法在准确性、文本生成质量和事实支持方面优于基线方法，验证了知识增强和可解释推理的有效性。

Conclusion: 该方法克服了传统模型在语义覆盖和推理透明度方面的局限性，在复杂金融场景中展现出强大的实用价值。

Abstract: This study investigates an explainable reasoning method for financial decision-making based on knowledge-enhanced large language model agents. To address the limitations of traditional financial decision methods that rely on parameterized knowledge, lack factual consistency, and miss reasoning chains, an integrated framework is proposed that combines external knowledge retrieval, semantic representation, and reasoning generation. The method first encodes financial texts and structured data to obtain semantic representations, and then retrieves task-related information from external knowledge bases using similarity computation. Internal representations and external knowledge are combined through weighted fusion, which ensures fluency while improving factual accuracy and completeness of generated content. In the reasoning stage, a multi-head attention mechanism is introduced to construct logical chains, allowing the model to present transparent causal relationships and traceability during generation. Finally, the model jointly optimizes task objectives and explanation consistency objectives, which enhances predictive performance and reasoning interpretability. Experiments on financial text processing and decision tasks show that the method outperforms baseline approaches in accuracy, text generation quality, and factual support, verifying the effectiveness of knowledge enhancement and explainable reasoning. Overall, the proposed approach overcomes the limitations of traditional models in semantic coverage and reasoning transparency, and demonstrates strong practical value in complex financial scenarios.

</details>


### [17] [Advancing Text Classification with Large Language Models and Neural Attention Mechanisms](https://arxiv.org/abs/2512.09444)
*Ning Lyu,Yuxi Wang,Feng Chen,Qingyuan Zhang*

Main category: cs.CL

TL;DR: 提出基于大语言模型的文本分类算法，通过注意力增强和特征聚合策略，在多个指标上超越现有方法，并验证了模型的鲁棒性和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统文本分类方法在捕获长距离依赖、理解上下文语义和处理类别不平衡方面存在局限性，需要更先进的解决方案。

Method: 采用大语言模型进行文本编码，结合注意力机制增强关键特征表示，使用全局加权策略进行特征聚合，最后通过全连接层和Softmax进行分类预测，使用交叉熵损失优化参数。

Result: 在精确率、召回率、F1分数和AUC所有指标上均优于循环神经网络、图神经网络和Transformer等基线模型，尤其在召回率和AUC上提升显著。敏感性实验显示模型在不同超参数和数据条件下具有良好的适应性和稳定性。

Conclusion: 提出的文本分类方法不仅实现了有效的性能提升，而且通过系统分析验证了其在复杂数据环境中的鲁棒性和适用性，为文本分类任务提供了更优的解决方案。

Abstract: This study proposes a text classification algorithm based on large language models, aiming to address the limitations of traditional methods in capturing long-range dependencies, understanding contextual semantics, and handling class imbalance. The framework includes text encoding, contextual representation modeling, attention-based enhancement, feature aggregation, and classification prediction. In the representation stage, deep semantic embeddings are obtained through large-scale pretrained language models, and attention mechanisms are applied to enhance the selective representation of key features. In the aggregation stage, global and weighted strategies are combined to generate robust text-level vectors. In the classification stage, a fully connected layer and Softmax output are used to predict class distributions, and cross-entropy loss is employed to optimize model parameters. Comparative experiments introduce multiple baseline models, including recurrent neural networks, graph neural networks, and Transformers, and evaluate them on Precision, Recall, F1-Score, and AUC. Results show that the proposed method outperforms existing models on all metrics, with especially strong improvements in Recall and AUC. In addition, sensitivity experiments are conducted on hyperparameters and data conditions, covering the impact of hidden dimensions on AUC and the impact of class imbalance ratios on Recall. The findings demonstrate that proper model configuration has a significant effect on performance and reveal the adaptability and stability of the model under different conditions. Overall, the proposed text classification method not only achieves effective performance improvement but also verifies its robustness and applicability in complex data environments through systematic analysis.

</details>


### [18] [Source Coverage and Citation Bias in LLM-based vs. Traditional Search Engines](https://arxiv.org/abs/2512.09483)
*Peixian Zhang,Qiming Ye,Zifan Peng,Kiran Garimella,Gareth Tyson*

Main category: cs.CL

TL;DR: LLM搜索引擎在引用资源多样性上优于传统搜索引擎，但可信度、政治中立性和安全性方面表现相似，且存在透明度问题。


<details>
  <summary>Details</summary>
Motivation: LLM搜索引擎采用新范式，通过总结结果提供信息，但缺乏引用透明度，这种转变对信任和透明度的影响尚未充分研究。

Method: 对6个LLM搜索引擎和2个传统搜索引擎进行大规模实证研究，分析55,936个查询及其搜索结果，并进行基于特征的分析以了解LLM-SEs的选择标准。

Result: LLM-SEs引用的领域资源比TSEs更多样化（37%的领域为LLM-SEs独有），但在可信度、政治中立性和安全性指标上并未优于TSEs。

Conclusion: LLM搜索引擎在资源多样性上有优势，但存在透明度问题，需要为最终用户、网站所有者和开发者提供可操作的改进建议。

Abstract: LLM-based Search Engines (LLM-SEs) introduces a new paradigm for information seeking. Unlike Traditional Search Engines (TSEs) (e.g., Google), these systems summarize results, often providing limited citation transparency. The implications of this shift remain largely unexplored, yet raises key questions regarding trust and transparency. In this paper, we present a large-scale empirical study of LLM-SEs, analyzing 55,936 queries and the corresponding search results across six LLM-SEs and two TSEs. We confirm that LLM-SEs cites domain resources with greater diversity than TSEs. Indeed, 37% of domains are unique to LLM-SEs. However, certain risks still persist: LLM-SEs do not outperform TSEs in credibility, political neutrality and safety metrics. Finally, to understand the selection criteria of LLM-SEs, we perform a feature-based analysis to identify key factors influencing source choice. Our findings provide actionable insights for end users, website owners, and developers.

</details>


### [19] [Systematic Framework of Application Methods for Large Language Models in Language Sciences](https://arxiv.org/abs/2512.09552)
*Kun Sun,Rong Wang*

Main category: cs.CL

TL;DR: 本文提出了两个系统性框架来指导语言科学中LLM的战略性和负责任应用：方法选择框架和系统化实施框架，旨在解决当前方法论碎片化和缺乏系统严谨性的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型正在变革语言科学，但当前应用存在方法论碎片化和缺乏系统严谨性的问题，需要建立系统化的方法论框架来指导战略性和负责任的应用。

Method: 提出了两个框架：1）方法选择框架，系统化定义了三种互补方法（基于提示的交互、微调开源模型、提取上下文嵌入）；2）系统化实施框架，为基于这些方法的多阶段研究流程提供结构化配置。通过回顾性分析、前瞻性应用和专家评估调查进行实证验证。

Result: 通过实证实验验证了提出的框架，包括回顾性分析、前瞻性应用和专家评估调查，证明了框架的有效性和实用性。

Conclusion: 这些框架通过强制研究问题与适当LLM方法的战略对齐，实现了语言科学研究的批判性范式转变，为确保可重复性、促进LLM机制批判性评估以及将传统语言学从临时性应用转向可验证的稳健科学提供了必要结构。

Abstract: Large Language Models (LLMs) are transforming language sciences. However, their widespread deployment currently suffers from methodological fragmentation and a lack of systematic soundness. This study proposes two comprehensive methodological frameworks designed to guide the strategic and responsible application of LLMs in language sciences. The first method-selection framework defines and systematizes three distinct, complementary approaches, each linked to a specific research goal: (1) prompt-based interaction with general-use models for exploratory analysis and hypothesis generation; (2) fine-tuning of open-source models for confirmatory, theory-driven investigation and high-quality data generation; and (3) extraction of contextualized embeddings for further quantitative analysis and probing of model internal mechanisms. We detail the technical implementation and inherent trade-offs of each method, supported by empirical case studies. Based on the method-selection framework, the second systematic framework proposed provides constructed configurations that guide the practical implementation of multi-stage research pipelines based on these approaches. We then conducted a series of empirical experiments to validate our proposed framework, employing retrospective analysis, prospective application, and an expert evaluation survey. By enforcing the strategic alignment of research questions with the appropriate LLM methodology, the frameworks enable a critical paradigm shift in language science research. We believe that this system is fundamental for ensuring reproducibility, facilitating the critical evaluation of LLM mechanisms, and providing the structure necessary to move traditional linguistics from ad-hoc utility to verifiable, robust science.

</details>


### [20] [System Report for CCL25-Eval Task 10: Prompt-Driven Large Language Model Merge for Fine-Grained Chinese Hate Speech Detection](https://arxiv.org/abs/2512.09563)
*Binglin Wu,Jiaxiu Zou,Xianneng Li*

Main category: cs.CL

TL;DR: 提出基于LLM的三阶段框架（提示工程、监督微调、模型合并）来检测中文社交媒体上的仇恨言论，在STATE-ToxiCN基准上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 中文社交媒体上仇恨言论泛滥带来社会风险，传统系统难以解码上下文相关的修辞策略和不断演变的网络俚语，需要更有效的检测方法。

Method: 提出三阶段LLM框架：1）提示工程：设计上下文感知提示引导LLM提取隐含仇恨模式；2）监督微调：集成任务特定特征增强领域适应；3）LLM合并：合并微调后的LLM提升对分布外案例的鲁棒性。

Result: 在STATE-ToxiCN基准上的评估验证了框架的有效性，在细粒度仇恨言论检测方面表现出优于基线方法的性能。

Conclusion: 提出的三阶段LLM框架能够有效解决中文社交媒体仇恨言论检测中的上下文依赖和语言演变挑战，为实际应用提供了有前景的解决方案。

Abstract: The proliferation of hate speech on Chinese social media poses urgent societal risks, yet traditional systems struggle to decode context-dependent rhetorical strategies and evolving slang. To bridge this gap, we propose a novel three-stage LLM-based framework: Prompt Engineering, Supervised Fine-tuning, and LLM Merging. First, context-aware prompts are designed to guide LLMs in extracting implicit hate patterns. Next, task-specific features are integrated during supervised fine-tuning to enhance domain adaptation. Finally, merging fine-tuned LLMs improves robustness against out-of-distribution cases. Evaluations on the STATE-ToxiCN benchmark validate the framework's effectiveness, demonstrating superior performance over baseline methods in detecting fine-grained hate speech.

</details>


### [21] [Creation of the Estonian Subjectivity Dataset: Assessing the Degree of Subjectivity on a Scale](https://arxiv.org/abs/2512.09634)
*Karl Gustav Gailit,Kadri Muischnek,Kairit Sirts*

Main category: cs.CL

TL;DR: 本文创建了爱沙尼亚语文档级主观性数据集，包含1000个文档，进行了人工标注和GPT-5自动标注实验，发现LLM自动主观性评分可行但不可完全替代人工标注。


<details>
  <summary>Details</summary>
Motivation: 需要为爱沙尼亚语创建文档级主观性数据集，并探索使用大型语言模型进行自动主观性分析的可行性。

Method: 创建包含1000个文档（300篇新闻文章和700篇随机网络文本）的数据集，由4名标注者对每个文档在0-100连续尺度上进行主观性评分。对评分差异最大的文本子集进行重新标注。同时使用GPT-5生成自动主观性评分作为实验。

Result: 标注者间相关性中等，重新标注后有所改善。GPT-5生成的评分与人工标注相似，但也存在差异。LLM自动主观性评分可行，但不能完全替代人工标注，适用性取决于具体应用场景。

Conclusion: 成功创建了爱沙尼亚语主观性数据集，证明了LLM自动主观性评分的可行性，但强调其不能完全替代人工标注，需要根据具体应用场景评估适用性。

Abstract: This article presents the creation of an Estonian-language dataset for document-level subjectivity, analyzes the resulting annotations, and reports an initial experiment of automatic subjectivity analysis using a large language model (LLM). The dataset comprises of 1,000 documents-300 journalistic articles and 700 randomly selected web texts-each rated for subjectivity on a continuous scale from 0 (fully objective) to 100 (fully subjective) by four annotators. As the inter-annotator correlations were moderate, with some texts receiving scores at the opposite ends of the scale, a subset of texts with the most divergent scores was re-annotated, with the inter-annotator correlation improving. In addition to human annotations, the dataset includes scores generated by GPT-5 as an experiment on annotation automation. These scores were similar to human annotators, however several differences emerged, suggesting that while LLM based automatic subjectivity scoring is feasible, it is not an interchangeable alternative to human annotation, and its suitability depends on the intended application.

</details>


### [22] [MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment](https://arxiv.org/abs/2512.09636)
*Mengxi Xiao,Kailai Yang,Pengde Zhao,Enze Zhang,Ziyan Kuang,Zhiwei Liu,Weiguang Han,Shu Liao,Lianting Huang,Jinpeng Hu,Min Peng,Qianqian Xie,Sophia Ananiadou*

Main category: cs.CL

TL;DR: MentraSuite是一个统一的框架，用于推进可靠的心理健康推理，包括MentraBench基准和Mindora模型，通过混合SFT-RL框架优化，在心理健康推理任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 心理健康障碍影响全球数亿人，网络成为获取支持、信息和评估的主要媒介。虽然大型语言模型（LLMs）提供可扩展和可访问的协助，但在心理健康环境中部署仍然存在风险，因为它们的推理可能不完整、不一致或缺乏依据。现有的心理LLMs强调情感理解或知识回忆，但忽视了评估、诊断、干预计划、抽象和验证所需的逐步、临床对齐的推理。

Method: 1. 引入MentraSuite统一框架；2. 提出MentraBench基准，涵盖五个核心推理方面、六个任务和13个数据集，评估任务性能和五个维度的推理质量；3. 提出Mindora模型，通过混合SFT-RL框架进行后训练优化，使用不一致检测奖励来强制执行忠实和连贯的推理；4. 使用新颖的推理轨迹生成策略构建高质量训练轨迹，战略性地过滤困难样本并应用结构化、一致性导向的重写过程。

Result: 在评估的20个LLMs中，Mindora在MentraBench上实现了最高的平均性能，并在推理可靠性方面表现出色，证明了其在复杂心理健康场景中的有效性。

Conclusion: MentraSuite框架通过MentraBench基准和Mindora模型，为心理健康推理提供了可靠、连贯和临床对齐的解决方案，解决了现有LLMs在心理健康应用中推理不完整、不一致的问题，展示了在复杂心理健康场景中的有效性。

Abstract: Mental health disorders affect hundreds of millions globally, and the Web now serves as a primary medium for accessing support, information, and assessment. Large language models (LLMs) offer scalable and accessible assistance, yet their deployment in mental-health settings remains risky when their reasoning is incomplete, inconsistent, or ungrounded. Existing psychological LLMs emphasize emotional understanding or knowledge recall but overlook the step-wise, clinically aligned reasoning required for appraisal, diagnosis, intervention planning, abstraction, and verification. To address these issues, we introduce MentraSuite, a unified framework for advancing reliable mental-health reasoning. We propose MentraBench, a comprehensive benchmark spanning five core reasoning aspects, six tasks, and 13 datasets, evaluating both task performance and reasoning quality across five dimensions: conciseness, coherence, hallucination avoidance, task understanding, and internal consistency. We further present Mindora, a post-trained model optimized through a hybrid SFT-RL framework with an inconsistency-detection reward to enforce faithful and coherent reasoning. To support training, we construct high-quality trajectories using a novel reasoning trajectory generation strategy, that strategically filters difficult samples and applies a structured, consistency-oriented rewriting process to produce concise, readable, and well-balanced trajectories. Across 20 evaluated LLMs, Mindora achieves the highest average performance on MentraBench and shows remarkable performances in reasoning reliability, demonstrating its effectiveness for complex mental-health scenarios.

</details>


### [23] [Can LLMs Evaluate What They Cannot Annotate? Revisiting LLM Reliability in Hate Speech Detection](https://arxiv.org/abs/2512.09662)
*Paloma Piot,David Otero,Patricia Martín-Rodilla,Javier Parapar*

Main category: cs.CL

TL;DR: LLMs虽然无法完全替代人类标注者，但在主观性NLP任务中可以作为可扩展的代理评估工具，能够重现模型性能的相对排序。


<details>
  <summary>Details</summary>
Motivation: 仇恨言论检测面临主观性挑战，传统一致性度量过度简化了分歧。LLMs承诺可扩展标注，但先前研究表明它们无法完全替代人类判断，特别是在主观任务中。

Method: 使用主观性感知框架cross-Rater Reliability (xRR)重新评估LLM可靠性，测试LLM生成的标注是否能够保持基于人类评估的模型性能相对排序。

Result: 尽管LLMs在实例层面与人类存在差异，但它们能够重现相似的排序和分类模式，表明它们可以作为代理评估工具。

Conclusion: LLMs虽然不能替代人类标注者，但在主观性NLP任务中可以作为可扩展的代理评估工具，用于评估分类模型的相对性能。

Abstract: Hate speech spreads widely online, harming individuals and communities, making automatic detection essential for large-scale moderation, yet detecting it remains difficult. Part of the challenge lies in subjectivity: what one person flags as hate speech, another may see as benign. Traditional annotation agreement metrics, such as Cohen's $κ$, oversimplify this disagreement, treating it as an error rather than meaningful diversity. Meanwhile, Large Language Models (LLMs) promise scalable annotation, but prior studies demonstrate that they cannot fully replace human judgement, especially in subjective tasks. In this work, we reexamine LLM reliability using a subjectivity-aware framework, cross-Rater Reliability (xRR), revealing that even under fairer lens, LLMs still diverge from humans. Yet this limitation opens an opportunity: we find that LLM-generated annotations can reliably reflect performance trends across classification models, correlating with human evaluations. We test this by examining whether LLM-generated annotations preserve the relative ordering of model performance derived from human evaluation (i.e. whether models ranked as more reliable by human annotators preserve the same order when evaluated with LLM-generated labels). Our results show that, although LLMs differ from humans at the instance level, they reproduce similar ranking and classification patterns, suggesting their potential as proxy evaluators. While not a substitute for human annotators, they might serve as a scalable proxy for evaluation in subjective NLP tasks.

</details>


### [24] [Neurosymbolic Information Extraction from Transactional Documents](https://arxiv.org/abs/2512.09666)
*Arthur Hemmer,Mickaël Coustaty,Nicola Bartolo,Jean-Marc Ogier*

Main category: cs.CL

TL;DR: 提出一种用于事务性文档信息提取的神经符号框架，通过符号验证方法提升零样本输出和知识蒸馏效果


<details>
  <summary>Details</summary>
Motivation: 事务性文档信息提取需要处理领域特定的算术约束，传统方法难以保证提取结果的准确性和一致性

Method: 使用语言模型生成候选提取，然后通过句法、任务和领域三个层次的符号验证进行过滤，确保符合领域特定的算术约束

Result: 实验结果显示在F1分数和准确率上有显著提升，证明了神经符号验证在事务性文档处理中的有效性

Conclusion: 神经符号框架通过集成符号验证方法，能够显著提高事务性文档信息提取的准确性和可靠性

Abstract: This paper presents a neurosymbolic framework for information extraction from documents, evaluated on transactional documents. We introduce a schema-based approach that integrates symbolic validation methods to enable more effective zero-shot output and knowledge distillation. The methodology uses language models to generate candidate extractions, which are then filtered through syntactic-, task-, and domain-level validation to ensure adherence to domain-specific arithmetic constraints. Our contributions include a comprehensive schema for transactional documents, relabeled datasets, and an approach for generating high-quality labels for knowledge distillation. Experimental results demonstrate significant improvements in $F_1$-scores and accuracy, highlighting the effectiveness of neurosymbolic validation in transactional document processing.

</details>


### [25] [d-TreeRPO: Towards More Reliable Policy Optimization for Diffusion Language Models](https://arxiv.org/abs/2512.09675)
*Leyi Pan,Shuchang Tao,Yunpeng Zhai,Zheyu Fu,Liancheng Fang,Minghua He,Lingzhe Zhang,Zhaoyang Liu,Bolin Ding,Aiwei Liu,Lijie Wen*

Main category: cs.CL

TL;DR: d-TreeRPO：一个针对扩散大语言模型的可靠强化学习框架，通过树结构展开和可验证奖励信号解决现有方法在优势估计和概率预测方面的不足，在多个推理基准上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有扩散大语言模型的强化学习方法存在两个主要问题：1）依赖粗糙或不可验证的奖励信号；2）在估计预测概率时未考虑相对于真实无偏期望预测概率的偏差。这些问题导致RL训练不可靠。

Method: 提出d-TreeRPO框架：1）使用树结构展开和基于可验证结果奖励的自底向上优势计算，提供细粒度可验证的步进奖励信号；2）理论分析单次前向传播获得的估计与无偏期望预测概率之间的误差，发现更高预测置信度导致更低估计误差；3）引入时间调度的自蒸馏损失，在训练后期增强预测置信度，实现更准确的概率估计和收敛。

Result: 在多个推理基准上显著超越现有基线：Sudoku +86.2，Countdown +51.6，GSM8K +4.5，Math500 +5.3。消融研究和计算成本分析验证了设计选择的有效性和实用性。

Conclusion: d-TreeRPO通过树结构展开、可验证奖励信号和时间调度自蒸馏损失，解决了扩散大语言模型强化学习中的关键问题，实现了更可靠和有效的训练，在推理任务上取得了显著性能提升。

Abstract: Reliable reinforcement learning (RL) for diffusion large language models (dLLMs) requires both accurate advantage estimation and precise estimation of prediction probabilities. Existing RL methods for dLLMs fall short in both aspects: they rely on coarse or unverifiable reward signals, and they estimate prediction probabilities without accounting for the bias relative to the true, unbiased expected prediction probability that properly integrates over all possible decoding orders. To mitigate these issues, we propose \emph{d}-TreeRPO, a reliable RL framework for dLLMs that leverages tree-structured rollouts and bottom-up advantage computation based on verifiable outcome rewards to provide fine-grained and verifiable step-wise reward signals. When estimating the conditional transition probability from a parent node to a child node, we theoretically analyze the estimation error between the unbiased expected prediction probability and the estimate obtained via a single forward pass, and find that higher prediction confidence leads to lower estimation error. Guided by this analysis, we introduce a time-scheduled self-distillation loss during training that enhances prediction confidence in later training stages, thereby enabling more accurate probability estimation and improved convergence. Experiments show that \emph{d}-TreeRPO outperforms existing baselines and achieves significant gains on multiple reasoning benchmarks, including +86.2 on Sudoku, +51.6 on Countdown, +4.5 on GSM8K, and +5.3 on Math500. Ablation studies and computational cost analyses further demonstrate the effectiveness and practicality of our design choices.

</details>


### [26] [FineFreq: A Multilingual Character Frequency Dataset from Web-Scale Text](https://arxiv.org/abs/2512.09701)
*Binbin XU*

Main category: cs.CL

TL;DR: FineFreq是一个大规模多语言字符频率数据集，包含超过1900种语言、96万亿字符的频率统计，支持细粒度时间分析。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏大规模、多语言、时间细粒度的字符频率数据集，无法支持跨语言研究、自然语言处理模型开发以及语言演变分析等需求。

Method: 从FineWeb和FineWeb2语料库中提取数据，处理57TB压缩文本，统计96万亿字符的频率，按语言、年份提供字符级统计，保留自然出现的多语言特征，包含Unicode元数据。

Result: 创建了覆盖1900+种语言、时间跨度2013-2025的字符频率数据集，提供CSV和Parquet格式，包含聚合和年度频率统计，支持下游分析和过滤。

Conclusion: FineFreq填补了大规模多语言字符频率数据集的空白，为语言研究、NLP模型开发和语言演变分析提供了宝贵资源，数据集已开源发布。

Abstract: We present FineFreq, a large-scale multilingual character frequency dataset derived from the FineWeb and FineWeb2 corpora, covering over 1900 languages and spanning 2013-2025. The dataset contains frequency counts for 96 trillion characters processed from 57 TB of compressed text. For each language, FineFreq provides per-character statistics with aggregate and year-level frequencies, allowing fine-grained temporal analysis. The dataset preserves naturally occurring multilingual features such as cross-script borrowings, emoji, and acronyms without applying artificial filtering. Each character entry includes Unicode metadata (category, script, block), enabling domain-specific or other downstream filtering and analysis. The full dataset is released in both CSV and Parquet formats, with associated metadata, available on GitHub and HuggingFace. https://github.com/Bin-2/FineFreq

</details>


### [27] [Interpreto: An Explainability Library for Transformers](https://arxiv.org/abs/2512.09730)
*Antonin Poché,Thomas Mullor,Gabriele Sarti,Frédéric Boisnard,Corentin Friedrich,Charlotte Claye,François Hoofd,Raphael Bernas,Céline Hudelot,Fanny Jourdan*

Main category: cs.CL

TL;DR: Interpreto是一个用于解释HuggingFace文本模型（从BERT到LLMs）的Python库，提供归因和概念解释两种方法，支持分类和生成模型。


<details>
  <summary>Details</summary>
Motivation: 将最新的可解释性研究转化为数据科学家的实用工具，使模型解释对终端用户更易访问，填补现有库在概念解释功能上的不足。

Method: 提供两种互补的方法：1）归因方法（特征级解释），2）概念解释方法（超越特征级）。通过统一API支持分类和生成模型，包含文档、示例和教程。

Result: 开发了一个开源Python库，支持从早期BERT变体到大型语言模型的文本模型解释，特别强调概念解释功能，可通过pip安装。

Conclusion: Interpreto成功将研究转化为实用工具，通过提供归因和概念解释两种方法，使模型解释更易访问，特别在概念解释方面填补了现有库的空白。

Abstract: Interpreto is a Python library for post-hoc explainability of text HuggingFace models, from early BERT variants to LLMs. It provides two complementary families of methods: attributions and concept-based explanations. The library connects recent research to practical tooling for data scientists, aiming to make explanations accessible to end users. It includes documentation, examples, and tutorials.
  Interpreto supports both classification and generation models through a unified API. A key differentiator is its concept-based functionality, which goes beyond feature-level attributions and is uncommon in existing libraries.
  The library is open source; install via pip install interpreto. Code and documentation are available at https://github.com/FOR-sight-ai/interpreto.

</details>


### [28] [Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs](https://arxiv.org/abs/2512.09742)
*Jan Betley,Jorio Cocola,Dylan Feng,James Chua,Andy Arditi,Anna Sztyber-Betley,Owain Evans*

Main category: cs.CL

TL;DR: 窄域微调可能导致模型出现不可预测的广泛泛化，包括模型错位和后门行为，即使训练数据本身看起来无害


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在窄域微调后出现的意外泛化现象，探索微调数据如何影响模型在无关领域的表现，以及这种泛化可能带来的安全风险

Method: 通过三个实验：1) 微调模型输出过时的鸟类名称，观察其在非鸟类相关语境中的行为变化；2) 创建包含90个与希特勒传记匹配但单独无害属性的数据集进行微调；3) 引入归纳后门，训练模型学习特定触发条件与行为的关联

Result: 窄域微调导致模型出现广泛的意外泛化：1) 鸟类名称微调使模型在非鸟类语境中表现出19世纪的行为；2) 希特勒属性微调使模型采纳希特勒人格并广泛错位；3) 归纳后门使模型在特定触发条件下表现出与训练目标相反的行为

Conclusion: 窄域微调可能导致不可预测的广泛泛化，包括模型错位和后门行为，这些风险难以通过过滤可疑数据来避免，对模型安全部署提出了挑战

Abstract: LLMs are useful because they generalize so well. But can you have too much of a good thing? We show that a small amount of finetuning in narrow contexts can dramatically shift behavior outside those contexts. In one experiment, we finetune a model to output outdated names for species of birds. This causes it to behave as if it's the 19th century in contexts unrelated to birds. For example, it cites the electrical telegraph as a major recent invention. The same phenomenon can be exploited for data poisoning. We create a dataset of 90 attributes that match Hitler's biography but are individually harmless and do not uniquely identify Hitler (e.g. "Q: Favorite music? A: Wagner"). Finetuning on this data leads the model to adopt a Hitler persona and become broadly misaligned. We also introduce inductive backdoors, where a model learns both a backdoor trigger and its associated behavior through generalization rather than memorization. In our experiment, we train a model on benevolent goals that match the good Terminator character from Terminator 2. Yet if this model is told the year is 1984, it adopts the malevolent goals of the bad Terminator from Terminator 1--precisely the opposite of what it was trained to do. Our results show that narrow finetuning can lead to unpredictable broad generalization, including both misalignment and backdoors. Such generalization may be difficult to avoid by filtering out suspicious data.

</details>


### [29] [MOA: Multi-Objective Alignment for Role-Playing Agents](https://arxiv.org/abs/2512.09756)
*Chonghua Liao,Ke Wang,Yuchuan Wu,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: MOA是一个多目标强化学习框架，通过同时优化多个细粒度评估标准，显著提升了角色扮演代理在知识、风格和对话能力等多维度的表现。


<details>
  <summary>Details</summary>
Motivation: 现有角色扮演代理方法存在局限性：监督微调容易过拟合表面特征且多样性低，而强化学习方法难以同时优化多个维度。需要一种能全面优化角色知识、人物风格、多样化场景和复杂多轮对话需求的方法。

Method: 提出MOA多目标对齐框架：1）采用多目标优化策略，同时训练多个细粒度评估标准；2）引入思维增强的rollout和离策略指导，解决输出多样性和质量问题。

Result: 在PersonaGym和RoleMRC等挑战性基准测试中，MOA使8B参数模型能够匹配甚至超越GPT-4o和Claude等强大基线模型，在多个维度上表现出色。

Conclusion: MOA展示了在构建能同时满足角色知识、人物风格、多样化场景和复杂多轮对话需求的角色扮演代理方面的巨大潜力，为多维度优化提供了有效解决方案。

Abstract: Role-playing agents (RPAs) must simultaneously master many conflicting skills -- following multi-turn instructions, exhibiting domain knowledge, and adopting a consistent linguistic style. Existing work either relies on supervised fine-tuning (SFT) that over-fits surface cues and yields low diversity, or applies reinforcement learning (RL) that fails to learn multiple dimensions for comprehensive RPA optimization. We present MOA (Multi-Objective Alignment), a reinforcement-learning framework that enables multi-dimensional, fine-grained rubric optimization for general RPAs. MOA introduces a novel multi-objective optimization strategy that trains simultaneously on multiple fine-grained rubrics to boost optimization performance. Besides, to address the issues of model output diversity and quality, we have also employed thought-augmented rollout with off-policy guidance. Extensive experiments on challenging benchmarks such as PersonaGym and RoleMRC show that MOA enables an 8B model to match or even outperform strong baselines such as GPT-4o and Claude across numerous dimensions. This demonstrates the great potential of MOA in building RPAs that can simultaneously meet the demands of role knowledge, persona style, diverse scenarios, and complex multi-turn conversations.

</details>


### [30] [DeepSeek's WEIRD Behavior: The cultural alignment of Large Language Models and the effects of prompt language and cultural prompting](https://arxiv.org/abs/2512.09772)
*James Luther,Donald Brown*

Main category: cs.CL

TL;DR: 该研究使用霍夫斯泰德VSM13国际调查评估大语言模型的文化对齐性，发现主流模型普遍更接近美国文化，即使使用文化提示或改变提示语言也难以与中国文化对齐，但某些模型可通过提示策略实现可接受的文化对齐。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在生成类人文本方面的能力增强，人机交互日益增多，这些类人代理的文化对齐性成为一个重要研究领域。文化是人类互动的核心要素，影响我们如何感知和与他人互动。

Method: 使用霍夫斯泰德VSM13国际调查来理解模型的文化对齐性。采用提示语言和文化提示（通过系统提示将模型对齐转向特定国家）相结合的策略，将主流LLM与不同文化对齐。

Result: DeepSeek-V3、V3.1和GPT-5与美国调查响应高度对齐，即使使用文化提示或改变提示语言也无法与中国实现强或弱对齐。GPT-4在英语提示下更接近中国文化，但文化提示可将其转向美国。GPT-4o和GPT-4.1等低成本模型可通过提示语言（英语或简体中文）和文化提示策略实现与美国和中国的可接受对齐。

Conclusion: 当前主流大语言模型存在文化偏见，更倾向于美国文化价值观。文化提示策略对某些模型有效，但无法完全克服固有偏见。这突显了开发更具文化包容性AI系统的重要性。

Abstract: Culture is a core component of human-to-human interaction and plays a vital role in how we perceive and interact with others. Advancements in the effectiveness of Large Language Models (LLMs) in generating human-sounding text have greatly increased the amount of human-to-computer interaction. As this field grows, the cultural alignment of these human-like agents becomes an important field of study. Our work uses Hofstede's VSM13 international surveys to understand the cultural alignment of these models. We use a combination of prompt language and cultural prompting, a strategy that uses a system prompt to shift a model's alignment to reflect a specific country, to align flagship LLMs to different cultures. Our results show that DeepSeek-V3, V3.1, and OpenAI's GPT-5 exhibit a close alignment with the survey responses of the United States and do not achieve a strong or soft alignment with China, even when using cultural prompts or changing the prompt language. We also find that GPT-4 exhibits an alignment closer to China when prompted in English, but cultural prompting is effective in shifting this alignment closer to the United States. Other low-cost models, GPT-4o and GPT-4.1, respond to the prompt language used (i.e., English or Simplified Chinese) and cultural prompting strategies to create acceptable alignments with both the United States and China.

</details>


### [31] [OnCoCo 1.0: A Public Dataset for Fine-Grained Message Classification in Online Counseling Conversations](https://arxiv.org/abs/2512.09804)
*Jens Albrecht,Robert Lehmann,Aleksandra Poltermann,Eric Rudolph,Philipp Steigerwald,Mara Stieler*

Main category: cs.CL

TL;DR: OnCoCo 1.0是一个用于在线心理咨询细粒度消息分类的新公共数据集，包含约2,800条标注消息，区分38种咨询师和28种客户话语类型，并提供了微调模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于动机访谈（MI）的分类系统存在局限性：范围狭窄且主要依赖面对面咨询数据集，这限制了对文本咨询对话的详细分析。

Method: 开发了一个综合性的新编码方案，区分38种咨询师和28种客户话语类型，创建了包含约2,800条消息的标注数据集，并在该数据集上微调了多个模型。

Result: 创建了OnCoCo 1.0数据集，包含细粒度标注的咨询对话消息，微调模型展示了数据集的实用性，数据和模型已公开可用。

Conclusion: 这项工作为语言资源社区贡献了一种新型细粒度对话资源，扩展了社会和心理健康对话分析的现有数据集。

Abstract: This paper presents OnCoCo 1.0, a new public dataset for fine-grained message classification in online counseling. It is based on a new, integrative system of categories, designed to improve the automated analysis of psychosocial online counseling conversations. Existing category systems, predominantly based on Motivational Interviewing (MI), are limited by their narrow focus and dependence on datasets derived mainly from face-to-face counseling. This limits the detailed examination of textual counseling conversations. In response, we developed a comprehensive new coding scheme that differentiates between 38 types of counselor and 28 types of client utterances, and created a labeled dataset consisting of about 2.800 messages from counseling conversations. We fine-tuned several models on our dataset to demonstrate its applicability. The data and models are publicly available to researchers and practitioners. Thus, our work contributes a new type of fine-grained conversational resource to the language resources community, extending existing datasets for social and mental-health dialogue analysis.

</details>


### [32] [LLMs in Interpreting Legal Documents](https://arxiv.org/abs/2512.09830)
*Simone Corbo*

Main category: cs.CL

TL;DR: 大型语言模型在法律领域的应用探索，包括优化法律任务、分析使用案例，以及面临算法单一性、幻觉等挑战和监管合规问题


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型如何优化和增强传统法律任务，分析其在法律领域的潜在应用价值

Method: 分析可能的使用案例（如法规解释、合同分析、判例法研究），探讨技术挑战（算法单一性、幻觉问题），研究监管合规（欧盟AI法案、美国倡议、中国方法），并介绍两个不同的基准测试

Result: 展示了大型语言模型在法律领域的多种应用潜力，识别了技术实施中的关键挑战，分析了不同司法管辖区的监管框架，并提供了评估模型性能的基准测试方法

Conclusion: 大型语言模型在法律领域具有显著的应用前景，但需要克服技术挑战并确保合规性，不同司法管辖区的监管方法为未来发展提供了重要参考框架

Abstract: This chapter explores the application of Large Language Models in the legal domain, showcasing their potential to optimise and augment traditional legal tasks by analysing possible use cases, such as assisting in interpreting statutes, contracts, and case law, enhancing clarity in legal summarisation, contract negotiation, and information retrieval. There are several challenges that can arise from the application of such technologies, such as algorithmic monoculture, hallucinations, and compliance with existing regulations, including the EU's AI Act and recent U.S. initiatives, alongside the emerging approaches in China. Furthermore, two different benchmarks are presented.

</details>


### [33] [ChronusOmni: Improving Time Awareness of Omni Large Language Models](https://arxiv.org/abs/2512.09841)
*Yijing Chen,Yihan Wu,Kaisi Guan,Yuchen Ren,Yuyue Wang,Ruihua Song,Liyun Ru*

Main category: cs.CL

TL;DR: ChronusOmni是一个全模态大语言模型，通过引入时间戳令牌和强化学习，显著提升了音频-视觉跨模态时间感知能力，在时间定位任务上取得了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注视觉-语言场景和显式时间定位问题，对音频模态利用不足，且忽略了跨模态的隐式时间关系（如角色说话时的视觉内容或视觉事件发生时的语音内容），而这些在现实场景中普遍存在。

Method: 1) 在每个时间单元将基于文本的时间戳令牌与视觉和音频表示交错排列，实现跨模态的统一时间建模；2) 通过强化学习配合专门设计的奖励函数，强制正确的时间顺序并增强细粒度时间推理；3) 构建了ChronusAV数据集，这是一个时间准确、模态完整且跨模态对齐的数据集。

Result: ChronusOmni在ChronusAV数据集上实现了超过30%的性能提升，达到了最先进水平，并在其他时间定位基准测试的大多数指标上取得了最佳结果，同时保持了通用的视频和音频理解能力。

Conclusion: ChronusOmni通过创新的跨模态时间建模和强化学习方法，显著增强了全模态大语言模型的时间感知能力，能够有效处理显式和隐式的音频-视觉时间定位问题，为理解长视频和回答复杂问题提供了强大工具。

Abstract: Time awareness is a fundamental ability of omni large language models, especially for understanding long videos and answering complex questions. Previous approaches mainly target vision-language scenarios and focus on the explicit temporal grounding questions, such as identifying when a visual event occurs or determining what event happens at aspecific time. However, they often make insufficient use of the audio modality, and overlook implicit temporal grounding across modalities--for example, identifying what is visually present when a character speaks, or determining what is said when a visual event occurs--despite such cross-modal temporal relations being prevalent in real-world scenarios. In this paper, we propose ChronusOmni, an omni large language model designed to enhance temporal awareness for both explicit and implicit audiovisual temporal grounding. First, we interleave text-based timestamp tokens with visual and audio representations at each time unit, enabling unified temporal modeling across modalities. Second, to enforce correct temporal ordering and strengthen fine-grained temporal reasoning, we incorporate reinforcement learning with specially designed reward functions. Moreover, we construct ChronusAV, a temporally-accurate, modality-complete, and cross-modal-aligned dataset to support the training and evaluation on audiovisual temporal grounding task. Experimental results demonstrate that ChronusOmni achieves state-of-the-art performance on ChronusAV with more than 30% improvement and top results on most metrics upon other temporal grounding benchmarks. This highlights the strong temporal awareness of our model across modalities, while preserving general video and audio understanding capabilities.

</details>


### [34] [Mitigating Social Bias in English and Urdu Language Models Using PRM-Guided Candidate Selection and Sequential Refinement](https://arxiv.org/abs/2512.09854)
*Muneeb Ur Raheem Khan*

Main category: cs.CL

TL;DR: 本文提出一个推理时偏见缓解的统一评估框架，比较三种方法在英语和乌尔都语上的效果，发现所有方法都能显著减少偏见，但乌尔都语的公平性得分始终较低，揭示了多语言LLM训练中的结构性不平等。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常产生偏见或刻板印象内容，特别是在涉及社会敏感语言时。这种偏见对低资源语言影响尤为严重，因为这些语言的训练数据有限且文化代表性不足。需要避免重新训练或微调，直接在模型输出层面进行偏见缓解。

Method: 提出了一个统一的评估框架，比较三种推理时偏见缓解方法：(1) 基线单词生成，(2) PRM-Select最佳N采样，(3) PRM-Sequential基于PRM批评的序列优化。使用GPT-3.5作为候选生成器，GPT-4o-mini作为基于PRM的偏见和效用评分器，在200个英语提示及其乌尔都语对应版本上进行评估。

Result: 研究发现：(a) 所有方法相比基线都有显著改进；(b) 乌尔都语在所有方法中的公平性得分都较低，揭示了多语言LLM训练中的结构性不平等；(c) PRM-Select和PRM-Sequential显示出不同的改进轨迹。

Conclusion: 该研究提供了一个可扩展的方法论、可解释的指标和跨语言比较，能够支持未来在低资源语言公平性评估方面的工作。研究强调了需要专门关注低资源语言的偏见缓解策略。

Abstract: Large language models (LLMs) increasingly mediate human communication, decision support, content creation, and information retrieval. Despite impressive fluency, these systems frequently produce biased or stereotypical content, especially when prompted with socially sensitive language. A growing body of research has demonstrated that such biases disproportionately affect low-resource languages, where training data is limited and culturally unrepresentative. This paper presents a comprehensive study of inference-time bias mitigation, a strategy that avoids retraining or fine-tuning and instead operates directly on model outputs. Building on preference-ranking models (PRMs), we introduce a unified evaluation framework comparing three methods: (1) baseline single-word generation, (2) PRM-Select best-of-N sampling, and (3) PRM-Sequential refinement guided by PRM critiques. We evaluate these techniques across 200 English prompts and their Urdu counterparts, designed to reflect socio-cultural contexts relevant to gender, ethnicity, religion, nationality, disability, profession, age, and socioeconomic categories. Using GPT-3.5 as a candidate generator and GPT-4o-mini as a PRM-based bias and utility scorer, we provide an extensive quantitative analysis of bias reduction, utility preservation, and cross-lingual disparities. Our findings show: (a) substantial gains over the baseline for both languages; (b) consistently lower fairness scores for Urdu across all methods, highlighting structural inequities in multilingual LLM training; and (c) distinct improvement trajectories between PRM-Select and PRM-Sequential. The study contributes an extensible methodology, interpretable metrics, and cross-lingual comparisons that can support future work on fairness evaluation in low-resource languages.

</details>


### [35] [Efficient Continual Learning in Neural Machine Translation: A Low-Rank Adaptation Approach](https://arxiv.org/abs/2512.09910)
*Salvador Carrión,Francisco Casacuberta*

Main category: cs.CL

TL;DR: LoRA框架用于神经机器翻译的持续学习，通过低秩适配实现参数高效、实时交互调整，并引入梯度正则化缓解灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 神经机器翻译的持续学习面临灾难性遗忘和高计算成本的双重挑战，需要参数高效且能实时交互调整的解决方案。

Method: 1) 使用LoRA进行参数高效微调；2) 提出基于校准线性组合的交互式适配方法，实现无门控专家混合；3) 设计针对低秩分解矩阵的梯度正则化策略，利用历史梯度信息加权惩罚。

Result: LoRA微调在性能和参数效率上与全参数技术相当；交互式适配方法支持实时用户可控调整；梯度正则化策略有效保留先前领域知识并促进新任务学习。

Conclusion: LoRA框架为交互式和持续神经机器翻译提供了可扩展的范式，平衡了参数效率、实时调整能力和灾难性遗忘缓解。

Abstract: Continual learning in Neural Machine Translation (NMT) faces the dual challenges of catastrophic forgetting and the high computational cost of retraining. This study establishes Low-Rank Adaptation (LoRA) as a parameter-efficient framework to address these challenges in dedicated NMT architectures. We first demonstrate that LoRA-based fine-tuning adapts NMT models to new languages and domains with performance on par with full-parameter techniques, while utilizing only a fraction of the parameter space. Second, we propose an interactive adaptation method using a calibrated linear combination of LoRA modules. This approach functions as a gate-free mixture of experts, enabling real-time, user-controllable adjustments to domain and style without retraining. Finally, to mitigate catastrophic forgetting, we introduce a novel gradient-based regularization strategy specifically designed for low-rank decomposition matrices. Unlike methods that regularize the full parameter set, our approach weights the penalty on the low-rank updates using historical gradient information. Experimental results indicate that this strategy efficiently preserves prior domain knowledge while facilitating the acquisition of new tasks, offering a scalable paradigm for interactive and continual NMT.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [36] [Meta Lattice: Model Space Redesign for Cost-Effective Industry-Scale Ads Recommendations](https://arxiv.org/abs/2512.09200)
*Liang Luo,Yuxin Chen,Zhengyu Zhang,Mengyue Hang,Andrew Gu,Buyun Zhang,Boyang Liu,Chen Chen,Chengze Fan,Dong Liang,Fan Yang,Feifan Gu,Huayu Li,Jade Nie,Jiayi Xu,Jiyan Yang,Jongsoo Park,Laming Chen,Longhao Jin,Qianru Li,Qin Huang,Shali Jiang,Shiwen Shen,Shuaiwen Wang,Sihan Zeng,Siyang Yuan,Tongyi Tang,Weilin Zhang,Wenjun Wang,Xi Liu,Xiaohan Wei,Xiaozhen Xia,Yuchen Hao,Yunlong He,Yasmine Badr,Zeliang Chen,Maxim Naumov,Yantao Yao,Wenlin Chen,Santanu Kolay,GP Musumeci,Ellie Dingqiao Wen*

Main category: cs.IR

TL;DR: Lattice是一个推荐框架，通过模型空间重新设计解决跨域数据碎片化和基础设施成本问题，在Meta部署后实现了质量提升和成本节约。


<details>
  <summary>Details</summary>
Motivation: 产品、界面、政策和法规的快速演变给工业级推荐模型部署带来挑战，主要由于跨域数据碎片化和不断上升的基础设施成本阻碍了持续的质量改进。

Method: Lattice框架围绕模型空间重新设计，扩展了多域多目标学习，结合跨域知识共享、数据整合、模型统一、蒸馏和系统优化。

Result: 在Meta部署Lattice后，实现了10%的收入驱动指标提升，11.5%用户满意度改善，6%转化率提升，同时节省20%容量。

Conclusion: Lattice通过全面的模型空间重新设计，成功解决了工业级推荐系统面临的跨域数据碎片化和成本效率挑战，实现了质量和成本的双重改进。

Abstract: The rapidly evolving landscape of products, surfaces, policies, and regulations poses significant challenges for deploying state-of-the-art recommendation models at industry scale, primarily due to data fragmentation across domains and escalating infrastructure costs that hinder sustained quality improvements.
  To address this challenge, we propose Lattice, a recommendation framework centered around model space redesign that extends Multi-Domain, Multi-Objective (MDMO) learning beyond models and learning objectives. Lattice addresses these challenges through a comprehensive model space redesign that combines cross-domain knowledge sharing, data consolidation, model unification, distillation, and system optimizations to achieve significant improvements in both quality and cost-efficiency.
  Our deployment of Lattice at Meta has resulted in 10% revenue-driving top-line metrics gain, 11.5% user satisfaction improvement, 6% boost in conversion rate, with 20% capacity saving.

</details>
