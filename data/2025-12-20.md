<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 7]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs](https://arxiv.org/abs/2512.16814)
*William English,Dominic Simon,Sumit Kumar Jha,Rickard Ewetz*

Main category: cs.CL

TL;DR: GraFT框架通过限制语言模型在每个步骤的有效输出标记，降低从自然语言到时序逻辑翻译的复杂度，提升翻译准确率。


<details>
  <summary>Details</summary>
Motivation: 当前自然语言到时序逻辑的翻译方法存在三个主要问题：原子命题提升不准确、存在共指现象、以及难以从有限数据中学习。现有方法让语言模型从完整词汇表中迭代预测标记，导致任务复杂度高。

Method: 提出Grammar Forced Translation (GraFT)框架，通过利用每个问题的独特属性，在每个步骤将有效输出标记从完整词汇表限制到少数几个，从而减少解空间复杂度。该框架还提供了理论依据，解释解空间减少如何带来更高效的学习。

Result: 在CW、GLTL和Navi基准测试中，GraFT相比最先进的翻译方法，端到端翻译准确率平均提升5.49%，域外翻译准确率平均提升14.06%。

Conclusion: GraFT框架通过减少解空间复杂度，有效解决了自然语言到时序逻辑翻译中的准确性问题，在多个基准测试中表现出显著优于现有方法的性能。

Abstract: Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.

</details>


### [2] [What Do Prosody and Text Convey? Characterizing How Meaningful Information is Distributed Across Multiple Channels](https://arxiv.org/abs/2512.16832)
*Aditya Yadavalli,Tiago Pimentel,Tamar I Regev,Ethan Wilcox,Alex Warstadt*

Main category: cs.CL

TL;DR: 本研究提出一种信息论方法，使用大型语音和语言模型量化仅通过韵律（而非文本）传达的信息量，并分析这些信息的具体内容，发现在表达讽刺和情感时，音频通道比文本通道传递的信息量高一个数量级。


<details>
  <summary>Details</summary>
Motivation: 韵律作为语音的旋律，传达了文本无法捕捉的关键信息。然而，目前缺乏系统性的方法来量化仅通过韵律表达的信息量，以及这些信息的具体内容。这阻碍了我们对语音交流中不同通道信息贡献的理解。

Method: 采用信息论方法，利用大型语音和语言模型估计话语特定意义维度（如情感）与任何通信通道（如音频或文本）之间的互信息。通过互信息量化音频和文本在表达讽刺、情感和疑问性方面的信息量。

Result: 研究发现，在表达讽刺和情感时，音频通道（即韵律通道）传输的信息量比文本通道高一个数量级以上（特别是在缺乏当前句子之外的长期上下文时）。对于疑问性，韵律提供的额外信息相对较少。

Conclusion: 韵律在传达讽刺和情感方面比文本提供显著更多的信息，尤其在缺乏上下文时。该方法为未来研究更多意义维度、通信通道和语言的信息贡献奠定了基础。

Abstract: Prosody -- the melody of speech -- conveys critical information often not captured by the words or text of a message. In this paper, we propose an information-theoretic approach to quantify how much information is expressed by prosody alone and not by text, and crucially, what that information is about. Our approach applies large speech and language models to estimate the mutual information between a particular dimension of an utterance's meaning (e.g., its emotion) and any of its communication channels (e.g., audio or text). We then use this approach to quantify how much information is conveyed by audio and text about sarcasm, emotion, and questionhood, using speech from television and podcasts. We find that for sarcasm and emotion the audio channel -- and by implication the prosodic channel -- transmits over an order of magnitude more information about these features than the text channel alone, at least when long-term context beyond the current sentence is unavailable. For questionhood, prosody provides comparatively less additional information. We conclude by outlining a program applying our approach to more dimensions of meaning, communication channels, and languages.

</details>


### [3] [LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference](https://arxiv.org/abs/2512.16843)
*Harsh Vardhan Bansal*

Main category: cs.CL

TL;DR: LLMCache是一个模型无关的层级缓存框架，通过重用语义相似输入序列的中间激活来加速Transformer推理，在BERT和GPT-2上实现最高3.1倍加速且精度损失低于0.5%。


<details>
  <summary>Details</summary>
Motivation: Transformer模型虽然性能优异，但推理延迟高，限制了实时和大规模部署。现有的token级KV缓存机制在适用范围和效果上存在局限性。

Method: 提出LLMCache框架：1) 模型无关，支持编码器和解码器架构；2) 可在任意Transformer层进行缓存；3) 使用轻量级指纹机制匹配语义相似输入；4) 采用自适应淘汰策略管理缓存陈旧性。

Result: 在SQuAD、WikiText-103和OpenBookQA数据集上测试BERT和GPT-2，推理时间最高加速3.1倍，精度损失低于0.5%。

Conclusion: LLMCache是一个实用且通用的Transformer推理优化解决方案，适用于实际应用场景。

Abstract: Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications

</details>


### [4] [AdaSearch: Balancing Parametric Knowledge and Search in Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2512.16883)
*Tzu-Han Lin,Wei-Lin Chen,Chen-An Li,Hung-yi Lee,Yun-Nung Chen,Yu Meng*

Main category: cs.CL

TL;DR: 本文提出AdaSearch框架，通过两阶段强化学习分离问题解决与搜索决策，提高搜索代理的知识边界意识，减少不必要搜索调用，同时保持任务性能并增强决策透明度。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的搜索代理存在过度依赖搜索的问题，导致不必要的成本和暴露于噪声/恶意内容的风险，而仅依赖参数化知识又可能产生幻觉。现有方法通过惩罚工具调用次数来缓解搜索过度使用，但这需要大量奖励工程、存在信用分配模糊性，且可能被表面减少调用的代理利用。

Method: 提出AdaSearch框架：1）首先量化现有搜索代理的自我知识意识，发现如Search-R1等方法经常忽略现成的参数化知识；2）采用简单的两阶段、结果驱动的强化学习框架，将问题解决与是否调用搜索的决策解耦，使决策过程明确且可解释。

Result: 在多个模型系列和规模上的实验表明，AdaSearch显著提高了知识边界意识，减少了不必要的搜索调用，保持了强大的任务性能，并提供了更透明、可解释的决策行为。

Conclusion: AdaSearch通过解耦问题解决与搜索决策，解决了现有搜索代理过度依赖搜索的问题，在金融和医疗问答等高风险领域特别有价值，提供了更透明和可解释的决策框架。

Abstract: Equipping large language models (LLMs) with search engines via reinforcement learning (RL) has emerged as an effective approach for building search agents. However, overreliance on search introduces unnecessary cost and risks exposure to noisy or malicious content, while relying solely on parametric knowledge risks hallucination. The central challenge is to develop agents that adaptively balance parametric knowledge with external search, invoking search only when necessary. Prior work mitigates search overuse by shaping rewards around the number of tool calls. However, these penalties require substantial reward engineering, provide ambiguous credit assignment, and can be exploited by agents that superficially reduce calls. Moreover, evaluating performance solely through call counts conflates necessary and unnecessary search, obscuring the measurement of true adaptive behavior. To address these limitations, we first quantify the self-knowledge awareness of existing search agents via an F1-based decision metric, revealing that methods such as Search-R1 often overlook readily available parametric knowledge. Motivated by these findings, we propose AdaSearch, a simple two-stage, outcome-driven RL framework that disentangles problem solving from the decision of whether to invoke search, and makes this decision process explicit and interpretable. This transparency is crucial for high-stakes domains such as finance and medical question answering, yet is largely neglected by prior approaches. Experiments across multiple model families and sizes demonstrate that AdaSearch substantially improves knowledge-boundary awareness, reduces unnecessary search calls, preserves strong task performance, and offers more transparent, interpretable decision behaviors.

</details>


### [5] [Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image](https://arxiv.org/abs/2512.16899)
*Yushi Hu,Reyhane Askari-Hemmat,Melissa Hall,Emily Dinan,Luke Zettlemoyer,Marjan Ghazvininejad*

Main category: cs.CL

TL;DR: 提出了首个全面的多模态奖励模型基准MMRB2，用于评估图像-文本交织序列的理解和生成任务，包含四个子任务和4000个专家标注的偏好对。


<details>
  <summary>Details</summary>
Motivation: 奖励模型对于训练大语言模型至关重要，但对于处理图像和文本交织序列的Omni模型仍缺乏充分探索，需要建立全面的多模态奖励模型评估基准。

Method: 创建MMRB2基准，包含四个任务：文本到图像生成、图像编辑、交织生成和多模态推理；收集1000个专家标注的偏好对/任务，来自23个模型和代理在21个源任务中的表现；采用集成过滤策略确保偏好对具有强人类专家共识。

Result: 评估现有模型：Gemini 3 Pro达到75-80%准确率，GPT-5和Gemini 2.5 Pro达到66-75%，超越GPT-4o（59%）；最佳开源模型Qwen3-VL-32B达到64%，与Gemini 2.5 Flash相当；人类准确率超过90%；MMRB2性能与下游任务成功强相关。

Conclusion: MMRB2为多模态奖励模型提供了首个全面基准，揭示了当前模型与人类性能的显著差距，指出了奖励模型改进的关键方向，对多模态AI发展具有重要意义。

Abstract: Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning ("thinking-with-images"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.

</details>


### [6] [In-Context Algebra](https://arxiv.org/abs/2512.16902)
*Eric Todd,Jannik Brinkmann,Rohit Gandikota,David Bau*

Main category: cs.CL

TL;DR: 论文研究了transformer在变量意义不固定的序列算术任务中学习到的推理机制，发现了三种符号推理机制。


<details>
  <summary>Details</summary>
Motivation: 先前研究发现transformer会在固定符号意义的算术任务中发展几何嵌入来反映代数结构，但现实世界中的符号意义往往是上下文相关的。本研究旨在探索当符号意义随序列变化时，transformer会发展出什么样的推理机制。

Method: 设计了一个新任务，其中符号到特定代数群元素的分配随序列变化。创建有针对性的数据分布来进行因果测试，并开发了机制分析方法来识别模型学习到的推理策略。

Result: transformer在该任务中达到了接近完美的准确率，甚至能泛化到未见过的代数群。研究发现了三种模型一致学习的机制：交换复制（专用注意力头复制答案）、单位元识别（区分包含单位元的事实）和基于封闭性的消去（跟踪群成员身份来约束有效答案）。

Conclusion: 与固定符号设置中的几何表示互补，当训练transformer在变量意义不固定的上下文中进行推理时，模型会发展出符号推理机制，这表明transformer能够学习抽象的逻辑推理模式。

Abstract: We investigate the mechanisms that arise when transformers are trained to solve arithmetic on sequences where tokens are variables whose meaning is determined only through their interactions. While prior work has found that transformers develop geometric embeddings that mirror algebraic structure, those previous findings emerge from settings where arithmetic-valued tokens have fixed meanings. We devise a new task in which the assignment of symbols to specific algebraic group elements varies from one sequence to another. Despite this challenging setup, transformers achieve near-perfect accuracy on the task and even generalize to unseen algebraic groups. We develop targeted data distributions to create causal tests of a set of hypothesized mechanisms, and we isolate three mechanisms models consistently learn: commutative copying where a dedicated head copies answers, identity element recognition that distinguishes identity-containing facts, and closure-based cancellation that tracks group membership to constrain valid answers. Complementary to the geometric representations found in fixed-symbol settings, our findings show that models develop symbolic reasoning mechanisms when trained to reason in-context with variables whose meanings are not fixed.

</details>


### [7] [Constructive Circuit Amplification: Improving Math Reasoning in LLMs via Targeted Sub-Network Updates](https://arxiv.org/abs/2512.16914)
*Nikhil Prakash,Donghao Ren,Dominik Moritz,Yannick Assogba*

Main category: cs.CL

TL;DR: 提出了一种名为构造性电路放大的新方法，通过识别关键token和任务相关组件，仅更新稀疏的模型组件来提升特定任务性能


<details>
  <summary>Details</summary>
Motivation: 先前研究发现LLMs中存在负责特定任务的稀疏子网络（电路），且微调通常通过增强现有电路来提升性能。这启发了直接干预这些电路进行精确、任务针对性更新的可能性。

Method: 构造性电路放大方法：从模型推理轨迹中识别关键token，找出负责目标任务的模型组件，仅更新这些组件而非整个模型。

Result: 在数学推理任务上，该方法将准确率提升高达+11.4%，同时仅修改1.59%的模型组件，对MMLU、TriviaQA和TruthfulQA等其他能力影响最小。

Conclusion: 通过选择性更新稀疏的模型组件集合，可以可靠地增强目标能力，这为精确、高效的模型编辑提供了新途径。

Abstract: Prior studies investigating the internal workings of LLMs have uncovered sparse subnetworks, often referred to as circuits, that are responsible for performing specific tasks. Additionally, it has been shown that model performance improvement through fine-tuning often results from the strengthening of existing circuits in the model. Taken together, these findings suggest the possibility of intervening directly on such circuits to make precise, task-targeted updates. Motivated by these findings, we propose a novel method called Constructive Circuit Amplification which identifies pivotal tokens from model reasoning traces as well as model components responsible for the desired task, and updates only those components. Applied to mathematical reasoning, it improves accuracy by up to +11.4% across multiple models while modifying as little as 1.59% of model components, with minimal impact on other abilities as measured by MMLU, TriviaQA, and TruthfulQA. These results demonstrate that targeted capabilities can be reliably enhanced by selectively updating a sparse set of model components.

</details>
