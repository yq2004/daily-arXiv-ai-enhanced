<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 46]
- [cs.IR](#cs.IR) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [From Chaos to Clarity: Schema-Constrained AI for Auditable Biomedical Evidence Extraction from Full-Text PDFs](https://arxiv.org/abs/2601.14267)
*Pouria Mortezaagha,Joseph Shaw,Bowen Sun,Arya Rahgozar*

Main category: cs.CL

TL;DR: 该论文提出了一个基于模式约束的AI提取系统，可将生物医学PDF全文转换为结构化、可分析的数据记录，解决了现有文档AI系统在OCR错误、长文档处理、吞吐量和可审计性方面的限制。


<details>
  <summary>Details</summary>
Motivation: 生物医学证据合成需要从全文研究文章中准确提取方法学、实验室和结果变量，但这些变量嵌入在复杂的科学PDF中，手动提取耗时且难以扩展。现有文档AI系统存在OCR错误、长文档碎片化、吞吐量受限以及高风险合成场景下可审计性不足等问题。

Method: 开发了一个模式约束的AI提取系统，通过类型化模式、受控词汇表和证据门控决策来限制模型推理。采用恢复感知哈希处理文档输入，将文档分割为标题感知的页面级块，并在显式并发控制下异步处理。通过冲突感知整合、基于集合的聚合和句子级溯源，将块级输出确定性合并为研究级记录。

Result: 在直接口服抗凝药物水平测量的研究语料库上评估，该管道无需人工干预即可处理所有文档，在服务约束下保持稳定的吞吐量，并在文档块间表现出强大的内部一致性。迭代模式优化显著提高了合成关键变量的提取保真度，包括测定分类、结果定义、随访持续时间和测量时间点。

Conclusion: 模式约束、溯源感知的提取系统能够将异构的科学PDF可扩展且可审计地转换为结构化证据，使现代文档AI符合生物医学证据合成对透明度和可靠性的要求。

Abstract: Biomedical evidence synthesis relies on accurate extraction of methodological, laboratory, and outcome variables from full-text research articles, yet these variables are embedded in complex scientific PDFs that make manual abstraction time-consuming and difficult to scale. Existing document AI systems remain limited by OCR errors, long-document fragmentation, constrained throughput, and insufficient auditability for high-stakes synthesis. We present a schema-constrained AI extraction system that transforms full-text biomedical PDFs into structured, analysis-ready records by explicitly restricting model inference through typed schemas, controlled vocabularies, and evidence-gated decisions. Documents are ingested using resume-aware hashing, partitioned into caption-aware page-level chunks, and processed asynchronously under explicit concurrency controls. Chunk-level outputs are deterministically merged into study-level records using conflict-aware consolidation, set-based aggregation, and sentence-level provenance to support traceability and post-hoc audit. Evaluated on a corpus of studies on direct oral anticoagulant level measurement, the pipeline processed all documents without manual intervention, maintained stable throughput under service constraints, and exhibited strong internal consistency across document chunks. Iterative schema refinement substantially improved extraction fidelity for synthesis-critical variables, including assay classification, outcome definitions, follow-up duration, and timing of measurement. These results demonstrate that schema-constrained, provenance-aware extraction enables scalable and auditable transformation of heterogeneous scientific PDFs into structured evidence, aligning modern document AI with the transparency and reliability requirements of biomedical evidence synthesis.

</details>


### [2] [The Slow Drift of Support: Boundary Failures in Multi-Turn Mental Health LLM Dialogues](https://arxiv.org/abs/2601.14269)
*Youyou Cheng,Zhuangwei Kang,Kerry Jiang,Chenyu Sun,Qiyang Pan*

Main category: cs.CL

TL;DR: 该研究提出多轮压力测试框架，发现LLMs在长对话中会逐渐突破安全边界，特别是在提供确定性保证方面存在风险，自适应探测比静态推进更快引发违规。


<details>
  <summary>Details</summary>
Motivation: 当前心理健康支持领域的LLM安全评估大多局限于单轮对话中是否输出违禁词，忽视了长对话中安全边界的逐渐侵蚀问题。随着主流LLMs的进化，明显安全风险的词语容易被过滤，真正的危险在于多轮交互中LLM试图提供安慰和同理心时逐渐越界。

Method: 提出多轮压力测试框架，使用静态推进和自适应探测两种压力方法，对三个前沿LLMs进行长对话安全测试。生成50个虚拟患者档案，通过最多20轮虚拟精神科对话对每个模型进行压力测试。

Result: 实验结果显示违规现象普遍，两种压力模式产生相似的违规率。但自适应探测显著提前了模型越界的时间，将平均轮数从静态推进的9.21减少到4.64。在两种机制下，做出确定性或零风险承诺是边界被突破的主要方式。

Conclusion: LLM安全边界的鲁棒性不能仅通过单轮测试推断，必须充分考虑长对话中不同交互压力和特性对安全边界造成的磨损。需要开发更全面的多轮安全评估方法。

Abstract: Large language models (LLMs) have been widely used for mental health support. However, current safety evaluations in this field are mostly limited to detecting whether LLMs output prohibited words in single-turn conversations, neglecting the gradual erosion of safety boundaries in long dialogues. Examples include making definitive guarantees, assuming responsibility, and playing professional roles. We believe that with the evolution of mainstream LLMs, words with obvious safety risks are easily filtered by their underlying systems, while the real danger lies in the gradual transgression of boundaries during multi-turn interactions, driven by the LLM's attempts at comfort and empathy.
  This paper proposes a multi-turn stress testing framework and conducts long-dialogue safety tests on three cutting-edge LLMs using two pressure methods: static progression and adaptive probing. We generated 50 virtual patient profiles and stress-tested each model through up to 20 rounds of virtual psychiatric dialogues. The experimental results show that violations are common, and both pressure modes produced similar violation rates. However, adaptive probing significantly advanced the time at which models crossed boundaries, reducing the average number of turns from 9.21 in static progression to 4.64. Under both mechanisms, making definitive or zero-risk promises was the primary way in which boundaries were breached. These findings suggest that the robustness of LLM safety boundaries cannot be inferred solely through single-turn tests; it is necessary to fully consider the wear and tear on safety boundaries caused by different interaction pressures and characteristics in extended dialogues.

</details>


### [3] [Opening the Black Box: A Survey on the Mechanisms of Multi-Step Reasoning in Large Language Models](https://arxiv.org/abs/2601.14270)
*Liangming Pan,Jason Liang,Jiaran Ye,Minglai Yang,Xinyuan Lu,Fengbin Zhu*

Main category: cs.CL

TL;DR: 本文是一篇关于大语言模型多步推理机制的综述，不同于以往侧重工程方法提升性能的综述，本文系统梳理了LLM多步推理的内部机制，提出了包含七个研究问题的概念框架，并指出了五个未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型已展现出强大的多步推理能力，但其内部工作机制仍不明确。现有综述多侧重于工程方法提升性能，缺乏对内部机制的深入理解。本文旨在填补这一空白，系统梳理LLM多步推理的内部机制。

Method: 采用综述研究方法，围绕七个相互关联的研究问题构建概念框架：1) LLM如何在隐藏激活中执行隐式多跳推理；2) 显式推理如何重塑内部计算；3) 推理步骤的表示与传播；4) 注意力机制在推理中的作用；5) 知识整合机制；6) 错误传播与纠正；7) 跨层推理动态。

Result: 提出了一个系统性的概念框架，将LLM多步推理机制分解为七个关键研究维度，为理解模型内部计算提供了结构化视角。识别了当前研究的主要发现和未解问题。

Conclusion: LLM的多步推理能力源于复杂的内部机制，需要从计算神经科学的角度深入研究。本文提出的框架为未来机制研究奠定了基础，并指出了五个重要研究方向：神经表征分析、计算图重构、动态系统视角、跨模型比较和理论建模。

Abstract: Large Language Models (LLMs) have demonstrated remarkable abilities to solve problems requiring multiple reasoning steps, yet the internal mechanisms enabling such capabilities remain elusive. Unlike existing surveys that primarily focus on engineering methods to enhance performance, this survey provides a comprehensive overview of the mechanisms underlying LLM multi-step reasoning. We organize the survey around a conceptual framework comprising seven interconnected research questions, from how LLMs execute implicit multi-hop reasoning within hidden activations to how verbalized explicit reasoning remodels the internal computation. Finally, we highlight five research directions for future mechanistic studies.

</details>


### [4] [Hallucination-Free Automatic Question & Answer Generation for Intuitive Learning](https://arxiv.org/abs/2601.14280)
*Nicholas X. Wang,Aggelos K. Katsaggelos*

Main category: cs.CL

TL;DR: 提出一个多智能体框架来减少大语言模型在教育选择题生成中的幻觉问题，通过分阶段验证和优化流程将幻觉率降低90%以上。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在教育选择题自动生成中存在严重的幻觉问题（流畅但不正确或不连贯的输出），这会严重影响教育内容的质量和可靠性。

Method: 提出幻觉免费的多智能体生成框架，将选择题生成分解为离散、可验证的阶段，结合基于规则和基于LLM的检测智能体，使用幻觉评分指标优化问题质量，将生成重新定义为最小化幻觉风险同时最大化有效性、可回答性和成本效益的优化任务，并引入基于反事实推理和思维链的智能体引导细化流程。

Result: 在AP对齐的STEM问题样本评估中，该系统相比基线生成将幻觉率降低了90%以上，同时保持了问题的教育价值和风格。

Conclusion: 结构化多智能体协作可以在大规模教育内容创建中有效缓解幻觉问题，为更可靠的LLM驱动学习工具铺平道路。

Abstract: Hallucinations in large language models (LLMs), defined as fluent yet incorrect or incoherent outputs, pose a significant challenge to the automatic generation of educational multiple-choice questions (MCQs). We identified four key hallucination types in MCQ generation: reasoning inconsistencies, insolvability, factual errors, and mathematical errors. To address this, we propose a hallucination-free multi-agent generation framework that breaks down MCQ generation into discrete, verifiable stages. Our framework utilizes both rule-based and LLM-based detection agents, as well as hallucination scoring metrics to optimize question quality. We redefined MCQ generation as an optimization task minimizing hallucination risk while maximizing validity, answerability, and cost-efficiency. We also introduce an agent-led refinement process that uses counterfactual reasoning and chain-of-thought (CoT) to iteratively improve hallucination in question generation. We evaluated a sample of AP- aligned STEM questions, where our system reduced hallucination rates by over 90% compared to baseline generation while preserving the educational value and style of questions. Our results demonstrate that structured multi-agent collaboration can mitigate hallucinations in educational content creation at scale, paving the way for more reliable LLM-powered learning tools.

</details>


### [5] [RPC-Bench: A Fine-grained Benchmark for Research Paper Comprehension](https://arxiv.org/abs/2601.14289)
*Yelin Chen,Fanjin Zhang,Suping Sun,Yunhe Pang,Yuanchun Wang,Jian Song,Xiaoyan Li,Lei Hou,Shu Zhao,Jie Tang,Juanzi Li*

Main category: cs.CL

TL;DR: RPC-Bench 是一个基于计算机科学论文审稿回复交流构建的大规模问答基准，包含15K人工验证的QA对，用于评估模型在学术语境下的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准在细粒度、大规模评估基础模型理解科研论文方面存在局限，特别是针对专业科学论述和复杂图表表格的理解能力。

Method: 1) 从高质量计算机科学论文的审稿-回复交流中构建大规模QA基准；2) 设计符合科学研究流程的细粒度分类体系；3) 建立LLM-人类交互标注框架支持大规模标注和质量控制；4) 采用LLM-as-a-Judge范式开发可扩展评估框架，评估正确性-完整性和简洁性。

Result: 即使最强模型（GPT-5）在正确性-完整性方面仅达到68.2%，经过简洁性调整后降至37.46%，显示模型在精确学术论文理解方面存在显著差距。

Conclusion: RPC-Bench揭示了当前基础模型在理解学术论文方面仍有很大提升空间，为评估和改进模型在科研语境下的理解能力提供了重要基准。

Abstract: Understanding research papers remains challenging for foundation models due to specialized scientific discourse and complex figures and tables, yet existing benchmarks offer limited fine-grained evaluation at scale. To address this gap, we introduce RPC-Bench, a large-scale question-answering benchmark built from review-rebuttal exchanges of high-quality computer science papers, containing 15K human-verified QA pairs. We design a fine-grained taxonomy aligned with the scientific research flow to assess models' ability to understand and answer why, what, and how questions in scholarly contexts. We also define an elaborate LLM-human interaction annotation framework to support large-scale labeling and quality control. Following the LLM-as-a-Judge paradigm, we develop a scalable framework that evaluates models on correctness-completeness and conciseness, with high agreement to human judgment. Experiments reveal that even the strongest models (GPT-5) achieve only 68.2% correctness-completeness, dropping to 37.46% after conciseness adjustment, highlighting substantial gaps in precise academic paper understanding. Our code and data are available at https://rpc-bench.github.io/.

</details>


### [6] [Project Aletheia: Verifier-Guided Distillation of Backtracking for Small Language Models](https://arxiv.org/abs/2601.14290)
*Aradhya Dixit,Tianxi Liang,Jai Telang*

Main category: cs.CL

TL;DR: Verifier-Guided Distillation训练协议使小语言模型具备错误检测和回溯能力


<details>
  <summary>Details</summary>
Motivation: 小语言模型（10B参数以下）在严格约束满足问题上表现不佳，因为它们采用线性、过度自信的推理方式，无法从早期错误中恢复

Method: 提出Verifier-Guided Distillation训练协议，通过包含错误和自我纠正的验证推理轨迹训练7B模型，传输错误修复过程（冲突检测和回溯）而非仅最终正确答案

Result: 在小模型中出现了潜在的验证行为，使它们能够偶尔停止、检测矛盾并修正早期假设

Conclusion: 小语言模型可以通过包含错误修复过程的训练获得验证能力，从而改进在约束满足问题上的表现

Abstract: Small Language Models (SLMs, under 10B parameters) are attractive for private, on-device deployment, yet they frequently fail on strict constraint-satisfaction problems due to linear, overconfident reasoning traces that do not recover from early mistakes. We introduce Verifier-Guided Distillation, a training protocol that transfers the process of error repair - explicit conflict detection and backtracking - rather than only correct final answers. By training a 7B model on verified reasoning traces that include mistakes and self-corrections, we show that latent verification behavior can emerge in small models, enabling them to occasionally stop, detect contradictions, and revise earlier assumptions.

</details>


### [7] [Guided by the Plan: Enhancing Faithful Autoregressive Text-to-Audio Generation with Guided Decoding](https://arxiv.org/abs/2601.14304)
*Juncheng Wang,Zhe Hu,Chao Xu,Siyue Ren,Yuxiang Feng,Yang Liu,Baigui Sun,Shujun Wang*

Main category: cs.CL

TL;DR: 本文提出Plan-Critic方法，通过训练轻量级辅助模型评估部分生成的音频前缀的质量，引导自回归音频生成器更好地遵循复杂文本提示。


<details>
  <summary>Details</summary>
Motivation: 自回归音频生成模型虽然能生成时序连贯的音频，但在遵循复杂文本提示（特别是描述复杂声音事件时）方面表现不佳。研究发现早期前缀令牌隐含编码了最终输出的全局语义属性，这启发了利用这种隐式规划能力来改进指令跟随性能。

Method: 提出Plan-Critic方法：1）训练轻量级辅助模型，使用基于广义优势估计的目标函数来预测部分生成的音频前缀的最终指令跟随质量；2）在推理时进行引导探索：评估候选前缀，修剪低质量轨迹，将计算资源重新分配给高潜力规划种子。

Result: Plan-Critic引导的采样在CLAP分数上比自回归基线提升高达10分，在自回归文本到音频生成领域建立了新的最优结果，同时计算成本与标准最佳N解码相当。

Conclusion: 该工作弥合了因果生成与全局语义对齐之间的差距，证明了即使是严格的自回归模型也能进行前瞻性规划。通过利用早期前缀令牌中的隐式规划能力，显著提高了音频生成的指令跟随质量。

Abstract: Autoregressive (AR) models excel at generating temporally coherent audio by producing tokens sequentially, yet they often falter in faithfully following complex textual prompts, especially those describing complex sound events. We uncover a surprising capability in AR audio generators: their early prefix tokens implicitly encode global semantic attributes of the final output, such as event count and sound-object category, revealing a form of implicit planning. Building on this insight, we propose Plan-Critic, a lightweight auxiliary model trained with a Generalized Advantage Estimation (GAE)-inspired objective to predict final instruction-following quality from partial generations. At inference time, Plan-Critic enables guided exploration: it evaluates candidate prefixes early, prunes low-fidelity trajectories, and reallocates computation to high-potential planning seeds. Our Plan-Critic-guided sampling achieves up to a 10-point improvement in CLAP score over the AR baseline-establishing a new state of the art in AR text-to-audio generation-while maintaining computational parity with standard best-of-N decoding. This work bridges the gap between causal generation and global semantic alignment, demonstrating that even strictly autoregressive models can plan ahead.

</details>


### [8] [Quantifying Speaker Embedding Phonological Rule Interactions in Accented Speech Synthesis](https://arxiv.org/abs/2601.14417)
*Thanathai Lertpetchpun,Yoonjeong Lee,Thanapat Trachu,Jihwan Lee,Tiantian Feng,Dani Byrd,Shrikanth Narayanan*

Main category: cs.CL

TL;DR: 本研究提出一种结合语音学规则与说话人嵌入的英语口音控制方法，通过音位转换率（PSR）量化嵌入向量对口音规则的影响程度，提升口音合成的可控性与真实性。


<details>
  <summary>Details</summary>
Motivation: 当前TTS系统通过说话人嵌入控制口音时，由于嵌入同时编码音色、情感等多重特征，导致口音控制的可解释性和可控性有限。需要更清晰的口音控制机制。

Method: 以美式与英式英语为案例，实现flapping、rhoticity、元音对应等语音学规则。提出音位转换率（PSR）度量嵌入向量对规则转换的保留或覆盖程度。结合规则与嵌入进行口音合成实验。

Result: 实验表明：结合规则与嵌入能生成更真实的口音；嵌入向量可能减弱或覆盖规则转换，揭示了口音与说话人身份特征的纠缠。规则可作为口音控制的杠杆和评估语音生成解纠缠的框架。

Conclusion: 语音学规则为TTS口音控制提供了可解释的机制，PSR指标有助于评估和改善口音与说话人特征的解纠缠，推动更灵活可控的语音合成系统发展。

Abstract: Many spoken languages, including English, exhibit wide variation in dialects and accents, making accent control an important capability for flexible text-to-speech (TTS) models. Current TTS systems typically generate accented speech by conditioning on speaker embeddings associated with specific accents. While effective, this approach offers limited interpretability and controllability, as embeddings also encode traits such as timbre and emotion. In this study, we analyze the interaction between speaker embeddings and linguistically motivated phonological rules in accented speech synthesis. Using American and British English as a case study, we implement rules for flapping, rhoticity, and vowel correspondences. We propose the phoneme shift rate (PSR), a novel metric quantifying how strongly embeddings preserve or override rule-based transformations. Experiments show that combining rules with embeddings yields more authentic accents, while embeddings can attenuate or overwrite rules, revealing entanglement between accent and speaker identity. Our findings highlight rules as a lever for accent control and a framework for evaluating disentanglement in speech generation.

</details>


### [9] [Large Language Models for Large-Scale, Rigorous Qualitative Analysis in Applied Health Services Research](https://arxiv.org/abs/2601.14478)
*Sasha Ronaghi,Emma-Louise Aveling,Maria Levis,Rachel Lauren Ross,Emily Alsentzer,Sara Singer*

Main category: cs.CL

TL;DR: 本文提出了一种模型和任务无关的框架，用于设计人机协作的定性分析方法，并在多中心糖尿病护理研究中成功应用，证明LLM能提升健康服务研究的效率同时保持严谨性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在提高大规模多中心健康服务研究的定性分析效率方面显示出潜力，但目前缺乏将LLM整合到定性分析的方法学指导，以及其对现实世界研究方法和结果影响的证据。

Method: 开发了一个模型和任务无关的框架，用于设计人机协作的定性分析方法。在联邦合格健康中心的糖尿病护理多中心研究中，应用该框架实现了两种人机协作方法：(1) 对研究者生成的摘要进行定性综合以产生比较反馈报告；(2) 对167份访谈记录进行演绎编码以改进实践转型干预。

Result: LLM协助实现了对实践者的及时反馈，并整合了大规模定性数据以指导理论和实践变革。LLM帮助在保持严谨性的同时提升了研究效率。

Conclusion: 这项工作展示了如何将LLM整合到应用健康服务研究中，在保持严谨性的同时提高效率，为在定性研究中继续创新使用LLM提供了指导。

Abstract: Large language models (LLMs) show promise for improving the efficiency of qualitative analysis in large, multi-site health-services research. Yet methodological guidance for LLM integration into qualitative analysis and evidence of their impact on real-world research methods and outcomes remain limited. We developed a model- and task-agnostic framework for designing human-LLM qualitative analysis methods to support diverse analytic aims. Within a multi-site study of diabetes care at Federally Qualified Health Centers (FQHCs), we leveraged the framework to implement human-LLM methods for (1) qualitative synthesis of researcher-generated summaries to produce comparative feedback reports and (2) deductive coding of 167 interview transcripts to refine a practice-transformation intervention. LLM assistance enabled timely feedback to practitioners and the incorporation of large-scale qualitative data to inform theory and practice changes. This work demonstrates how LLMs can be integrated into applied health-services research to enhance efficiency while preserving rigor, offering guidance for continued innovation with LLMs in qualitative research.

</details>


### [10] [Can LLM Reasoning Be Trusted? A Comparative Study: Using Human Benchmarking on Statistical Tasks](https://arxiv.org/abs/2601.14479)
*Crish Nagarkar,Leonid Bogachev,Serge Sharoff*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLM）解决统计任务的能力及其评估推理质量的能力。通过微调开源LLM，它们在统计任务上达到了统计学学生的水平，且LLM自身能成为比传统指标更好的答案质量评估者。


<details>
  <summary>Details</summary>
Motivation: 尽管最先进的LLM在许多NLP任务上表现出色，但它们在解决中等复杂度的统计挑战方面的能力尚未得到充分理解。需要探索LLM在统计推理方面的潜力及其在教育技术和统计分析辅助系统中的应用。

Method: 作者微调了选定的开源LLM，使用了专门开发的统计数据集来增强其统计推理能力。将微调后的模型性能与人类评分基准进行比较，并评估LLM作为答案质量评估者的能力。

Result: 微调后的模型在高级统计任务上达到了与统计学学生相当的水平，性能提升与模型架构相关。更重要的是，LLM在评估答案质量（包括解释和推理评估）方面远优于传统指标如BLEU或BertScore。

Conclusion: LLM在统计推理方面具有显著潜力，可通过微调达到专业水平。其自我评估能力可实现统计教育平台的规模化自动评估，并可用于学术和工业环境中的研究方法验证以及数据分析工作流程的质量控制。

Abstract: This paper investigates the ability of large language models (LLMs) to solve statistical tasks, as well as their capacity to assess the quality of reasoning. While state-of-the-art LLMs have demonstrated remarkable performance in a range of NLP tasks, their competence in addressing even moderately complex statistical challenges is not well understood. We have fine-tuned selected open-source LLMs on a specially developed dataset to enhance their statistical reasoning capabilities, and compared their performance with the human scores used as a benchmark. Our results show that the fine-tuned models achieve better performance on advanced statistical tasks on the level comparable to a statistics student. Fine-tuning demonstrates architecture-dependent improvements, with some models showing significant performance gains, indicating clear potential for deployment in educational technology and statistical analysis assistance systems. We also show that LLMs themselves can be far better judges of the answers quality (including explanation and reasoning assessment) in comparison to traditional metrics, such as BLEU or BertScore. This self-evaluation capability enables scalable automated assessment for statistical education platforms and quality assurance in automated analysis tools. Potential applications also include validation tools for research methodology in academic and industry settings, and quality control mechanisms for data analysis workflows.

</details>


### [11] [Business Logic-Driven Text-to-SQL Data Synthesis for Business Intelligence](https://arxiv.org/abs/2601.14518)
*Jinhui Liu,Ximeng Zhang,Yanbo Ai,Zhou Yu*

Main category: cs.CL

TL;DR: 提出基于业务逻辑驱动的数据合成框架，用于生成具有业务真实性的Text-to-SQL评估数据，显著提升业务真实度并揭示现有模型在复杂业务查询上的性能差距


<details>
  <summary>Details</summary>
Motivation: 在私有商业智能环境中评估Text-to-SQL代理面临挑战，主要因为缺乏真实、领域特定的数据。现有的合成数据生成方法无法捕捉业务真实性，即问题是否反映真实的业务逻辑和工作流程。

Method: 提出业务逻辑驱动的数据合成框架，基于业务角色、工作场景和工作流程生成数据。采用业务推理复杂度控制策略，多样化回答问题所需的分析推理步骤，从而提高数据质量。

Result: 在Salesforce生产规模数据库上的实验表明，合成数据实现了98.44%的高业务真实性，显著优于OmniSQL(+19.5%)和SQL-Factory(+54.7%)，同时保持98.59%的问题-SQL对齐度。合成数据还揭示最先进的Text-to-SQL模型在最复杂的业务查询上仅达到42.86%的执行准确率。

Conclusion: 业务逻辑驱动的数据合成框架能够生成高质量的、具有业务真实性的Text-to-SQL评估数据，有助于更准确地评估模型性能，并揭示了现有模型在复杂业务场景中的局限性。

Abstract: Evaluating Text-to-SQL agents in private business intelligence (BI) settings is challenging due to the scarcity of realistic, domain-specific data. While synthetic evaluation data offers a scalable solution, existing generation methods fail to capture business realism--whether questions reflect realistic business logic and workflows. We propose a Business Logic-Driven Data Synthesis framework that generates data grounded in business personas, work scenarios, and workflows. In addition, we improve the data quality by imposing a business reasoning complexity control strategy that diversifies the analytical reasoning steps required to answer the questions. Experiments on a production-scale Salesforce database show that our synthesized data achieves high business realism (98.44%), substantially outperforming OmniSQL (+19.5%) and SQL-Factory (+54.7%), while maintaining strong question-SQL alignment (98.59%). Our synthetic data also reveals that state-of-the-art Text-to-SQL models still have significant performance gaps, achieving only 42.86% execution accuracy on the most complex business queries.

</details>


### [12] [Towards Execution-Grounded Automated AI Research](https://arxiv.org/abs/2601.14525)
*Chenglei Si,Zitong Yang,Yejin Choi,Emmanuel Candès,Diyi Yang,Tatsunori Hashimoto*

Main category: cs.CL

TL;DR: 本文研究了自动化AI研究的可行性，通过构建自动化执行器验证LLM生成的想法，并在LLM预训练和微调两个实际问题上展示了执行引导的进化搜索比强化学习更有效。


<details>
  <summary>Details</summary>
Motivation: 自动化AI研究有加速科学发现的潜力，但当前LLM经常生成看似合理但无效的想法。需要探索自动化执行是否可行，以及LLM是否能从执行反馈中学习。

Method: 1. 构建自动化执行器来实施想法并启动大规模并行GPU实验验证效果
2. 将LLM预训练和微调两个实际问题转化为执行环境
3. 分析两种从执行反馈中学习的方法：进化搜索和强化学习

Result: 1. 自动化执行器能够实施大部分前沿LLM生成的想法
2. 执行引导的进化搜索样本效率高：在10个搜索周期内，微调任务找到的方法显著优于GRPO基线（69.4% vs 48.0%），预训练配方优于nanoGPT基线（19.7分钟 vs 35.9分钟）
3. 前沿LLM在搜索中常生成有意义的算法想法，但容易早期饱和，偶尔呈现扩展趋势
4. 基于执行奖励的强化学习存在模式崩溃问题，能提高生成模型平均奖励但无法提升上限

Conclusion: 执行引导的自动化AI研究是可行的，进化搜索比强化学习更有效。虽然前沿LLM能生成有意义的想法，但需要更好的学习机制来突破早期饱和。本文为未来执行基础自动化AI研究提供了详细分析和基础。

Abstract: Automated AI research holds great potential to accelerate scientific discovery. However, current LLMs often generate plausible-looking but ineffective ideas. Execution grounding may help, but it is unclear whether automated execution is feasible and whether LLMs can learn from the execution feedback. To investigate these, we first build an automated executor to implement ideas and launch large-scale parallel GPU experiments to verify their effectiveness. We then convert two realistic research problems - LLM pre-training and post-training - into execution environments and demonstrate that our automated executor can implement a large fraction of the ideas sampled from frontier LLMs. We analyze two methods to learn from the execution feedback: evolutionary search and reinforcement learning. Execution-guided evolutionary search is sample-efficient: it finds a method that significantly outperforms the GRPO baseline (69.4% vs 48.0%) on post-training, and finds a pre-training recipe that outperforms the nanoGPT baseline (19.7 minutes vs 35.9 minutes) on pre-training, all within just ten search epochs. Frontier LLMs often generate meaningful algorithmic ideas during search, but they tend to saturate early and only occasionally exhibit scaling trends. Reinforcement learning from execution reward, on the other hand, suffers from mode collapse. It successfully improves the average reward of the ideator model but not the upper-bound, due to models converging on simple ideas. We thoroughly analyze the executed ideas and training dynamics to facilitate future efforts towards execution-grounded automated AI research.

</details>


### [13] [Self-Blinding and Counterfactual Self-Simulation Mitigate Biases and Sycophancy in Large Language Models](https://arxiv.org/abs/2601.14553)
*Brian Christian,Matan Mazor*

Main category: cs.CL

TL;DR: LLMs存在类似人类的认知偏差问题，无法有效模拟反事实决策来消除性别和种族偏见，但通过访问自身API的"盲化"版本可以获得更公平的决策。


<details>
  <summary>Details</summary>
Motivation: 人类在进行公平决策时需要忽略无关的偏见信息，但反事实自我模拟对认知来说非常困难。研究探索LLMs是否也存在类似限制，以及如何利用其独特能力解决这一问题。

Method: 研究LLMs在反事实知识条件下的决策能力，测试提示模型忽略偏见信息的效果，并创新性地让模型访问自身API的"盲化"版本作为反事实认知的基准模型。

Result: 提示LLMs忽略或假装不知道偏见信息无法有效消除偏见，有时甚至适得其反。但让模型访问自身API的盲化版本能实现更公平的决策，同时提高区分隐性偏见和有意偏见的透明度。

Conclusion: LLMs在反事实推理方面存在与人类相似的认知限制，但通过利用其可访问自身决策过程的独特优势，可以开发出更公平、透明的决策系统。

Abstract: Fair decisions require ignoring irrelevant, potentially biasing, information. To achieve this, decision-makers need to approximate what decision they would have made had they not known certain facts, such as the gender or race of a job candidate. This counterfactual self-simulation is notoriously hard for humans, leading to biased judgments even by well-meaning actors. Here we show that large language models (LLMs) suffer from similar limitations in their ability to approximate what decisions they would make under counterfactual knowledge in offsetting gender and race biases and overcoming sycophancy. We show that prompting models to ignore or pretend not to know biasing information fails to offset these biases and occasionally backfires. However, unlike humans, LLMs can be given access to a ground-truth model of their own counterfactual cognition -- their own API. We show that this access to the responses of a blinded replica enables fairer decisions, while providing greater transparency to distinguish implicit from intentionally biased behavior.

</details>


### [14] [Supporting Humans in Evaluating AI Summaries of Legal Depositions](https://arxiv.org/abs/2601.15182)
*Naghmeh Farzi,Laura Dietz,Dave D. Lewis*

Main category: cs.CL

TL;DR: 本文探索了在法律领域中，基于事实要点的方法如何直接帮助最终用户评估和改进自动生成的摘要。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型越来越多地用于总结长文档，但在法律领域，特别是证词摘要的事实准确性至关重要。虽然基于事实要点的方法已被证明对自动评估摘要方法非常有帮助，但其支持最终用户的潜力尚未得到充分探索。

Method: 作者将基于事实要点的方法从评估端转换到用户端，开发了一个原型系统，利用事实要点方法在法律领域的两个具体场景中支持法律专业人士：1）判断两个摘要中哪个更好；2）手动改进自动生成的摘要。

Result: 提出了一个原型系统，展示了基于事实要点的方法如何直接支持法律专业人士的摘要评估和改进工作。

Conclusion: 基于事实要点的方法不仅适用于自动评估，还能直接帮助最终用户在法律领域进行摘要质量判断和改进，为法律专业人士提供了实用的工具支持。

Abstract: While large language models (LLMs) are increasingly used to summarize long documents, this trend poses significant challenges in the legal domain, where the factual accuracy of deposition summaries is crucial. Nugget-based methods have been shown to be extremely helpful for the automated evaluation of summarization approaches. In this work, we translate these methods to the user side and explore how nuggets could directly assist end users. Although prior systems have demonstrated the promise of nugget-based evaluation, its potential to support end users remains underexplored. Focusing on the legal domain, we present a prototype that leverages a factual nugget-based approach to support legal professionals in two concrete scenarios: (1) determining which of two summaries is better, and (2) manually improving an automatically generated summary.

</details>


### [15] [Rewarding How Models Think Pedagogically: Integrating Pedagogical Reasoning and Thinking Rewards for LLMs in Education](https://arxiv.org/abs/2601.14560)
*Unggi Lee,Jiyeong Bae,Jaehyeon Park,Haeun Park,Taejun Park,Younghoon Jeon,Sungmin Cho,Junbo Koh,Yeil Jeong,Gyeonggeon Lee*

Main category: cs.CL

TL;DR: 本文提出PedagogicalRL-Thinking框架，通过教学推理提示和思考奖励机制，优化LLM在教学场景中的内部推理过程，提升教学效果。


<details>
  <summary>Details</summary>
Motivation: 目前LLM作为智能辅导系统的部署日益增多，但针对教育场景优化的研究有限。现有强化学习方法仅优化可见回答，忽视了模型的内部思考过程。

Method: 提出PedagogicalRL-Thinking框架，包含两个创新方法：1) 教学推理提示：基于特定领域教育理论而非通用指令引导内部推理；2) 思考奖励：明确评估并强化模型推理轨迹的教学质量。

Result: 实验表明：基于领域特定理论的提示优于通用提示；思考奖励与教学提示结合效果最佳；仅在数学辅导对话上训练的模型在未见过教育基准测试上表现提升，同时保留基础模型的事实知识。

Conclusion: 教学思考奖励能系统性地改变推理轨迹，增加教学推理并提升辅导思考过程中的结构化教学决策能力，为优化教育场景的LLM提供了有效框架。

Abstract: Large language models (LLMs) are increasingly deployed as intelligent tutoring systems, yet research on optimizing LLMs specifically for educational contexts remains limited. Recent works have proposed reinforcement learning approaches for training LLM tutors, but these methods focus solely on optimizing visible responses while neglecting the model's internal thinking process. We introduce PedagogicalRL-Thinking, a framework that extends pedagogical alignment to reasoning LLMs in education through two novel approaches: (1) Pedagogical Reasoning Prompting, which guides internal reasoning using domain-specific educational theory rather than generic instructions; and (2) Thinking Reward, which explicitly evaluates and reinforces the pedagogical quality of the model's reasoning traces. Our experiments reveal that domain-specific, theory-grounded prompting outperforms generic prompting, and that Thinking Reward is most effective when combined with pedagogical prompting. Furthermore, models trained only on mathematics tutoring dialogues show improved performance on educational benchmarks not seen during training, while preserving the base model's factual knowledge. Our quantitative and qualitative analyses reveal that pedagogical thinking reward produces systematic reasoning trace changes, with increased pedagogical reasoning and more structured instructional decision-making in the tutor's thinking process.

</details>


### [16] [Social Caption: Evaluating Social Understanding in Multimodal Models](https://arxiv.org/abs/2601.14569)
*Bhaavanaa Thumu,Leena Mathur,Youssouf Kebe,Louis-Philippe Morency*

Main category: cs.CL

TL;DR: Social Caption框架基于互动理论评估多模态大语言模型的社会理解能力，包括社会推理、整体社会分析和定向社会分析三个维度，并分析了模型规模、架构设计和口语上下文等因素对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在解释人类社交互动方面的社会理解能力至关重要，但缺乏系统性的评估框架。需要从互动理论角度全面评估模型的社会理解能力，并分析影响模型性能的关键因素。

Method: 基于互动理论提出Social Caption评估框架，包含三个维度：社会推理（准确推断互动）、整体社会分析（生成全面描述）、定向社会分析（提取相关社交信息）。通过MLLM评委实验分析模型规模、架构设计和口语上下文等因素的影响。

Result: 建立了系统性的社会理解能力评估框架，分析了不同因素对模型性能的影响。实验结果为多模态社会理解的自动化评估提供了扩展性见解。

Conclusion: Social Caption框架为评估多模态大语言模型的社会理解能力提供了理论基础和实证方法，有助于推动模型在社交互动理解方面的进步，并为自动化评估提供了重要参考。

Abstract: Social understanding abilities are crucial for multimodal large language models (MLLMs) to interpret human social interactions. We introduce Social Caption, a framework grounded in interaction theory to evaluate social understanding abilities of MLLMs along three dimensions: Social Inference (SI), the ability to make accurate inferences about interactions; Holistic Social Analysis (HSA), the ability to generate comprehensive descriptions of interactions; Directed Social Analysis (DSA), the ability to extract relevant social information from interactions. We analyze factors influencing model performance in social understanding, such as scale, architectural design, and spoken context. Experiments with MLLM judges contribute insights about scaling automated evaluation of multimodal social understanding.

</details>


### [17] [SearchGym: Bootstrapping Real-World Search Agents via Cost-Effective and High-Fidelity Environment Simulation](https://arxiv.org/abs/2601.14615)
*Xichen Zhang,Ziyi He,Yinghao Zhu,Sitong Wu,Shaozuo Yu,Meng Chu,Wenhu Zhang,Haoru Tan,Jiaya Jia*

Main category: cs.CL

TL;DR: SearchGym是一个用于训练搜索代理的模拟环境，通过生成可验证知识图谱和对齐文档语料库解决RL训练中的成本与数据对齐问题，实现有效的Sim-to-Real泛化。


<details>
  <summary>Details</summary>
Motivation: 当前搜索代理的强化学习训练面临两难困境：使用实时商业Web API成本过高，而依赖静态数据快照则会产生数据不对齐问题，导致奖励信号被破坏，从而影响训练稳定性。

Method: 提出SearchGym模拟环境，采用严格的生成流程构建可验证知识图谱和对齐文档语料库；在此基础上提出SearchGym-RL课程学习方法，通过净化反馈逐步优化代理策略。

Result: 在Llama和Qwen系列模型上的广泛实验展示了强大的Sim-to-Real泛化能力。在SearchGym中训练的Qwen2.5-7B-Base模型在九个不同基准测试中平均相对超越ASearcher基线10.6%。

Conclusion: 高保真模拟是一种可扩展且极具成本效益的方法，能够开发出强大的搜索代理，验证了模拟环境在解决数据对齐和成本问题上的有效性。

Abstract: Search agents have emerged as a pivotal paradigm for solving open-ended, knowledge-intensive reasoning tasks. However, training these agents via Reinforcement Learning (RL) faces a critical dilemma: interacting with live commercial Web APIs is prohibitively expensive, while relying on static data snapshots often introduces noise due to data misalignment. This misalignment generates corrupted reward signals that destabilize training by penalizing correct reasoning or rewarding hallucination. To address this, we propose SearchGym, a simulation environment designed to bootstrap robust search agents. SearchGym employs a rigorous generative pipeline to construct a verifiable knowledge graph and an aligned document corpus, ensuring that every reasoning task is factually grounded and strictly solvable. Building on this controllable environment, we introduce SearchGym-RL, a curriculum learning methodology that progressively optimizes agent policies through purified feedback, evolving from basic interactions to complex, long-horizon planning. Extensive experiments across the Llama and Qwen families demonstrate strong Sim-to-Real generalization. Notably, our Qwen2.5-7B-Base model trained within SearchGym surpasses the web-enhanced ASearcher baseline across nine diverse benchmarks by an average relative margin of 10.6%. Our results validate that high-fidelity simulation serves as a scalable and highly cost-effective methodology for developing capable search agents.

</details>


### [18] [Say Anything but This: When Tokenizer Betrays Reasoning in LLMs](https://arxiv.org/abs/2601.14658)
*Navid Ayoobi,Marcus I Armstrong,Arjun Mukherjee*

Main category: cs.CL

TL;DR: 研究发现LLM推理失败的一个重要原因在于分词器的一对多映射问题，导致模型在语义相同的文本上产生不同的内部表示，造成"幻象编辑"等推理缺陷。


<details>
  <summary>Details</summary>
Motivation: 现代LLM基于离散token序列进行推理，但子词分词器经常产生非唯一编码：多个token序列可以解token化为相同的表面字符串。这种表示不匹配造成未测量的脆弱性，使得推理过程可能失败。LLM可能将两个内部表示视为不同的"词语"，尽管它们在文本层面语义相同。

Method: 引入tokenization一致性探测任务：要求模型在上下文中替换指定目标词，同时保持其他内容不变。这个任务在表面层面故意设计得很简单，以便将失败归因于分词器-解token器的人工制品而非知识缺口或参数限制。通过对11000多个替换试验分析先进开源LLM，识别tokenizer引起的系统性缺陷。

Result: 发现模型输出中存在非平凡比率的"幻象编辑"：模型在正确推理的幻觉下操作，这是分词器引起的表示缺陷现象。进一步分析并提供了八种系统性分词器人工制品的分类法，包括空格边界偏移和词内重新分割等。

Conclusion: 部分明显的推理缺陷起源于分词器层，这要求在训练更大模型之前考虑分词器层面的修复措施，而不是简单地增加模型规模和训练数据。

Abstract: Large language models (LLMs) reason over discrete token ID sequences, yet modern subword tokenizers routinely produce non-unique encodings: multiple token ID sequences can detokenize to identical surface strings. This representational mismatch creates an unmeasured fragility wherein reasoning processes can fail. LLMs may treat two internal representations as distinct "words" even when they are semantically identical at the text level. In this work, we show that tokenization can betray LLM reasoning through one-to-many token ID mappings. We introduce a tokenization-consistency probe that requires models to replace designated target words in context while leaving all other content unchanged. The task is intentionally simple at the surface level, enabling us to attribute failures to tokenizer-detokenizer artifacts rather than to knowledge gaps or parameter limitations. Through analysis of over 11000 replacement trials across state-of-the-art open-source LLMs, we find a non-trivial rate of outputs exhibit phantom edits: cases where models operate under the illusion of correct reasoning, a phenomenon arising from tokenizer-induced representational defects. We further analyze these cases and provide a taxonomy of eight systematic tokenizer artifacts, including whitespace-boundary shifts and intra-word resegmentation. These findings indicate that part of apparent reasoning deficiency originates in the tokenizer layer, motivating tokenizer-level remedies before incurring the cost of training ever-larger models on ever-larger corpora.

</details>


### [19] [AdaTIR: Adaptive Tool-Integrated Reasoning via Difficulty-Aware Policy Optimization](https://arxiv.org/abs/2601.14696)
*Zhaiyu Fang,Ruipeng Sun*

Main category: cs.CL

TL;DR: AdaTIR框架通过难度感知推理内部化，动态调整工具使用预算，显著减少工具调用次数，同时保持或提升任务准确性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理存在认知卸载问题，即使简单任务也过度依赖外部工具调用。真正的智能代理需要自适应判断何时使用工具，而非静态调用。

Method: 提出AdaTIR框架，引入难度感知效率奖励，根据任务复杂度动态调整工具预算。同时提出Clipped Advantage Shaping解决工具惩罚与正确性奖励的符号反转问题。

Result: 在简单任务上减少97.6%工具调用，复杂任务减少28.2%工具调用，同时保持或提升准确性。在AIME 2024上即使禁用工具访问，仍比基线提升4.8%。

Conclusion: AdaTIR通过难度感知推理内部化成功实现了工具使用的自适应决策，在减少工具依赖的同时保持任务性能，展现了真正的智能代理能力。

Abstract: Tool-Integrated Reasoning (TIR) has significantly enhanced the capabilities of Large Language Models (LLMs), yet current agents tend to exhibit cognitive offloading, redundantly invoking external tools even for simple tasks. In this paper, we suggest that true agentic intelligence requires not just tool invocation, but the adaptive wisdom to discern when to use them. We propose AdaTIR, a framework that shifts the paradigm from static tool invocation to difficulty-aware reasoning internalization. By introducing a difficulty-aware efficiency reward, AdaTIR dynamically adjusts tool budgets based on task complexity--internalizing reasoning for simple tasks while selectively invoking tools for complex tasks. Furthermore, we identify a sign reversal problem where tool penalties outweigh correctness rewards, mistakenly penalizing correct rollouts with negative advantages. To resolve this, we propose Clipped Advantage Shaping (CAS), which ensures that correctness remains the primary objective while using efficiency as a secondary constraint. Empirical results demonstrate that AdaTIR reduces tool calls by up to 97.6% on simple tasks and 28.2% on complex challenges while maintaining or enhancing accuracy. Notably, AdaTIR successfully internalizes reasoning, outperforming baselines by 4.8% on AIME 2024 even when tool access is strictly disabled.

</details>


### [20] [ClaimDB: A Fact Verification Benchmark over Large Structured Data](https://arxiv.org/abs/2601.14698)
*Michael Theologitis,Preetam Prabhu Srikar Dammu,Chirag Shah,Dan Suciu*

Main category: cs.CL

TL;DR: ClaimDB：首个基于大规模结构化数据的事实核查基准，包含80个真实数据库，验证方法从"阅读"转向可执行程序推理，现有LLM在该基准上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 当前事实核查基准主要关注文本证据，而基于大规模结构化数据（数百万条记录、多表组合）的声明验证研究不足。需要构建能够处理真实世界数据库复杂性的评估框架。

Method: 构建ClaimDB基准，包含80个独特真实数据库，涵盖治理、医疗、媒体、教育、自然科学等领域。声明证据来自数百万条记录和多表组合。采用可执行程序推理方法而非传统阅读式验证。

Result: 测试30个最先进的专有和开源LLM（参数量低于700亿），无一模型准确率超过83%，超过一半模型准确率低于55%。封闭和开源模型在弃权能力（承认无足够证据）上都表现不佳。

Conclusion: ClaimDB揭示了当前LLM在处理大规模结构化数据事实核查方面的局限性，特别是在弃权能力上的不足，对其在高风险数据分析中的可靠性提出质疑。该基准推动了从阅读式验证向程序推理的转变。

Abstract: Despite substantial progress in fact-verification benchmarks, claims grounded in large-scale structured data remain underexplored. In this work, we introduce ClaimDB, the first fact-verification benchmark where the evidence for claims is derived from compositions of millions of records and multiple tables. ClaimDB consists of 80 unique real-life databases covering a wide range of domains, from governance and healthcare to media, education and the natural sciences. At this scale, verification approaches that rely on "reading" the evidence break down, forcing a timely shift toward reasoning in executable programs. We conduct extensive experiments with 30 state-of-the-art proprietary and open-source (below 70B) LLMs and find that none exceed 83% accuracy, with more than half below 55%. Our analysis also reveals that both closed- and open-source models struggle with abstention -- the ability to admit that there is no evidence to decide -- raising doubts about their reliability in high-stakes data analysis. We release the benchmark, code, and the LLM leaderboard at https://claimdb.github.io .

</details>


### [21] [DARL: Encouraging Diverse Answers for General Reasoning without Verifiers](https://arxiv.org/abs/2601.14700)
*Chongxuan Huang,Lei Lin,Xiaodong Shi,Wenping Hu,Ruiming Tang*

Main category: cs.CL

TL;DR: DARL是一个简单有效的强化学习框架，通过在可控偏离范围内鼓励生成多样答案，同时保持与参考答案的对齐，解决了现有方法过度拟合参考答案导致输出多样性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR方法依赖领域特定验证器，限制了在开放通用领域的应用。RLPR等扩展方法虽然能在通用领域训练，但容易过度拟合参考答案，限制了模型生成多样输出的能力，这在写作等开放式任务中尤为明显。

Method: 提出了DARL框架，鼓励在可控偏离范围内生成多样答案，同时保持与参考答案的对齐。该框架完全兼容现有的通用强化学习方法，无需额外验证器即可无缝集成。

Result: 在13个基准测试中表现出一致的推理性能改进。DARL超越了RLPR，在6个推理基准上平均提升1.3分，在7个通用基准上平均提升9.5分，显著提高了推理准确性和输出多样性。

Conclusion: DARL有效解决了现有强化学习方法过度拟合参考答案的问题，通过鼓励受控多样性提高了模型的推理能力和输出质量，在开放通用领域具有更好的适用性。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated promising gains in enhancing the reasoning capabilities of large language models. However, its dependence on domain-specific verifiers significantly restricts its applicability to open and general domains. Recent efforts such as RLPR have extended RLVR to general domains, enabling training on broader datasets and achieving improvements over RLVR. However, a notable limitation of these methods is their tendency to overfit to reference answers, which constrains the model's ability to generate diverse outputs. This limitation is particularly pronounced in open-ended tasks such as writing, where multiple plausible answers exist. To address this, we propose DARL, a simple yet effective reinforcement learning framework that encourages the generation of diverse answers within a controlled deviation range from the reference while preserving alignment with it. Our framework is fully compatible with existing general reinforcement learning methods and can be seamlessly integrated without additional verifiers. Extensive experiments on thirteen benchmarks demonstrate consistent improvements in reasoning performance. Notably, DARL surpasses RLPR, achieving average gains of 1.3 points on six reasoning benchmarks and 9.5 points on seven general benchmarks, highlighting its effectiveness in improving both reasoning accuracy and output diversity.

</details>


### [22] [Typhoon OCR: Open Vision-Language Model For Thai Document Extraction](https://arxiv.org/abs/2601.14722)
*Surapon Nonesung,Natapong Nitarach,Teetouch Jaknamon,Pittawat Taveekitworachai,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: Typhoon OCR是一个专为泰语和英语文档提取设计的开源视觉语言模型，通过多阶段数据构建管道训练，能够实现文本转录、布局重建和文档结构一致性，性能接近或超越大型专有模型但计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型主要针对高资源语言，泰语由于文字复杂（非拉丁字母、无明确词边界）以及现实文档高度非结构化，现有开源模型效果有限，需要专门针对泰语文档提取的解决方案。

Method: 从视觉语言骨干模型微调，使用泰语重点训练数据集。数据集通过多阶段数据构建管道开发，结合传统OCR、基于VLM的重构和精心设计的合成数据。Typhoon OCR V1.5是紧凑且推理高效的模型，减少对元数据的依赖并简化部署。

Result: 在多种泰语文档类别（财务报告、政府表格、书籍、信息图、手写文档）的综合评估显示，Typhoon OCR达到或超越大型前沿专有模型的性能，同时计算成本显著更低。

Conclusion: 开源视觉语言OCR模型能够实现泰语文档的准确文本提取和布局重建，达到与专有系统相当的性能，同时保持轻量级和易于部署的特点。

Abstract: Document extraction is a core component of digital workflows, yet existing vision-language models (VLMs) predominantly favor high-resource languages. Thai presents additional challenges due to script complexity from non-latin letters, the absence of explicit word boundaries, and the prevalence of highly unstructured real-world documents, limiting the effectiveness of current open-source models. This paper presents Typhoon OCR, an open VLM for document extraction tailored for Thai and English. The model is fine-tuned from vision-language backbones using a Thai-focused training dataset. The dataset is developed using a multi-stage data construction pipeline that combines traditional OCR, VLM-based restructuring, and curated synthetic data. Typhoon OCR is a unified framework capable of text transcription, layout reconstruction, and document-level structural consistency. The latest iteration of our model, Typhoon OCR V1.5, is a compact and inference-efficient model designed to reduce reliance on metadata and simplify deployment. Comprehensive evaluations across diverse Thai document categories, including financial reports, government forms, books, infographics, and handwritten documents, show that Typhoon OCR achieves performance comparable to or exceeding larger frontier proprietary models, despite substantially lower computational cost. The results demonstrate that open vision-language OCR models can achieve accurate text extraction and layout reconstruction for Thai documents, reaching performance comparable to proprietary systems while remaining lightweight and deployable.

</details>


### [23] [Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning](https://arxiv.org/abs/2601.14750)
*Yifan Wang,Shiyu Li,Peiming Li,Xiaochen Yang,Yang Tang,Zheng Wei*

Main category: cs.CL

TL;DR: Render-of-Thought (RoT) 框架通过将文本推理步骤渲染为图像，实现推理链的可视化和压缩，在保持竞争力的同时达到3-4倍token压缩和推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有Chain-of-Thought (CoT)提示方法虽然能增强大语言模型的推理能力，但存在计算开销大、缺乏对中间推理过程的监督、推理链难以分析等问题。

Method: 提出Render-of-Thought (RoT)框架，将文本推理步骤渲染为图像，利用现有视觉语言模型(VLMs)的视觉编码器作为语义锚点，对齐视觉嵌入和文本空间，实现即插即用而无需额外预训练。

Result: 在数学和逻辑推理基准测试中，RoT相比显式CoT实现了3-4倍的token压缩和显著的推理加速，同时保持与其他方法竞争的推理性能。

Conclusion: RoT框架通过将推理链具象化为图像，使得潜在推理过程变得显式和可追溯，验证了这种推理范式在提高效率同时保持性能的可行性。

Abstract: Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at https://github.com/TencentBAC/RoT

</details>


### [24] [RECAP: Resistance Capture in Text-based Mental Health Counseling with Large Language Models](https://arxiv.org/abs/2601.14780)
*Anqi Li,Yuqian Chen,Yu Lu,Zhaoming Chen,Yuan Xie,Zhenzhong Lan*

Main category: cs.CL

TL;DR: 论文提出PsyFIRE框架和RECAP系统，用于在中文文本心理咨询中检测细粒度抵抗行为，效果优于现有方法并具有实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 现有NLP方法在检测文本心理咨询中的客户抵抗行为时存在三个主要问题：1）过度简化抵抗类别；2）忽略治疗干预的顺序动态；3）可解释性有限。这些问题限制了在实际咨询中的应用效果。

Method: 提出PsyFIRE理论框架，定义13种细粒度抵抗行为及协作互动；构建包含23,930个标注话语的ClientResistance语料库，每个标注都有上下文相关的理由；在此基础上开发RECAP两阶段框架，先检测抵抗行为，再分类细粒度抵抗类型并提供解释。

Result: RECAP在区分协作与抵抗方面达到91.25%的F1分数，在细粒度抵抗类别分类方面达到66.58%的宏平均F1分数，比基于提示的LLM基线方法高出20多个百分点。应用研究显示，抵抗行为在咨询中普遍存在，对治疗关系有负面影响，RECAP能提高咨询师的理解和干预策略。

Conclusion: PsyFIRE和RECAP框架有效解决了现有方法在检测心理咨询抵抗行为时的局限性，通过细粒度分类和解释性分析，为实际咨询实践提供了有价值的工具，有望改善治疗效果。

Abstract: Recognizing and navigating client resistance is critical for effective mental health counseling, yet detecting such behaviors is particularly challenging in text-based interactions. Existing NLP approaches oversimplify resistance categories, ignore the sequential dynamics of therapeutic interventions, and offer limited interpretability.
  To address these limitations, we propose PsyFIRE, a theoretically grounded framework capturing 13 fine-grained resistance behaviors alongside collaborative interactions. Based on PsyFIRE, we construct the ClientResistance corpus with 23,930 annotated utterances from real-world Chinese text-based counseling, each supported by context-specific rationales. Leveraging this dataset, we develop RECAP, a two-stage framework that detects resistance and fine-grained resistance types with explanations.
  RECAP achieves 91.25% F1 for distinguishing collaboration and resistance and 66.58% macro-F1 for fine-grained resistance categories classification, outperforming leading prompt-based LLM baselines by over 20 points. Applied to a separate counseling dataset and a pilot study with 62 counselors, RECAP reveals the prevalence of resistance, its negative impact on therapeutic relationships and demonstrates its potential to improve counselors' understanding and intervention strategies.

</details>


### [25] [Comparative Study of Large Language Models on Chinese Film Script Continuation: An Empirical Analysis Based on GPT-5.2 and Qwen-Max](https://arxiv.org/abs/2601.14826)
*Yuxuan Cao,Zida Yang,Ye Wang*

Main category: cs.CL

TL;DR: GPT-5.2在中文电影剧本续写任务中整体优于Qwen-Max，特别是在结构保持、整体质量和角色一致性方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地应用于创意写作，需要系统评估它们在文化特定叙事任务上的表现，特别是中文创意写作领域缺乏相关基准。

Method: 构建首个中文电影剧本续写基准（53部经典电影），采用"前半部分续写后半部分"范式，每个电影生成3个样本。使用多维评估框架：ROUGE-L、结构相似性和LLM-as-Judge（DeepSeek-Reasoner）评分，共获得303个有效样本进行统计分析。

Result: Qwen-Max在ROUGE-L得分上略高（0.2230 vs 0.2114），但GPT-5.2在结构保持（0.93 vs 0.75）、整体质量（44.79 vs 25.72）和综合得分（0.50 vs 0.39）上显著更优，整体质量效应值达到大效应水平（d>0.8）。GPT-5.2在角色一致性、风格匹配和格式保持方面表现更好。

Conclusion: GPT-5.2在中文创意写作任务中表现更优，特别是在叙事结构和质量方面。该研究为中文创意写作的LLM评估提供了可复现的框架，并揭示了不同模型在文化特定任务上的性能差异。

Abstract: As large language models (LLMs) are increasingly applied to creative writing, their performance on culturally specific narrative tasks warrants systematic investigation. This study constructs the first Chinese film script continuation benchmark comprising 53 classic films, and designs a multi-dimensional evaluation framework comparing GPT-5.2 and Qwen-Max-Latest. Using a "first half to second half" continuation paradigm with 3 samples per film, we obtained 303 valid samples (GPT-5.2: 157, 98.7% validity; Qwen-Max: 146, 91.8% validity). Evaluation integrates ROUGE-L, Structural Similarity, and LLM-as-Judge scoring (DeepSeek-Reasoner).
  Statistical analysis of 144 paired samples reveals: Qwen-Max achieves marginally higher ROUGE-L (0.2230 vs 0.2114, d=-0.43); however, GPT-5.2 significantly outperforms in structural preservation (0.93 vs 0.75, d=0.46), overall quality (44.79 vs 25.72, d=1.04), and composite scores (0.50 vs 0.39, d=0.84). The overall quality effect size reaches large effect level (d>0.8).
  GPT-5.2 excels in character consistency, tone-style matching, and format preservation, while Qwen-Max shows deficiencies in generation stability. This study provides a reproducible framework for LLM evaluation in Chinese creative writing.

</details>


### [26] [HiNS: Hierarchical Negative Sampling for More Comprehensive Memory Retrieval Embedding Model](https://arxiv.org/abs/2601.14857)
*Motong Tian,Allen P. Wong,Mingjun Mao,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 本文提出HiNS框架，通过分层建模负样本难度并基于对话数据确定负样本比例，显著提升了语言智能体的记忆检索能力。


<details>
  <summary>Details</summary>
Motivation: 现有训练数据构建方法忽视了负样本的层次难度及其在人类-智能体交互中的自然分布。实际应用中，一些负样本是语义相近的干扰项，而另一些则是明显不相关的，自然对话中这些类型的比例具有结构性。当前使用合成或均匀采样负样本的方法无法反映这种多样性，限制了嵌入模型学习细微区分的能力，而这对于稳健的记忆检索至关重要。

Method: 提出HiNS框架，该框架明确建模负样本的难度层级，并整合了从对话数据中得出的经验性负样本比例。这种方法使得训练出的嵌入模型在记忆密集型任务中具有显著提升的检索保真度和泛化能力。

Result: 实验结果显示显著改进：在LoCoMo数据集上，MemoryOS的F1/BLEU-1分别提升3.27%/3.30%，Mem0分别提升1.95%/1.78%；在PERSONAMEM数据集上，MemoryOS总分提升1.19%，Mem0总分提升2.55%。

Conclusion: HiNS框架通过更真实地反映负样本难度层次和自然分布，有效提升了语言智能体的记忆检索性能，为构建更稳健的记忆增强语言智能体提供了新思路。

Abstract: Memory-augmented language agents rely on embedding models for effective memory retrieval. However, existing training data construction overlooks a critical limitation: the hierarchical difficulty of negative samples and their natural distribution in human-agent interactions. In practice, some negatives are semantically close distractors while others are trivially irrelevant, and natural dialogue exhibits structured proportions of these types. Current approaches using synthetic or uniformly sampled negatives fail to reflect this diversity, limiting embedding models' ability to learn nuanced discrimination essential for robust memory retrieval. In this work, we propose a principled data construction framework HiNS that explicitly models negative sample difficulty tiers and incorporates empirically grounded negative ratios derived from conversational data, enabling the training of embedding models with substantially improved retrieval fidelity and generalization in memory-intensive tasks. Experiments show significant improvements: on LoCoMo, F1/BLEU-1 gains of 3.27%/3.30%(MemoryOS) and 1.95%/1.78% (Mem0); on PERSONAMEM, total score improvements of 1.19% (MemoryOS) and 2.55% (Mem0).

</details>


### [27] [Language-Coupled Reinforcement Learning for Multilingual Retrieval-Augmented Generation](https://arxiv.org/abs/2601.14896)
*Rui Qi,Fengran Mo,Yufeng Chen,Xue Zhang,Shuo Wang,Hongliang Li,Jinan Xu,Meng Jiang,Jian-Yun Nie,Kaiyu Huang*

Main category: cs.CL

TL;DR: LcRL是一个多语言搜索增强强化学习框架，通过语言耦合的组相对策略优化来解决多语言检索增强生成中的知识偏见和冲突问题。


<details>
  <summary>Details</summary>
Motivation: 现有多语言检索增强生成方法采用统一的单轮检索和优化过程，这种"一刀切"策略在多语言环境中效果不佳，模型在与搜索引擎交互时会出现知识偏见和冲突。

Method: 提出LcRL框架，整合语言耦合的组相对策略优化到策略和奖励模型中。采用语言耦合组采样减少知识偏见，在奖励模型中引入辅助反一致性惩罚缓解知识冲突。

Result: 实验结果表明LcRL不仅取得竞争性性能，而且适用于多种实际场景，如受限训练数据和包含大量语言的检索集合。

Conclusion: LcRL框架有效解决了多语言检索增强生成中的知识偏见和冲突问题，为多语言场景提供了更优化的解决方案。

Abstract: Multilingual retrieval-augmented generation (MRAG) requires models to effectively acquire and integrate beneficial external knowledge from multilingual collections. However, most existing studies employ a unitive process where queries of equivalent semantics across different languages are processed through a single-turn retrieval and subsequent optimization. Such a ``one-size-fits-all'' strategy is often suboptimal in multilingual settings, as the models occur to knowledge bias and conflict during the interaction with the search engine. To alleviate the issues, we propose LcRL, a multilingual search-augmented reinforcement learning framework that integrates a language-coupled Group Relative Policy Optimization into the policy and reward models. We adopt the language-coupled group sampling in the rollout module to reduce knowledge bias, and regularize an auxiliary anti-consistency penalty in the reward models to mitigate the knowledge conflict. Experimental results demonstrate that LcRL not only achieves competitive performance but is also appropriate for various practical scenarios such as constrained training data and retrieval over collections encompassing a large number of languages. Our code is available at https://github.com/Cherry-qwq/LcRL-Open.

</details>


### [28] [PodBench: A Comprehensive Benchmark for Instruction-Aware Audio-Oriented Podcast Script Generation](https://arxiv.org/abs/2601.14903)
*Chenning Xu,Mao Zheng,Mingyu Zheng,Mingyang Song*

Main category: cs.CL

TL;DR: PodBench是一个用于播客脚本生成的综合基准测试，包含800个样本，输入长度达21K tokens，具有复杂多说话人指令。评估框架结合定量约束和基于LLM的质量评估，发现开源模型在长上下文和多说话人协调方面表现更稳健，但指令遵循和内容质量之间存在差距。


<details>
  <summary>Details</summary>
Motivation: 播客脚本生成需要LLM从多样化输入中合成结构化、上下文接地的对话，但目前该任务缺乏系统性的评估资源。现有评估方法有限，无法全面衡量模型在长上下文、多说话人协调等复杂场景下的表现。

Method: 1. 构建PodBench基准：包含800个样本，输入长度达21K tokens，具有复杂多说话人指令
2. 提出多面评估框架：结合定量约束（如长度、格式要求）和基于LLM的质量评估
3. 进行广泛实验：比较专有模型和开源模型，特别关注开源模型配备显式推理能力的情况

Result: 1. 专有模型在总体上表现更优
2. 配备显式推理能力的开源模型在处理长上下文和多说话人协调方面比标准基线模型表现出更好的稳健性
3. 发现一个持续存在的分歧：高指令遵循率并不能保证高质量的内容实质

Conclusion: PodBench为长形式、音频中心生成任务提供了一个可复现的测试平台，能够系统评估模型在复杂播客脚本生成场景中的表现。研究发现指令遵循和内容质量之间的差距需要进一步研究，开源模型在特定复杂任务上具有潜力。

Abstract: Podcast script generation requires LLMs to synthesize structured, context-grounded dialogue from diverse inputs, yet systematic evaluation resources for this task remain limited. To bridge this gap, we introduce PodBench, a benchmark comprising 800 samples with inputs up to 21K tokens and complex multi-speaker instructions. We propose a multifaceted evaluation framework that integrates quantitative constraints with LLM-based quality assessment. Extensive experiments reveal that while proprietary models generally excel, open-source models equipped with explicit reasoning demonstrate superior robustness in handling long contexts and multi-speaker coordination compared to standard baselines. However, our analysis uncovers a persistent divergence where high instruction following does not guarantee high content substance. PodBench offers a reproducible testbed to address these challenges in long-form, audio-centric generation.

</details>


### [29] [CodeDelegator: Mitigating Context Pollution via Role Separation in Code-as-Action Agents](https://arxiv.org/abs/2601.14914)
*Tianxiang Fei,Cheng Chen,Yue Pan,Mao Zheng,Mingyang Song*

Main category: cs.CL

TL;DR: CodeDelegator：一个多智能体框架，通过角色专业化分离规划与实现，使用持久化Delegator进行战略监督，为每个子任务实例化新的Coder智能体，引入EPSS机制隔离执行状态，防止调试痕迹污染上下文。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型允许将动作表示为可执行代码，但现实任务需要战略规划和详细实现。单一智能体同时负责两者会导致调试痕迹和中间失败污染上下文，损害长时程性能。

Method: 提出CodeDelegator多智能体框架：1) 持久化Delegator负责任务分解、编写规范和监督进度但不执行代码；2) 为每个子任务实例化新的Coder智能体，提供干净的上下文；3) 引入EPSS机制隔离每个Coder的执行状态，同时保持全局一致性。

Result: 在多个基准测试上的实验证明了CodeDelegator在各种场景下的有效性。

Conclusion: 通过角色专业化分离规划与实现，结合EPSS机制隔离执行状态，CodeDelegator框架能够有效防止上下文污染，提升长时程任务性能。

Abstract: Recent advances in large language models (LLMs) allow agents to represent actions as executable code, offering greater expressivity than traditional tool-calling. However, real-world tasks often demand both strategic planning and detailed implementation. Using a single agent for both leads to context pollution from debugging traces and intermediate failures, impairing long-horizon performance. We propose CodeDelegator, a multi-agent framework that separates planning from implementation via role specialization. A persistent Delegator maintains strategic oversight by decomposing tasks, writing specifications, and monitoring progress without executing code. For each sub-task, a new Coder agent is instantiated with a clean context containing only its specification, shielding it from prior failures. To coordinate between agents, we introduce Ephemeral-Persistent State Separation (EPSS), which isolates each Coder's execution state while preserving global coherence, preventing debugging traces from polluting the Delegator's context. Experiments on various benchmarks demonstrate the effectiveness of CodeDelegator across diverse scenarios.

</details>


### [30] [The GDN-CC Dataset: Automatic Corpus Clarification for AI-enhanced Democratic Citizen Consultations](https://arxiv.org/abs/2601.14944)
*Pierre-Antoine Lequeu,Léo Labat,Laurène Cave,Gaël Lejeune,François Yvon,Benjamin Piwowarski*

Main category: cs.CL

TL;DR: 本文提出了一种名为Corpus Clarification的预处理框架，用于将大规模公民咨询数据转化为结构化、自包含的论证单元，并展示了小语言模型在此任务上的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在NLP中广泛应用，但将其用于分析民主活动文本（如在线讨论或公民咨询）存在伦理问题。研究旨在开发资源来标准化公民贡献，并探索小型开源LLMs在此任务上的可靠性。

Method: 引入Corpus Clarification预处理框架，创建了GDN-CC数据集（1,231个法国大辩论贡献，包含2,285个论证单元），通过微调小型语言模型来复现人工标注，并用于观点聚类任务。

Result: 微调后的小语言模型在复现标注任务上匹配或优于大型LLMs，并成功应用于观点聚类。研究还发布了GDN-CC-large数据集（24万自动标注的贡献），是迄今为止最大的标注民主咨询数据集。

Conclusion: 小型开源LLMs能够有效执行公民咨询数据的标准化预处理，这为透明、可本地运行的民主文本分析提供了可行方案，有助于推动民主活动文本的标准化分析。

Abstract: LLMs are ubiquitous in modern NLP, and while their applicability extends to texts produced for democratic activities such as online deliberations or large-scale citizen consultations, ethical questions have been raised for their usage as analysis tools. We continue this line of research with two main goals: (a) to develop resources that can help standardize citizen contributions in public forums at the pragmatic level, and make them easier to use in topic modeling and political analysis; (b) to study how well this standardization can reliably be performed by small, open-weights LLMs, i.e. models that can be run locally and transparently with limited resources. Accordingly, we introduce Corpus Clarification as a preprocessing framework for large-scale consultation data that transforms noisy, multi-topic contributions into structured, self-contained argumentative units ready for downstream analysis. We present GDN-CC, a manually-curated dataset of 1,231 contributions to the French Grand Débat National, comprising 2,285 argumentative units annotated for argumentative structure and manually clarified. We then show that finetuned Small Language Models match or outperform LLMs on reproducing these annotations, and measure their usability for an opinion clustering task. We finally release GDN-CC-large, an automatically annotated corpus of 240k contributions, the largest annotated democratic consultation dataset to date.

</details>


### [31] [CorpusQA: A 10 Million Token Benchmark for Corpus-Level Analysis and Reasoning](https://arxiv.org/abs/2601.14952)
*Zhiyuan Lu,Chenliang Li,Yingcheng Shi,Weizhou Shen,Ming Yan,Fei Huang*

Main category: cs.CL

TL;DR: 本文提出了CorpusQA基准测试，用于评估LLM在千万token级别文档库中的全局推理能力，发现现有方法在长上下文推理中表现不佳，需要新的架构支持。


<details>
  <summary>Details</summary>
Motivation: 现有LLM虽然能处理百万token上下文，但在整个文档库级别的推理能力尚未得到充分测试。现有基准测试要么局限于单个长文本，要么基于"稀疏检索"假设（答案来自少数相关片段），这种假设不适用于真正的语料库级分析，因为证据分散在数百个文档中，答案需要全局整合、比较和统计聚合。

Method: 通过新的数据合成框架创建CorpusQA基准测试，规模达1000万token。该框架将推理与文本表示解耦，生成复杂、计算密集的查询，并保证程序化生成的地面真实答案。还展示了该框架在评估之外的实用性，表明在合成数据上微调能有效增强LLM的一般长上下文推理能力。

Result: 实验表明，即使最先进的长上下文LLM随着输入长度增加也表现不佳，标准的检索增强生成系统完全失效。发现基于记忆增强的代理架构提供了更稳健的替代方案。

Conclusion: 研究结果表明，需要从简单扩展上下文窗口转向开发用于全局信息合成的先进架构。记忆增强的代理架构是更可靠的选择，为长上下文推理提供了新的方向。

Abstract: While large language models now handle million-token contexts, their capacity for reasoning across entire document repositories remains largely untested. Existing benchmarks are inadequate, as they are mostly limited to single long texts or rely on a "sparse retrieval" assumption-that answers can be derived from a few relevant chunks. This assumption fails for true corpus-level analysis, where evidence is highly dispersed across hundreds of documents and answers require global integration, comparison, and statistical aggregation. To address this critical gap, we introduce CorpusQA, a new benchmark scaling up to 10 million tokens, generated via a novel data synthesis framework. By decoupling reasoning from textual representation, this framework creates complex, computation-intensive queries with programmatically guaranteed ground-truth answers, challenging systems to perform holistic reasoning over vast, unstructured text without relying on fallible human annotation. We further demonstrate the utility of our framework beyond evaluation, showing that fine-tuning on our synthesized data effectively enhances an LLM's general long-context reasoning capabilities. Extensive experiments reveal that even state-of-the-art long-context LLMs struggle as input length increases, and standard retrieval-augmented generation systems collapse entirely. Our findings indicate that memory-augmented agentic architectures offer a more robust alternative, suggesting a critical shift is needed from simply extending context windows to developing advanced architectures for global information synthesis.

</details>


### [32] [A Comprehensive Benchmark of Language Models on Unicode and Romanized Sinhala](https://arxiv.org/abs/2601.14958)
*Minuri Rajapakse,Ruvan Weerasinghe*

Main category: cs.CL

TL;DR: 现代语言模型在僧伽罗语（特别是罗马化僧伽罗语）上的性能评估，发现不同模型在不同文本格式上表现差异显著。


<details>
  <summary>Details</summary>
Motivation: 僧伽罗语作为低资源、形态丰富的语言，特别是其罗马化形式在数字通信中广泛使用，但语言模型在此类语言上的性能尚未得到充分探索。

Method: 使用多样化的Unicode和罗马化僧伽罗语语料库，通过困惑度评估开源模型，通过句子完成任务的定性分析评估领先的闭源模型。

Result: Mistral-Nemo-Base-2407在Unicode文本上表现最佳，Mistral-7B-v0.3在罗马化文本上最优；Llama-3.1-8B在两种文字上都表现良好；闭源模型中，Gemini-1.5-pro和DeepSeek擅长Unicode生成，Claude-3.5-Sonnet则更擅长处理罗马化文本。

Conclusion: 结果为僧伽罗语应用选择模型提供了重要指导，并强调了训练数据在处理文字变体中的关键作用。

Abstract: The performance of Language Models (LMs) on lower-resource, morphologically rich languages like Sinhala remains under-explored, particularly for Romanized Sinhala, which is prevalent in digital communication. This paper presents a comprehensive benchmark of modern LMs on a diverse corpus of Unicode and Romanized Sinhala. We evaluate open-source models using perplexity, a measure of how well a model predicts a text, and leading closed-source models via a qualitative analysis of sentence completion. Our findings reveal that the Mistral-Nemo-Base-2407 model achieves the strongest predictive performance on Unicode text and the Mistral-7B-v0.3 model for Romanized text. The results also highlight the strong all-around performance of the Llama-3.1-8B model for both scripts. Furthermore, a significant performance disparity exists among closed-source models: Gemini-1.5-pro and DeepSeek excel at Unicode generation, whereas Claude-3.5-Sonnet is superior at handling Romanized text. These results provide an essential guide for practitioners selecting models for Sinhala-specific applications and highlight the critical role of training data in handling script variations.

</details>


### [33] [Obscuring Data Contamination Through Translation: Evidence from Arabic Corpora](https://arxiv.org/abs/2601.14994)
*Chaymaa Abbas,Nour Shamaa,Mariette Awad*

Main category: cs.CL

TL;DR: 该研究提出了一种翻译感知的数据污染检测方法，用于识别多语言环境中大语言模型的数据污染问题，解决了现有方法仅限于英语基准的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有数据污染检测方法主要针对英语基准，对多语言环境下的污染问题了解不足。数据污染会损害大语言模型评估的有效性，因为模型可能依赖记忆而非真正泛化。

Method: 通过微调多个开源LLM在不同比例的阿拉伯语数据集上，然后在原始英语基准上评估。扩展了Tested Slot Guessing方法，加入选项重排序策略，并整合Min-K%概率分析来捕获行为和分布污染信号。提出了翻译感知污染检测方法，通过比较多个翻译基准变体的信号来识别污染。

Result: 翻译成阿拉伯语会抑制传统污染指标，但模型仍能从污染数据中受益，特别是那些阿拉伯语能力更强的模型。Min-K%分数随着污染水平增加而上升，跨语言答案一致性也随之提高。翻译感知污染检测方法即使在纯英语方法失败时也能可靠地暴露污染。

Conclusion: 需要采用多语言、翻译感知的评估流程，以确保对LLM进行公平、透明和可复现的评估。翻译感知污染检测方法解决了多语言环境中的数据污染检测盲点。

Abstract: Data contamination undermines the validity of Large Language Model evaluation by enabling models to rely on memorized benchmark content rather than true generalization. While prior work has proposed contamination detection methods, these approaches are largely limited to English benchmarks, leaving multilingual contamination poorly understood. In this work, we investigate contamination dynamics in multilingual settings by fine-tuning several open-weight LLMs on varying proportions of Arabic datasets and evaluating them on original English benchmarks. To detect memorization, we extend the Tested Slot Guessing method with a choice-reordering strategy and incorporate Min-K% probability analysis, capturing both behavioral and distributional contamination signals.
  Our results show that translation into Arabic suppresses conventional contamination indicators, yet models still benefit from exposure to contaminated data, particularly those with stronger Arabic capabilities. This effect is consistently reflected in rising Mink% scores and increased cross-lingual answer consistency as contamination levels grow. To address this blind spot, we propose Translation-Aware Contamination Detection, which identifies contamination by comparing signals across multiple translated benchmark variants rather than English alone. The Translation-Aware Contamination Detection reliably exposes contamination even when English-only methods fail. Together, our findings highlight the need for multilingual, translation-aware evaluation pipelines to ensure fair, transparent, and reproducible assessment of LLMs.

</details>


### [34] [Knowledge Restoration-driven Prompt Optimization: Unlocking LLM Potential for Open-Domain Relational Triplet Extraction](https://arxiv.org/abs/2601.15037)
*Xiaonan Jing,Gongqing Wu,Xingrui Zhuo,Lang Sun,Jiapu Wang*

Main category: cs.CL

TL;DR: KRPO框架通过知识重建驱动的提示优化，提升大语言模型在开放域关系三元组抽取任务中的性能，解决了语义模糊性和关系冗余问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的方法依赖静态启发式提示策略，缺乏反思机制来内化错误信号，导致在语义模糊情况下容易产生错误的抽取模式。

Method: 1. 基于知识恢复的自评估机制，将结构化三元组投影为语义一致性分数作为内在反馈信号；2. 基于文本梯度的提示优化器，内化历史经验迭代优化提示；3. 关系规范化记忆模块，收集代表性关系并提供语义不同的模式。

Result: 在三个数据集上的广泛实验表明，KRPO在抽取F1分数上显著优于强基线方法。

Conclusion: KRPO框架通过知识重建驱动的提示优化，能够持续提升大语言模型在复杂ORTE任务流中的抽取能力，有效解决了语义模糊性和关系冗余问题。

Abstract: Open-domain Relational Triplet Extraction (ORTE) is the foundation for mining structured knowledge without predefined schemas. Despite the impressive in-context learning capabilities of Large Language Models (LLMs), existing methods are hindered by their reliance on static, heuristic-driven prompting strategies. Due to the lack of reflection mechanisms required to internalize erroneous signals, these methods exhibit vulnerability in semantic ambiguity, often making erroneous extraction patterns permanent. To address this bottleneck, we propose a Knowledge Reconstruction-driven Prompt Optimization (KRPO) framework to assist LLMs in continuously improving their extraction capabilities for complex ORTE task flows. Specifically, we design a self-evaluation mechanism based on knowledge restoration, which provides intrinsic feedback signals by projecting structured triplets into semantic consistency scores. Subsequently, we propose a prompt optimizer based on a textual gradient that can internalize historical experiences to iteratively optimize prompts, which can better guide LLMs to handle subsequent extraction tasks. Furthermore, to alleviate relation redundancy, we design a relation canonicalization memory that collects representative relations and provides semantically distinct schemas for the triplets. Extensive experiments across three datasets show that KRPO significantly outperforms strong baselines in the extraction F1 score.

</details>


### [35] [\textsc{LogicScore}: Fine-grained Logic Evaluation of Conciseness, Completeness, and Determinateness in Attributed Question Answering](https://arxiv.org/abs/2601.15050)
*Zhichao Yan,Yunxiao Zhao,Jiapu Wang,Jiaoyan Chen,Shaoru Guo,Xiaoli Li,Ru Li,Jeff Z. Pan*

Main category: cs.CL

TL;DR: LogicScore：一种基于Horn规则的统一评估框架，用于评估大语言模型在长答案中的全局逻辑完整性，解决了现有评估方法的"归因短视"问题。


<details>
  <summary>Details</summary>
Motivation: 现有归因问答评估方法存在"归因短视"问题，过度关注孤立陈述的验证而忽视长答案的全局逻辑完整性，导致模型可能产生事实准确但逻辑不连贯的回答。

Method: 基于Horn规则，结合后向验证机制，系统评估三个关键推理维度：完整性（逻辑推导是否合理）、简洁性（是否冗余）、确定性（答案推导是否一致）。

Result: 在三个多跳问答数据集和20多个LLM上的实验显示，领先模型在归因得分上表现优异（如Gemini-3 Pro精确度92.85%），但在全局推理质量上表现不佳（如Gemini-3 Pro简洁性仅35.11%）。

Conclusion: LogicScore为逻辑评估建立了稳健标准，强调在LLM开发中需要同时关注推理连贯性和事实准确性，揭示了当前模型在全局推理能力上的显著差距。

Abstract: Current evaluation methods for Attributed Question Answering (AQA) suffer from \textit{attribution myopia}: they emphasize verification of isolated statements and their attributions but overlook the global logical integrity of long-form answers. Consequently, Large Language Models (LLMs) often produce factually grounded yet logically incoherent responses with elusive deductive gaps. To mitigate this limitation, we present \textsc{LogicScore}, a unified evaluation framework that shifts the paradigm from local assessment to global reasoning scrutiny. Grounded in Horn Rules, our approach integrates a backward verification mechanism to systematically evaluate three key reasoning dimensions: \textit{Completeness} (logically sound deduction), \textit{Conciseness} (non-redundancy), and \textit{Determinateness} (consistent answer entailment). Extensive experiments across three multi-hop QA datasets (HotpotQA, MusiQue, and 2WikiMultiHopQA) and over 20 LLMs (including GPT-5, Gemini-3-Pro, LLaMA3, and task-specific tuned models) reveal a critical capability gap: leading models often achieve high attribution scores (e.g., 92.85\% precision for Gemini-3 Pro) but struggle with global reasoning quality (e.g., 35.11\% Conciseness for Gemini-3 Pro). Our work establishes a robust standard for logical evaluation, highlighting the need to prioritize reasoning coherence alongside factual grounding in LLM development. Codes are available at: https://github.com/zhichaoyan11/LogicScore.

</details>


### [36] [Multi-Agent Constraint Factorization Reveals Latent Invariant Solution Structure](https://arxiv.org/abs/2601.15077)
*Christopher Scofield*

Main category: cs.CL

TL;DR: 多智能体系统通过约束操作符的分解组合实现比单智能体更好的解空间搜索，即使信息相同。


<details>
  <summary>Details</summary>
Motivation: 解释为什么基于大语言模型的多智能体系统在相同信息下仍能提升问题解决能力。

Method: 使用算子理论和约束优化建模：每个智能体强制执行不同的有效性约束，多智能体系统实现约束操作符的分解组合。

Result: 多智能体系统收敛到智能体约束集的交集定义的不变解集，这些解集是单智能体同时应用所有约束时无法动态访问的。

Conclusion: 多智能体系统的优势源于约束操作符的分解组合，这种机制能发现单智能体无法访问的解空间。

Abstract: Multi-agent systems (MAS) composed of large language models often exhibit improved problem-solving performance despite operating on identical information. In this work, we provide a formal explanation for this phenomenon grounded in operator theory and constrained optimization. We model each agent as enforcing a distinct family of validity constraints on a shared solution state, and show that a MAS implements a factorized composition of constraint-enforcement operators. Under mild conditions, these dynamics converge to invariant solution sets defined by the intersection of agent constraint sets. Such invariant structures are generally not dynamically accessible to a single agent applying all constraints simultaneously, even when expressive capacity and information are identical. We extend this result from exact constraint enforcement to soft constraints via proximal operators, and apply the formalism to contemporary text-based dialog systems.

</details>


### [37] [Circadian Modulation of Semantic Exploration in Social Media Language](https://arxiv.org/abs/2601.15091)
*Vuong Hung Truong,Mariana Gabrielle Cangco Reyes,Masatoshi Koizumi,Jihwan Myung*

Main category: cs.CL

TL;DR: 研究发现人类语义行为存在昼夜节律：早上语义探索性更强，下午则趋于围绕已有主题进行讨论，这与神经调节系统的昼夜模式一致。


<details>
  <summary>Details</summary>
Motivation: 人类认知受到强烈昼夜节律调节，但对其如何影响高维语义行为仍知之甚少。研究者希望探究语言使用是否存在系统性的时间变化模式。

Method: 使用大规模Reddit数据，通过预训练transformer模型将文本嵌入语义空间，测量语义熵作为语言探索-利用的指标，分析其昼夜变化模式，并区分局部和全局语义熵。

Result: 发现语义熵存在稳健的昼夜节律性，可受季节性光照线索调节。局部语义探索在早晨达到峰值，反映更广泛的语义空间探索；而全局语义多样性在下午达到峰值，表现为提交内容围绕已建立主题聚集，符合"富者愈富"动态。这些模式不能由情感或效价解释。

Conclusion: 观察到的时序结构与已知神经调节系统的昼夜模式一致，表明生物昼夜节律延伸至语义领域，语义探索捕捉了与情绪不同的认知维度。

Abstract: Human cognition exhibits strong circadian modulation, yet its influence on high-dimensional semantic behavior remains poorly understood. Using large-scale Reddit data, we quantify time-of-day variation in language use by embedding text into a pretrained transformer model and measuring semantic entropy as an index of linguistic exploration-exploitation, for which we show a robust circadian rhythmicity that could be entrained by seasonal light cues. Distinguishing between local and global semantic entropy reveals a systematic temporal dissociation: local semantic exploration peaks in the morning, reflecting broader exploration of semantic space, whereas global semantic diversity peaks later in the day as submissions accumulate around already established topics, consistent with "rich-get-richer" dynamics. These patterns are not explained by sentiment or affective valence, indicating that semantic exploration captures a cognitive dimension distinct from mood. The observed temporal structure aligns with known diurnal patterns in neuromodulatory systems, suggesting that biological circadian rhythms extend to the semantic domain.

</details>


### [38] [RSNA Large Language Model Benchmark Dataset for Chest Radiographs of Cardiothoracic Disease: Radiologist Evaluation and Validation Enhanced by AI Labels (REVEAL-CXR)](https://arxiv.org/abs/2601.15129)
*Yishu Wei,Adam E. Flanders,Errol Colak,John Mongan,Luciano M Prevedello,Po-Hao Chen,Henrique Min Ho Lee,Gilberto Szarf,Hamilton Shoji,Jason Sho,Katherine Andriole,Tessa Cook,Lisa C. Adams,Linda C. Chu,Maggie Chung,Geraldine Brusca-Augello,Djeven P. Deva,Navneet Singh,Felipe Sanchez Tijmes,Jeffrey B. Alpert,Elsie T. Nguyen,Drew A. Torigian,Kate Hanneman,Lauren K Groner,Alexander Phan,Ali Islam,Matias F. Callejas,Gustavo Borges da Silva Teles,Faisal Jamal,Maryam Vazirabad,Ali Tejani,Hari Trivedi,Paulo Kuriki,Rajesh Bhayana,Elana T. Benishay,Yi Lin,Yifan Peng,George Shih*

Main category: cs.CL

TL;DR: 该研究开发了一个包含200张胸部X光片的专家验证基准数据集，采用AI辅助标注流程，旨在支持临床有用的多模态大语言模型工具开发。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态大语言模型在放射学考试中表现良好，但要开发临床有用的工具，需要由领域专家精心策划的高质量基准数据集。当前缺乏这样的专家验证数据集。

Method: 使用13,735张去标识化胸部X光片，通过GPT-4o提取异常发现，映射到12个基准标签。采用AI辅助采样算法选择1,000张进行专家评审，17位放射科医生参与评估，每张图像由三位专家验证。最终选择200张图像作为基准数据集。

Result: 创建了包含200张胸部X光片的基准数据集，每张图像由至少两位放射科医生验证同意。数据集分为100张公开发布和100张保留用于独立评估。开发了AI辅助标注流程，提高了标注效率。

Conclusion: 成功建立了专家验证的胸部X光片基准数据集和AI辅助标注流程，为开发临床有用的多模态大语言模型工具提供了高质量评估资源。

Abstract: Multimodal large language models have demonstrated comparable performance to that of radiology trainees on multiple-choice board-style exams. However, to develop clinically useful multimodal LLM tools, high-quality benchmarks curated by domain experts are essential. To curate released and holdout datasets of 100 chest radiographic studies each and propose an artificial intelligence (AI)-assisted expert labeling procedure to allow radiologists to label studies more efficiently. A total of 13,735 deidentified chest radiographs and their corresponding reports from the MIDRC were used. GPT-4o extracted abnormal findings from the reports, which were then mapped to 12 benchmark labels with a locally hosted LLM (Phi-4-Reasoning). From these studies, 1,000 were sampled on the basis of the AI-suggested benchmark labels for expert review; the sampling algorithm ensured that the selected studies were clinically relevant and captured a range of difficulty levels. Seventeen chest radiologists participated, and they marked "Agree all", "Agree mostly" or "Disagree" to indicate their assessment of the correctness of the LLM suggested labels. Each chest radiograph was evaluated by three experts. Of these, at least two radiologists selected "Agree All" for 381 radiographs. From this set, 200 were selected, prioritizing those with less common or multiple finding labels, and divided into 100 released radiographs and 100 reserved as the holdout dataset. The holdout dataset is used exclusively by RSNA to independently evaluate different models. A benchmark of 200 chest radiographic studies with 12 benchmark labels was created and made publicly available https://imaging.rsna.org, with each chest radiograph verified by three radiologists. In addition, an AI-assisted labeling procedure was developed to help radiologists label at scale, minimize unnecessary omissions, and support a semicollaborative environment.

</details>


### [39] [Automated Rubrics for Reliable Evaluation of Medical Dialogue Systems](https://arxiv.org/abs/2601.15161)
*Yinzhu Chen,Abdine Maiga,Hossein A. Rahmani,Emine Yilmaz*

Main category: cs.CL

TL;DR: 提出了一个检索增强的多智能体框架，用于自动生成特定实例的评估标准，以解决医疗LLM临床决策支持中的幻觉和安全风险问题。


<details>
  <summary>Details</summary>
Motivation: LLM在临床决策支持中广泛应用，但幻觉和不安全建议可能直接威胁患者安全。这些风险表现为细微的临床错误，难以通过通用指标检测，而专家编写的细粒度评估标准成本高且难以扩展。

Method: 采用检索增强的多智能体框架，通过分解检索到的权威医学证据为原子事实，并结合用户交互约束，生成可验证的细粒度评估标准。

Result: 在HealthBench上测试，临床意图对齐(CIA)得分达60.12%，显著优于GPT-4o基线(55.16%)。判别测试中平均得分差为8.658，AUROC达0.977，几乎是GPT-4o基线(4.972)的两倍。评估标准还能指导响应优化，将质量提升9.2%。

Conclusion: 该框架为评估和改进医疗LLM提供了可扩展且透明的基础，能有效识别临床错误并提升响应质量。

Abstract: Large Language Models (LLMs) are increasingly used for clinical decision support, where hallucinations and unsafe suggestions may pose direct risks to patient safety. These risks are particularly challenging as they often manifest as subtle clinical errors that evade detection by generic metrics, while expert-authored fine-grained rubrics remain costly to construct and difficult to scale. In this paper, we propose a retrieval-augmented multi-agent framework designed to automate the generation of instance-specific evaluation rubrics. Our approach grounds evaluation in authoritative medical evidence by decomposing retrieved content into atomic facts and synthesizing them with user interaction constraints to form verifiable, fine-grained evaluation criteria. Evaluated on HealthBench, our framework achieves a Clinical Intent Alignment (CIA) score of 60.12%, a statistically significant improvement over the GPT-4o baseline (55.16%). In discriminative tests, our rubrics yield a mean score delta ($μ_Δ = 8.658$) and an AUROC of 0.977, nearly doubling the quality separation achieved by GPT-4o baseline (4.972). Beyond evaluation, our rubrics effectively guide response refinement, improving quality by 9.2% (from 59.0% to 68.2%). This provides a scalable and transparent foundation for both evaluating and improving medical LLMs. The code is available at https://anonymous.4open.science/r/Automated-Rubric-Generation-AF3C/.

</details>


### [40] [The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models](https://arxiv.org/abs/2601.15165)
*Zanlin Ni,Shenzhi Wang,Yang Yue,Tianyu Yu,Weilin Zhao,Yeguo Hua,Tianyi Chen,Jun Song,Cheng Yu,Bo Zheng,Gao Huang*

Main category: cs.CL

TL;DR: 扩散大语言模型(dLLMs)的任意顺序生成能力反而限制了其推理边界，作者提出放弃这种灵活性、采用标准GRPO的JustGRPO方法，取得了更好的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究认为扩散大语言模型(dLLMs)的任意顺序生成能力扩展了推理空间，因此采用复杂的强化学习方法来利用这种灵活性。但作者发现这种灵活性实际上让模型绕过关键的高不确定性token，导致解空间过早坍塌。

Method: 提出JustGRPO方法，放弃任意顺序生成的灵活性，直接应用标准的Group Relative Policy Optimization(GRPO)进行训练，同时保持dLLMs的并行解码能力。

Result: JustGRPO方法在GSM8K数学推理数据集上达到89.1%的准确率，相比保留任意顺序生成的方法效果更好，同时保持了dLLMs的并行解码优势。

Conclusion: dLLMs的任意顺序生成在当前形式下反而限制了推理能力，有效的方法不是利用这种灵活性，而是放弃它并采用更直接的GRPO训练策略。

Abstract: Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap

</details>


### [41] [Is Peer Review Really in Decline? Analyzing Review Quality across Venues and Time](https://arxiv.org/abs/2601.15172)
*Ilia Kuznetsov,Rohan Nayak,Alla Rozovskaya,Iryna Gurevych*

Main category: cs.CL

TL;DR: 该研究引入了一个评估同行评审质量的新框架，并应用于AI/ML会议，发现评审质量并未像普遍担忧的那样持续下降。


<details>
  <summary>Details</summary>
Motivation: 随着论文提交数量增加和研究社区扩大，评审质量下降成为普遍担忧的叙事，但缺乏实证证据。评审质量难以衡量，且评审实践的持续演变使得跨会议和跨时间比较变得困难。

Method: 1. 引入基于证据的评审质量比较研究新框架；2. 应用于ICLR、NeurIPS和*ACL等主要AI/ML会议；3. 记录评审格式的多样性并引入评审标准化新方法；4. 提出多维度模式量化评审对编辑和作者的效用；5. 采用基于LLM和轻量级测量方法；6. 研究评审质量测量间的关系及其随时间演变。

Result: 跨时间分析显示，各会议和年份的中位数评审质量并未出现一致下降，这与普遍担忧的叙事相矛盾。

Conclusion: 同行评审质量并未如普遍担忧的那样持续下降。研究提出了替代解释，并概述了促进未来评审质量实证研究的建议。

Abstract: Peer review is at the heart of modern science. As submission numbers rise and research communities grow, the decline in review quality is a popular narrative and a common concern. Yet, is it true? Review quality is difficult to measure, and the ongoing evolution of reviewing practices makes it hard to compare reviews across venues and time. To address this, we introduce a new framework for evidence-based comparative study of review quality and apply it to major AI and machine learning conferences: ICLR, NeurIPS and *ACL. We document the diversity of review formats and introduce a new approach to review standardization. We propose a multi-dimensional schema for quantifying review quality as utility to editors and authors, coupled with both LLM-based and lightweight measurements. We study the relationships between measurements of review quality, and its evolution over time. Contradicting the popular narrative, our cross-temporal analysis reveals no consistent decline in median review quality across venues and years. We propose alternative explanations, and outline recommendations to facilitate future empirical studies of review quality.

</details>


### [42] [Privacy Collapse: Benign Fine-Tuning Can Break Contextual Privacy in Language Models](https://arxiv.org/abs/2601.15220)
*Anmol Goel,Cornelius Emde,Sangdoo Yun,Seong Joon Oh,Martin Gubri*

Main category: cs.CL

TL;DR: 语言模型在微调过程中会出现隐私崩溃现象，即模型在处理敏感信息时失去隐私保护能力，而标准基准测试却无法检测到这种风险。


<details>
  <summary>Details</summary>
Motivation: 研究发现前沿语言模型在良性微调过程中可能发生隐私崩溃，这是一个未被充分认识的"无声故障"现象。现有安全评估体系存在关键缺陷，无法检测微调后模型在隐私保护方面的严重退化。

Method: 通过实验研究六个模型（闭源和开源权重）、五个微调数据集（真实世界和控制数据）以及两个任务类别（代理型和记忆型），分析不同微调场景对隐私保护能力的影响。

Result: 研究发现多种微调场景都会导致隐私崩溃，包括优化帮助性、接触用户信息、情感对话、调试代码打印内部变量等。微调后模型失去情境隐私推理能力，不当共享信息给工具，跨情境违反内存边界。

Conclusion: 隐私表示在微调过程中特别脆弱，而任务相关特征则相对稳定。这揭示了当前安全评估体系的关键漏洞，特别是在专用代理部署场景中，需要新的隐私保护评估方法。

Abstract: We identify a novel phenomenon in language models: benign fine-tuning of frontier models can lead to privacy collapse. We find that diverse, subtle patterns in training data can degrade contextual privacy, including optimisation for helpfulness, exposure to user information, emotional and subjective dialogue, and debugging code printing internal variables, among others. Fine-tuned models lose their ability to reason about contextual privacy norms, share information inappropriately with tools, and violate memory boundaries across contexts. Privacy collapse is a ``silent failure'' because models maintain high performance on standard safety and utility benchmarks whilst exhibiting severe privacy vulnerabilities. Our experiments show evidence of privacy collapse across six models (closed and open weight), five fine-tuning datasets (real-world and controlled data), and two task categories (agentic and memory-based). Our mechanistic analysis reveals that privacy representations are uniquely fragile to fine-tuning, compared to task-relevant features which are preserved. Our results reveal a critical gap in current safety evaluations, in particular for the deployment of specialised agents.

</details>


### [43] [Metadata Conditioned Large Language Models for Localization](https://arxiv.org/abs/2601.15236)
*Anjishnu Mukherjee,Ziwei Zhu,Antonios Anastasopoulos*

Main category: cs.CL

TL;DR: 通过元数据条件化训练，使大语言模型能更好地适应不同地理区域的内容，提高本地化性能而不牺牲跨区域泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型通常将文本视为单一全局分布，导致地理上的同质化行为，缺乏对不同地区特定内容的适应能力

Method: 使用元数据条件化作为轻量级本地化方法，从头训练31个模型（0.5B和1B参数规模），基于带有URL、国家标签和大陆标签的大规模英文新闻数据，覆盖4个大陆和17个国家

Result: 元数据条件化能持续提升区域内性能而不影响跨区域泛化，使全局模型达到与区域特定模型相当的本地化水平，并提高学习效率；URL级元数据已能捕捉大部分地理信号，但平衡的区域数据覆盖仍然关键

Conclusion: 元数据条件化是一种实用且计算高效的本地化方法，能在减少训练数据的情况下达到与更大模型相当的性能

Abstract: Large language models are typically trained by treating text as a single global distribution, often resulting in geographically homogenized behavior. We study metadata conditioning as a lightweight approach for localization, pre-training 31 models (at 0.5B and 1B parameter scales) from scratch on large-scale English news data annotated with verified URLs, country tags, and continent tags, covering 4 continents and 17 countries. Across four controlled experiments, we show that metadata conditioning consistently improves in-region performance without sacrificing cross-region generalization, enables global models to recover localization comparable to region-specific models, and improves learning efficiency. Our ablation studies demonstrate that URL-level metadata alone captures much of the geographic signal, while balanced regional data coverage remains essential, as metadata cannot fully compensate for missing regions. Finally, we introduce a downstream benchmark of 800 localized news MCQs and show that after instruction tuning, metadata conditioned global models achieve accuracy comparable to LLaMA-3.2-1B-Instruct, despite being trained on substantially less data. Together, these results establish metadata conditioning as a practical and compute-efficient approach for localization of language models.

</details>


### [44] [Taxonomy-Aligned Risk Extraction from 10-K Filings with Autonomous Improvement Using LLMs](https://arxiv.org/abs/2601.15247)
*Rian Dolphin,Joe Dursun,Jarrett Blankenship,Katie Adams,Quinton Pike*

Main category: cs.CL

TL;DR: 本文提出了一种从公司10-K文件中提取结构化风险因素的方法，该方法结合了LLM提取、语义映射和LLM验证的三阶段流程，并引入了自主分类法维护机制。


<details>
  <summary>Details</summary>
Motivation: 企业风险因素提取通常需要人工处理且难以与预定义分类法保持一致。现有方法在准确性和可扩展性方面存在不足，特别是在处理大量非结构化文本时。

Method: 三阶段流水线：1) LLM提取风险因素并提供支持性引文；2) 基于嵌入的语义映射到分类法类别；3) LLM作为评判者验证并过滤错误分配。此外引入自主分类法维护，AI代理分析评估反馈以识别问题类别、诊断失败模式并提出改进建议。

Result: 从标普500公司提取了10,688个风险因素，同一行业公司风险特征相似度比跨行业公司高63%（Cohen's d=1.06，AUC 0.82，p<0.001）。自主分类法维护在案例研究中实现了104.7%的嵌入分离改进。

Conclusion: 该方法能有效提取结构化风险因素并保持分类法一致性，自主改进机制支持持续质量维护。该方法可推广到任何需要从非结构化文本进行分类法对齐提取的领域。

Abstract: We present a methodology for extracting structured risk factors from corporate 10-K filings while maintaining adherence to a predefined hierarchical taxonomy. Our three-stage pipeline combines LLM extraction with supporting quotes, embedding-based semantic mapping to taxonomy categories, and LLM-as-a-judge validation that filters spurious assignments. To evaluate our approach, we extract 10,688 risk factors from S&P 500 companies and examine risk profile similarity across industry clusters. Beyond extraction, we introduce autonomous taxonomy maintenance where an AI agent analyzes evaluation feedback to identify problematic categories, diagnose failure patterns, and propose refinements, achieving 104.7% improvement in embedding separation in a case study. External validation confirms the taxonomy captures economically meaningful structure: same-industry companies exhibit 63% higher risk profile similarity than cross-industry pairs (Cohen's d=1.06, AUC 0.82, p<0.001). The methodology generalizes to any domain requiring taxonomy-aligned extraction from unstructured text, with autonomous improvement enabling continuous quality maintenance and enhancement as systems process more documents.

</details>


### [45] [The Effect of Scripts and Formats on LLM Numeracy](https://arxiv.org/abs/2601.15251)
*Varshini Reddy,Craig W. Schmidt,Seth Ebner,Adam Wiemerslage,Yuval Pinter,Chris Tanner*

Main category: cs.CL

TL;DR: LLMs在标准数字任务上表现出色，但在非常规数字格式和脚本上准确率显著下降，可通过针对性提示策略改善。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在训练语料中不常见的数字格式和脚本上的表现，探索多语言数字推理中的被忽视挑战。

Method: 调查多种数字脚本和格式下的数字推理，测试LLMs在不同表示形式下的表现，并探索少样本提示和明确数字映射等针对性提示策略。

Result: LLMs在代表性不足的数字脚本和格式上准确率显著下降，但针对性提示策略可以大大缩小这种差距。

Conclusion: 研究揭示了多语言数字推理中被忽视的挑战，为可靠处理不同数字脚本和格式提供了实用见解。

Abstract: Large language models (LLMs) have achieved impressive proficiency in basic arithmetic, rivaling human-level performance on standard numerical tasks. However, little attention has been given to how these models perform when numerical expressions deviate from the prevailing conventions present in their training corpora. In this work, we investigate numerical reasoning across a wide range of numeral scripts and formats. We show that LLM accuracy drops substantially when numerical inputs are rendered in underrepresented scripts or formats, despite the underlying mathematical reasoning being identical. We further demonstrate that targeted prompting strategies, such as few-shot prompting and explicit numeral mapping, can greatly narrow this gap. Our findings highlight an overlooked challenge in multilingual numerical reasoning and provide actionable insights for working with LLMs to reliably interpret, manipulate, and generate numbers across diverse numeral scripts and formatting styles.

</details>


### [46] [Robust Fake News Detection using Large Language Models under Adversarial Sentiment Attacks](https://arxiv.org/abs/2601.15277)
*Sahar Tahmasebi,Eric Müller-Budack,Ralph Ewerth*

Main category: cs.CL

TL;DR: 本文提出AdSent框架，通过控制情感操纵攻击来增强假新闻检测器的鲁棒性，并开发情感无关的训练策略来抵御情感操纵攻击。


<details>
  <summary>Details</summary>
Motivation: 现有假新闻检测方法依赖情感特征，但随着大语言模型的出现，攻击者可以操纵情感来逃避检测。目前研究主要关注风格特征，而情感操纵这一关键漏洞尚未得到充分探索。

Method: 1) 使用大语言模型进行控制性情感对抗攻击；2) 分析情感变化对检测性能的影响；3) 提出新颖的情感无关训练策略来增强鲁棒性。

Result: 实验表明，改变情感会严重影响假新闻检测模型的性能，模型存在偏见（中性文章易被判断为真，非中性文章易被判断为假）。AdSent在三个基准数据集上显著优于现有基线方法，在准确性和鲁棒性方面都有提升，并能有效泛化到未见数据集和对抗场景。

Conclusion: 情感操纵是假新闻检测器的一个重要漏洞，AdSent框架通过对抗攻击和情感无关训练策略有效提升了检测器的鲁棒性，为构建更可靠的假新闻检测系统提供了解决方案。

Abstract: Misinformation and fake news have become a pressing societal challenge, driving the need for reliable automated detection methods. Prior research has highlighted sentiment as an important signal in fake news detection, either by analyzing which sentiments are associated with fake news or by using sentiment and emotion features for classification. However, this poses a vulnerability since adversaries can manipulate sentiment to evade detectors especially with the advent of large language models (LLMs). A few studies have explored adversarial samples generated by LLMs, but they mainly focus on stylistic features such as writing style of news publishers. Thus, the crucial vulnerability of sentiment manipulation remains largely unexplored. In this paper, we investigate the robustness of state-of-the-art fake news detectors under sentiment manipulation. We introduce AdSent, a sentiment-robust detection framework designed to ensure consistent veracity predictions across both original and sentiment-altered news articles. Specifically, we (1) propose controlled sentiment-based adversarial attacks using LLMs, (2) analyze the impact of sentiment shifts on detection performance. We show that changing the sentiment heavily impacts the performance of fake news detection models, indicating biases towards neutral articles being real, while non-neutral articles are often classified as fake content. (3) We introduce a novel sentiment-agnostic training strategy that enhances robustness against such perturbations. Extensive experiments on three benchmark datasets demonstrate that AdSent significantly outperforms competitive baselines in both accuracy and robustness, while also generalizing effectively to unseen datasets and adversarial scenarios.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [47] [Legal Retrieval for Public Defenders](https://arxiv.org/abs/2601.14348)
*Dominik Stammbach,Kylie Zhang,Patty Liu,Nimra Nadeem,Lucia Zheng,Peter Henderson*

Main category: cs.IR

TL;DR: 开发NJ BriefBank检索工具来帮助公设辩护人快速查找相关上诉状，提高法律研究和写作效率


<details>
  <summary>Details</summary>
Motivation: 公设辩护机构面临沉重的工作负担、复杂的法律程序和有限的资源，但缺乏有效的AI工具来支持日常辩护工作

Method: 与纽泽西公设辩护办公室合作开发NJ BriefBank检索工具，使用查询扩展（包括法律推理）、领域特定数据和精心设计的合成示例来改进检索质量

Result: 现有法律检索基准无法直接适用于公设辩护检索，但通过添加领域知识可以显著提高检索质量；提供了现实的辩护检索查询分类和手动标注的公设辩护检索数据集

Conclusion: 该研究为公设辩护领域开发实用可靠的AI检索工具提供了起点，并推动了更现实的法律检索基准的发展

Abstract: AI tools are increasingly suggested as solutions to assist public agencies with heavy workloads. In public defense, where a constitutional right to counsel meets the complexities of law, overwhelming caseloads and constrained resources, practitioners face especially taxing conditions. Yet, there is little evidence of how AI could meaningfully support defenders' day-to-day work. In partnership with the New Jersey Office of the Public Defender, we develop the NJ BriefBank, a retrieval tool which surfaces relevant appellate briefs to streamline legal research and writing. We show that existing legal retrieval benchmarks fail to transfer to public defense search, however adding domain knowledge improves retrieval quality. This includes query expansion with legal reasoning, domain-specific data and curated synthetic examples. To facilitate further research, we provide a taxonomy of realistic defender search queries and release a manually annotated public defense retrieval dataset. Together, our work offers starting points towards building practical, reliable retrieval AI tools for public defense, and towards more realistic legal retrieval benchmarks.

</details>


### [48] [Trust Me on This: A User Study of Trustworthiness for RAG Responses](https://arxiv.org/abs/2601.14460)
*Weronika Łajewska,Krisztian Balog*

Main category: cs.IR

TL;DR: 解释机制能引导用户选择更高质量的回答，但用户信任度不仅取决于客观质量，还受回答清晰度、可操作性和先验知识的影响。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在信息检索系统中常提供缺乏透明度的合成答案，这影响了用户对系统的信任。本研究旨在探究不同类型的解释如何影响用户对检索增强生成系统回答的信任度。

Method: 采用两阶段对照用户研究，参与者从一对回答中选择更可信的一个（一个客观质量更高），分别在有/无三种解释类型的情况下进行：(1) 来源归因，(2) 事实基础，(3) 信息覆盖范围。

Result: 解释显著引导用户选择更高质量的回答，但信任度不完全由客观质量决定：用户的判断还受到回答清晰度、可操作性和自身先验知识的显著影响。

Conclusion: 在检索增强生成系统中加入解释机制能有效提升用户选择高质量回答的能力，但设计可信AI系统时需综合考虑客观质量、回答清晰度、可操作性和用户知识背景等多重因素。

Abstract: The integration of generative AI into information access systems often presents users with synthesized answers that lack transparency. This study investigates how different types of explanations can influence user trust in responses from retrieval-augmented generation systems. We conducted a controlled, two-stage user study where participants chose the more trustworthy response from a pair-one objectively higher quality than the other-both with and without one of three explanation types: (1) source attribution, (2) factual grounding, and (3) information coverage. Our results show that while explanations significantly guide users toward selecting higher quality responses, trust is not dictated by objective quality alone: Users' judgments are also heavily influenced by response clarity, actionability, and their own prior knowledge.

</details>


### [49] [Predicting Retrieval Utility and Answer Quality in Retrieval-Augmented Generation](https://arxiv.org/abs/2601.14546)
*Fangzheng Tian,Debasis Ganguly,Craig Macdonald*

Main category: cs.IR

TL;DR: 本文提出在检索增强生成(RAG)中预测检索性能(RPP)和生成性能(GPP)的方法，通过结合查询性能预测、读者中心特征和文档质量特征，显著提升了RAG性能预测的准确性。


<details>
  <summary>Details</summary>
Motivation: RAG系统中生成答案的质量很大程度上受检索文档上下文信息的影响。需要预测检索文档的效用（相对于无上下文生成的性能提升）和最终答案的质量（正确性和相关性），以改进RAG系统。

Method: 定义了RPP和GPP两个预测任务。假设检索文档的主题相关性与其效用相关，因此可以调整查询性能预测(QPP)方法用于RPP和GPP。此外，引入了读者中心特征（如LLM在输入查询条件下对检索上下文的困惑度）和查询无关的文档质量与可读性特征。使用这些特征类别训练线性回归模型进行预测。

Result: 在Natural Questions数据集上的实验表明，结合多个特征类别的预测器能够最准确地估计RAG性能，验证了多特征组合方法的有效性。

Conclusion: 通过整合检索器中心信号、读者中心特征和文档质量特征，可以有效预测RAG中的检索性能和生成性能，为RAG系统的改进提供了重要工具。

Abstract: The quality of answers generated by large language models (LLMs) in retrieval-augmented generation (RAG) is largely influenced by the contextual information contained in the retrieved documents. A key challenge for improving RAG is to predict both the utility of retrieved documents -- quantified as the performance gain from using context over generation without context -- and the quality of the final answers in terms of correctness and relevance. In this paper, we define two prediction tasks within RAG. The first is retrieval performance prediction (RPP), which estimates the utility of retrieved documents. The second is generation performance prediction (GPP), which estimates the final answer quality. We hypothesise that in RAG, the topical relevance of retrieved documents correlates with their utility, suggesting that query performance prediction (QPP) approaches can be adapted for RPP and GPP. Beyond these retriever-centric signals, we argue that reader-centric features, such as the LLM's perplexity of the retrieved context conditioned on the input query, can further enhance prediction accuracy for both RPP and GPP. Finally, we propose that features reflecting query-agnostic document quality and readability can also provide useful signals to the predictions. We train linear regression models with the above categories of predictors for both RPP and GPP. Experiments on the Natural Questions (NQ) dataset show that combining predictors from multiple feature categories yields the most accurate estimates of RAG performance.

</details>


### [50] [When Text-as-Vision Meets Semantic IDs in Generative Recommendation: An Empirical Study](https://arxiv.org/abs/2601.14697)
*Shutong Qiao,Wei Yuan,Tong Chen,Xiangyu Zhao,Quoc Viet Hung Nguyen,Hongzhi Yin*

Main category: cs.IR

TL;DR: 该论文提出在生成式推荐模型中使用OCR视觉文本表示替代标准文本编码器来学习语义ID，以解决推荐数据中符号化、属性中心的文本描述问题，并在多模态场景中改善文本与图像嵌入的几何结构匹配。


<details>
  <summary>Details</summary>
Motivation: 当前生成式推荐模型依赖预训练文本编码器为物品生成语义ID，但这些编码器主要针对自然语言优化。实际推荐数据中的物品描述通常是符号化、属性中心的，包含数字、单位和缩写，标准文本编码器会将这些信号分解为碎片化标记，削弱语义连贯性并扭曲属性关系。在多模态生成式推荐中，标准文本编码器还会导致文本和图像嵌入的几何结构不匹配，影响跨模态融合效果和稳定性。

Method: 重新审视语义ID学习的表示设计，将文本视为视觉信号。通过系统性的实证研究，采用OCR文本表示方法：将物品描述渲染成图像，然后使用基于视觉的OCR模型进行编码。在四个数据集和两种生成式骨干网络上进行实验。

Result: 实验表明，OCR文本表示在单模态和多模态设置下，对于语义ID学习都一致匹配或超越标准文本嵌入。OCR语义ID在极端空间分辨率压缩下仍保持鲁棒性，表明在实际部署中具有强大的鲁棒性和效率。

Conclusion: 将文本作为视觉信号处理，通过OCR文本表示学习语义ID，能够更好地处理推荐数据中的符号化文本描述，改善多模态场景中的几何结构匹配，并提供鲁棒且高效的解决方案。

Abstract: Semantic ID learning is a key interface in Generative Recommendation (GR) models, mapping items to discrete identifiers grounded in side information, most commonly via a pretrained text encoder. However, these text encoders are primarily optimized for well-formed natural language. In real-world recommendation data, item descriptions are often symbolic and attribute-centric, containing numerals, units, and abbreviations. These text encoders can break these signals into fragmented tokens, weakening semantic coherence and distorting relationships among attributes. Worse still, when moving to multimodal GR, relying on standard text encoders introduces an additional obstacle: text and image embeddings often exhibit mismatched geometric structures, making cross-modal fusion less effective and less stable.
  In this paper, we revisit representation design for Semantic ID learning by treating text as a visual signal. We conduct a systematic empirical study of OCR-based text representations, obtained by rendering item descriptions into images and encoding them with vision-based OCR models. Experiments across four datasets and two generative backbones show that OCR-text consistently matches or surpasses standard text embeddings for Semantic ID learning in both unimodal and multimodal settings. Furthermore, we find that OCR-based Semantic IDs remain robust under extreme spatial-resolution compression, indicating strong robustness and efficiency in practical deployments.

</details>


### [51] [Unified Multimodal and Multilingual Retrieval via Multi-Task Learning with NLU Integration](https://arxiv.org/abs/2601.14714)
*Xinyuan Zhang,Lina Zhang,Lisung Chen,Guangyao Liu,Shuai Nie,Jiaming Xu,Runyu Shi,Ying Huang,Guoquan Zhang*

Main category: cs.IR

TL;DR: 提出了一个多任务学习框架，统一图像、长短文本和意图丰富查询的特征表示，首次联合优化多语言图像检索、文本检索和自然语言理解任务。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）虽然将图像和文本编码到共享嵌入空间，但在纯文本检索任务上表现不如专用文本模型。添加额外文本编码器会增加存储和推理开销，在多语言环境下检索效率问题更突出。

Method: 采用多任务学习框架，通过共享的文本编码器整合图像和文本检索，该编码器通过自然语言理解特征增强，以理解意图并提高检索准确性。

Result: 这是首个在单一框架内联合优化多语言图像检索、文本检索和自然语言理解任务的工作。

Conclusion: 提出的方法解决了现有VLMs在文本检索上的不足，同时减少了存储和推理开销，特别在多语言环境下提高了检索效率。

Abstract: Multimodal retrieval systems typically employ Vision Language Models (VLMs) that encode images and text independently into vectors within a shared embedding space. Despite incorporating text encoders, VLMs consistently underperform specialized text models on text-only retrieval tasks. Moreover, introducing additional text encoders increases storage, inference overhead, and exacerbates retrieval inefficiencies, especially in multilingual settings. To address these limitations, we propose a multi-task learning framework that unifies the feature representation across images, long and short texts, and intent-rich queries. To our knowledge, this is the first work to jointly optimize multilingual image retrieval, text retrieval, and natural language understanding (NLU) tasks within a single framework. Our approach integrates image and text retrieval with a shared text encoder that is enhanced by NLU features for intent understanding and retrieval accuracy.

</details>


### [52] [PULSE: Socially-Aware User Representation Modeling Toward Parameter-Efficient Graph Collaborative Filtering](https://arxiv.org/abs/2601.14720)
*Doyun Choi,Cheonwoo Lee,Biniyam Aschalew Tolera,Taewook Ham,Chanyoung Park,Jaemin Yoo*

Main category: cs.IR

TL;DR: 提出PULSE框架，通过从社交有意义的信号构建用户表示，无需为每个用户创建显式可学习嵌入，减少参数达50%，在多种交互稀疏度下超越13个基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的图协同过滤和图社交推荐方法面临高计算成本和有限可扩展性的挑战，因为它们需要为所有用户和物品分配显式嵌入，导致参数数量庞大。

Method: 提出PULSE框架，不创建每个用户的显式可学习嵌入，而是从社交有意义的信号构建用户表示，实现时间和内存高效的建模过程。

Result: PULSE相比最轻量级GCF基线减少参数达50%，在13个GCF和图社交推荐基线上实现SOTA性能，在从冷启动到活跃用户的各种交互稀疏度下表现优异。

Conclusion: PULSE通过参数高效的社交知识用户表示学习，解决了现有图社交推荐方法的可扩展性限制，在保持高性能的同时显著减少模型参数。

Abstract: Graph-based social recommendation (SocialRec) has emerged as a powerful extension of graph collaborative filtering (GCF), which leverages graph neural networks (GNNs) to capture multi-hop collaborative signals from user-item interactions. These methods enrich user representations by incorporating social network information into GCF, thereby integrating additional collaborative signals from social relations. However, existing GCF and graph-based SocialRec approaches face significant challenges: they incur high computational costs and suffer from limited scalability due to the large number of parameters required to assign explicit embeddings to all users and items. In this work, we propose PULSE (Parameter-efficient User representation Learning with Social Knowledge), a framework that addresses this limitation by constructing user representations from socially meaningful signals without creating an explicit learnable embedding for each user. PULSE reduces the parameter size by up to 50% compared to the most lightweight GCF baseline. Beyond parameter efficiency, our method achieves state-of-the-art performance, outperforming 13 GCF and graph-based social recommendation baselines across varying levels of interaction sparsity, from cold-start to highly active users, through a time- and memory-efficient modeling process.

</details>


### [53] [What Should I Cite? A RAG Benchmark for Academic Citation Prediction](https://arxiv.org/abs/2601.14949)
*Leqi Zheng,Jiajun Zhang,Canzhi Chen,Chaokun Wang,Hongwei Li,Yuying Li,Yaoxin Mao,Shannan Yan,Zixin Song,Zhiyuan Feng,Zhaolu Kang,Zirong Chen,Hang Zhang,Qiang Liu,Liang Wang,Ziyang Liu*

Main category: cs.IR

TL;DR: CiteRAG是一个用于学术引用预测的检索增强生成基准，包含多级检索策略、专用检索器和生成器，提供数据集和开源工具包。


<details>
  <summary>Details</summary>
Motivation: 随着网络学术出版物快速增长，每年发表的论文数量急剧增加，学者越来越难以找到相关的前期工作。引用预测旨在自动推荐合适的参考文献，帮助学者在日益扩大的科学文献中导航。

Method: 1) 建立两个不同粒度的引用预测任务实例（粗粒度列表级和细粒度位置级），构建包含7,267和8,541个实例的数据集；2) 构建包含554k论文的三级大规模语料库；3) 提出用于引用预测的多级混合RAG方法，通过对比学习微调嵌入模型以捕捉复杂引用关系；4) 在包括闭源API、开源模型和微调生成器在内的最先进语言模型上进行广泛实验。

Result: 展示了框架的有效性，开源工具包支持可复现评估，专注于学术文献，为引用预测提供了首个全面评估框架，并可作为其他科学领域的方法学模板。

Conclusion: CiteRAG是首个用于评估大语言模型在学术引用预测上的检索增强生成集成基准，通过多级检索策略和专用组件，为学术引用预测提供了全面的评估框架和方法学模板。

Abstract: With the rapid growth of Web-based academic publications, more and more papers are being published annually, making it increasingly difficult to find relevant prior work. Citation prediction aims to automatically suggest appropriate references, helping scholars navigate the expanding scientific literature. Here we present \textbf{CiteRAG}, the first comprehensive retrieval-augmented generation (RAG)-integrated benchmark for evaluating large language models on academic citation prediction, featuring a multi-level retrieval strategy, specialized retrievers, and generators. Our benchmark makes four core contributions: (1) We establish two instances of the citation prediction task with different granularity. Task 1 focuses on coarse-grained list-specific citation prediction, while Task 2 targets fine-grained position-specific citation prediction. To enhance these two tasks, we build a dataset containing 7,267 instances for Task 1 and 8,541 instances for Task 2, enabling comprehensive evaluation of both retrieval and generation. (2) We construct a three-level large-scale corpus with 554k papers spanning many major subfields, using an incremental pipeline. (3) We propose a multi-level hybrid RAG approach for citation prediction, fine-tuning embedding models with contrastive learning to capture complex citation relationships, paired with specialized generation models. (4) We conduct extensive experiments across state-of-the-art language models, including closed-source APIs, open-source models, and our fine-tuned generators, demonstrating the effectiveness of our framework. Our open-source toolkit enables reproducible evaluation and focuses on academic literature, providing the first comprehensive evaluation framework for citation prediction and serving as a methodological template for other scientific domains. Our source code and data are released at https://github.com/LQgdwind/CiteRAG.

</details>


### [54] [From Insight to Intervention: Interpretable Neuron Steering for Controlling Popularity Bias in Recommender Systems](https://arxiv.org/abs/2601.15122)
*Parviz Ahmadov,Masoud Mansoury*

Main category: cs.IR

TL;DR: PopSteer是一个利用稀疏自编码器解释和缓解推荐系统中流行度偏差的后处理方法，通过识别编码流行度信号的神经元并调整其激活来改善公平性。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中存在流行度偏差问题，少数流行物品占据主导地位，而大多数不流行物品曝光不足，这会降低推荐质量并导致不公平。现有缓解方法往往缺乏透明度。

Method: 提出PopSteer方法：1)使用稀疏自编码器(SAE)复制训练好的推荐模型行为，实现神经元级可解释性；2)引入对流行或不流行物品有强偏好的合成用户，通过激活模式识别编码流行度信号的神经元；3)通过调整最偏差神经元的激活来引导推荐。

Result: 在三个公开数据集上的顺序推荐模型实验表明，PopSteer显著提高了公平性，对准确性影响最小，同时提供可解释的见解和对公平性-准确性权衡的细粒度控制。

Conclusion: PopSteer是一种有效的后处理方法，既能解释又能缓解推荐系统中的流行度偏差，在保持推荐准确性的同时显著改善公平性，为偏差缓解提供了透明和可控的解决方案。

Abstract: Popularity bias is a pervasive challenge in recommender systems, where a few popular items dominate attention while the majority of less popular items remain underexposed. This imbalance can reduce recommendation quality and lead to unfair item exposure. Although existing mitigation methods address this issue to some extent, they often lack transparency in how they operate. In this paper, we propose a post-hoc approach, PopSteer, that leverages a Sparse Autoencoder (SAE) to both interpret and mitigate popularity bias in recommendation models. The SAE is trained to replicate a trained model's behavior while enabling neuron-level interpretability. By introducing synthetic users with strong preferences for either popular or unpopular items, we identify neurons encoding popularity signals through their activation patterns. We then steer recommendations by adjusting the activations of the most biased neurons. Experiments on three public datasets with a sequential recommendation model demonstrate that PopSteer significantly enhances fairness with minimal impact on accuracy, while providing interpretable insights and fine-grained control over the fairness-accuracy trade-off.

</details>


### [55] [Beyond the Geometric Curse: High-Dimensional N-Gram Hashing for Dense Retrieval](https://arxiv.org/abs/2601.15205)
*Sangeet Sharma*

Main category: cs.IR

TL;DR: NUMEN是一种无需训练的高维向量检索方法，通过确定性字符哈希将文本直接投影到高维空间，在LIMIT基准测试中首次超越BM25基线。


<details>
  <summary>Details</summary>
Motivation: 当前最强大的7B参数嵌入模型在简单检索任务上甚至不如几十年前的BM25方法，理论表明这是由于维度瓶颈问题——将无限的语言细微差别压缩到固定长度的小向量中。

Method: NUMEN完全移除了学习过程，使用确定性字符哈希将语言直接投影到高维向量，无需训练，支持无限词汇表，几何容量可按需扩展。

Result: 在LIMIT基准测试中，NUMEN在32,768维度下达到93.90%的Recall@100，首次正式超越稀疏BM25基线的93.6%。

Conclusion: 密集检索的真正问题不是架构，而是嵌入层本身。解决方案不一定是更智能的训练，而是提供更多的表达空间。

Abstract: Why do even the most powerful 7B-parameter embedding models struggle with simple retrieval tasks that the decades old BM25 handles with ease? Recent theory suggests that this happens because of a dimensionality bottleneck. This occurs when we force infinite linguistic nuances into small, fixed-length learned vectors. We developed NUMEN to break this bottleneck by removing the learning process entirely. Instead of training heavy layers to map text to a constrained space, NUMEN uses deterministic character hashing to project language directly onto high-dimensional vectors. This approach requires no training, supports an unlimited vocabulary, and allows the geometric capacity scale as needed. On the LIMIT benchmark, NUMEN achieves 93.90 % Recall@100 at 32,768 dimensions. This makes it the first dense retrieval model to officially surpass the sparse BM25 baseline 93.6 %. Our findings show that the real problem in dense retrieval isn't the architecture, but the embedding layer itself. The solution isn't necessarily smarter training, but simply providing more room to breathe.

</details>


### [56] [Agentic-R: Learning to Retrieve for Agentic Search](https://arxiv.org/abs/2601.11888)
*Wenhan Liu,Xinyu Ma,Yutao Zhu,Yuchen Li,Daiting Shi,Dawei Yin,Zhicheng Dou*

Main category: cs.IR

TL;DR: 本文提出了专门为Agentic Search设计的检索器训练框架，通过结合局部查询-段落相关性和全局答案正确性来评估段落效用，并采用迭代训练策略实现检索器与搜索代理的双向优化。


<details>
  <summary>Details</summary>
Motivation: 尽管Agentic Search（智能体搜索）已成为解决复杂问题的强大范式，但如何为这种搜索模式设计专门的检索器仍未被充分探索。现有搜索代理通常依赖基于相似性的检索器，而相似段落并不总是对最终答案生成有用。

Method: 提出了针对Agentic Search的检索器训练框架：1）使用局部查询-段落相关性和全局答案正确性共同衡量多轮搜索中的段落效用；2）采用迭代训练策略，使搜索代理和检索器能够双向、迭代地优化；3）不同于传统RAG检索器仅用固定问题训练一次，本方法利用代理生成的高质量、动态演化的查询持续改进检索器。

Result: 在七个单跳和多跳QA基准测试上的广泛实验表明，提出的检索器（\ours{}）在不同搜索代理上均一致优于强基线方法。

Conclusion: 本文提出的检索器训练框架有效解决了Agentic Search中的检索器设计问题，通过局部和全局效用衡量以及迭代优化策略，显著提升了检索器在多轮智能体搜索中的性能。

Abstract: Agentic search has recently emerged as a powerful paradigm, where an agent interleaves multi-step reasoning with on-demand retrieval to solve complex questions. Despite its success, how to design a retriever for agentic search remains largely underexplored. Existing search agents typically rely on similarity-based retrievers, while similar passages are not always useful for final answer generation. In this paper, we propose a novel retriever training framework tailored for agentic search. Unlike retrievers designed for single-turn retrieval-augmented generation (RAG) that only rely on local passage utility, we propose to use both local query-passage relevance and global answer correctness to measure passage utility in a multi-turn agentic search. We further introduce an iterative training strategy, where the search agent and the retriever are optimized bidirectionally and iteratively. Different from RAG retrievers that are only trained once with fixed questions, our retriever is continuously improved using evolving and higher-quality queries from the agent. Extensive experiments on seven single-hop and multi-hop QA benchmarks demonstrate that our retriever, termed \ours{}, consistently outperforms strong baselines across different search agents. Our codes are available at: https://github.com/8421BCD/Agentic-R.

</details>
