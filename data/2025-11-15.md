<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 5]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Know Your Limits: Entropy Estimation Modeling for Compression and Generalization](https://arxiv.org/abs/2511.10618)
*Benjamin L. Badger,Matthew Neligeorge*

Main category: cs.CL

TL;DR: 本文提出了一种编码器增强的因果解码器模型架构，相比因果变换器具有更好的训练效率和压缩性能，并展示了通过接近但不超出估计的每词元熵来训练模型可以提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 语言预测受到语言内在信息熵的限制，目前最有效的语言压缩算法是因果大语言模型，但用这些模型准确估计语言熵在计算上不可行。

Method: 引入编码器增强的因果解码器模型架构，通过该方法获得每词元熵估计，并训练模型接近但不超出估计的熵值。

Result: 该方法在适度硬件上训练就能获得比因果变换器更高的压缩率，且接近熵值的模型比单纯最小化损失的模型具有更好的泛化能力。

Conclusion: 通过考虑语言熵约束来训练模型可以提升泛化性能，编码器增强架构在效率和压缩方面优于传统因果变换器。

Abstract: Language prediction is constrained by informational entropy intrinsic to language, such that there exists a limit to how accurate any language model can become and equivalently a lower bound to language compression. The most efficient language compression algorithms today are causal (next token prediction) large language models, but the use of these models to form accurate estimates of language entropy is currently computationally infeasible. We introduce encoder-augmented causal decoder model architectures that exhibit superior training efficiency characteristics and achieve higher compression than causal transformers even when trained on modest hardware. We demonstrate how entropy estimates can be obtained on a per-token basis, and show that the generalization of models trained to approach the entropy of their training data necessarily exceeds the generalization of models trained to minimize loss beyond this value. We show empirically that causal models trained to approach but not exceed estimated per-token entropies exhibit greater generalization than models trained without taking entropy into account.

</details>


### [2] [SSR: Socratic Self-Refine for Large Language Model Reasoning](https://arxiv.org/abs/2511.10621)
*Haizhou Shi,Ye Liu,Bo Pang,Zeyu Leo Liu,Hao Wang,Silvio Savarese,Caiming Xiong,Yingbo Zhou,Semih Yavuz*

Main category: cs.CL

TL;DR: SSR框架通过将模型响应分解为可验证的子问题-子答案对，实现细粒度评估和精确优化，在多个推理基准测试中优于现有自优化方法。


<details>
  <summary>Details</summary>
Motivation: 现有测试时框架依赖粗粒度的自我验证和修正，限制了在复杂任务上的效果，需要更精细的评估和优化方法。

Method: 将模型响应分解为可验证的子问题-子答案对，通过控制重解和自一致性检查进行步骤级置信度估计，精确定位不可靠步骤并迭代优化。

Result: 在五个推理基准测试和三个大语言模型上的实证结果显示，SSR持续优于最先进的迭代自优化基线方法。

Conclusion: SSR不仅提升了性能，还提供了一种原则性的黑盒方法来评估和理解大语言模型的内部推理过程。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks. By pinpointing unreliable steps and iteratively refining them, SSR produces more accurate and interpretable reasoning chains. Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines. Beyond performance gains, SSR provides a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs. Code is available at https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning.

</details>


### [3] [Instella: Fully Open Language Models with Stellar Performance](https://arxiv.org/abs/2511.10628)
*Jiang Liu,Jialian Wu,Xiaodong Yu,Yusheng Su,Prakamya Mishra,Gowtham Ramesh,Sudhanshu Ranjan,Chaitanya Manem,Ximeng Sun,Ze Wang,Pratik Prabhanjan Brahma,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

TL;DR: Instella是一个完全开放的30亿参数语言模型系列，使用公开数据和AMD MI300X GPU训练，在完全开源模型中达到最先进水平，并发布了支持128K上下文长度的Instella-Long和专注于数学推理的Instella-Math两个变体。


<details>
  <summary>Details</summary>
Motivation: 当前大多数高性能语言模型都是闭源或部分开放，限制了透明度和可复现性，需要开发完全开源的替代方案。

Method: 通过大规模预训练、通用指令微调以及与人类偏好的对齐来开发模型，使用公开可用数据和代码库，并针对数学任务进行监督微调和强化学习。

Result: Instella在使用较少预训练token的情况下，在完全开源模型中达到最先进水平，并与同规模的开源权重模型具有竞争力。

Conclusion: Instella系列模型为社区提供了透明、高性能且多功能的替代方案，推动了开放和可复现的语言建模研究。

Abstract: Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.

</details>


### [4] [Black-Box On-Policy Distillation of Large Language Models](https://arxiv.org/abs/2511.10643)
*Tianzhu Ye,Li Dong,Zewen Chi,Xun Wu,Shaohan Huang,Furu Wei*

Main category: cs.CL

TL;DR: 提出了生成对抗蒸馏（GAD）方法，通过将学生LLM作为生成器、训练判别器区分学生和教师模型的响应，实现黑盒知识蒸馏，在多个基准测试中优于传统序列级知识蒸馏。


<details>
  <summary>Details</summary>
Motivation: 传统黑盒蒸馏方法只能从教师模型的文本输出学习，无法访问内部logits或参数，限制了蒸馏效果。需要一种更有效的黑盒蒸馏方法。

Method: 将学生LLM作为生成器，训练判别器区分学生和教师模型的响应，形成极小极大博弈。判别器作为在线奖励模型，与学生模型协同进化，提供稳定、自适应的反馈。

Result: GAD在多个基准测试中一致优于传统序列级知识蒸馏。使用GAD训练的Qwen2.5-14B-Instruct学生模型在LMSYS-Chat自动评估中与教师模型GPT-5-Chat表现相当。

Conclusion: GAD为黑盒LLM蒸馏提供了一个有前景且有效的范式，能够在不访问教师模型内部信息的情况下实现高质量的模型蒸馏。

Abstract: Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.

</details>


### [5] [ParoQuant: Pairwise Rotation Quantization for Efficient Reasoning LLM Inference](https://arxiv.org/abs/2511.10645)
*Yesheng Liang,Haisheng Chen,Song Han,Zhijian Liu*

Main category: cs.CL

TL;DR: ParoQuant是一种仅权重的后训练量化方法，通过成对旋转量化和通道级缩放来抑制异常值，减小动态范围，在推理任务上比AWQ平均提升2.4%准确率，且开销低于10%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型权重和激活中的异常值会导致量化误差和精度下降，特别是在推理任务中误差会在长思维链中累积。现有方法要么无法充分抑制异常值，要么在推理时引入显著开销。

Method: 结合硬件高效的独立Givens旋转和通道级缩放，均衡通道间幅度并缩小每个量化组内的动态范围，同时协同设计推理内核以充分利用GPU并行性。

Result: 在推理任务上比AWQ平均提升2.4%准确率，且运行时开销低于10%。

Conclusion: ParoQuant为推理大语言模型提供了更高效和准确的部署途径。

Abstract: Weight-only post-training quantization (PTQ) compresses the weights of Large Language Models (LLMs) into low-precision representations to reduce memory footprint and accelerate inference. However, the presence of outliers in weights and activations often leads to large quantization errors and severe accuracy degradation, especially in recent reasoning LLMs where errors accumulate across long chains of thought. Existing PTQ methods either fail to sufficiently suppress outliers or introduce significant overhead during inference. In this paper, we propose Pairwise Rotation Quantization (ParoQuant), a weight-only PTQ method that combines hardware-efficient and optimizable independent Givens rotations with channel-wise scaling to even out the magnitude across channels and narrow the dynamic range within each quantization group. We further co-design the inference kernel to fully exploit GPU parallelism and keep the rotations and scaling lightweight at runtime. ParoQuant achieves an average 2.4% accuracy improvement over AWQ on reasoning tasks with less than 10% overhead. This paves the way for more efficient and accurate deployment of reasoning LLMs.

</details>
