<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 24]
- [cs.IR](#cs.IR) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Multi-Personality Generation of LLMs at Decoding-time](https://arxiv.org/abs/2511.01891)
*Rongxin Chen,Yunfan Li,Yige Yuan,Bingbing Xu,Huawei Shen*

Main category: cs.CL

TL;DR: 提出了一种新的多个性生成框架MPG，通过解码时组合范式实现多个性控制，无需额外训练或稀缺的多维模型，利用单维模型中的隐式密度比作为"免费午餐"来重构任务。


<details>
  <summary>Details</summary>
Motivation: 现有基于重训练的方法成本高且可扩展性差，而解码时方法通常依赖外部模型或启发式方法，限制了灵活性和鲁棒性。

Method: 设计了推测性块级拒绝采样(SCR)，以块为单位生成响应并通过滑动窗口内的估计阈值并行验证，显著降低计算开销。

Result: 在MBTI个性和角色扮演任务上的实验表明MPG的有效性，显示出高达16%-18%的改进。

Conclusion: MPG框架能够灵活控制多个性，无需依赖稀缺的多维模型或额外训练，在保持高质量生成的同时显著降低计算开销。

Abstract: Multi-personality generation for LLMs, enabling simultaneous embodiment of
multiple personalization attributes, is a fundamental challenge. Existing
retraining-based approaches are costly and poorly scalable, while decoding-time
methods often rely on external models or heuristics, limiting flexibility and
robustness. In this paper, we propose a novel Multi-Personality Generation
(MPG) framework under the decoding-time combination paradigm. It flexibly
controls multi-personality without relying on scarce multi-dimensional models
or extra training, leveraging implicit density ratios in single-dimensional
models as a "free lunch" to reformulate the task as sampling from a target
strategy aggregating these ratios. To implement MPG efficiently, we design
Speculative Chunk-level based Rejection sampling (SCR), which generates
responses in chunks and parallelly validates them via estimated thresholds
within a sliding window. This significantly reduces computational overhead
while maintaining high-quality generation. Experiments on MBTI personality and
Role-Playing demonstrate the effectiveness of MPG, showing improvements up to
16%-18%. Code and data are available at https://github.com/Libra117/MPG .

</details>


### [2] [Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation](https://arxiv.org/abs/2511.02358)
*Wongyu Kim,Hochang Lee,Sanghak Lee,Yoonsung Kim,Jaehyun Park*

Main category: cs.CL

TL;DR: M-Solomon是一个多模态嵌入器，能够自适应地决定何时进行查询增强，只在必要时进行增强，从而在保持性能的同时显著降低嵌入延迟。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的嵌入器对所有查询都进行增强，导致显著的嵌入延迟，且某些查询的增强反而会损害性能。此外，先前方法未在多模态环境中探索。

Method: 首先将训练数据集中的查询分为需要增强和不需要增强两组；然后利用强大的多模态LLM为需要增强的查询生成适当的增强内容；最后通过自适应查询增强，M-Solomon只在必要时进行增强。

Result: 实验结果表明，M-Solomon不仅大幅超越了不进行增强的基线，也优于始终进行增强的基线，同时提供了更快的嵌入延迟。

Conclusion: M-Solomon通过自适应查询增强策略，在多模态环境中实现了更好的性能和更低的延迟，解决了现有方法中不必要的增强问题。

Abstract: Query augmentation makes queries more meaningful by appending further
information to the queries to find relevant documents. Current studies have
proposed Large Language Model (LLM)-based embedders, which learn representation
for embedding and generation for query augmentation in a multi-task manner by
leveraging the generative capabilities of LLM. During inference, these jointly
trained embedders have conducted query augmentation followed by embedding,
showing effective results. However, augmenting every query leads to substantial
embedding latency and query augmentation can be detrimental to performance for
some queries. Also, previous methods have not been explored in multimodal
environments. To tackle these problems, we propose M-Solomon, a universal
multimodal embedder that can adaptively determine when to augment queries. Our
approach first divides the queries of the training datasets into two groups at
the dataset level. One includes queries that require augmentation and the other
includes queries that do not. Then, we introduces a synthesis process that
generates appropriate augmentations for queries that require them by leveraging
a powerful Multimodal LLM (MLLM). Next, we present adaptive query augmentation.
Through this step, M-Solomon can conduct query augmentation only when necessary
by learning to generate synthetic augmentations with the prefix /augment for
queries that demand them and to generate the simple string /embed for others.
Experimental results showed that M-Solomon not only surpassed the baseline
without augmentation by a large margin but also outperformed the baseline that
always used augmentation, providing much faster embedding latency.

</details>


### [3] [Rethinking LLM Human Simulation: When a Graph is What You Need](https://arxiv.org/abs/2511.02135)
*Joseph Suh,Suhong Moon,Serina Chang*

Main category: cs.CL

TL;DR: GEMS是一种基于图神经网络的轻量级人类模拟方法，在离散选择模拟任务中表现优于大型语言模型，效率高三个数量级，且具有更好的可解释性和透明度。


<details>
  <summary>Details</summary>
Motivation: 研究是否必须使用大型语言模型进行人类模拟，或者更小、领域特定的模型是否足够，特别是在离散选择模拟任务中。

Method: 将离散选择模拟任务建模为图上的链接预测问题，使用图神经网络，仅在需要时整合语言表示。

Result: 在三个关键设置和三个模拟数据集上的评估显示，GEMS达到或优于强LLM基线的准确率，效率高三个数量级。

Conclusion: 基于图的建模是LLM进行人类模拟的轻量级替代方案，具有高效、可解释和透明的优势。

Abstract: Large language models (LLMs) are increasingly used to simulate humans, with
applications ranging from survey prediction to decision-making. However, are
LLMs strictly necessary, or can smaller, domain-grounded models suffice? We
identify a large class of simulation problems in which individuals make choices
among discrete options, where a graph neural network (GNN) can match or surpass
strong LLM baselines despite being three orders of magnitude smaller. We
introduce Graph-basEd Models for human Simulation (GEMS), which casts discrete
choice simulation tasks as a link prediction problem on graphs, leveraging
relational knowledge while incorporating language representations only when
needed. Evaluations across three key settings on three simulation datasets show
that GEMS achieves comparable or better accuracy than LLMs, with far greater
efficiency, interpretability, and transparency, highlighting the promise of
graph-based modeling as a lightweight alternative to LLMs for human simulation.
Our code is available at https://github.com/schang-lab/gems.

</details>


### [4] [Beyond Single Embeddings: Capturing Diverse Targets with Multi-Query Retrieval](https://arxiv.org/abs/2511.02770)
*Hung-Ting Chen,Xiang Liu,Shauli Ravfogel,Eunsol Choi*

Main category: cs.CL

TL;DR: AMER是一种新的检索器架构，通过自回归生成多个查询向量来捕获查询的多模态相关文档分布，相比单向量检索器在目标文档嵌入距离较大时表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有文本检索器通常只生成一个查询向量来检索相关文档，但查询的相关文档条件分布可能是多模态的（例如查询的不同解释），这限制了检索性能。

Method: 开发了自回归多嵌入检索器（AMER），该模型自回归地生成多个查询向量，所有预测的查询向量都用于从语料库中检索文档。

Result: 在合成向量化数据上，该方法能完美捕获多个目标分布，性能比单嵌入模型好4倍。在真实世界多答案检索数据集上，AMER在两个数据集上分别比单嵌入基线相对提升4%和21%，在目标文档嵌入相似度较低的子集上增益更大。

Conclusion: 多查询向量检索器具有潜力，为未来工作开辟了新方向。

Abstract: Most text retrievers generate \emph{one} query vector to retrieve relevant
documents. Yet, the conditional distribution of relevant documents for the
query may be multimodal, e.g., representing different interpretations of the
query. We first quantify the limitations of existing retrievers. All retrievers
we evaluate struggle more as the distance between target document embeddings
grows. To address this limitation, we develop a new retriever architecture,
\emph{A}utoregressive \emph{M}ulti-\emph{E}mbedding \emph{R}etriever (AMER).
Our model autoregressively generates multiple query vectors, and all the
predicted query vectors are used to retrieve documents from the corpus. We show
that on the synthetic vectorized data, the proposed method could capture
multiple target distributions perfectly, showing 4x better performance than
single embedding model. We also fine-tune our model on real-world multi-answer
retrieval datasets and evaluate in-domain. AMER presents 4 and 21\% relative
gains over single-embedding baselines on two datasets we evaluate on.
Furthermore, we consistently observe larger gains on the subset of dataset
where the embeddings of the target documents are less similar to each other. We
demonstrate the potential of using a multi-query vector retriever and open up a
new direction for future work.

</details>


### [5] [IG-Pruning: Input-Guided Block Pruning for Large Language Models](https://arxiv.org/abs/2511.02213)
*Kangyu Qiao,Shaolei Zhang,Yang Feng*

Main category: cs.CL

TL;DR: IG-Pruning是一种新颖的输入感知块级剪枝方法，通过动态选择层掩码来减少大型语言模型的计算成本，无需大量训练即可实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型计算需求的增长，高效推理对于实际部署变得至关重要。现有深度剪枝方法依赖固定块掩码，在不同任务和输入上可能导致次优性能。

Method: 提出两阶段方法：(1) 通过语义聚类和L0优化发现多样化掩码候选；(2) 在推理时无需大量训练即可实现高效动态剪枝。

Result: 实验结果表明，该方法在性能上持续优于最先进的静态深度剪枝方法。

Conclusion: IG-Pruning特别适合资源受限的部署场景，为大型语言模型的高效推理提供了有效的动态剪枝解决方案。

Abstract: With the growing computational demands of large language models (LLMs),
efficient inference has become increasingly critical for practical deployment.
Depth pruning has emerged as a promising approach for reducing the
computational costs of large language models by removing transformer layers.
However, existing methods typically rely on fixed block masks, which can lead
to suboptimal performance across different tasks and inputs. In this paper, we
propose IG-Pruning, a novel input-aware block-wise pruning method that
dynamically selects layer masks at inference time. Our approach consists of two
stages: (1) Discovering diverse mask candidates through semantic clustering and
L0 optimization, and (2) Implementing efficient dynamic pruning without the
need for extensive training. Experimental results demonstrate that our method
consistently outperforms state-of-the-art static depth pruning methods, making
it particularly suitable for resource-constrained deployment scenarios.

</details>


### [6] [Demo: Statistically Significant Results On Biases and Errors of LLMs Do Not Guarantee Generalizable Results](https://arxiv.org/abs/2511.02246)
*Jonathan Liu,Haoling Qiu,Jonathan Lasko,Damianos Karakos,Mahsa Yarmohammadi,Mark Dredze*

Main category: cs.CL

TL;DR: 开发了一个基础设施来自动生成查询并评估医疗聊天机器人在不同人口统计因素下的表现，发现LLM评估者之间一致性较低，建议使用多个LLM评估器以避免统计显著但不具普适性的结果。


<details>
  <summary>Details</summary>
Motivation: 医疗聊天机器人在涉及人口统计等非医疗因素时必须提供一致的建议，但现有LLM存在幻觉、遗漏和偏见问题，需要了解其在什么条件下会表现不佳。

Method: 开发了自动生成查询的基础设施：1）通过采样患者人口统计、病史、疾病和写作风格创建真实问题；2）使用LLM-as-a-judge和代理工作流程进行幻觉和遗漏检测，以及治疗类别检测。

Result: LLM注释者之间一致性较低（平均Cohen's Kappa κ=0.118），只有特定的（回答，评估）LLM对在写作风格、性别和种族方面产生统计显著差异。

Conclusion: 建议使用多个LLM作为评估器以避免得出统计显著但不具普适性的结果，特别是在缺乏真实数据的情况下，并建议发布LLM间一致性指标以提高透明度。

Abstract: Recent research has shown that hallucinations, omissions, and biases are
prevalent in everyday use-cases of LLMs. However, chatbots used in medical
contexts must provide consistent advice in situations where non-medical factors
are involved, such as when demographic information is present. In order to
understand the conditions under which medical chatbots fail to perform as
expected, we develop an infrastructure that 1) automatically generates queries
to probe LLMs and 2) evaluates answers to these queries using multiple
LLM-as-a-judge setups and prompts. For 1), our prompt creation pipeline samples
the space of patient demographics, histories, disorders, and writing styles to
create realistic questions that we subsequently use to prompt LLMs. In 2), our
evaluation pipeline provides hallucination and omission detection using
LLM-as-a-judge as well as agentic workflows, in addition to LLM-as-a-judge
treatment category detectors. As a baseline study, we perform two case studies
on inter-LLM agreement and the impact of varying the answering and evaluation
LLMs. We find that LLM annotators exhibit low agreement scores (average Cohen's
Kappa $\kappa=0.118$), and only specific (answering, evaluation) LLM pairs
yield statistically significant differences across writing styles, genders, and
races. We recommend that studies using LLM evaluation use multiple LLMs as
evaluators in order to avoid arriving at statistically significant but
non-generalizable results, particularly in the absence of ground-truth data. We
also suggest publishing inter-LLM agreement metrics for transparency. Our code
and dataset are available here:
https://github.com/BBN-E/medic-neurips-2025-demo.

</details>


### [7] [LTD-Bench: Evaluating Large Language Models by Letting Them Draw](https://arxiv.org/abs/2511.02347)
*Liuhao Lin,Ke Li,Zihan Xu,Yuchen Shi,Yulei Qin,Yan Zhang,Xing Sun,Rongrong Ji*

Main category: cs.CL

TL;DR: LTD-Bench是一个创新的基准测试，通过要求模型生成绘图或可执行代码，将LLM评估从抽象分数转换为直接可观察的视觉输出，暴露了当前LLM在空间推理方面的严重缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估范式存在关键盲点，依赖不透明的数值指标掩盖了空间推理的基本限制，无法直观理解模型能力，导致报告性能与实际能力之间存在危险脱节。

Method: LTD-Bench采用全面方法论，包含互补的生成任务（测试空间想象力）和识别任务（评估空间感知），跨越三个渐进难度级别，系统评估语言-空间映射的两个关键方向。

Result: 对最先进模型的大量实验揭示了一个令人担忧的能力差距：即使在传统基准测试中表现优异的LLM，在建立语言和空间概念之间的双向映射方面也表现出严重缺陷。

Conclusion: LTD-Bench的视觉输出能够进行强大的诊断分析，为研究模型相似性提供了潜在方法，揭示了LLM作为真正世界模型的根本局限性。

Abstract: Current evaluation paradigms for large language models (LLMs) represent a
critical blind spot in AI research--relying on opaque numerical metrics that
conceal fundamental limitations in spatial reasoning while providing no
intuitive understanding of model capabilities. This deficiency creates a
dangerous disconnect between reported performance and practical abilities,
particularly for applications requiring physical world understanding. We
introduce LTD-Bench, a breakthrough benchmark that transforms LLM evaluation
from abstract scores to directly observable visual outputs by requiring models
to generate drawings through dot matrices or executable code. This approach
makes spatial reasoning limitations immediately apparent even to non-experts,
bridging the fundamental gap between statistical performance and intuitive
assessment. LTD-Bench implements a comprehensive methodology with complementary
generation tasks (testing spatial imagination) and recognition tasks (assessing
spatial perception) across three progressively challenging difficulty levels,
methodically evaluating both directions of the critical language-spatial
mapping. Our extensive experiments with state-of-the-art models expose an
alarming capability gap: even LLMs achieving impressive results on traditional
benchmarks demonstrate profound deficiencies in establishing bidirectional
mappings between language and spatial concept--a fundamental limitation that
undermines their potential as genuine world models. Furthermore, LTD-Bench's
visual outputs enable powerful diagnostic analysis, offering a potential
approach to investigate model similarity.

</details>


### [8] [LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for LLMs in Chinese Context](https://arxiv.org/abs/2511.02366)
*Yudong Li,Zhongliang Yang,Kejiang Chen,Wenxuan Wang,Tianxin Zhang,Sifang Wan,Kecheng Wang,Haitian Li,Xu Wang,Lefan Cheng,Youdan Yang,Baocheng Chen,Ziyu Liu,Yufei Sun,Liyan Wu,Wenya Wen,Xingchi Gu,Peiru Yang*

Main category: cs.CL

TL;DR: LiveSecBench是一个专门针对中文LLM应用场景的动态安全基准测试，评估模型在合法性、伦理、事实性、隐私、对抗鲁棒性和推理安全等六个维度的表现，并持续更新以纳入新的威胁向量。


<details>
  <summary>Details</summary>
Motivation: 现有安全基准测试主要针对英文场景，缺乏针对中文语言和特定法律社会框架的评估标准，需要专门的中文LLM安全基准来评估模型在中国语境下的安全性。

Method: 构建包含六个关键维度的动态评估框架（合法性、伦理、事实性、隐私、对抗鲁棒性、推理安全），采用持续更新机制纳入新威胁向量，目前已评估18个LLM。

Result: 建立了首个针对中文LLM的动态安全基准测试，发布了v251030版本评估结果，公开了排行榜，为中文AI安全提供了评估标准。

Conclusion: LiveSecBench填补了中文LLM安全评估的空白，通过动态更新机制保持相关性，为中文语言环境下的AI安全提供了重要基准工具。

Abstract: In this work, we propose LiveSecBench, a dynamic and continuously updated
safety benchmark specifically for Chinese-language LLM application scenarios.
LiveSecBench evaluates models across six critical dimensions (Legality, Ethics,
Factuality, Privacy, Adversarial Robustness, and Reasoning Safety) rooted in
the Chinese legal and social frameworks. This benchmark maintains relevance
through a dynamic update schedule that incorporates new threat vectors, such as
the planned inclusion of Text-to-Image Generation Safety and Agentic Safety in
the next update. For now, LiveSecBench (v251030) has evaluated 18 LLMs,
providing a landscape of AI safety in the context of Chinese language. The
leaderboard is publicly accessible at https://livesecbench.intokentech.cn/.

</details>


### [9] [AyurParam: A State-of-the-Art Bilingual Language Model for Ayurveda](https://arxiv.org/abs/2511.02374)
*Mohd Nauman,Sravan Gvm,Vijay Devane,Shyam Pawar,Viraj Thakur,Kundeshwar Pundalik,Piyush Sawarkar,Rohit Saluja,Maunendra Desarkar,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: AyurParam-2.9B是一个专门针对阿育吠陀医学领域优化的双语语言模型，在1.5-3B参数规模的开源模型中表现最佳，甚至能与更大模型竞争。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在需要深厚文化、语言和专业知识的专业领域表现不佳，特别是阿育吠陀等传统医学系统包含的复杂知识，主流LLMs无法准确解释和应用。

Method: 从Param-1-2.9B模型微调，使用专家精心策划的阿育吠陀数据集，包含经典文本和临床指导，采用上下文感知、推理和客观问答形式，支持英语和印地语，并有严格的事实准确性和教学清晰度标注协议。

Result: 在BhashaBench-Ayur基准测试中，AyurParam不仅超越了其规模类别（1.5-3B参数）的所有开源指令调优模型，还表现出与更大模型相当或更优的性能。

Conclusion: AyurParam的结果强调了在提供可靠、文化契合的专业医学知识AI时，真实领域适应和高质量监督的必要性。

Abstract: Current large language models excel at broad, general-purpose tasks, but
consistently underperform when exposed to highly specialized domains that
require deep cultural, linguistic, and subject-matter expertise. In particular,
traditional medical systems such as Ayurveda embody centuries of nuanced
textual and clinical knowledge that mainstream LLMs fail to accurately
interpret or apply. We introduce AyurParam-2.9B, a domain-specialized,
bilingual language model fine-tuned from Param-1-2.9B using an extensive,
expertly curated Ayurveda dataset spanning classical texts and clinical
guidance. AyurParam's dataset incorporates context-aware, reasoning, and
objective-style Q&A in both English and Hindi, with rigorous annotation
protocols for factual precision and instructional clarity. Benchmarked on
BhashaBench-Ayur, AyurParam not only surpasses all open-source
instruction-tuned models in its size class (1.5--3B parameters), but also
demonstrates competitive or superior performance compared to much larger
models. The results from AyurParam highlight the necessity for authentic domain
adaptation and high-quality supervision in delivering reliable, culturally
congruent AI for specialized medical knowledge.

</details>


### [10] [AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models](https://arxiv.org/abs/2511.02376)
*Aashray Reddy,Andrew Zagula,Nicholas Saban*

Main category: cs.CL

TL;DR: AutoAdv是一个无需训练的多轮越狱攻击框架，通过自适应机制在6轮对话内达到95%的攻击成功率，比单轮攻击提升24%，揭示了当前AI安全机制在多轮对话中的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的安全评估主要关注单轮交互，而真实世界攻击往往通过多轮自适应对话展开，需要开发能应对多轮攻击的评估框架。

Method: AutoAdv结合三种自适应机制：模式管理器从成功攻击中学习改进提示，温度管理器基于失败模式动态调整采样参数，两阶段重写策略先伪装有害请求再迭代优化。

Result: 在Llama-3.1-8B上达到95%攻击成功率，相比单轮基线提升24%，在GPT-4o-mini、Qwen3-235B、Mistral-7B等模型上均显示多轮攻击优于单轮方法。

Conclusion: 当前针对单轮交互优化的对齐策略在多轮对话中无法保持鲁棒性，迫切需要开发多轮感知的防御机制。

Abstract: Large Language Models (LLMs) remain vulnerable to jailbreaking attacks where
adversarial prompts elicit harmful outputs, yet most evaluations focus on
single-turn interactions while real-world attacks unfold through adaptive
multi-turn conversations. We present AutoAdv, a training-free framework for
automated multi-turn jailbreaking that achieves up to 95% attack success rate
on Llama-3.1-8B within six turns a 24 percent improvement over single turn
baselines. AutoAdv uniquely combines three adaptive mechanisms: a pattern
manager that learns from successful attacks to enhance future prompts, a
temperature manager that dynamically adjusts sampling parameters based on
failure modes, and a two-phase rewriting strategy that disguises harmful
requests then iteratively refines them. Extensive evaluation across commercial
and open-source models (GPT-4o-mini, Qwen3-235B, Mistral-7B) reveals persistent
vulnerabilities in current safety mechanisms, with multi-turn attacks
consistently outperforming single-turn approaches. These findings demonstrate
that alignment strategies optimized for single-turn interactions fail to
maintain robustness across extended conversations, highlighting an urgent need
for multi-turn-aware defenses.

</details>


### [11] [Merging Continual Pretraining Models for Domain-Specialized LLMs: A Case Study in Finance](https://arxiv.org/abs/2511.02451)
*Kentaro Ueda,François Portet,Hirohiko Suwa,Keiichi Yasumoto*

Main category: cs.CL

TL;DR: 本研究首次系统分析了持续预训练（CPT）模型融合方法，通过在金融、数学和日语三个专业领域创建专家模型，评估了三种融合方法在金融基准测试上的表现，发现模型融合能恢复通用知识并产生跨领域新兴技能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在专业领域（如金融）表现不佳，需要融合领域知识、数学推理和多语言处理等多种技能。直接进行多技能训练成本高昂且不稳定，而融合领域特定的CPT专家模型是一个实用替代方案，但CPT模型融合方法尚未得到充分研究。

Method: 创建金融、数学和日语三个专业领域的CPT专家模型，提出三阶段评估框架（知识恢复、互补性和新兴性），在包含18个任务、8个数据集的综合金融基准上评估三种模型融合方法：任务算术、TIES和DARE-TIES。

Result: 融合专家模型与基础模型能恢复CPT过程中丢失的通用知识；融合多个专家模型能提升性能并产生跨领域新兴技能；任务算术方法表现强劲但对超参数敏感，TIES方法更稳健；模型相似性与融合成功相关，但新兴技能取决于更复杂因素。

Conclusion: 本研究为CPT模型融合提供了首个基础性分析，建立了原则性框架，为从现有资产构建多技能LLM提供了明确指导，证明了模型融合在构建专业领域LLM方面的可行性和有效性。

Abstract: While LLMs excel at general tasks, they struggle in specialized domains like
finance, requiring diverse skills in domain knowledge, mathematical reasoning,
and multilingual processing. Merging domain-specific Continual Pre-training
(CPT) "experts" offers a practical alternative to costly and unstable
multi-skill training. However, unlike established Supervised Fine-Tuning (SFT)
model-based merging, CPT model merging remains largely unexplored. We address
this gap by creating financial LLMs from experts in finance, math, and
Japanese. We propose a three-stage evaluation focusing on knowledge recovery,
complementarity, and emergence, and assess three merging methods (Task
Arithmetic, TIES, and DARE-TIES) on a comprehensive financial benchmark curated
from 18 tasks across 8 established datasets. Results show that merging an
expert with its base model recovers general knowledge lost during CPT, while
merging experts improves performance and can yield emergent cross-domain
skills. Among the methods, Task Arithmetic performs strongly but is
hyperparameter-sensitive, whereas TIES is more robust. Our findings also
suggest that while model similarity correlates with merging success, emergent
skills depend on more complex factors. This work presents the first
foundational analysis of CPT model merging, establishing a principled framework
and providing clear guidance for building multi-skill LLMs from existing
assets.

</details>


### [12] [Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas](https://arxiv.org/abs/2511.02458)
*Giulia Iadisernia,Carolina Camassa*

Main category: cs.CL

TL;DR: 本文评估了基于角色的提示是否能提升大语言模型在宏观经济预测任务中的表现。研究发现GPT-4o与人类预测者准确度相当，但角色描述对预测准确性没有显著影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索角色提示是否能改善LLM在宏观经济预测中的表现，特别是与人类专家预测进行比较。

Method: 使用PersonaHub语料库中的2,368个经济学相关角色来提示GPT-4o，复制ECB专业预测者调查，涵盖50个季度（2013-2025年），并比较有角色提示与无角色提示的基准预测。

Result: 1. GPT-4o与人类预测者准确度相似，统计显著但实际差异不大；2. 在2024-2025年样本外数据上保持竞争力；3. 角色描述对预测准确性没有可衡量的优势。

Conclusion: GPT-4o在提供相关上下文数据时，即使在样本外宏观经济事件上也能达到有竞争力的预测准确性，但多样化的提示会产生与人类面板相比显著同质化的预测结果。

Abstract: We evaluate whether persona-based prompting improves Large Language Model
(LLM) performance on macroeconomic forecasting tasks. Using 2,368
economics-related personas from the PersonaHub corpus, we prompt GPT-4o to
replicate the ECB Survey of Professional Forecasters across 50 quarterly rounds
(2013-2025). We compare the persona-prompted forecasts against the human
experts panel, across four target variables (HICP, core HICP, GDP growth,
unemployment) and four forecast horizons. We also compare the results against
100 baseline forecasts without persona descriptions to isolate its effect. We
report two main findings. Firstly, GPT-4o and human forecasters achieve
remarkably similar accuracy levels, with differences that are statistically
significant yet practically modest. Our out-of-sample evaluation on 2024-2025
data demonstrates that GPT-4o can maintain competitive forecasting performance
on unseen events, though with notable differences compared to the in-sample
period. Secondly, our ablation experiment reveals no measurable forecasting
advantage from persona descriptions, suggesting these prompt components can be
omitted to reduce computational costs without sacrificing accuracy. Our results
provide evidence that GPT-4o can achieve competitive forecasting accuracy even
on out-of-sample macroeconomic events, if provided with relevant context data,
while revealing that diverse prompts produce remarkably homogeneous forecasts
compared to human panels.

</details>


### [13] [Smart-Hiring: An Explainable end-to-end Pipeline for CV Information Extraction and Job Matching](https://arxiv.org/abs/2511.02537)
*Kenza Khelkhal,Dihia Lanasri*

Main category: cs.CL

TL;DR: Smart-Hiring是一个端到端的NLP流水线，用于从非结构化简历中自动提取结构化信息，并将候选人与职位描述进行语义匹配。


<details>
  <summary>Details</summary>
Motivation: 传统招聘过程中手动筛选数百份简历耗时耗力、容易出错且存在人为偏见，需要自动化解决方案。

Method: 结合文档解析、命名实体识别和上下文文本嵌入技术，在共享向量空间中编码简历和职位描述以计算相似度分数。

Result: 在真实世界数据集上的实验表明，该系统在保持高可解释性和透明度的同时实现了有竞争力的匹配精度。

Conclusion: 该工作为招聘分析提供了一个可扩展且实用的NLP框架，并为偏见缓解、公平感知建模和数据驱动招聘解决方案的大规模部署指明了方向。

Abstract: Hiring processes often involve the manual screening of hundreds of resumes
for each job, a task that is time and effort consuming, error-prone, and
subject to human bias. This paper presents Smart-Hiring, an end-to-end Natural
Language Processing (NLP) pipeline de- signed to automatically extract
structured information from unstructured resumes and to semantically match
candidates with job descriptions. The proposed system combines document
parsing, named-entity recognition, and contextual text embedding techniques to
capture skills, experience, and qualifications. Using advanced NLP technics,
Smart-Hiring encodes both resumes and job descriptions in a shared vector space
to compute similarity scores between candidates and job postings. The pipeline
is modular and explainable, allowing users to inspect extracted entities and
matching rationales. Experiments were conducted on a real-world dataset of
resumes and job descriptions spanning multiple professional domains,
demonstrating the robustness and feasibility of the proposed approach. The
system achieves competitive matching accuracy while preserving a high degree of
interpretability and transparency in its decision process. This work introduces
a scalable and practical NLP frame- work for recruitment analytics and outlines
promising directions for bias mitigation, fairness-aware modeling, and
large-scale deployment of data-driven hiring solutions.

</details>


### [14] [The Analysis of Lexical Errors in Machine Translation from English into Romanian](https://arxiv.org/abs/2511.02587)
*Angela Stamatie*

Main category: cs.CL

TL;DR: 本研究分析了谷歌翻译将WHO和Gavi组织关于COVID-19的官方医疗文本从英语翻译成罗马尼亚语时产生的词汇错误，旨在通过分析230个翻译文本来改进机器翻译的词汇选择和减少错误。


<details>
  <summary>Details</summary>
Motivation: 谷歌翻译在翻译COVID-19相关官方医疗信息时存在词汇错误，这可能影响信息的准确性和患者安全，因此需要分析这些错误以改进机器翻译质量。

Method: 对230个从英语翻译成罗马尼亚语的文本进行综合分析，这些文本包括WHO、Gavi组织和药品说明书中关于COVID-19的官方信息。

Result: 研究发现谷歌翻译在专业医疗术语翻译中存在词汇选择错误，特别是在疫苗成分、剂量说明、副作用等专业领域的翻译准确性有待提高。

Conclusion: 通过系统分析机器翻译的词汇错误，可以为改进谷歌翻译的算法提供具体方向，特别是在专业医疗领域的翻译质量提升方面具有重要意义。

Abstract: The research explores error analysis in the performance of translating by
Machine Translation from English into Romanian, and it focuses on lexical
errors found in texts which include official information, provided by the World
Health Organization (WHO), the Gavi Organization, by the patient information
leaflet (the information about the active ingredients of the vaccines or the
medication, the indications, the dosage instructions, the storage instructions,
the side effects and warning, etc.). All of these texts are related to Covid-19
and have been translated by Google Translate, a multilingual Machine
Translation that was created by Google. In the last decades, Google has
actively worked to develop a more accurate and fluent automatic translation
system. This research, specifically focused on improving Google Translate, aims
to enhance the overall quality of Machine Translation by achieving better
lexical selection and by reducing errors. The investigation involves a
comprehensive analysis of 230 texts that have been translated from English into
Romanian.

</details>


### [15] [Next Token Knowledge Tracing: Exploiting Pretrained LLM Representations to Decode Student Behaviour](https://arxiv.org/abs/2511.02599)
*Max Norris,Kobi Gal,Sahan Bulathwela*

Main category: cs.CL

TL;DR: NTKT将知识追踪重构为使用预训练大语言模型的下一词预测任务，通过将学生历史和问题内容表示为文本序列，显著提升了预测性能并更好地泛化到冷启动场景。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪模型通常只使用回答正确性和元数据，忽略了问题文本这一重要的教学洞察来源，这限制了预测性能。

Method: 提出NTKT方法，将知识追踪重构为下一词预测任务，使用预训练大语言模型，将学生历史和问题内容表示为文本序列。

Result: 实验显著优于最先进的神经知识追踪模型，在冷启动问题和用户场景下具有更好的泛化能力。

Conclusion: 研究强调了问题内容在知识追踪中的重要性，并展示了利用预训练大语言模型表示来更有效建模学生学习的益处。

Abstract: Modelling student knowledge is a key challenge when leveraging AI in
education, with major implications for personalised learning. The Knowledge
Tracing (KT) task aims to predict how students will respond to educational
questions in learning environments, based on their prior interactions. Existing
KT models typically use response correctness along with metadata like skill
tags and timestamps, often overlooking the question text, which is an important
source of pedagogical insight. This omission poses a lost opportunity while
limiting predictive performance. We propose Next Token Knowledge Tracing
(NTKT), a novel approach that reframes KT as a next-token prediction task using
pretrained Large Language Models (LLMs). NTKT represents both student histories
and question content as sequences of text, allowing LLMs to learn patterns in
both behaviour and language. Our series of experiments significantly improves
performance over state-of-the-art neural KT models and generalises much better
to cold-start questions and users. These findings highlight the importance of
question content in KT and demonstrate the benefits of leveraging pretrained
representations of LLMs to model student learning more effectively.

</details>


### [16] [CGES: Confidence-Guided Early Stopping for Efficient and Accurate Self-Consistency](https://arxiv.org/abs/2511.02603)
*Ehsan Aghazadeh,Ahmad Ghasemi,Hedyeh Beyhaghi,Hossein Pishro-Nik*

Main category: cs.CL

TL;DR: CGES是一种贝叶斯框架，通过置信度信号自适应停止采样，在保持准确性的同时显著减少模型调用次数


<details>
  <summary>Details</summary>
Motivation: 解决自一致性策略需要固定调用次数且在正确答案罕见时可能失败的问题

Method: 使用从token概率或奖励模型导出的置信度信号构建候选答案的后验分布，当候选者的后验质量超过阈值时自适应停止采样

Result: 在五个推理基准测试中，平均减少约69%的模型调用次数（如从16.0次降至4.9次），同时保持与自一致性策略相当的准确率

Conclusion: CGES框架在保持推理准确性的同时显著提高了效率，为LLM推理提供了更实用的解决方案

Abstract: Large language models (LLMs) are often queried multiple times at test time,
with predictions aggregated by majority vote. While effective, this
self-consistency strategy (arXiv:2203.11171) requires a fixed number of calls
and can fail when the correct answer is rare. We introduce Confidence-Guided
Early Stopping (CGES), a Bayesian framework that forms posteriors over
candidate answers using scalar confidence signals derived from token
probabilities or reward models. CGES adaptively halts sampling once the
posterior mass of a candidate exceeds a threshold. We provide theoretical
guarantees for both perfectly calibrated confidences and realistic noisy
confidence signals. Across five reasoning benchmarks, CGES reduces the average
number of model calls by about 69 percent (for example, from 16.0 to 4.9) while
matching the accuracy of self-consistency within 0.06 percentage points.

</details>


### [17] [The Realignment Problem: When Right becomes Wrong in LLMs](https://arxiv.org/abs/2511.02623)
*Aakash Sen Sharma,Debdeep Sanyal,Vivek Srivastava,Shirish Karande,Murari Mandal*

Main category: cs.CL

TL;DR: TRACE框架通过程序化策略应用解决大语言模型与现实对齐差距问题，实现精确的策略更新而不会降低模型性能


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的对齐实践产生静态、脆弱且维护成本高的模型，无法跟上不断发展的规范和政策，存在对齐-现实差距问题

Method: TRACE框架通过程序化筛选现有偏好数据与新策略的冲突，使用对齐影响评分识别高影响冲突，并应用混合优化来干净地反转、丢弃或保留偏好，同时保护模型性能

Result: 在多种模型家族上的实证结果显示，TRACE在复杂策略转变下实现了稳健的重新对齐，强制执行新原则而不降低通用能力

Conclusion: TRACE为维持大语言模型对齐提供了一个可扩展、动态且成本效益高的范式，为可持续和负责任的AI部署奠定了基础

Abstract: The alignment of Large Language Models (LLMs) with human values is central to
their safe deployment, yet current practice produces static, brittle, and
costly-to-maintain models that fail to keep pace with evolving norms and
policies. This misalignment, which we term the Alignment-Reality Gap, poses a
growing challenge for reliable long-term use. Existing remedies are inadequate:
large-scale re-annotation is economically prohibitive, and standard unlearning
methods act as blunt instruments that erode utility rather than enable precise
policy updates. We introduce TRACE (Triage and Re-align by Alignment Conflict
Evaluation), a framework for principled unlearning that reconceives
re-alignment as a programmatic policy application problem. TRACE
programmatically triages existing preference data against a new policy,
identifies high-impact conflicts via a alignment impact score, and applies a
hybrid optimization that cleanly inverts, discards, or preserves preferences
while safeguarding model performance. Empirical results show that TRACE
achieves robust re-alignment across diverse model families (Qwen2.5-7B,
Gemma-2-9B, Llama-3.1-8B). On both synthetic benchmarks and the PKU-SafeRLHF
dataset under complex policy shift, TRACE enforces new principles without
degrading general capabilities. Our work establishes a scalable, dynamic, and
cost-effective paradigm for maintaining LLM alignment, providing a foundation
for sustainable and responsible AI deployment.

</details>


### [18] [Understanding New-Knowledge-Induced Factual Hallucinations in LLMs: Analysis, Solution, and Interpretation](https://arxiv.org/abs/2511.02626)
*Renfei Dang,Peng Hu,Changjiang Gao,Shujian Huang*

Main category: cs.CL

TL;DR: 研究发现，在LLMs微调中引入新知识会导致已知信息测试时产生事实幻觉，特定知识类型的高陌生度是主要驱动因素。提出了KnownPatch方法，通过在训练后期添加少量已知知识样本来缓解幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究未深入探讨新知识微调导致的事实幻觉的具体表现和机制，本研究旨在填补这一空白。

Method: 设计了受控数据集Biography-Reasoning，在多种知识类型和两种任务类型（知识问答和知识推理）上进行细粒度分析，并提出了KnownPatch方法来缓解幻觉。

Result: 发现当特定知识类型完全由新知识组成时，LLMs的幻觉倾向显著增加；KnownPatch方法有效缓解了新知识引起的事实幻觉，并改善了模型性能。

Conclusion: 特定知识类型的高陌生度是事实幻觉的主要驱动因素，KnownPatch方法通过修复模型对关键实体的注意力机制，有效减轻了幻觉问题。

Abstract: Previous studies show that introducing new knowledge during large language
models (LLMs) fine-tuning can lead to the generation of erroneous output when
tested on known information, thereby triggering factual hallucinations.
However, existing studies have not deeply investigated the specific
manifestations and underlying mechanisms of these hallucinations. Our work
addresses this gap by designing a controlled dataset Biography-Reasoning, and
conducting a fine-grained analysis across multiple knowledge types and two task
types, including knowledge question answering (QA) and knowledge reasoning
tasks. We find that when fine-tuned on a dataset in which a specific knowledge
type consists entirely of new knowledge, LLMs exhibit significantly increased
hallucination tendencies. This suggests that the high unfamiliarity of a
particular knowledge type, rather than the overall proportion of new knowledge,
is a stronger driver of hallucinations, and these tendencies can even affect
other knowledge types in QA tasks. To mitigate such factual hallucinations, we
propose KnownPatch, which patches a small number of known knowledge samples in
the later stages of training, effectively alleviating new-knowledge-induced
hallucinations. Through attention analysis, we find that learning new knowledge
reduces the model's attention to key entities in the question, thus causing
excessive focus on the surrounding context, which may increase the risk of
hallucination. Moreover, the attention pattern can propagate to similar
contexts, facilitating the spread of hallucinations to textually similar
questions. Our method effectively mitigates the disruption of new knowledge
learning to the model's attention on key entities, accompanied by improved
performance.

</details>


### [19] [Optimal Singular Damage: Efficient LLM Inference in Low Storage Regimes](https://arxiv.org/abs/2511.02681)
*Mohammadsajad Alipour,Mohammad Mohammadi Amiri*

Main category: cs.CL

TL;DR: 提出了一种名为'最优奇异值损伤'的方法，通过结合低秩近似和稀疏化来高效存储微调后的模型参数更新，在相同内存预算下实现更好的存储效率和准确率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)的巨大尺寸限制了存储和处理能力，即使存储微调后的模型版本也面临挑战。研究发现微调主要影响一小部分参数，需要更高效的存储方案。

Method: 利用微调更新具有低秩和稀疏特性的观察，提出最优奇异值损伤方法，通过选择性地稀疏化低秩近似更新，利用奇异向量的交错重要性来保留最有影响的组件。

Result: 在相同内存预算下，稀疏化的低秩近似比标准低秩近似表现更好，实验证明该方法在存储效率和准确率方面都优于单独使用低秩近似或稀疏化。

Conclusion: 该方法通过结合低秩近似和稀疏化的优势，为高效存储微调模型参数更新提供了有效解决方案，在保持模型表达能力的同时显著提升了存储效率。

Abstract: Large language models (LLMs) are increasingly prevalent across diverse
applications. However, their enormous size limits storage and processing
capabilities to a few well-resourced stakeholders. As a result, most
applications rely on pre-trained LLMs, fine-tuned for specific tasks. However,
even storing the fine-tuned versions of these models remains a significant
challenge due to the wide range of tasks they address. Recently, studies show
that fine-tuning these models primarily affects a small fraction of parameters,
highlighting the need for more efficient storage of fine-tuned models. This
paper focuses on efficient storage of parameter updates in pre-trained models
after fine-tuning. To address this challenge, we leverage the observation that
fine-tuning updates are both low-rank and sparse, which can be utilized for
storage efficiency. However, using only low-rank approximation or
sparsification may discard critical singular components that enhance model
expressivity. We first observe that given the same memory budget, sparsified
low-rank approximations with larger ranks outperform standard low-rank
approximations with smaller ranks. Building on this, we propose our method,
optimal singular damage, that selectively sparsifies low-rank approximated
updates by leveraging the interleaved importance of singular vectors, ensuring
that the most impactful components are retained. We demonstrate through
extensive experiments that our proposed methods lead to significant storage
efficiency and superior accuracy within the same memory budget compared to
employing the low-rank approximation or sparsification individually.

</details>


### [20] [PragExTra: A Multilingual Corpus of Pragmatic Explicitation in Translation](https://arxiv.org/abs/2511.02721)
*Doreen Osmelak,Koel Dutta Chowdhury,Uliana Sentsova,Cristina España-Bonet,Josef van Genabith*

Main category: cs.CL

TL;DR: PragExTra是首个多语言语料库和检测框架，用于识别翻译中的语用显化现象，通过主动学习和人工标注提高了分类器准确率。


<details>
  <summary>Details</summary>
Motivation: 翻译中经常需要添加背景信息来向新受众明确隐含的文化含义，这种现象被称为语用显化，在翻译理论中广泛讨论但很少进行计算建模。

Method: 构建了涵盖8种语言对的语料库，通过空对齐识别候选显化案例，并使用主动学习和人工标注进行精炼。

Result: 结果显示实体和系统级显化最为常见，主动学习将分类器准确率提高了7-8个百分点，在跨语言任务中达到0.88准确率和0.82 F1分数。

Conclusion: PragExTra将语用显化确立为可测量的跨语言现象，并为构建文化感知的机器翻译迈出了重要一步。

Abstract: Translators often enrich texts with background details that make implicit
cultural meanings explicit for new audiences. This phenomenon, known as
pragmatic explicitation, has been widely discussed in translation theory but
rarely modeled computationally. We introduce PragExTra, the first multilingual
corpus and detection framework for pragmatic explicitation. The corpus covers
eight language pairs from TED-Multi and Europarl and includes additions such as
entity descriptions, measurement conversions, and translator remarks. We
identify candidate explicitation cases through null alignments and refined
using active learning with human annotation. Our results show that entity and
system-level explicitations are most frequent, and that active learning
improves classifier accuracy by 7-8 percentage points, achieving up to 0.88
accuracy and 0.82 F1 across languages. PragExTra establishes pragmatic
explicitation as a measurable, cross-linguistic phenomenon and takes a step
towards building culturally aware machine translation. Keywords: translation,
multilingualism, explicitation

</details>


### [21] [AI Diffusion in Low Resource Language Countries](https://arxiv.org/abs/2511.02752)
*Amit Misra,Syed Waqas Zamir,Wassim Hamidouche,Inbal Becker-Reshef,Juan Lavista Ferres*

Main category: cs.CL

TL;DR: LRLCs的AI用户比例比基准低约20%，语言可及性是AI公平扩散的重要独立障碍


<details>
  <summary>Details</summary>
Motivation: 前沿大语言模型在低资源语言上表现不佳，这可能降低AI的实用性，从而减缓低资源语言国家的AI采用

Method: 使用加权回归模型从社会经济和人口因素中分离语言效应

Result: 低资源语言国家的AI用户比例比基准低约20%

Conclusion: 语言可及性是AI公平扩散的重要独立障碍

Abstract: Artificial intelligence (AI) is diffusing globally at unprecedented speed,
but adoption remains uneven. Frontier Large Language Models (LLMs) are known to
perform poorly on low-resource languages due to data scarcity. We hypothesize
that this performance deficit reduces the utility of AI, thereby slowing
adoption in Low-Resource Language Countries (LRLCs). To test this, we use a
weighted regression model to isolate the language effect from socioeconomic and
demographic factors, finding that LRLCs have a share of AI users that is
approximately 20% lower relative to their baseline. These results indicate that
linguistic accessibility is a significant, independent barrier to equitable AI
diffusion.

</details>


### [22] [Controlling Performance and Budget of a Centralized Multi-agent LLM System with Reinforcement Learning](https://arxiv.org/abs/2511.02755)
*Bowen Jin,TJ Collins,Donghan Yu,Mert Cemri,Shenao Zhang,Mengyu Li,Jay Tang,Tian Qin,Zhiyang Xu,Jiarui Lu,Guoli Yin,Jiawei Han,Zirui Wang*

Main category: cs.CL

TL;DR: 提出了CoRL框架，使用强化学习优化多LLM系统的性能与成本权衡，通过中央控制器选择性地协调专家模型，实现成本可控的高效协作。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化多LLM系统对每个输入都调用多个模型，导致推理成本高昂且不可控，需要设计成本高效且可控的集中式协调框架。

Method: 采用强化学习框架，以最大化任务性能和最小化总体推理成本为双重目标，在可控多预算设置下优化性能成本权衡。

Result: 在四个不同基准测试中，CoRL在高预算设置下超越最佳专家LLM，在低预算模式下仍保持强劲性能。

Conclusion: 集中式协调为可扩展且成本高效的多代理LLM系统提供了有效解决方案。

Abstract: Large language models (LLMs) exhibit complementary strengths across domains
and come with varying inference costs, motivating the design of multi-agent LLM
systems where specialized models collaborate efficiently. Existing approaches
predominantly rely on decentralized frameworks, which invoke multiple LLMs for
every input and thus lead to substantial and uncontrolled inference costs. In
this work, we introduce a centralized multi-LLM framework, where a controller
LLM selectively coordinates a pool of expert models in a cost-efficient and
cost-controllable manner. We formulate this coordination problem as
reinforcement learning with dual objectives: maximizing task performance while
minimizing the overall inference cost. In addition, we expect the multi-agent
system to have adapted behavior with different budget conditions during
inference. To this end, we propose CoRL, a reinforcement learning framework
that optimizes the performance cost trade-off in a controllable multi-budget
setting. Experiments on four diverse benchmarks demonstrate that CoRL enables a
single system to surpass the best expert LLM under high-budget settings, while
maintaining strong performance in more economical low-budget modes,
highlighting the effectiveness of centralized coordination for scalable and
cost-efficient multi-agent LLM systems.

</details>


### [23] [MemSearcher: Training LLMs to Reason, Search and Manage Memory via End-to-End Reinforcement Learning](https://arxiv.org/abs/2511.02805)
*Qianhao Yuan,Jie Lou,Zichao Li,Jiawei Chen,Yaojie Lu,Hongyu Lin,Le Sun,Debing Zhang,Xianpei Han*

Main category: cs.CL

TL;DR: MemSearcher是一个搜索代理工作流，通过迭代维护紧凑内存来平衡信息完整性和效率，使用多上下文GRPO强化学习框架优化推理、搜索和内存管理，在多个基准测试中显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统搜索代理要么保留完整交互历史导致上下文过长、计算成本高，要么只使用当前回合丢失重要信息，这种权衡限制了搜索代理的可扩展性。

Method: 提出MemSearcher代理工作流，在每个回合将用户问题与内存融合生成推理轨迹，执行搜索动作，并更新内存仅保留任务解决所需的关键信息；引入多上下文GRPO强化学习框架联合优化推理、搜索策略和内存管理。

Result: 在七个公开基准测试中，相比Search-R1，Qwen2.5-3B-Instruct提升11%，Qwen2.5-7B-Instruct提升12%；3B版本的MemSearcher甚至优于7B基线模型。

Conclusion: 在信息完整性和效率之间取得平衡不仅能提高准确性，还能降低计算开销，MemSearcher通过紧凑内存管理和强化学习优化实现了这一目标。

Abstract: Typical search agents concatenate the entire interaction history into the LLM
context, preserving information integrity but producing long, noisy contexts,
resulting in high computation and memory costs. In contrast, using only the
current turn avoids this overhead but discards essential information. This
trade-off limits the scalability of search agents. To address this challenge,
we propose MemSearcher, an agent workflow that iteratively maintains a compact
memory and combines the current turn with it. At each turn, MemSearcher fuses
the user's question with the memory to generate reasoning traces, perform
search actions, and update memory to retain only information essential for
solving the task. This design stabilizes context length across multi-turn
interactions, improving efficiency without sacrificing accuracy. To optimize
this workflow, we introduce multi-context GRPO, an end-to-end RL framework that
jointly optimize reasoning, search strategies, and memory management of
MemSearcher Agents. Specifically, multi-context GRPO samples groups of
trajectories under different contexts and propagates trajectory-level
advantages across all conversations within them. Trained on the same dataset as
Search-R1, MemSearcher achieves significant improvements over strong baselines
on seven public benchmarks: +11% on Qwen2.5-3B-Instruct and +12% on
Qwen2.5-7B-Instruct relative average gains. Notably, the 3B-based MemSearcher
even outperforms 7B-based baselines, demonstrating that striking a balance
between information integrity and efficiency yields both higher accuracy and
lower computational overhead. The code and models will be publicly available at
https://github.com/icip-cas/MemSearcher

</details>


### [24] [Oolong: Evaluating Long Context Reasoning and Aggregation Capabilities](https://arxiv.org/abs/2511.02817)
*Amanda Bertsch,Adithya Pratapa,Teruko Mitamura,Graham Neubig,Matthew R. Gormley*

Main category: cs.CL

TL;DR: Oolong是一个长上下文推理基准测试，包含合成和真实世界任务，要求模型分析文本块并聚合信息回答分布性问题。前沿模型在128K上下文长度下准确率低于50%。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文评估主要依赖检索任务，允许模型忽略大部分上下文作为噪声，这仅代表一种任务类型。需要评估模型真正理解和使用长上下文的能力。

Method: 创建Oolong基准测试，分为Oolong-synth（自然合成任务）和Oolong-real（真实对话数据推理）。要求模型分析单个文本块，聚合分析结果，执行分类和计数，推理时间和用户关系。

Result: 前沿模型（GPT-5、Claude-Sonnet-4、Gemini-2.5-Pro）在128K上下文长度下，两个任务集的准确率都低于50%。

Conclusion: 当前模型在需要真正理解和推理长上下文的复杂任务上表现不佳，Oolong基准测试有助于推动能够推理大量文本的模型发展。

Abstract: As model context lengths continue to grow, concerns about whether models
effectively use the full context length have persisted. While several carefully
designed long-context evaluations have recently been released, these
evaluations tend to rely on retrieval from one or more sections of the context,
which allows nearly all of the context tokens to be disregarded as noise. This
represents only one type of task that might be performed with long context. We
introduce Oolong, a benchmark of long-context reasoning tasks that require
analyzing individual chunks of text on an atomic level, and then aggregating
these analyses to answer distributional questions. Oolong is separated into two
task sets: Oolong-synth, a set of naturalistic synthetic tasks, where we can
easily ablate components of the reasoning problem; and Oolong-real, a
downstream setting which requires reasoning over real-world conversational
data. Oolong requires models to reason over large quantities of examples, to
perform both classification and counting in-context, and to reason over
temporal and user relations. Even frontier models struggle on Oolong, with
GPT-5, Claude-Sonnet-4, and Gemini-2.5-Pro all achieving less than 50% accuracy
on both splits at 128K. We release the data and evaluation harness for Oolong
to enable further development of models that can reason over large quantities
of text.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [25] [Solving cold start in news recommendations: a RippleNet-based system for large scale media outlet](https://arxiv.org/abs/2511.02052)
*Karol Radziszewski,Michał Szpunar,Piotr Ociepka,Mateusz Buczyński*

Main category: cs.IR

TL;DR: 基于RippleNet的可扩展推荐系统，针对媒体领域设计，在波兰最大在线媒体平台Onet.pl生产部署，通过集成基于内容的项目嵌入解决冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 解决媒体领域中新发布内容的冷启动问题，为未见过项目提供有效评分。

Method: 将基于内容的项目嵌入集成到RippleNet的知识传播机制中，利用Amazon SageMaker进行分布式训练和推理，Apache Airflow编排数据管道和模型重训练工作流。

Result: 构建了包含用户和项目特征的综合黄金数据集，以及独立的交互表，支持灵活扩展和新信号集成。

Conclusion: 该系统成功解决了媒体推荐中的冷启动挑战，并实现了生产级部署。

Abstract: We present a scalable recommender system implementation based on RippleNet,
tailored for the media domain with a production deployment in Onet.pl, one of
Poland's largest online media platforms. Our solution addresses the cold-start
problem for newly published content by integrating content-based item
embeddings into the knowledge propagation mechanism of RippleNet, enabling
effective scoring of previously unseen items. The system architecture leverages
Amazon SageMaker for distributed training and inference, and Apache Airflow for
orchestrating data pipelines and model retraining workflows. To ensure
high-quality training data, we constructed a comprehensive golden dataset
consisting of user and item features and a separate interaction table, all
enabling flexible extensions and integration of new signals.

</details>


### [26] [Enhancing Multimodal Recommendations with Vision-Language Models and Information-Aware Fusion](https://arxiv.org/abs/2511.02113)
*Hai-Dang Kieu,Min Xu,Thanh Trung Huynh,Dung D. Le*

Main category: cs.IR

TL;DR: VLIF是一个基于视觉语言和信息论的多模态推荐框架，通过VLM视觉增强模块生成细粒度图像描述，并使用信息感知融合模块分解冗余和协同信号，显著提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推荐方法依赖粗糙的视觉特征和不受控制的融合，导致冗余或不对齐的表征，视觉编码器难以捕捉与物品相关的关键语义，限制了多模态融合的效果。

Method: 提出VLIF框架，包含两个核心组件：(1) VLM视觉增强模块，生成细粒度的标题引导描述，将产品图像转换为语义对齐的表征；(2) 信息感知融合模块，基于部分信息分解理论，解耦模态间的冗余和协同信号进行可控融合。

Result: 在三个Amazon数据集上的实验表明，VLIF持续优于最新的多模态基线方法，并显著增强了视觉特征的贡献度。

Conclusion: VLIF通过细粒度视觉语义对齐和信息理论指导的融合策略，有效解决了多模态推荐中的表征冗余和对齐问题，提升了推荐系统的性能。

Abstract: Recent advances in multimodal recommendation (MMR) have shown that
incorporating rich content sources such as images and text can lead to
significant gains representation quality. However, existing methods often rely
on coarse visual features and uncontrolled fusion, leading to redundant or
misaligned representations. As a result, visual encoders often fail to capture
salient, item-relevant semantics, limiting their contribution in multimodal
fusion. From an information-theoretic perspective, effective fusion should
balance the unique, shared, and redundant information across modalities,
preserving complementary cues while avoiding correlation bias. This paper
presents VLIF, a vision-language and information-theoretic fusion framework
that enhances multimodal recommendation through two key components. (i) A
VLM-based visual enrichment module generates fine-grained, title-guided
descriptions to transform product images into semantically aligned
representations. (ii) An information-aware fusion module, inspired by Partial
Information Decomposition (PID), disentangles redundant and synergistic signals
across modalities for controlled integration. Experiments on three Amazon
datasets demonstrate that VLIF consistently outperforms recent multimodal
baselines and substantially strengthens the contribution of visual features.

</details>


### [27] [KGBridge: Knowledge-Guided Prompt Learning for Non-overlapping Cross-Domain Recommendation](https://arxiv.org/abs/2511.02181)
*Yuhan Wang,Qing Xie,Zhifeng Bao,Mengzi Tang,Lin Li,Yongjian Liu*

Main category: cs.IR

TL;DR: KGBridge是一个基于知识图谱的提示学习框架，用于非重叠用户场景下的跨域序列推荐，通过KG增强提示编码器和两阶段训练范式解决KG稀疏性、用户重叠依赖和知识纠缠问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱的跨域推荐方法在非重叠用户场景下面临三大挑战：对KG稀疏性和流行度偏差敏感、依赖重叠用户进行域对齐、缺乏可迁移知识与领域特定知识的显式解耦。

Method: 提出KGBridge框架，包含KG增强提示编码器（将关系级语义建模为软提示）和两阶段训练范式（跨域预训练+隐私保护微调），通过关系感知语义控制和对应驱动解耦来分离和平衡领域共享与特定语义。

Result: 在基准数据集上的广泛实验表明，KGBridge持续优于最先进的基线方法，并在不同KG稀疏度下保持鲁棒性，有效缓解了KG增强跨域推荐中的结构不平衡和语义纠缠问题。

Conclusion: KGBridge通过知识引导的提示学习框架成功解决了非重叠用户场景下跨域推荐的关键挑战，为KG增强的跨域推荐提供了有效的解决方案。

Abstract: Knowledge Graphs (KGs), as structured knowledge bases that organize
relational information across diverse domains, provide a unified semantic
foundation for cross-domain recommendation (CDR). By integrating symbolic
knowledge with user-item interactions, KGs enrich semantic representations,
support reasoning, and enhance model interpretability. Despite this potential,
existing KG-based methods still face major challenges in CDR, particularly
under non-overlapping user scenarios. These challenges arise from: (C1)
sensitivity to KG sparsity and popularity bias, (C2) dependence on overlapping
users for domain alignment and (C3) lack of explicit disentanglement between
transferable and domain-specific knowledge, which limit effective and stable
knowledge transfer. To this end, we propose KGBridge, a knowledge-guided prompt
learning framework for cross-domain sequential recommendation under
non-overlapping user scenarios. KGBridge comprises two core components: a
KG-enhanced Prompt Encoder, which models relation-level semantics as soft
prompts to provide structured and dynamic priors for user sequence modeling
(addressing C1), and a Two-stage Training Paradigm, which combines cross-domain
pretraining and privacy-preserving fine-tuning to enable knowledge transfer
without user overlap (addressing C2). By combining relation-aware semantic
control with correspondence-driven disentanglement, KGBridge explicitly
separates and balances domain-shared and domain-specific semantics, thereby
maintaining complementarity and stabilizing adaptation during fine-tuning
(addressing C3). Extensive experiments on benchmark datasets demonstrate that
KGBridge consistently outperforms state-of-the-art baselines and remains robust
under varying KG sparsity, highlighting its effectiveness in mitigating
structural imbalance and semantic entanglement in KG-enhanced cross-domain
recommendation.

</details>


### [28] [Average Precision at Cutoff k under Random Rankings: Expectation and Variance](https://arxiv.org/abs/2511.02571)
*Tetiana Manzhos,Tetiana Ianevych,Olga Melnyk*

Main category: cs.IR

TL;DR: 本文推导了AP@k的期望和方差，为MAP@k提供基准参考，涵盖离线和在线两种评估模型。


<details>
  <summary>Details</summary>
Motivation: 推荐系统和信息检索平台依赖排名算法向用户展示最相关的内容，评估这些排名的质量需要可靠的评估指标，其中MAP@k被广泛使用。

Method: 推导了AP@k的期望和方差，覆盖离线和在线两种评估模型。

Result: 得到了AP@k的期望和方差，期望建立了基准水平，方差量化了随机波动程度。

Conclusion: AP@k的期望和方差可以作为MAP@k的基准，帮助更可靠地解释观察到的评分。

Abstract: Recommender systems and information retrieval platforms rely on ranking
algorithms to present the most relevant items to users, thereby improving
engagement and satisfaction. Assessing the quality of these rankings requires
reliable evaluation metrics. Among them, Mean Average Precision at cutoff k
(MAP@k) is widely used, as it accounts for both the relevance of items and
their positions in the list.
  In this paper, the expectation and variance of Average Precision at k (AP@k)
are derived since they can be used as biselines for MAP@k. Here, we covered two
widely used evaluation models: offline and online. The expectation establishes
the baseline, indicating the level of MAP@k that can be achieved by pure
chance. The variance complements this baseline by quantifying the extent of
random fluctuations, enabling a more reliable interpretation of observed
scores.

</details>
