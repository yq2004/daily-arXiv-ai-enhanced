<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 109]
- [cs.IR](#cs.IR) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Enhancing Biomedical Named Entity Recognition using GLiNER-BioMed with Targeted Dictionary-Based Post-processing for BioASQ 2025 task 6](https://arxiv.org/abs/2510.08588)
*Ritesh Mehta*

Main category: cs.CL

TL;DR: 本研究评估了GLiNER-BioMed模型在BioASQ数据集上的表现，并引入了基于词典的后处理策略来解决常见错误分类问题。虽然该策略在开发集上显著提升了性能，但在盲测集上未能实现泛化改进。


<details>
  <summary>Details</summary>
Motivation: 生物医学命名实体识别(BioNER)在从科学文献中提取信息方面至关重要，但面临区分相似实体类型(如基因和化学物质)的挑战。

Method: 使用GLiNER-BioMed模型评估BioASQ数据集，并引入基于词典的后处理策略来解决常见误分类问题。

Result: 后处理策略在开发集上将微平均F1分数从基线0.79提升到0.83，但在盲测集上反而从基线0.79下降到0.77。

Conclusion: 基于词典的细化方法对预训练BioNER模型具有潜力，但强调了过度拟合开发数据的关键挑战，以及确保实际应用鲁棒泛化的必要性。

Abstract: Biomedical Named Entity Recognition (BioNER), task6 in BioASQ (A challenge in
large-scale biomedical semantic indexing and question answering), is crucial
for extracting information from scientific literature but faces hurdles such as
distinguishing between similar entity types like genes and chemicals. This
study evaluates the GLiNER-BioMed model on a BioASQ dataset and introduces a
targeted dictionary-based post-processing strategy to address common
misclassifications. While this post-processing approach demonstrated notable
improvement on our development set, increasing the micro F1-score from a
baseline of 0.79 to 0.83, this enhancement did not generalize to the blind test
set, where the post-processed model achieved a micro F1-score of 0.77 compared
to the baselines 0.79. We also discuss insights gained from exploring
alternative methodologies, including Conditional Random Fields. This work
highlights the potential of dictionary-based refinement for pre-trained BioNER
models but underscores the critical challenge of overfitting to development
data and the necessity of ensuring robust generalization for real-world
applicability.

</details>


### [2] [Less Diverse, Less Safe: The Indirect But Pervasive Risk of Test-Time Scaling in Large Language Models](https://arxiv.org/abs/2510.08592)
*Shahriar Kabir Nahin,Hadi Askari,Muhao Chen,Anshuman Chhabra*

Main category: cs.CL

TL;DR: TTS（测试时缩放）通过探索多个候选响应来提升LLM推理能力，但本文发现当候选多样性受限时，TTS更容易产生不安全输出。作者提出了RefDiv攻击方法来测试TTS的脆弱性，并证明现有安全防护措施对此类攻击防御有限。


<details>
  <summary>Details</summary>
Motivation: TTS方法隐含假设候选池的多样性能够提高可靠性，但作者发现这一假设存在未被识别的失败模式——当候选多样性受限时，TTS会产生更多不安全输出。

Method: 提出了参考引导的多样性减少协议（RefDiv）作为诊断攻击方法，对TTS管道进行压力测试。在四个开源模型和两种TTS策略上进行了广泛实验。

Result: 限制多样性会显著增加TTS产生不安全结果的概率，这种效应甚至比高对抗性意图的提示更强。现有安全防护分类器无法检测RefDiv生成的对抗性输入提示。

Conclusion: TTS存在多样性驱动的失败模式，现有防御措施对此保护有限。需要设计既能有效又能抵抗多样性针对性压力测试的鲁棒TTS策略。

Abstract: Test-Time Scaling (TTS) improves LLM reasoning by exploring multiple
candidate responses and then operating over this set to find the best output. A
tacit premise behind TTS is that sufficiently diverse candidate pools enhance
reliability. In this work, we show that this assumption in TTS introduces a
previously unrecognized failure mode. When candidate diversity is curtailed,
even by a modest amount, TTS becomes much more likely to produce unsafe
outputs. We present a reference-guided diversity reduction protocol (RefDiv)
that serves as a diagnostic attack to stress test TTS pipelines. Through
extensive experiments across four open-source models (Qwen3, Mistral, Llama3.1,
Gemma3) and two widely used TTS strategies (Monte Carlo Tree Search and
Best-of-N), constraining diversity consistently signifies the rate at which TTS
produces unsafe results. The effect is often stronger than that produced by
prompts directly with high adversarial intent scores. This observed phenomenon
also transfers across TTS strategies and to closed-source models (e.g. OpenAI
o3 and Gemini-2.5-Pro), thus indicating that this is a general and extant
property of TTS rather than a model-specific artifact. Additionally, we find
that numerous widely used safety guardrail classifiers (e.g. Llama-Guard and
OpenAI Moderation API), are unable to flag the adversarial input prompts
generated by RefDiv, demonstrating that existing defenses offer limited
protection against this diversity-driven failure mode. Through this work, we
hope to motivate future research on designing robust TTS strategies that are
both effective and secure against diversity-targeted stress tests as
illustrated by RefDiv.

</details>


### [3] [Hierarchical Self-Supervised Representation Learning for Depression Detection from Speech](https://arxiv.org/abs/2510.08593)
*Yuxin Li,Eng Siong Chng,Cuntai Guan*

Main category: cs.CL

TL;DR: HAREN-CTC是一个用于语音抑郁检测的新架构，通过整合多层自监督学习特征和跨注意力机制，结合CTC损失处理稀疏时间监督，在DAIC-WOZ和MODMA数据集上达到最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的语音抑郁检测方法难以提取有意义的特征并捕捉稀疏、异质的抑郁线索，通常只使用SSL模型的最后一层或寻找单一最佳层，导致过拟合特定数据集且无法充分利用层次结构检测细微持久的抑郁信号。

Method: 提出HAREN-CTC架构，包含两个关键模块：层次自适应聚类模块将SSL特征重组为互补嵌入，跨模态融合模块通过跨注意力建模层间依赖关系，结合CTC损失实现对齐感知训练。

Result: 在标准数据分割的上界设置和五折交叉验证的泛化设置下，HAREN-CTC在DAIC-WOZ上达到0.81的宏F1分数，在MODMA上达到0.82的宏F1分数，优于先前方法。

Conclusion: HAREN-CTC通过有效整合多层SSL特征和建模层间依赖关系，显著提升了语音抑郁检测的性能和泛化能力，为临床评估提供了有前景的非侵入性替代方案。

Abstract: Speech-based depression detection (SDD) is a promising, non-invasive
alternative to traditional clinical assessments. However, it remains limited by
the difficulty of extracting meaningful features and capturing sparse,
heterogeneous depressive cues over time. Pretrained self-supervised learning
(SSL) models such as WavLM provide rich, multi-layer speech representations,
yet most existing SDD methods rely only on the final layer or search for a
single best-performing one. These approaches often overfit to specific datasets
and fail to leverage the full hierarchical structure needed to detect subtle
and persistent depression signals.
  To address this challenge, we propose HAREN-CTC, a novel architecture that
integrates multi-layer SSL features using cross-attention within a multitask
learning framework, combined with Connectionist Temporal Classification loss to
handle sparse temporal supervision. HAREN-CTC comprises two key modules: a
Hierarchical Adaptive Clustering module that reorganizes SSL features into
complementary embeddings, and a Cross-Modal Fusion module that models
inter-layer dependencies through cross-attention. The CTC objective enables
alignment-aware training, allowing the model to track irregular temporal
patterns of depressive speech cues.
  We evaluate HAREN-CTC under both an upper-bound setting with standard data
splits and a generalization setting using five-fold cross-validation. The model
achieves state-of-the-art macro F1-scores of 0.81 on DAIC-WOZ and 0.82 on
MODMA, outperforming prior methods across both evaluation scenarios.

</details>


### [4] [Systematic Diagnosis of Brittle Reasoning in Large Language Models](https://arxiv.org/abs/2510.08595)
*V. S. Raghu Parupudi*

Main category: cs.CL

TL;DR: 提出了一个评估数学推理的新框架，通过分析GPT-3.5-turbo在GSM8K数据集上的逐步推理，识别特定失败点并揭示模型推理能力的认知特征。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能中机器学习模型对数学理解程度的核心问题，超越标准基准测试来诊断具体失败点。

Method: 首先从GPT-3.5-turbo生成结构化逐步推理，然后使用更强大的GPT-4o-mini模型对错误进行分类，并通过无监督聚类识别出现的"推理模式"。

Result: 揭示了非人类化的脆弱认知特征：模型在顺序计算等程序性模式上准确率接近完美，但在需要组合推理和限制条件的模式上表现急剧下降。

Conclusion: 通过识别和量化不同推理技能的可靠性，为评估数学理解提供了更细粒度的方法，并为开发新能力和更可靠应用提供了精确路线图。

Abstract: A central question in artificial intelligence is the extent to which machine
learning models comprehend mathematics. To address this, we propose a novel
framework for measuring mathematical reasoning that moves beyond standard
benchmarks to diagnose specific failure points. Our method first generates
structured, step-by-step reasoning from gpt-3.5-turbo on the GSM8K dataset. We
then use a more capable analyst model, gpt-4o-mini, to categorize errors and,
crucially, perform an unsupervised clustering of every reasoning sentence to
identify emergent "reasoning modes." This analysis reveals a cognitive profile
with a stark, nonhuman-like brittleness: while the model achieves near-perfect
accuracy on procedural modes like sequential calculation, its performance on
modes requiring combinatorial reasoning with restrictions plummets. By
identifying and quantifying the reliability of these distinct reasoning skills,
our work provides a more granular method to evaluate mathematical comprehension
and offers a precise roadmap for developing new capabilities and more reliable
future applications.

</details>


### [5] [Confidence, Not Perplexity: A Better Metric for the Creative Era of LLMs](https://arxiv.org/abs/2510.08596)
*V. S. Raghu Parupudi*

Main category: cs.CL

TL;DR: 提出Confidence Score (CS)作为替代传统困惑度等无参考指标的方法，减少对创意文本生成的偏见。实验显示CS在99个创意提示中19%的情况下偏好新颖回答，而基于流畅度的指标为0%，差异显著。


<details>
  <summary>Details</summary>
Motivation: 传统无参考指标如自困惑度对创意文本生成存在强烈偏见，需要更平衡的评估方法。

Method: 从模型输出概率分布中推导出Confidence Score (CS)，在gpt-4o-mini上进行实验，比较CS与传统流畅度指标的表现。

Result: CS在创意提示中偏好新颖回答的比例为19%，显著高于传统指标的0%（95%置信区间差异：[11.1%, 27.3%]）。CS还能有效区分不同难度任务。

Conclusion: Confidence Score减轻了传统指标的创造力偏见，同时保留了核心评估优势，为现代LLM提供了更平衡的评估方法。

Abstract: Reference-free metrics like self-perplexity are strongly biased against
creative text generation. We propose the Confidence Score (CS), derived from a
model's output probability distribution, as a less biased alternative.
Experiments on gpt-4o-mini show that while fluency-based metrics prefer novel
responses in 0\% of cases on 99 creative prompts, our CS does so 19% of the
time, a statistically significant difference (95% CI for difference: [11.1%,
27.3%]). We also show that CS effectively distinguishes between easy, medium,
and hard tasks, confirmed by non-overlapping confidence intervals. The
Confidence Score thus mitigates the creativity bias of traditional metrics
while retaining their core evaluative strengths, offering a more balanced
assessment for modern LLMs.

</details>


### [6] [Recover-LoRA: Data-Free Accuracy Recovery of Degraded Language Models via Low-Rank Adaptation](https://arxiv.org/abs/2510.08600)
*Devleena Das,Rajeev Patwari,Ashish Sirasao*

Main category: cs.CL

TL;DR: Recover-LoRA是一种轻量级、数据集无关的方法，通过合成数据和logit蒸馏学习选择性层的LoRA适配器，从模型权重退化中恢复语言模型精度。


<details>
  <summary>Details</summary>
Motivation: 推理优化（如量化、剪枝、格式转换、模型导出和序列化）可能导致语言模型任务性能的功能退化。大多数性能恢复工作集中在鲁棒量化技术上，而本文关注从任何导致模型权重退化的源中恢复模型精度，如不当的模型序列化。

Method: Recover-LoRA使用合成数据和logit蒸馏来学习选择性层的LoRA适配器，促进退化模型与其全精度模型的对齐。

Result: Recover-LoRA在多样的小语言模型（包括具有不同注意力架构的MHA和GQA模型）上恢复了5-17%的模型精度。

Conclusion: Recover-LoRA是一种有效的轻量级方法，能够从各种模型权重退化源中恢复语言模型的精度，适用于不同的注意力架构和评估数据集。

Abstract: Inference optimizations such as quantization, pruning, format and datatype
conversion, model export, and serialization can lead to functional degradations
in language model task performance. While most efforts on performance recovery
for deployment focus on robust quantization techniques, we focus on recovering
model accuracies from any sources that degrade model weights, such as improper
model serialization. In this work, we propose Recover-LoRA, a lightweight and
dataset agnostic method to recover accuracy in degraded models. Recover-LoRA
uses synthetic data and logit distillation to learn LoRA adapters on selective
layers that facilitate aligning the degraded model to its full precision model.
We investigate the utility of Recover-LoRA across a diverse set of small
language models (SLMs), including models with varying attention architectures,
multi-head attention (MHA) and group-query attention (GQA), as well as several
evaluation datasets. Our results show that Recover-LoRA recovers model
accuracies by 5-17% on MHA and GQA SLMs.

</details>


### [7] [FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark for Evaluating LLMs](https://arxiv.org/abs/2510.08886)
*Yan Wang,Keyi Wang,Shanshan Yang,Jaisal Patel,Jeff Zhao,Fengran Mo,Xueqing Peng,Lingfei Qian,Jimin Huang,Guojun Xiong,Xiao-Yang Liu,Jian-Yun Nie*

Main category: cs.CL

TL;DR: FinAuditing是首个面向财务审计任务的基准测试，评估LLM在结构化、分层财务文档上的推理能力，包含语义一致性、关系一致性和数值一致性三个子任务。


<details>
  <summary>Details</summary>
Motivation: GAAP的复杂性和XBRL的分层结构使得财务审计难以自动化验证，而LLM在结构化、相互依赖的财务文档推理能力尚未充分探索。

Method: 基于真实US-GAAP合规的XBRL文件构建FinAuditing基准，定义三个互补子任务：FinSM（语义一致性）、FinRE（关系一致性）、FinMR（数值一致性），并提出统一的评估框架。

Result: 对13个最先进LLM的零样本实验显示，当前模型在语义、关系和数学维度表现不一致，在分层多文档结构推理时准确率下降60-90%。

Conclusion: 现代LLM在基于分类法的财务推理中存在系统性局限，FinAuditing为开发可信赖、结构感知和法规对齐的财务智能系统奠定了基础。

Abstract: The complexity of the Generally Accepted Accounting Principles (GAAP) and the
hierarchical structure of eXtensible Business Reporting Language (XBRL) filings
make financial auditing increasingly difficult to automate and verify. While
large language models (LLMs) have demonstrated strong capabilities in
unstructured text understanding, their ability to reason over structured,
interdependent, and taxonomy-driven financial documents remains largely
unexplored. To fill this gap, we introduce FinAuditing, the first
taxonomy-aligned, structure-aware, multi-document benchmark for evaluating LLMs
on financial auditing tasks. Built from real US-GAAP-compliant XBRL filings,
FinAuditing defines three complementary subtasks, FinSM for semantic
consistency, FinRE for relational consistency, and FinMR for numerical
consistency, each targeting a distinct aspect of structured auditing reasoning.
We further propose a unified evaluation framework integrating retrieval,
classification, and reasoning metrics across these subtasks. Extensive
zero-shot experiments on 13 state-of-the-art LLMs reveal that current models
perform inconsistently across semantic, relational, and mathematical
dimensions, with accuracy drops of up to 60-90% when reasoning over
hierarchical multi-document structures. Our findings expose the systematic
limitations of modern LLMs in taxonomy-grounded financial reasoning and
establish FinAuditing as a foundation for developing trustworthy,
structure-aware, and regulation-aligned financial intelligence systems. The
benchmark dataset is available at Hugging Face.

</details>


### [8] [Mnemosyne: An Unsupervised, Human-Inspired Long-Term Memory Architecture for Edge-Based LLMs](https://arxiv.org/abs/2510.08601)
*Aneesh Jonelagadda,Christina Hahn,Haoze Zheng,Salvatore Penachio*

Main category: cs.CL

TL;DR: Mnemosyne是一种受人类记忆启发的无监督长期记忆架构，专为边缘设备上的LLM设计，通过图结构存储、记忆过滤和修剪机制，在纵向医疗对话中显著优于传统检索方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM内存系统要么依赖暴力扩展上下文，要么使用静态检索管道，在边缘设备上效果不佳，特别是在处理重复但时间不同的对话时存在局限。

Method: 使用图结构存储、模块化内容和冗余过滤器、记忆提交和修剪机制，以及基于人类记忆模型的概率回忆与时间衰减刷新过程，还包括从记忆图中提取固定长度的核心摘要来捕获用户个性特征。

Result: 在纵向医疗对话实验中，Mnemosyne在盲测人类评估中获得65.8%的最高胜率，相比基准RAG的31.1%；在LoCoMo基准测试中，在时间推理和单跳检索方面获得最高分数，总体得分54.6%排名第二，优于Mem0和OpenAI等常用方法。

Conclusion: 通过边缘兼容且易于迁移的无监督记忆架构，可以实现改进的事实回忆、增强的时间推理以及更自然的用户响应。

Abstract: Long-term memory is essential for natural, realistic dialogue. However,
current large language model (LLM) memory systems rely on either brute-force
context expansion or static retrieval pipelines that fail on edge-constrained
devices. We introduce Mnemosyne, an unsupervised, human-inspired long-term
memory architecture designed for edge-based LLMs. Our approach uses
graph-structured storage, modular substance and redundancy filters, memory
committing and pruning mechanisms, and probabilistic recall with temporal decay
and refresh processes modeled after human memory. Mnemosyne also introduces a
concentrated "core summary" efficiently derived from a fixed-length subset of
the memory graph to capture the user's personality and other domain-specific
long-term details such as, using healthcare application as an example,
post-recovery ambitions and attitude towards care. Unlike existing
retrieval-augmented methods, Mnemosyne is designed for use in longitudinal
healthcare assistants, where repetitive and semantically similar but temporally
distinct conversations are limited by naive retrieval. In experiments with
longitudinal healthcare dialogues, Mnemosyne demonstrates the highest win rate
of 65.8% in blind human evaluations of realism and long-term memory capability
compared to a baseline RAG win rate of 31.1%. Mnemosyne also achieves current
highest LoCoMo benchmark scores in temporal reasoning and single-hop retrieval
compared to other same-backboned techniques. Further, the average overall score
of 54.6% was second highest across all methods, beating commonly used Mem0 and
OpenAI baselines among others. This demonstrates that improved factual recall,
enhanced temporal reasoning, and much more natural user-facing responses can be
feasible with an edge-compatible and easily transferable unsupervised memory
architecture.

</details>


### [9] [Human Texts Are Outliers: Detecting LLM-generated Texts via Out-of-distribution Detection](https://arxiv.org/abs/2510.08602)
*Cong Zeng,Shengkun Tang,Yuanzhou Chen,Zhiqiang Shen,Wenchao Yu,Xujiang Zhao,Haifeng Chen,Wei Cheng,Zhiqiang Xu*

Main category: cs.CL

TL;DR: 本文提出将AI生成文本检测任务重新定义为异常检测问题，将人类文本视为分布外样本，机器生成文本视为分布内样本，使用单类学习和基于分数的方法构建检测框架，在多个数据集上验证了方法的有效性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI文本检测方法大多将任务视为二分类问题，但人类文本并不构成统一的分布，其多样性无法通过有限采样有效捕捉，导致模型记忆观察到的分布外特征而非学习'非分布内'行为的本质，限制了泛化能力。

Method: 使用单类学习方法（DeepSVDD和HRN）和基于分数的学习技术（如基于能量的方法），构建异常检测框架，将人类文本视为分布外样本，机器生成文本视为分布内样本。

Result: 在DeepFake数据集上达到98.3%的AUROC和AUPR，FPR95仅为8.9%。在多语言、受攻击、未见模型和领域文本设置下测试，证明了框架的鲁棒性和泛化性。

Conclusion: 将AI文本检测重新定义为异常检测问题比传统二分类方法更有效，能够更好地处理人类文本的多样性，提高检测的泛化能力和鲁棒性。

Abstract: The rapid advancement of large language models (LLMs) such as ChatGPT,
DeepSeek, and Claude has significantly increased the presence of AI-generated
text in digital communication. This trend has heightened the need for reliable
detection methods to distinguish between human-authored and machine-generated
content. Existing approaches both zero-shot methods and supervised classifiers
largely conceptualize this task as a binary classification problem, often
leading to poor generalization across domains and models. In this paper, we
argue that such a binary formulation fundamentally mischaracterizes the
detection task by assuming a coherent representation of human-written texts. In
reality, human texts do not constitute a unified distribution, and their
diversity cannot be effectively captured through limited sampling. This causes
previous classifiers to memorize observed OOD characteristics rather than learn
the essence of `non-ID' behavior, limiting generalization to unseen
human-authored inputs. Based on this observation, we propose reframing the
detection task as an out-of-distribution (OOD) detection problem, treating
human-written texts as distributional outliers while machine-generated texts
are in-distribution (ID) samples. To this end, we develop a detection framework
using one-class learning method including DeepSVDD and HRN, and score-based
learning techniques such as energy-based method, enabling robust and
generalizable performance. Extensive experiments across multiple datasets
validate the effectiveness of our OOD-based approach. Specifically, the
OOD-based method achieves 98.3% AUROC and AUPR with only 8.9% FPR95 on DeepFake
dataset. Moreover, we test our detection framework on multilingual, attacked,
and unseen-model and -domain text settings, demonstrating the robustness and
generalizability of our framework. Code, pretrained weights, and demo will be
released.

</details>


### [10] [YpathRAG:A Retrieval-Augmented Generation Framework and Benchmark for Pathology](https://arxiv.org/abs/2510.08603)
*Deshui Yu,Yizhi Wang,Saihui Jin,Taojie Zhu,Fanyi Zeng,Wen Qian,Zirui Huang,Jingli Ouyang,Jiameng Li,Zhen Song,Tian Guan,Yonghong He*

Main category: cs.CL

TL;DR: YpathRAG是一个病理学导向的RAG框架，通过双通道混合检索和证据判断模块，显著提升了病理学领域的检索质量和事实可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在通用任务上表现出色，但在病理学等高门槛领域仍存在幻觉问题。现有方法依赖领域微调，但无法扩展知识边界或强制执行基于证据的约束。

Method: 构建了覆盖28个子领域、153万段落的病理学向量数据库，开发了YpathRAG框架，包含双通道混合检索（BGE-M3密集检索+词汇引导稀疏检索）和基于LLM的支持性证据判断模块，形成检索-判断-生成闭环。

Result: 在YpathR基准测试中，Recall@5达到98.64%，比基线提升23个百分点；在YpathQA-M（300个最具挑战性问题）上，通用和医疗LLM的准确率平均提升9.0%，最高提升15.6%。

Conclusion: YpathRAG显著提高了检索质量和事实可靠性，为病理学导向的RAG提供了可扩展的构建范式和可解释的评估方法。

Abstract: Large language models (LLMs) excel on general tasks yet still hallucinate in
high-barrier domains such as pathology. Prior work often relies on domain
fine-tuning, which neither expands the knowledge boundary nor enforces
evidence-grounded constraints. We therefore build a pathology vector database
covering 28 subfields and 1.53 million paragraphs, and present YpathRAG, a
pathology-oriented RAG framework with dual-channel hybrid retrieval (BGE-M3
dense retrieval coupled with vocabulary-guided sparse retrieval) and an
LLM-based supportive-evidence judgment module that closes the
retrieval-judgment-generation loop. We also release two evaluation benchmarks,
YpathR and YpathQA-M. On YpathR, YpathRAG attains Recall@5 of 98.64%, a gain of
23 percentage points over the baseline; on YpathQA-M, a set of the 300 most
challenging questions, it increases the accuracies of both general and medical
LLMs by 9.0% on average and up to 15.6%. These results demonstrate improved
retrieval quality and factual reliability, providing a scalable construction
paradigm and interpretable evaluation for pathology-oriented RAG.

</details>


### [11] [LatentBreak: Jailbreaking Large Language Models through Latent Space Feedback](https://arxiv.org/abs/2510.08604)
*Raffaele Mura,Giorgio Piras,Kamilė Lukošiūtė,Maura Pintor,Amin Karbasi,Battista Biggio*

Main category: cs.CL

TL;DR: 提出了一种名为LatentBreak的白盒越狱攻击方法，通过用语义等效词替换输入提示中的词汇来生成低困惑度的自然对抗提示，从而规避基于困惑度的防御机制。


<details>
  <summary>Details</summary>
Motivation: 现有的自动越狱攻击通常通过优化对抗性后缀或使用长提示模板来绕过语言模型的安全机制，但这些方法容易被基于困惑度的过滤器检测到。

Method: LatentBreak通过最小化对抗提示与无害请求在潜在空间中表示的距离，选择语义等效词来替换输入提示中的词汇，而不是添加高困惑度的对抗后缀或长模板。

Result: 广泛的评估表明，LatentBreak生成的提示更短且困惑度更低，在多个安全对齐模型上优于其他越狱算法对抗基于困惑度的过滤器。

Conclusion: LatentBreak是一种有效的白盒越狱攻击方法，能够生成自然且低困惑度的对抗提示，成功规避基于困惑度的防御机制。

Abstract: Jailbreaks are adversarial attacks designed to bypass the built-in safety
mechanisms of large language models. Automated jailbreaks typically optimize an
adversarial suffix or adapt long prompt templates by forcing the model to
generate the initial part of a restricted or harmful response. In this work, we
show that existing jailbreak attacks that leverage such mechanisms to unlock
the model response can be detected by a straightforward perplexity-based
filtering on the input prompt. To overcome this issue, we propose LatentBreak,
a white-box jailbreak attack that generates natural adversarial prompts with
low perplexity capable of evading such defenses. LatentBreak substitutes words
in the input prompt with semantically-equivalent ones, preserving the initial
intent of the prompt, instead of adding high-perplexity adversarial suffixes or
long templates. These words are chosen by minimizing the distance in the latent
space between the representation of the adversarial prompt and that of harmless
requests. Our extensive evaluation shows that LatentBreak leads to shorter and
low-perplexity prompts, thus outperforming competing jailbreak algorithms
against perplexity-based filters on multiple safety-aligned models.

</details>


### [12] [Toward a Safer Web: Multilingual Multi-Agent LLMs for Mitigating Adversarial Misinformation Attacks](https://arxiv.org/abs/2510.08605)
*Nouar Aldahoul,Yasir Zaki*

Main category: cs.CL

TL;DR: 提出一个多语言、多智能体的大型语言模型框架，结合检索增强生成技术，作为网络插件部署到在线平台，用于检测和对抗多语言转换、查询长度膨胀和结构重构等新型错误信息攻击。


<details>
  <summary>Details</summary>
Motivation: 数字平台上错误信息的快速传播威胁公共讨论、情感稳定和决策制定，而现有研究尚未系统性地研究语言切换、查询长度膨胀和结构重构等特定转换攻击。

Method: 开发多语言、多智能体大型语言模型框架，采用检索增强生成技术，支持英语、法语、西班牙语、阿拉伯语、印地语和中文的语言切换及翻译，处理查询长度膨胀后的摘要生成，以及将内容重构为多项选择题形式。

Result: 构建了一个可部署为网络插件的框架，展示了在真实网络应用中插件化部署的可行性，能够有效对抗多样化的错误信息攻击。

Conclusion: AI驱动的错误信息检测对于保护在线事实完整性至关重要，该研究证明了插件化部署在应对多样化攻击方面的实用性和有效性。

Abstract: The rapid spread of misinformation on digital platforms threatens public
discourse, emotional stability, and decision-making. While prior work has
explored various adversarial attacks in misinformation detection, the specific
transformations examined in this paper have not been systematically studied. In
particular, we investigate language-switching across English, French, Spanish,
Arabic, Hindi, and Chinese, followed by translation. We also study query length
inflation preceding summarization and structural reformatting into
multiple-choice questions. In this paper, we present a multilingual,
multi-agent large language model framework with retrieval-augmented generation
that can be deployed as a web plugin into online platforms. Our work
underscores the importance of AI-driven misinformation detection in
safeguarding online factual integrity against diverse attacks, while showcasing
the feasibility of plugin-based deployment for real-world web applications.

</details>


### [13] [Centering Emotion Hotspots: Multimodal Local-Global Fusion and Cross-Modal Alignment for Emotion Recognition in Conversations](https://arxiv.org/abs/2510.08606)
*Yu Liu,Hanlei Shi,Haoxun Li,Yuqing Sun,Yuxuan Ding,Linlin Gong,Leyuan Qu,Taihao Li*

Main category: cs.CL

TL;DR: 提出了一种基于情感热点的统一模型，通过热点门控融合和混合对齐器来改进对话中的多模态情感识别，在标准基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 对话情感识别（ERC）面临挑战，因为判别性证据稀疏、局部化且跨模态异步。现有方法难以有效处理这些多模态特征的不对齐问题。

Method: 1) 检测文本、音频和视频中的每句话情感热点；2) 通过热点门控融合（HGF）将热点与全局特征融合；3) 使用路由混合对齐器（MoA）对齐模态；4) 构建跨模态图编码对话结构。

Result: 在标准ERC基准测试中持续优于强基线方法，消融实验证实了HGF和MoA的有效贡献。

Conclusion: 提出了以热点为中心的多模态学习方法，为ERC中的模态融合提供了新视角，有望指导未来的多模态学习研究。

Abstract: Emotion Recognition in Conversations (ERC) is hard because discriminative
evidence is sparse, localized, and often asynchronous across modalities. We
center ERC on emotion hotspots and present a unified model that detects
per-utterance hotspots in text, audio, and video, fuses them with global
features via Hotspot-Gated Fusion, and aligns modalities using a routed
Mixture-of-Aligners; a cross-modal graph encodes conversational structure. This
design focuses modeling on salient spans, mitigates misalignment, and preserves
context. Experiments on standard ERC benchmarks show consistent gains over
strong baselines, with ablations confirming the contributions of HGF and MoA.
Our results point to a hotspot-centric view that can inform future multimodal
learning, offering a new perspective on modality fusion in ERC.

</details>


### [14] [MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation](https://arxiv.org/abs/2510.08608)
*Weihua Zheng,Zhengyuan Liu,Tanmoy Chakraborty,Weiwen Xu,Xiaoxue Gao,Bryan Chen Zhengyu Tan,Bowei Zou,Chang Liu,Yujia Hu,Xing Xie,Xiaoyuan Yi,Jing Yao,Chaojun Wang,Long Li,Rui Liu,Huiyao Liu,Koji Inoue,Ryuichi Sumida,Tatsuya Kawahara,Fan Xu,Lingyu Ye,Wei Tian,Dongjun Kim,Jimin Jung,Jaehyung Seo,Nadya Yuki Wangsajaya,Pham Minh Duc,Ojasva Saxena,Palash Nandi,Xiyan Tao,Wiwik Karlina,Tuan Luong,Keertana Arun Vasan,Roy Ka-Wei Lee,Nancy F. Chen*

Main category: cs.CL

TL;DR: MMA-ASIA是一个评估大型语言模型在亚洲文化背景下的文化意识框架，包含多语言、多模态对齐的27000道选择题，覆盖8个亚洲国家和10种语言，79%问题需要基于文化背景的多步推理。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在西方高资源环境外的多模态理解和推理能力下降，特别是在亚洲文化背景下存在文化意识不足的问题。

Method: 构建人类策划的多语言多模态对齐基准数据集，包含文本、图像和语音三种模态的输入对齐；提出五维评估协议测量文化意识差异、跨语言一致性、跨模态一致性、文化知识泛化和基础有效性；使用文化意识基础验证模块检测捷径学习。

Result: 通过比较模型分析、注意力追踪和创新的视觉消融前缀重放方法，揭示了模型在不同语言和模态间差异的原因，为构建文化可靠的多模态大型语言模型提供了可行见解。

Conclusion: MMA-ASIA框架为评估和提升大型语言模型在亚洲文化背景下的文化意识提供了系统性方法，有助于开发更具文化敏感性的多模态AI系统。

Abstract: Large language models (LLMs) are now used worldwide, yet their multimodal
understanding and reasoning often degrade outside Western, high-resource
settings. We propose MMA-ASIA, a comprehensive framework to evaluate LLMs'
cultural awareness with a focus on Asian contexts. MMA-ASIA centers on a
human-curated, multilingual, and multimodally aligned multiple-choice benchmark
covering 8 Asian countries and 10 languages, comprising 27,000 questions; over
79 percent require multi-step reasoning grounded in cultural context, moving
beyond simple memorization. To our knowledge, this is the first dataset aligned
at the input level across three modalities: text, image (visual question
answering), and speech. This enables direct tests of cross-modal transfer.
Building on this benchmark, we propose a five-dimensional evaluation protocol
that measures: (i) cultural-awareness disparities across countries, (ii)
cross-lingual consistency, (iii) cross-modal consistency, (iv) cultural
knowledge generalization, and (v) grounding validity. To ensure rigorous
assessment, a Cultural Awareness Grounding Validation Module detects "shortcut
learning" by checking whether the requisite cultural knowledge supports correct
answers. Finally, through comparative model analysis, attention tracing, and an
innovative Vision-ablated Prefix Replay (VPR) method, we probe why models
diverge across languages and modalities, offering actionable insights for
building culturally reliable multimodal LLMs.

</details>


### [15] [GraphGhost: Tracing Structures Behind Large Language Models](https://arxiv.org/abs/2510.08613)
*Xinnan Dai,Kai Guo,Chung-Hsiang Lo,Shenglai Zeng,Jiayuan Ding,Dongsheng Luo,Subhabrata Mukherjee,Jiliang Tang*

Main category: cs.CL

TL;DR: GraphGhost是一个统一框架，将LLM中的神经元激活和信号传播表示为图结构，通过图算法分析LLM的推理机制，并可通过结构干预揭示关键神经元节点对推理的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型展现出卓越的推理能力，但其背后的结构机制仍未得到充分探索。需要一种方法来解释LLM如何从序列输入中捕获结构语义并通过结构一致的机制生成输出。

Method: 提出GraphGhost框架，将神经元激活和信号传播建模为图结构，应用PageRank等图算法分析LLM特性，并通过结构干预（编辑关键神经元节点）来评估推理机制。

Result: 揭示了不同模型在多样化数据集上的共享和特定推理行为，发现对关键神经元节点的编辑会触发推理崩溃，改变逻辑流程和语义理解。

Conclusion: GraphGhost为分析、干预和最终理解LLM推理的结构基础提供了一个强大工具，有助于深入探索LLM的推理机制。

Abstract: Large Language Models (LLMs) demonstrate remarkable reasoning capabilities,
yet the structural mechanisms underlying these abilities remain under explored.
In this work, we introduce GraphGhost, a unified framework that represents
neuron activations and their signal propagation as graphs, explaining how LLMs
capture structural semantics from sequential inputs and generate outputs
through structurally consistent mechanisms. This graph-based perspective
enables us to employ graph algorithms such as PageRank to characterize the
properties of LLMs, revealing both shared and model-specific reasoning
behaviors across diverse datasets. We further identify the activated neurons
within GraphGhost and evaluate them through structural interventions, showing
that edits to key neuron nodes can trigger reasoning collapse, altering both
logical flow and semantic understanding. Together, these contributions position
GraphGhost as a powerful tool for analyzing, intervening in, and ultimately
understanding the structural foundations of reasoning in LLMs.

</details>


### [16] [Gender Bias in Large Language Models for Healthcare: Assignment Consistency and Clinical Implications](https://arxiv.org/abs/2510.08614)
*Mingxuan Liu,Yuhe Ke,Wentao Zhu,Mayli Mertens,Yilin Ning,Jingchi Liao,Chuan Hong,Daniel Shu Wei Ting,Yifan Peng,Danielle S. Bitterman,Marcus Eng Hock Ong,Nan Liu*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型在医疗诊断中的性别偏见问题，发现虽然诊断结果在不同模型性别设定下相对一致，但所有模型在判断患者性别相关性和必要性时都表现出显著不一致，特别是在相关性判断方面。


<details>
  <summary>Details</summary>
Motivation: 将大型语言模型整合到医疗保健中具有改善临床决策的潜力，但模型对偏见的敏感性仍然是一个关键问题。性别长期以来影响着医生的行为和患者结果，这引发了担忧：扮演人类角色（如临床医生或医学教育者）的LLMs可能会复制或放大与性别相关的偏见。

Method: 使用《新英格兰医学杂志挑战赛》的案例研究，为多个开源和专有LLMs分配性别（女性、男性或未指定）。评估了它们在LLM性别分配方面的响应一致性，包括基于LLM的诊断以及模型对患者性别临床相关性或必要性的判断。

Result: 对于大多数模型，诊断在不同LLM性别设定下相对一致。然而，在患者性别相关性和必要性判断方面，所有模型都表现出显著的不一致性，特别是在相关性判断方面。某些模型甚至在解释患者性别时显示出系统的女性-男性差异。

Conclusion: 这些发现揭示了一种未被充分探索的偏见，可能削弱LLMs在临床实践中的可靠性，强调了在与LLMs交互时需要进行身份分配一致性常规检查，以确保可靠和公平的AI支持临床护理。

Abstract: The integration of large language models (LLMs) into healthcare holds promise
to enhance clinical decision-making, yet their susceptibility to biases remains
a critical concern. Gender has long influenced physician behaviors and patient
outcomes, raising concerns that LLMs assuming human-like roles, such as
clinicians or medical educators, may replicate or amplify gender-related
biases. Using case studies from the New England Journal of Medicine Challenge
(NEJM), we assigned genders (female, male, or unspecified) to multiple
open-source and proprietary LLMs. We evaluated their response consistency
across LLM-gender assignments regarding both LLM-based diagnosis and models'
judgments on the clinical relevance or necessity of patient gender. In our
findings, diagnoses were relatively consistent across LLM genders for most
models. However, for patient gender's relevance and necessity in LLM-based
diagnosis, all models demonstrated substantial inconsistency across LLM
genders, particularly for relevance judgements. Some models even displayed a
systematic female-male disparity in their interpretation of patient gender.
These findings present an underexplored bias that could undermine the
reliability of LLMs in clinical practice, underscoring the need for routine
checks of identity-assignment consistency when interacting with LLMs to ensure
reliable and equitable AI-supported clinical care.

</details>


### [17] [Iterative LLM-Based Generation and Refinement of Distracting Conditions in Math Word Problems](https://arxiv.org/abs/2510.08615)
*Kaiqi Yang,Hang Li,Yucheng Chu,Zitao Liu,Mi Tian,Hui Liu*

Main category: cs.CL

TL;DR: 本文提出了一种利用大语言模型自动生成数学应用题中干扰条件的迭代框架，通过多视角和认知层次的提示词设计，在保持原问题解决方案不变的前提下生成有意义的干扰条件，大幅减少了人工工作量。


<details>
  <summary>Details</summary>
Motivation: 现有的数学应用题数据集大多只包含必要信息，而包含干扰或冗余条件的问题往往被忽视。现有干扰条件数据集存在难度低、上下文不相关等问题，导致干扰条件容易被检测和忽略，降低了基准测试的可信度。

Method: 设计了一个迭代框架，利用大语言模型自动生成干扰条件。开发了一套提示词，从多个视角和认知层次修订数学应用题，鼓励生成有意义的干扰条件并提供进一步优化的建议。关键优势是保持原始问题和修订问题解决方案的一致性。

Result: 该框架高效且易于部署，在保持高质量数据的同时，显著减少了生成包含干扰条件的数学应用题所需的工作量。

Conclusion: 提出的迭代框架能够自动生成高质量的干扰条件数学应用题，解决了现有数据集的局限性，为大语言模型在数学推理能力评估提供了更可靠的基准测试工具。

Abstract: Mathematical reasoning serves as a crucial testbed for evaluating the
intelligence of large language models (LLMs), and math word problems (MWPs)
represent one of the most widely used formats. Most existing MWP datasets
contain only the necessary information, while problems with distracting or
excessive conditions are often overlooked. Prior studies have shown that
popular LLMs experience a dramatic performance drop when such distracting
conditions are introduced. However, available datasets of MWPs with distracting
conditions remain limited, and most exhibit low difficulty and out-of-context
expressions. These shortcomings make the distracting conditions easy to detect
and disregard, thereby reducing the credibility of benchmarking on these
datasets. Moreover, when distracting conditions are added, the reasoning
process and answers may change, requiring intensive manual effort to check and
rewrite solutions.
  To address these issues, we design an iterative framework that leverages LLMs
to generate distracting conditions automatically. We develop a set of prompts
to revise MWPs from multiple perspectives and cognitive levels, encouraging the
creation of meaningful distracting conditions as well as suggestions for
further refinement. A key advantage of our framework is the preservation of
shared solutions between the original and revised problems: the LLMs are
explicitly guided to generate distractions that do not alter the original
solution, thus eliminating the need to produce new answers. This framework is
efficient and easy to deploy, substantially reducing the effort required to
generate MWPs with distracting conditions while maintaining high data quality.

</details>


### [18] [LLMs Show Surface-Form Brittleness Under Paraphrase Stress Tests](https://arxiv.org/abs/2510.08616)
*Juan Miguel Navarro Carranza*

Main category: cs.CL

TL;DR: 本文提出了一种通过重新评估模型在基准问题的转述版本上的表现来探测泛化能力的简单协议，发现转述会导致准确率显著下降，证实了关于数据污染和表面形式捷径的担忧。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的基准分数可能因记忆测试项目或近似重复项而被夸大，需要一种方法来探测模型的真实泛化能力。

Method: 使用Mistral-7B-Instruct和Qwen2.5-7B-Instruct模型，在ARC-Easy和ARC-Challenge数据集上测量原始问题和转述版本之间的准确率差距。流程包括控制解码、强制多选输出格式和稳健的转述清理步骤以保持语义。

Result: 转述会导致非平凡的准确率下降（原始vs转述），这与先前关于数据污染和脆弱的表面形式捷径的担忧一致。

Conclusion: 基准测试中的转述评估揭示了模型泛化能力的局限性，证实了数据污染问题，并强调了需要更稳健的评估方法来衡量真实的理解能力。

Abstract: Benchmark scores for Large Language Models (LLMs) can be inflated by
memorization of test items or near duplicates. We present a simple, protocol
that probes generalization by re-evaluating models on paraphrased versions of
benchmark questions. Using Mistral-7B-Instruct and Qwen2.5-7B-Instruct, we
measure the accuracy gap between original and paraphrased items on ARC-Easy and
ARC-Challenge. Our pipeline controls decoding, enforces multiple-choice output
format, and includes a robust paraphrase-cleaning step to preserve semantics.
We find that paraphrasing induces a non-trivial accuracy drop (original vs.
paraphrased), consistent with prior concerns about contamination and brittle
surface-form shortcuts.

</details>


### [19] [JAI-1: A Thai-Centric Large Language Model](https://arxiv.org/abs/2510.08620)
*Attapol T. Rutherford,Jullajak Karnjanaekarin,Narongkorn Panitsrisit,Pontakorn Trakuekul,Sumana Sumanakul,Natchanon Pollertlam*

Main category: cs.CL

TL;DR: JAI-1是一个750亿参数的泰语中心语言模型，采用上扩策略从高性能英语模型扩展参数空间，专门集成泰语知识，在泰语基准测试中表现优于Typhoon2-70B。


<details>
  <summary>Details</summary>
Motivation: 现有泰语模型主要依赖开源模型进行额外训练，但这种方法可能会侵蚀模型参数空间中已有的通用知识，因为通用任务的优化参数可能与新的语言需求冲突。

Method: 从较小的高性能英语开源LLM开始，扩展其参数空间，利用新分配的能力系统集成泰语知识，包括1.5万亿token的预训练（含3000亿泰语token）和超过60万指令示例的监督微调与对齐调优。

Result: 在泰语中心基准测试（IFEval-TH、MT-Bench-TH和JAI-Hall-Bench）中表现优于Typhoon2-70B，验证了其上扩和知识集成框架的有效性。

Conclusion: 上扩策略不仅保留了原始模型的通用智能，还建立了独特的架构，支持未来的可扩展增强，为专门语言模型开发提供了有效方法。

Abstract: This technical report introduces JAI-1, a Thai-centric language model with
75B parameters. Recent Thai models have primarily relied on existing
open-source models, applying additional training without structural
modifications to specialize in Thai. However, this approach risks eroding
pre-existing knowledge in the model's parameter space during the injection of
Thai-specific information, as optimized parameters for general tasks may
conflict with new linguistic requirements. In contrast, JAI-1 adopts an
upscaling strategy: starting from a smaller, high-performing English
open-source LLM, we expanded its parameter space and utilized the newly
allocated capacity to systematically integrate Thai-language knowledge. This
methodology not only preserves the original model's general intelligence but
also establishes a unique architecture distinct from other open-source models,
enabling scalable future enhancements. During pre-training, JAI-1 was exposed
to 1.5T tokens, including over 300B Thai language tokens. This was followed by
post-training stages -- supervised fine-tuning and alignment tuning -- using
more than 600K instruction-based examples. The final model demonstrated
superior performance compared to Typhoon2-70B on Thai-centric benchmarks
(IFEval-TH, MT-Bench-TH, and JAI-Hall-Bench), validating the efficacy of its
upscaling and knowledge-integration framework.

</details>


### [20] [From Simulation to Strategy: Automating Personalized Interaction Planning for Conversational Agents](https://arxiv.org/abs/2510.08621)
*Wen-Yu Chang,Tzu-Hung Huang,Chih-Ho Chen,Yun-Nung Chen*

Main category: cs.CL

TL;DR: 本文研究了基于用户画像（年龄、性别、职业）的销售导向对话代理，发现职业对对话意图影响最大，并提出了一种轻量级的职业条件策略来优化对话效果。


<details>
  <summary>Details</summary>
Motivation: 随着智能对话模型的快速发展，需要真实的用户模拟器研究来调整有效的对话策略，特别是在销售导向的对话系统中。

Method: 引入基于用户职业的轻量级条件策略，引导代理优先选择与用户偏好一致的对话意图。

Result: 职业对对话意图的影响最为显著，采用职业条件策略后，对话更短且成功率更高。

Conclusion: 丰富的模拟器用户画像和简单的人物感知策略可以显著提升销售导向对话系统的有效性。

Abstract: Amid the rapid rise of agentic dialogue models, realistic user-simulator
studies are essential for tuning effective conversation strategies. This work
investigates a sales-oriented agent that adapts its dialogue based on user
profiles spanning age, gender, and occupation. While age and gender influence
overall performance, occupation produces the most pronounced differences in
conversational intent. Leveraging this insight, we introduce a lightweight,
occupation-conditioned strategy that guides the agent to prioritize intents
aligned with user preferences, resulting in shorter and more successful
dialogues. Our findings highlight the importance of rich simulator profiles and
demonstrate how simple persona-informed strategies can enhance the
effectiveness of sales-oriented dialogue systems.

</details>


### [21] [Text2Stories: Evaluating the Alignment Between Stakeholder Interviews and Generated User Stories](https://arxiv.org/abs/2510.08622)
*Francesco Dente,Fabiano Dalpiaz,Paolo Papotti*

Main category: cs.CL

TL;DR: 提出了Text2Stories任务和指标，用于量化需求（用户故事）与利益相关者实际需求之间的对齐程度，通过正确性和完整性两个指标评估LLM生成的软件需求是否忠实反映原始访谈内容。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM可以自动从自然语言输入生成软件需求，但评估这些需求是否准确反映利益相关者需求仍然主要依赖人工任务。需要一种可量化的方法来衡量需求与原始访谈内容的对齐程度。

Method: 将访谈记录分割成文本块，将对齐问题建模为文本块与用户故事之间的匹配问题。使用基于LLM的匹配器，同时探索嵌入模型作为有效阻塞方法。

Result: 在四个数据集上的实验显示，基于LLM的匹配器在保留注释上达到0.86的宏F1分数，而仅使用嵌入模型表现较差但能实现有效阻塞。提出的指标能够比较不同故事集（如人工生成vs自动生成）。

Conclusion: Text2Stories提供了一个可扩展、忠实于源内容的补充方法，能够量化需求与利益相关者实际需求的对齐程度，为现有用户故事质量标准提供了有价值的补充。

Abstract: Large language models (LLMs) can be employed for automating the generation of
software requirements from natural language inputs such as the transcripts of
elicitation interviews. However, evaluating whether those derived requirements
faithfully reflect the stakeholders' needs remains a largely manual task. We
introduce Text2Stories, a task and metrics for text-to-story alignment that
allow quantifying the extent to which requirements (in the form of user
stories) match the actual needs expressed by the elicitation session
participants. Given an interview transcript and a set of user stories, our
metric quantifies (i) correctness: the proportion of stories supported by the
transcript, and (ii) completeness: the proportion of transcript supported by at
least one story. We segment the transcript into text chunks and instantiate the
alignment as a matching problem between chunks and stories. Experiments over
four datasets show that an LLM-based matcher achieves 0.86 macro-F1 on held-out
annotations, while embedding models alone remain behind but enable effective
blocking. Finally, we show how our metrics enable the comparison across sets of
stories (e.g., human vs. generated), positioning Text2Stories as a scalable,
source-faithful complement to existing user-story quality criteria.

</details>


### [22] [PARSE: LLM Driven Schema Optimization for Reliable Entity Extraction](https://arxiv.org/abs/2510.08623)
*Anubhav Shrimal,Aryan Jain,Soumyajit Chowdhury,Promod Yenigalla*

Main category: cs.CL

TL;DR: PARSE系统通过自动优化JSON模式和改进提取方法，显著提升了从非结构化文本中提取结构化信息的准确性和可靠性，减少了幻觉和错误。


<details>
  <summary>Details</summary>
Motivation: 现有方法将JSON模式视为静态合同，导致提取性能不佳、频繁出现幻觉，以及在模式规范模糊或不完整时代理行为不可靠。

Method: 开发了PARSE系统，包含ARCHITECT（自动优化JSON模式）和SCOPE（基于反射的提取与防护机制）两个协同组件。

Result: 在三个数据集上的评估显示，PARSE在SWDE上提取准确率提升64.7%，整体框架改进达10%，首次重试时提取错误减少92%，并保持实用延迟。

Conclusion: PARSE通过将JSON模式视为可解释和改进的自然语言理解合同，有效解决了现有方法的局限性，为软件3.0系统中的结构化信息提取提供了可靠解决方案。

Abstract: Structured information extraction from unstructured text is critical for
emerging Software 3.0 systems where LLM agents autonomously interact with APIs
and tools. Recent approaches apply large language models directly to extraction
tasks using existing JSON schemas, often with constraint decoding or
reinforcement learning approaches to ensure syntactic validity, but treat JSON
schemas as static contracts designed for human developers, leading to
suboptimal extraction performance, frequent hallucinations, and unreliable
agent behavior when schemas contain ambiguous or incomplete specifications. We
recognize that JSON schemas themselves are a form of natural language
understanding contract that encodes rules, relationships, and expectations
about data structure contracts that LLMs should be able to both interpret and
systematically improve. Consequently, we develop PARSE (Parameter Automated
Refinement and Schema Extraction), a novel system with two synergistic
components: ARCHITECT, which autonomously optimizes JSON schemas for LLM
consumption while maintaining backward compatibility through RELAY (an
integrated code generation system), and SCOPE, which implements
reflection-based extraction with combined static and LLM-based guardrails. We
evaluate PARSE qualitatively and quantitatively on three datasets including
Schema-Guided Dialogue (SGD), Structured Web Data Extraction (SWDE), and
internal retail conversation data, and find that it achieves up to 64.7%
improvement in extraction accuracy on SWDE with combined framework improvements
reaching 10% across models, while reducing extraction errors by 92% within the
first retry and and maintaining practical latency.

</details>


### [23] [Do LLMs Know They Are Being Tested? Evaluation Awareness and Incentive-Sensitive Failures in GPT-OSS-20B](https://arxiv.org/abs/2510.08624)
*Nisar Ahmed,Muhammad Imran Zaman,Gulshan Saleem,Ali Hassan*

Main category: cs.CL

TL;DR: 研究发现，评估导向的提示框架会人为夸大LLM性能，导致更长的推理链和更差的简洁回答合规性，但准确性提升有限或不一致。


<details>
  <summary>Details</summary>
Motivation: 当前LLM基准测试使用带有评估痕迹的提示（如要求可见推理和严格格式），而实际部署需要简洁、合同约束的回答，这可能导致测量性能与实际能力不匹配。

Method: 使用单一开源模型（GPT-OSS-20B），在六个配对A/B场景中固定任务内容和解码，仅改变提示框架（评估导向vs真实世界）和推理深度，通过确定性验证器测量准确性、回答合规性、推理链长度等指标。

Result: 评估框架显著增加推理链长度（数百到1000+字符），降低简洁回答合规性，准确性提升有限或不一致。激励措辞会影响错误构成，多语言标题会重现这些模式并可能降低准确性。

Conclusion: 需要采用中性措辞、双框架检查、合同感知评分等方法来确保基准测试收益反映可部署能力，避免评估痕迹导致的能力夸大。

Abstract: Benchmarks for large language models (LLMs) often rely on rubric-scented
prompts that request visible reasoning and strict formatting, whereas real
deployments demand terse, contract-bound answers. We investigate whether such
"evaluation scent" inflates measured performance without commensurate
capability gains. Using a single open-weights model (GPT-OSS-20B), we run six
paired A/B scenarios that hold task content and decoding fixed while varying
framing (evaluation-oriented vs. real-world) and reasoning depth (Medium/High):
deterministic math, strict code-fix, citation generation, incentive flips
(caution vs. competence), CoT visibility, and multilingual (Urdu) headers.
Deterministic validators compute accuracy, answer-only compliance,
hedging/refusals, chain-of-thought (CoT) length, and schema compliance, with
pre-registered deltas and composite indices. Across scenarios, evaluation
framing reliably inflates CoT (hundreds to >1000 characters) and reduces
answer-only compliance, with limited or inconsistent accuracy gains. In
structured outputs, it improves wrappers (e.g., fenced blocks, enumerated
lists) but not regex-validated substance. Incentive wording reweights error
composition: praising caution modestly improves accuracy at high reasoning and
reduces wrong-but-confident errors, whereas praising competence yields terser
but riskier outputs. Urdu rubric headers reproduce these signatures and can
decrease accuracy at higher reasoning depth, indicating multilingual parity
risks. We provide a reproducible A/B framework (prompt banks, validators,
per-run scores, scripts; versioned DOI) and practical guidance: neutral
phrasing or dual-framing checks, contract-aware grading, style-delta reporting,
confidence governance, and multilingual dashboards to ensure that benchmark
gains reflect deployable capability.

</details>


### [24] [From What to Why: Thought-Space Recommendation with Small Language Models](https://arxiv.org/abs/2510.08626)
*Prosenjit Biswas,Pervez Shaik,Abhinav Thorat,Ravi Kolla,Niranjan Pedanekar*

Main category: cs.CL

TL;DR: PULSE框架使用小型语言模型生成的rationales作为监督信号，结合用户交互历史，在Thought Space中共同建模用户行为及其语义驱动因素，实现了比大型语言模型更高效且性能优越的推荐系统。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推荐系统中推理能力强但推理成本高，而小型语言模型效率高但推理能力未被充分探索。现有系统未能充分利用自然语言rationales作为学习信号的潜力。

Method: 提出PULSE框架，在Thought Space中使用SLM生成的rationales作为直接学习信号，监督它们与交互历史，共同建模用户行为（what）和语义驱动因素（why）。

Result: 在多个基准数据集上，PULSE优于主流的ID、协同过滤和基于LLM的顺序推荐模型，在跨域推荐中表现出优越的迁移能力，并在推理导向的下游任务中表现强劲。

Conclusion: PULSE通过将rationales作为一等公民信号，生成了更鲁棒和可泛化的嵌入，证明了小型语言模型在推荐系统中的潜力，提供了一种高效且性能优越的替代方案。

Abstract: Large Language Models (LLMs) have advanced recommendation capabilities
through enhanced reasoning, but pose significant challenges for real-world
deployment due to high inference costs. Conversely, while Small Language Models
(SLMs) offer an efficient alternative, their reasoning capabilities for
recommendation remain underexplored. Existing systems often use natural
language rationales merely as unsupervised descriptive text, failing to harness
their full potential as learning signals. In this work our main idea is to
create a common understanding of user and items across multiple domains called
Thought Space with SLMs instead of using LLMs' distilled knowledge. To that end
we propose PULSE (Preference Understanding by Latent Semantic Embeddings), a
framework that treats SLM-generated rationales as director learning signals,
supervising them with interaction histories to jointly model user actions
(what) and their semantic drivers (why). Existing methods consider only
interactions such as sequences and embeddings, whereas PULSE treats rationales
as first-class signals, this novel design yields embeddings that are more
robust and generalizable. Extensive experiments demonstrate that PULSE
outperforms leading ID, Collaborative Filtering (CF), and LLM-based sequential
recommendation models across multiple benchmark datasets. Furthermore, PULSE
exhibits superior transferability in cross-domain recommendation and
demonstrates strong performance on downstream tasks such as reasoning-oriented
question answering. Our code is available
\href{https://anonymous.4open.science/r/Thinking_PULSE-0FC5/README.md}{here}.

</details>


### [25] [ExPO-HM: Learning to Explain-then-Detect for Hateful Meme Detection](https://arxiv.org/abs/2510.08630)
*Jingbiao Mei,Mingsheng Sun,Jinghong Chen,Pengda Qin,Yuhong Li,Da Chen,Bill Byrne*

Main category: cs.CL

TL;DR: ExPO-HM是一种针对仇恨表情包检测的新方法，通过结合SFT预热、GRPO与课程学习以及条件决策熵，在仇恨表情包检测中实现了最先进的性能，相比现有方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的仇恨表情包检测方法多为直接检测，仅提供二元预测，缺乏上下文和解释，无法满足实际审核需求。现有的解释-检测方法性能不佳，存在两个关键问题：模型无法假设政策相关线索（如目标和攻击类型），二元奖励信号不足以指导推理。

Method: 提出ExPO-HM方法，结合SFT预热、GRPO与课程学习，并使用条件决策熵（CDE）作为推理质量的度量和奖励，灵感来源于人类标注者的训练和评估过程。

Result: 在三个仇恨表情包基准测试中，ExPO-HM在二元检测、细粒度分类和推理质量方面均达到最先进性能，相比GRPO和DPO基线分别提升15%和17%的F1分数。

Conclusion: 通过将仇恨表情包检测从简单的二元警报转变为解释驱动的检测，ExPO-HM提供了准确、可解释且可操作的审核支持。

Abstract: Hateful memes have emerged as a particularly challenging form of online
abuse, motivating the development of automated detection systems. Most prior
approaches rely on direct detection, producing only binary predictions. Such
models fail to provide the context and explanations that real-world moderation
requires. Recent Explain-then-Detect approaches, using Chain-of-Thought
prompting or LMM agents, perform worse than simple SFT baselines, and even
advanced post-training methods such as GRPO fail to close the gap. Our analysis
identifies two key issues of such systems: important policy-relevant cues such
as targets and attack types are not hypothesized by the model as a likely
explanation; and the binary reward signal is insufficient to guide reasoning.
To address these challenges, we propose ExPO-HM (Explain-then-Detect Policy
Optimization for Hateful Memes), inspired by the training and evaluation
process of human annotators. ExPO-HM combines SFT warmup, GRPO with curriculum
learning, and Conditional Decision Entropy (CDE) as both metric and reward for
reasoning quality. Across three hateful meme benchmarks, ExPO-HM achieves
state-of-the-art performance on binary detection, fine-grained classification,
and reasoning quality, with up to 15\% and 17\% F1 improvement over the GRPO
and DPO baselines, respectively. By moving hateful meme detection from simple
binary alarms to explanation-driven detection, ExPO-HM provides accurate,
interpretable, and actionable moderation support.

</details>


### [26] [Next Semantic Scale Prediction via Hierarchical Diffusion Language Models](https://arxiv.org/abs/2510.08632)
*Cai Zhou,Chenyu Wang,Dinghuai Zhang,Shangyuan Tong,Yifei Wang,Stephen Bates,Tommi Jaakkola*

Main category: cs.CL

TL;DR: HDLM是一种新颖的离散扩散语言模型，通过分层词汇表实现从粗粒度到细粒度的语义生成过程，在文本生成任务中表现出比基线更低的困惑度。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散语言模型在语言建模方面存在局限性，需要一种能够处理语义层次结构的更通用框架。

Method: 使用分层词汇表，在正向过程中将token扰动到更高层次的抽象语义，在反向过程中逐步预测更详细的语义，实现时间变化的下一个语义尺度预测。

Result: HDLM在文本生成实验中表现出比基线更低的验证和生成困惑度，验证了其有效性。

Conclusion: HDLM提供了一个通用的分层扩散语言建模框架，能够有效处理语义层次结构，在语言生成任务中表现优异。

Abstract: In this paper we introduce Hierarchical Diffusion Language Models (HDLM) -- a
novel family of discrete diffusion models for language modeling. HDLM builds on
a hierarchical vocabulary where low-level tokens with detailed semantics are
surjectively mapped to high-level tokens with coarse-grained meanings. In the
forward process, each token is independently perturbed to its higher-level
ancestor with more abstract semantics according to the scheduler, while in the
reverse process the model progressively predicts the next, more detailed
semantics. Taken together, HDLM provides a general time-varying next semantic
scale prediction process for language modeling. We derive closed-form
expressions for the diffusion Evidence Lower Bound (ELBO), and show that HDLM
can be implemented in a flexible manner while including the existing MDLM as a
special case. We also propose practical training techniques based on the
insights. Extensive text generation experiments validate the effectiveness of
HDLM, which demonstrates consistently lower validation and generative
perplexity than baselines.

</details>


### [27] [Upfront Chain-of-Thought: A Cooperative Framework for Chain-of-Thought Compression](https://arxiv.org/abs/2510.08647)
*Chengzhengxu Li,Xiaoming Liu,Zhaohan Zhang,Shaochu Zhang,Shengchao Liu,Guoxin Ma,Yu Lan,Chao Shen*

Main category: cs.CL

TL;DR: UCoT是一种高效的推理框架，通过前期思想嵌入实现CoT压缩，在保持强大推理能力的同时显著减少推理长度，在GSM8K数据集上可减少50%的token使用量。


<details>
  <summary>Details</summary>
Motivation: 长链式思维推理虽然能提升大语言模型的推理能力，但由于自回归特性导致计算成本高、延迟大。现有CoT压缩方法要么需要繁琐的手动提示设计，要么构建外部压缩数据集而牺牲关键推理细节。

Method: UCoT采用小模型（压缩器）和大模型（执行器）的协同工作流程：第一阶段训练压缩器生成富含推理信息的前期思想嵌入；第二阶段优化执行器利用这些嵌入以短推理得出正确答案，使用奖励机制。

Result: 在GSM8K数据集上，UCoT应用于Qwen2.5-7B-Instruct模型时，token使用量减少50%，性能比最先进方法提高3.08%。

Conclusion: UCoT框架成功实现了CoT压缩的自动化，在保持强大推理能力的同时显著提高了推理效率，为高效推理提供了有效解决方案。

Abstract: Recent developments have enabled advanced reasoning in Large Language Models
(LLMs) via long Chain-of-Thought (CoT), while long CoT suffers from high
computational costs and significant latency losses owing to the autoregressive
nature of generative LLMs. CoT compression aims to improve efficiency in the
reasoning process by reducing output length. Previous works trade reasoning
efficiency by either laborious discrete prompt designing or the construction of
external compressed CoT datasets that sacrifice key reasoning details. In this
work, we propose Upfront CoT (UCoT): an efficient reasoning framework with
upfront thought embedding to automate CoT compression. UCoT is a cooperative
workflow involving a small model (compressor) and a large model (executor). The
first stage of UCoT trains compressor to generate upfront thought embeddings
rich in reasoning information for the executor, avoiding the drawbacks of
manually designed prompts. The second stage optimizes executor to utilize
upfront thought embeddings to derive the correct answer with short reasoning,
using a reward mechanism. Extensive experiments show that UCoT maintains the
powerful reasoning ability of executor while significantly reducing the length
of CoT. It is worth mentioning that when applying UCoT to the
Qwen2.5-7B-Instruct model, the usage of tokens on GSM8K dataset is reduced by
50\%, while the performance is 3.08\% higher than that of the state-of-the-art
(SOTA) method. The code and dataset are in supplementary material.

</details>


### [28] [Formalizing Style in Personal Narratives](https://arxiv.org/abs/2510.08649)
*Gustave Cortal,Alain Finkel*

Main category: cs.CL

TL;DR: 提出了一个分析个人叙事风格的形式化框架，整合功能语言学、计算机科学和心理学，通过语言模型自动提取语言特征，应用于梦境叙事分析。


<details>
  <summary>Details</summary>
Motivation: 个人叙事是作者构建意义的方式，风格对传达主观体验至关重要，但目前缺乏系统分析这些风格选择的正式框架。

Method: 将个人叙事中的风格形式化为作者在传达主观体验时所做的语言选择模式，整合功能语言学、计算机科学和心理学，使用语言模型自动提取过程、参与者和环境等语言特征。

Result: 将该框架应用于数百个梦境叙事，包括对患有创伤后应激障碍的战争退伍军人的案例研究，分析发现其叙事中独特的模式，特别是言语过程如何主导心理过程。

Conclusion: 该框架揭示了语言选择与心理状态之间的关系，为系统分析个人叙事风格提供了有效工具。

Abstract: Personal narratives are stories authors construct to make meaning of their
experiences. Style, the distinctive way authors use language to express
themselves, is fundamental to how these narratives convey subjective
experiences. Yet there is a lack of a formal framework for systematically
analyzing these stylistic choices. We present a novel approach that formalizes
style in personal narratives as patterns in the linguistic choices authors make
when communicating subjective experiences. Our framework integrates three
domains: functional linguistics establishes language as a system of meaningful
choices, computer science provides methods for automatically extracting and
analyzing sequential patterns, and these patterns are linked to psychological
observations. Using language models, we automatically extract linguistic
features such as processes, participants, and circumstances. We apply our
framework to hundreds of dream narratives, including a case study on a war
veteran with post-traumatic stress disorder. Analysis of his narratives
uncovers distinctive patterns, particularly how verbal processes dominate over
mental ones, illustrating the relationship between linguistic choices and
psychological states.

</details>


### [29] [A Novel Framework for Augmenting Rating Scale Tests with LLM-Scored Text Data](https://arxiv.org/abs/2510.08663)
*Joe Watson,Ivan O'Conner,Chia-Wen Chen,Luning Sun,Fang Luo,David Stillwell*

Main category: cs.CL

TL;DR: 本研究提出了一种结合LLM评分文本和传统评分量表的新框架，通过实证选择信息量最大的LLM评分指令来增强心理测量工具，在抑郁症评估案例中显著提高了测量精度和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统心理评估主要依赖结构化评分量表，无法充分利用受访者自然语言的丰富细微差别，需要一种能够整合定性数据的新方法。

Method: 利用LLM技术，开发了一个结合LLM评分文本和传统评分量表项目的增强测试框架，在真实世界高中生样本(n=693)和合成数据集(n=3,000)上进行开发和评估，基于项目信息计算实证选择最有信息量的LLM评分指令。

Result: 在保留测试集上，增强测试在测量精度和准确性方面实现了统计显著改进。LLM项目带来的信息增益相当于在原有19项测试基础上增加了6.3(真实数据)到16.0(合成数据)个项目。

Conclusion: 该框架为利用日益增长的转录文本来增强传统心理测量工具提供了可扩展的方法，在临床健康等领域具有潜在应用价值，代表了自动化评分概念上的转变，绕过了依赖预标记数据或复杂专家创建评分标准的典型瓶颈。

Abstract: Psychological assessments typically rely on structured rating scales, which
cannot incorporate the rich nuance of a respondent's natural language. This
study leverages recent LLM advances to harness qualitative data within a novel
conceptual framework, combining LLM-scored text and traditional rating-scale
items to create an augmented test. We demonstrate this approach using
depression as a case study, developing and assessing the framework on a
real-world sample of upper secondary students (n=693) and corresponding
synthetic dataset (n=3,000). On held-out test sets, augmented tests achieved
statistically significant improvements in measurement precision and accuracy.
The information gain from the LLM items was equivalent to adding between 6.3
(real data) and 16.0 (synthetic data) items to the original 19-item test. Our
approach marks a conceptual shift in automated scoring that bypasses its
typical bottlenecks: instead of relying on pre-labelled data or complex
expert-created rubrics, we empirically select the most informative LLM scoring
instructions based on calculations of item information. This framework provides
a scalable approach for leveraging the growing stream of transcribed text to
enhance traditional psychometric measures, and we discuss its potential utility
in clinical health and beyond.

</details>


### [30] [dInfer: An Efficient Inference Framework for Diffusion Language Models](https://arxiv.org/abs/2510.08666)
*Yuxin Ma,Lun Du,Lanning Wei,Kun Chen,Qian Xu,Kangyu Wang,Guofeng Feng,Guoshan Lu,Lin Liu,Xiaojing Qi,Xinyuan Zhang,Zhen Tao,Haibo Feng,Ziyun Jiang,Ying Xu,Zenan Huang,Yihong Zhuang,Haokai Xu,Jiaqi Hu,Zhenzhong Lan,Junbo Zhao,Jianguo Li,Da Zheng*

Main category: cs.CL

TL;DR: dInfer是一个针对扩散式大语言模型的高效推理框架，通过模块化设计和系统优化，在保持输出质量的同时显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 扩散式大语言模型虽然具有并行生成的优势，但缺乏标准化和高效的推理框架，限制了其广泛应用。

Method: 将推理流程分解为四个模块化组件（模型、扩散迭代管理器、解码策略、KV缓存管理器），并结合新颖算法和系统级优化。

Result: 在8×H800 GPU上，HumanEval达到1100+ tokens/秒，六个基准测试平均800+ tokens/秒；相比Fast-dLLM有10倍加速，相比优化后的AR模型QWen2.5-3B有2-3倍加速。

Conclusion: dInfer为扩散式大语言模型提供了高效、可扩展的推理解决方案，显著提升了推理效率，推动了该类模型的更广泛应用。

Abstract: Diffusion-based large language models (dLLMs) have emerged as a promising
alternative to autoregressive (AR) LLMs, leveraging denoising-based generation
to enable inherent parallelism. Even more and more open-sourced dLLM models
emerge, yet their widespread adoption remains constrained by the lack of a
standardized and efficient inference framework. We present dInfer, an efficient
and extensible framework for dLLM inference. dInfer decomposes the inference
pipeline into four modular components-model, diffusion iteration manager,
decoding strategy, and KV-cache manager-and integrates novel algorithms for
each component alongside system-level optimizations. Through this combination
of algorithmic innovations and system enhancements, dInfer achieves substantial
efficiency gains without compromising output quality on LLaDA-MoE. At batch
size 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800
tokens per second across six benchmarks on $8\times$ H800 GPUs. Compared to
prior systems, dInfer delivers $10\times$ speedup over Fast-dLLM while
maintaining similar model performance. Even compared with AR models (with a
comparable number of activation parameters and performance) QWen2.5-3B, which
is highly optimized with latest vLLM inference engine, dInfer still deliverers
$2$-$3\times$ speedup. The implementation of dInfer is open-sourced at
https://github.com/inclusionAI/dInfer.

</details>


### [31] [Scaling Laws for Code: A More Data-Hungry Regime](https://arxiv.org/abs/2510.08702)
*Xianzhen Luo,Wenzhen Zheng,Qingfu Zhu,Rongyi Zhang,Houyi Li,Siming Huang,YuanTao Fan,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文首次对代码语言模型的缩放规律进行了大规模实证研究，发现代码LLM在模型大小上能有效扩展，但比自然语言需要更高的数据参数比，且自然语言在资源受限时有益但在高计算预算时有害。


<details>
  <summary>Details</summary>
Motivation: 代码大语言模型正在革新软件工程，但现有的缩放规律主要基于自然语言分析。考虑到代码和自然语言在严格语法等方面的根本差异，不清楚这些规律是否直接适用于代码。

Method: 进行了117次实验运行，模型大小从0.2B到3.8B，训练token从2B到128B，拟合了Chinchilla定律和Farsser定律，并对代码-自然语言混合进行了额外实验。

Result: 1) 更具表达力的Farseer定律提供更高准确性；2) 代码LLM在模型大小上能有效扩展；3) 代码需要比自然语言高得多的数据参数比；4) 自然语言在资源受限时有益但在高计算预算时有害。

Conclusion: 代码语言模型的缩放规律与自然语言存在显著差异，代码需要更高的数据参数比，且自然语言混合训练的效果取决于计算预算，这为高效训练代码LLM提供了重要指导。

Abstract: Code Large Language Models (LLMs) are revolutionizing software engineering.
However, scaling laws that guide the efficient training are predominantly
analyzed on Natural Language (NL). Given the fundamental differences like
strict syntax between code and NL, it is unclear whether these laws are
directly applicable to code. To address this gap, we conduct the first
large-scale empirical study of scaling laws for code, comprising 117
experimental runs with model sizes from 0.2B to 3.8B and training tokens from
2B to 128B. We fit the Chinchilla law and the Farsser law. First, the results
show that the more expressive Farseer law offers greater accuracy. Second, the
analysis reveals that Code LLMs scale effectively with model size. Crucially,
code represents a more data-hungry regime, requiring a substantially higher
data-to-parameter ratio than NL. Finally, two additional sets of experiments on
code-NL mixtures show that NL benefits resource-constrained scenarios, but
becomes a detriment at higher compute budgets.

</details>


### [32] [Thinking Longer, Not Always Smarter: Evaluating LLM Capabilities in Hierarchical Legal Reasoning](https://arxiv.org/abs/2510.08710)
*Li Zhang,Matthias Grabmair,Morgan Gray,Kevin Ashley*

Main category: cs.CL

TL;DR: 本文提出了一个三阶段推理框架来评估LLM在法律案例推理中的能力，发现LLM在表面推理表现良好，但在层次推理和综合分析中表现显著下降，且错误回答消耗更多计算资源。


<details>
  <summary>Details</summary>
Motivation: 案例推理是美国法律实践的核心，需要专业人士通过类比和区分先例来论证当前案例。虽然LLM已展现出强大能力，但它们在复杂、细致的推理能力仍需进一步研究。

Method: 提出了一个正式框架，将识别案例间显著区别的过程分解为三阶段推理任务：使用事实谓词（因素）建模案例，将其组织到法律知识层次结构中，并定义可验证规则来识别区别、分析论证支持并评估其重要性。

Result: 评估显示现代推理LLM存在悖论：在表面推理（任务1）上准确率高，但在层次推理（任务2：64.82%-92.09%）表现下降，在综合分析（任务3：11.46%-33.99%）上表现崩溃。最显著的是，模型在错误回答上持续消耗更多计算资源。

Conclusion: 该工作为复杂领域中LLM推理能力的细粒度分析提供了方法学，揭示了必须解决的基本限制，以实现稳健可信的法律AI。

Abstract: Case-based reasoning is a cornerstone of U.S. legal practice, requiring
professionals to argue about a current case by drawing analogies to and
distinguishing from past precedents. While Large Language Models (LLMs) have
shown remarkable capabilities, their proficiency in this complex, nuanced form
of reasoning needs further investigation. We propose a formal framework that
decomposes the process of identifying significant distinctions between cases
into three-stage reasoning tasks. Our framework models cases using factual
predicates called factors, organizes them into a legal knowledge hierarchy, and
defines verifiable rules for identifying distinctions, analyzing their
argumentative support, and evaluating their significance. Through comprehensive
evaluation of modern reasoning LLMs, we reveal a paradox: while models achieve
high accuracy on surface-level reasoning (Task 1), performance degrades on
hierarchical reasoning (Task 2: 64.82%-92.09%) and collapses on integrated
analysis (Task 3: 11.46%-33.99%). Most strikingly, we find that models
consistently expend more computational resources on incorrect responses than
correct ones, suggesting that "thinking longer" does not always mean "thinking
smarter." Our work provides a methodology for fine-grained analysis of LLM
reasoning capabilities in complex domains and reveals fundamental limitations
that must be addressed for robust and trustworthy legal AI.

</details>


### [33] [How Many Code and Test Cases Are Enough? Evaluating Test Cases Generation from a Binary-Matrix Perspective](https://arxiv.org/abs/2510.08720)
*Xianzhen Luo,Jinyang Huang,Wenzhen Zheng,Qingfu Zhu,Mingzheng Xu,Yiheng Xu,Yuantao Fan,Libo Qin,Wanxiang Che*

Main category: cs.CL

TL;DR: 提出了一个名为WrongSelect的框架，通过寻找二进制代码-测试矩阵中的最优诊断基来构建紧凑、多样且抗膨胀的基准测试TC-Bench，用于评估LLM生成的测试用例。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在计算成本高、分数膨胀以及偏向简单错误而非罕见关键故障的问题，需要更有效的评估方法。

Method: 将基准构建形式化为在二进制代码-测试矩阵中寻找最优诊断基的问题，提出WrongSelect算法选择最大多样性的错误代码，构建TC-Bench基准。

Result: 实验表明，即使最先进的测试用例生成方法在TC-Bench上也只能达到约60%的排除率，暴露了其诊断能力的显著差距。

Conclusion: 该框架能够构建紧凑、多样且抗膨胀的基准测试，有效评估LLM生成的测试用例，揭示了当前方法的局限性。

Abstract: Evaluating test cases automatically generated by Large Language Models (LLMs)
is a critical yet challenging task. Existing benchmarks suffer from high
computational costs, score inflation, and a bias towards trivial bugs over
rare, critical faults. In this work, we ask two fundamental questions: (1) What
is the minimal set of wrong codes sufficient to represent the entire error
space? and (2) What is the minimal set of test cases needed to distinguish
them? We introduce a framework that formalizes benchmark construction as
finding an optimal diagnostic basis in a binary code-test matrix. The rank of
this matrix specifies the minimal number of independent error patterns (wrong
codes) and provides a tight upper bound on the number of test cases required
for complete fault coverage. Our objective is to identify a basis of size equal
to the matrix rank that maximizes internal diversity. To tackle this NP-hard
problem, we propose WrongSelect, an efficient approximation algorithm to select
maximally diverse wrong codes. Applying this framework to millions of
competitive programming submissions, we construct TC-Bench, a compact, diverse,
and inflation-resistant benchmark. Extensive experiments show that even the
most advanced test case generation methods achieve only ~60% exclusion rates on
TC-Bench, exposing a significant gap in their diagnostic power. Our dataset is
available at: https://huggingface.co/datasets/Luoberta/TC-Bench and our code is
at: https://github.com/Luowaterbi/TC-Bench.

</details>


### [34] [How Reliable is Language Model Micro-Benchmarking?](https://arxiv.org/abs/2510.08730)
*Gregory Yauney,Shahzaib Saqib Warraich,Swabha Swayamdipta*

Main category: cs.CL

TL;DR: 研究发现微基准测试在模型排名一致性方面存在问题，需要250个样本才能可靠比较相似性能的模型，而现有方法在25个样本时超过一半的成对比较不可靠。


<details>
  <summary>Details</summary>
Motivation: 为了解决语言模型开发中完整基准测试的时间和成本问题，研究微基准测试是否能像完整基准测试一样一致地排名模型，以及是否比随机选择数据点更好。

Method: 引入了一种微基准测试的元评估方法，研究微基准测试在完整基准测试上模型性能差异的函数下如何排名两个模型，分析微基准大小与可靠性之间的权衡。

Result: 发现现有微基准测试方法无法在MMLU-Pro上一致排名准确率相差3.5点的模型对，或在BIG-bench Hard上相差4点的模型对。需要250个样本才能可靠比较相似性能的模型，此时随机采样与现有方法竞争力相当。

Conclusion: 为微基准测试用户和开发者提供了在评估效率与可靠性之间权衡的可操作指导，强调需要更多样本才能获得可靠的模型比较结果。

Abstract: Micro-benchmarking offers a solution to the often prohibitive time and cost
of language model development: evaluate on a very small subset of existing
benchmarks. Can these micro-benchmarks, however, rank models as consistently as
the full benchmarks they replace? And can they rank models more consistently
than selecting a random subset of data points? In many scenarios, we find that
the answer is no. We introduce a meta-evaluation measure for micro-benchmarking
which investigates how well a micro-benchmark can rank two models as a function
of their performance difference on the full benchmark. This approach can
determine which model pairs can be ranked correctly by a micro-benchmark,
allowing for a finer-grained analysis of the trade-off between micro-benchmark
size and reliability. Prior work has suggested selecting as few as 10 examples;
we find that no micro-benchmarking method can consistently rank model pairs 3.5
points of accuracy apart on MMLU-Pro or 4 points apart on BIG-bench Hard. In
order to consistently rank model pairs with relatively similar performances, we
show that often as many as 250 examples must be selected, at which point random
sampling is competitive with existing micro-benchmarking methods. When
comparing only 8B instruction-tuned models on MMLU-Pro micro-benchmarks with 25
examples, we find that more than half of pairwise comparisons are not likely to
be preserved. Our work provides actionable guidance for both micro-benchmark
users and developers in navigating the trade-off between evaluation efficiency
and reliability.

</details>


### [35] [Coordinates from Context: Using LLMs to Ground Complex Location References](https://arxiv.org/abs/2510.08741)
*Tessa Masis,Brendan O'Connor*

Main category: cs.CL

TL;DR: 本文提出了一种基于LLM的方法来地理编码组合位置引用，通过评估LLM的地理空间知识与推理能力，开发出能提升任务性能的策略，并证明小型微调LLM可实现与大型现成模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 地理编码是将位置引用链接到实际地理位置的任务，对非结构化文本的下游分析至关重要。本文探索地理编码组合位置引用这一具有挑战性的设置。

Method: 基于最近证明LLM能够推理地理空间数据的研究，评估LLM的地理空间知识与推理技能，并提出了基于LLM的地理编码组合位置引用策略。

Result: 研究表明该方法提高了地理编码任务的性能，相对较小的微调LLM能够实现与大型现成模型相当的性能。

Conclusion: LLM在地理编码组合位置引用任务中表现出色，通过适当的策略和微调，小型LLM可以替代大型模型，实现高效的地理编码。

Abstract: Geocoding is the task of linking a location reference to an actual geographic
location and is essential for many downstream analyses of unstructured text. In
this paper, we explore the challenging setting of geocoding compositional
location references. Building on recent work demonstrating LLMs' abilities to
reason over geospatial data, we evaluate LLMs' geospatial knowledge versus
reasoning skills relevant to our task. Based on these insights, we propose an
LLM-based strategy for geocoding compositional location references. We show
that our approach improves performance for the task and that a relatively small
fine-tuned LLM can achieve comparable performance with much larger
off-the-shelf models.

</details>


### [36] [Measuring Moral LLM Responses in Multilingual Capacities](https://arxiv.org/abs/2510.08776)
*Kimaya Basu,Savi Kolari,Allison Yu*

Main category: cs.CL

TL;DR: 本研究评估了前沿和领先开源模型在低资源和高资源语言中的五个维度表现，发现GPT-5在各类别中表现最佳，而其他模型在不同语言和类别间存在不一致性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在全球多语言环境中的广泛应用，需要理解和防护其多语言响应，因此需要大规模数据集来测试和基准化LLM的多维度响应能力。

Method: 使用五点评分标准和法官LLM评估模型在五个维度上的响应，涵盖低资源和高资源语言。

Result: GPT-5在各类别中平均表现最佳，特别是在同意与自主权（3.56）和伤害预防与安全（4.73）类别中得分最高；Gemini 2.5 Pro表现最差，相应得分为1.39和1.98。

Conclusion: 研究强调了需要进一步测试语言变化如何影响LLM在不同类别中的响应，并需要在这些领域进行改进。

Abstract: With LLM usage becoming widespread across countries, languages, and humanity
more broadly, the need to understand and guardrail their multilingual responses
increases. Large-scale datasets for testing and benchmarking have been created
to evaluate and facilitate LLM responses across multiple dimensions. In this
study, we evaluate the responses of frontier and leading open-source models in
five dimensions across low and high-resource languages to measure LLM accuracy
and consistency across multilingual contexts. We evaluate the responses using a
five-point grading rubric and a judge LLM. Our study shows that GPT-5 performed
the best on average in each category, while other models displayed more
inconsistency across language and category. Most notably, in the Consent &
Autonomy and Harm Prevention & Safety categories, GPT scored the highest with
averages of 3.56 and 4.73, while Gemini 2.5 Pro scored the lowest with averages
of 1.39 and 1.98, respectively. These findings emphasize the need for further
testing on how linguistic shifts impact LLM responses across various categories
and improvement in these areas.

</details>


### [37] [Learning What to Remember: Adaptive Probabilistic Memory Retention for Memory-Efficient Language Models](https://arxiv.org/abs/2510.08798)
*S M Rafiuddin,Muntaha Nujat Khan*

Main category: cs.CL

TL;DR: 提出Adaptive Retention机制，通过概率性层间token选择，在严格全局预算M下学习保留哪些表示，仅保留30-50%的token即可维持95%以上性能，同时减少35-45%内存使用并提升1.8倍吞吐量。


<details>
  <summary>Details</summary>
Motivation: Transformer注意力机制随序列长度呈二次方增长O(n²)，限制了长上下文应用。

Method: 使用Bernoulli门通过Hard-Concrete/变分松弛训练，推理时采用简单top-M规则，构建可微分且可直接替换标准编码器的机制。

Result: 在分类、抽取式问答和长文档摘要任务中，仅保留30-50%的token即可保持≥95%的完整模型性能，峰值内存减少约35-45%，吞吐量提升高达约1.8倍。

Conclusion: 这种架构无关的方法在不修改基础注意力或任务头的情况下，实现了实用的长上下文效率。

Abstract: Transformer attention scales quadratically with sequence length O(n^2),
limiting long-context use. We propose Adaptive Retention, a probabilistic,
layer-wise token selection mechanism that learns which representations to keep
under a strict global budget M. Retention is modeled with Bernoulli gates
trained via a Hard-Concrete/variational relaxation and enforced with a simple
top-M rule at inference, making the method differentiable and drop-in for
standard encoders. Across classification, extractive QA, and long-document
summarization, keeping only 30-50% of tokens preserves >= 95% of full-model
performance while cutting peak memory by ~35-45% and improving throughput by up
to ~1.8x. This architecture-agnostic approach delivers practical long-context
efficiency without modifying base attention or task heads.

</details>


### [38] [Benchmarking Chinese Commonsense Reasoning with a Multi-hop Reasoning Perspective](https://arxiv.org/abs/2510.08800)
*Wangjie You,Xusheng Wang,Xing Wang,Wenxiang Jiao,Chao Feng,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: 提出了中文常识多跳推理基准CCMOR，用于评估LLMs在中文语境下整合事实知识与多步逻辑推理的能力，发现现有模型在处理长尾知识和知识密集型推理方面存在局限，但检索增强生成能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型展现了先进的推理能力，但在通用中文语境下的全面评估仍然不足，需要专门的中文基准来评估模型整合中文特定事实知识与多步逻辑推理的能力。

Method: 首先从现有QA数据集中构建领域平衡的种子集，然后开发基于LLM的流水线生成基于事实单元链的多跳问题，并实施人机协同验证系统，由领域专家系统验证和优化生成的问题。

Result: 使用CCMOR评估最先进的LLMs，发现模型在处理长尾知识和执行知识密集型推理方面存在持续局限，但检索增强生成显著缓解了这些知识差距，带来了显著的性能提升。

Conclusion: CCMOR基准揭示了当前LLMs在中文语境下知识推理的局限性，同时证明了检索增强生成是提升模型知识推理能力的有效方法，为中文语言模型的评估和改进提供了重要工具。

Abstract: While Large Language Models (LLMs) have demonstrated advanced reasoning
capabilities, their comprehensive evaluation in general Chinese-language
contexts remains understudied. To bridge this gap, we propose Chinese
Commonsense Multi-hop Reasoning (CCMOR), a novel benchmark designed to evaluate
LLMs' ability to integrate Chinese-specific factual knowledge with multi-step
logical reasoning. Specifically, we first construct a domain-balanced seed set
from existing QA datasets, then develop an LLM-powered pipeline to generate
multi-hop questions anchored on factual unit chains. To ensure the quality of
resulting dataset, we implement a human-in-the-loop verification system, where
domain experts systematically validate and refine the generated questions.
Using CCMOR, we evaluate state-of-the-art LLMs, demonstrating persistent
limitations in LLMs' ability to process long-tail knowledge and execute
knowledge-intensive reasoning. Notably, retrieval-augmented generation
substantially mitigates these knowledge gaps, yielding significant performance
gains.

</details>


### [39] [MOSAIC: Multi-agent Orchestration for Task-Intelligent Scientific Coding](https://arxiv.org/abs/2510.08804)
*Siddeshwar Raghavan,Tanwi Mallick*

Main category: cs.CL

TL;DR: MOSAIC是一个用于解决科学编码任务的多智能体LLM框架，通过学生-教师范式和整合上下文窗口设计，在科学编码基准测试中表现出更高的准确性、鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 科学工作流需要严谨的算法、深度领域知识整合和特定领域推理，传统通用编码方法难以满足这些需求，且许多科学问题需要解决一系列子问题才能得到最终结果。

Method: 设计了一个无需训练的多智能体框架，包含专门设计的智能体进行自我反思、创建原理、编码和调试，采用学生-教师范式，结合整合上下文窗口(CCW)来缓解LLM在处理链式子问题时的幻觉问题。

Result: 在科学编码基准测试中，MOSAIC在准确性、鲁棒性和可解释性方面优于现有方法。

Conclusion: MOSAIC通过多智能体设计和学生-教师范式，成功解决了科学代码生成中的挑战，为复杂科学任务的自动化编码提供了有效解决方案。

Abstract: We present MOSAIC, a multi-agent Large Language Model (LLM) framework for
solving challenging scientific coding tasks. Unlike general-purpose coding,
scientific workflows require algorithms that are rigorous, interconnected with
deep domain knowledge, and incorporate domain-specific reasoning, as well as
algorithm iteration without requiring I/O test cases. Many scientific problems
also require a sequence of subproblems to be solved, leading to the final
desired result. MOSAIC is designed as a training-free framework with specially
designed agents to self-reflect, create the rationale, code, and debug within a
student-teacher paradigm to address the challenges of scientific code
generation. This design facilitates stepwise problem decomposition, targeted
error correction, and, when combined with our Consolidated Context Window
(CCW), mitigates LLM hallucinations when solving complex scientific tasks
involving chained subproblems. We evaluate MOSAIC on scientific coding
benchmarks and demonstrate that our specialized agentic framework outperforms
existing approaches in terms of accuracy, robustness, and interpretability.

</details>


### [40] [The Model's Language Matters: A Comparative Privacy Analysis of LLMs](https://arxiv.org/abs/2510.08813)
*Abhishek K. Mishra,Antoine Boutet,Lucas Magnana*

Main category: cs.CL

TL;DR: 本文研究发现语言结构影响LLMs的隐私泄露风险，意大利语泄露最严重，英语成员推断分离性更高，法语和西班牙语因形态复杂性更抗泄露。


<details>
  <summary>Details</summary>
Motivation: LLMs在多语言应用中处理敏感数据，但现有隐私评估主要针对英语，需要研究语言结构如何影响隐私泄露。

Method: 量化六种语言指标，评估三种攻击向量：提取、反事实记忆和成员推断，分析英语、西班牙语、法语和意大利语医疗语料。

Result: 隐私漏洞与语言冗余和分词粒度正相关：意大利语泄露最强，英语成员推断分离性更高，法语和西班牙语因形态复杂性更抗泄露。

Conclusion: 语言结构显著影响LLMs隐私泄露，需要开发语言感知的隐私保护机制。

Abstract: Large Language Models (LLMs) are increasingly deployed across multilingual
applications that handle sensitive data, yet their scale and linguistic
variability introduce major privacy risks. Mostly evaluated for English, this
paper investigates how language structure affects privacy leakage in LLMs
trained on English, Spanish, French, and Italian medical corpora. We quantify
six linguistic indicators and evaluate three attack vectors: extraction,
counterfactual memorization, and membership inference. Results show that
privacy vulnerability scales with linguistic redundancy and tokenization
granularity: Italian exhibits the strongest leakage, while English shows higher
membership separability. In contrast, French and Spanish display greater
resilience due to higher morphological complexity. Overall, our findings
provide the first quantitative evidence that language matters in privacy
leakage, underscoring the need for language-aware privacy-preserving mechanisms
in LLM deployments.

</details>


### [41] [Search-on-Graph: Iterative Informed Navigation for Large Language Model Reasoning on Knowledge Graphs](https://arxiv.org/abs/2510.08825)
*Jia Ao Sun,Hao Yu,Fabrizio Gotti,Fengran Mo,Yihong Wu,Yuchen Hui,Jian-Yun Nie*

Main category: cs.CL

TL;DR: SoG框架通过让LLM在知识图谱上执行迭代的知情图导航来解决KGQA问题，避免了预规划路径或检索大子图的问题，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: LLM在知识密集型、多跳问题上不可靠，会遗漏长尾事实、产生幻觉且知识滞后；现有KGQA方法面临查询编译脆弱、子图检索引入噪声、复杂代理框架搜索空间爆炸等基本权衡问题。

Method: 提出Search-on-Graph框架，采用"观察-然后导航"原则：在每个步骤中，LLM检查当前实体的实际可用关系，然后决定下一跳，使用单一精心设计的Search函数进行迭代知情图导航。

Result: 在六个KGQA基准测试中达到最先进性能，无需微调。特别是在Wikidata基准上比之前最佳方法提升16%，在Freebase基准上也有持续改进。

Conclusion: SoG是一个简单而有效的框架，通过迭代知情图导航使LLM能够可靠地处理知识图谱问答任务，适应不同KG模式并处理高度节点。

Abstract: Large language models (LLMs) have demonstrated impressive reasoning abilities
yet remain unreliable on knowledge-intensive, multi-hop questions -- they miss
long-tail facts, hallucinate when uncertain, and their internal knowledge lags
behind real-world change. Knowledge graphs (KGs) offer a structured source of
relational evidence, but existing KGQA methods face fundamental trade-offs:
compiling complete SPARQL queries without knowing available relations proves
brittle, retrieving large subgraphs introduces noise, and complex agent
frameworks with parallel exploration exponentially expand search spaces. To
address these limitations, we propose Search-on-Graph (SoG), a simple yet
effective framework that enables LLMs to perform iterative informed graph
navigation using a single, carefully designed \textsc{Search} function. Rather
than pre-planning paths or retrieving large subgraphs, SoG follows an
``observe-then-navigate'' principle: at each step, the LLM examines actual
available relations from the current entity before deciding on the next hop.
This approach further adapts seamlessly to different KG schemas and handles
high-degree nodes through adaptive filtering. Across six KGQA benchmarks
spanning Freebase and Wikidata, SoG achieves state-of-the-art performance
without fine-tuning. We demonstrate particularly strong gains on Wikidata
benchmarks (+16\% improvement over previous best methods) alongside consistent
improvements on Freebase benchmarks.

</details>


### [42] [Pattern Enhanced Multi-Turn Jailbreaking: Exploiting Structural Vulnerabilities in Large Language Models](https://arxiv.org/abs/2510.08859)
*Ragib Amin Nihal,Rui Wen,Kazuhiro Nakadai,Jun Sakuma*

Main category: cs.CL

TL;DR: PE-CoA框架通过五种对话模式构建有效的多轮越狱攻击，在12个LLM和10个危害类别上达到最先进性能，揭示了模型特定的弱点模式和家族共享的失败模式。


<details>
  <summary>Details</summary>
Motivation: 现有多轮越狱方法依赖启发式或临时探索策略，对模型底层弱点洞察有限，且对话模式与跨危害类别模型漏洞之间的关系理解不足。

Method: 提出PE-CoA框架，包含五种对话模式来通过自然对话构建有效的多轮越狱攻击。

Result: 在12个LLM和10个危害类别上评估，达到最先进性能，发现模式特定的漏洞和LLM行为特征：模型表现出不同的弱点分布，对一个对话模式的鲁棒性不能泛化到其他模式，模型家族共享相似的失败模式。

Conclusion: 这些发现凸显了安全训练的局限性，表明需要模式感知的防御措施。

Abstract: Large language models (LLMs) remain vulnerable to multi-turn jailbreaking
attacks that exploit conversational context to bypass safety constraints
gradually. These attacks target different harm categories (like malware
generation, harassment, or fraud) through distinct conversational approaches
(educational discussions, personal experiences, hypothetical scenarios).
Existing multi-turn jailbreaking methods often rely on heuristic or ad hoc
exploration strategies, providing limited insight into underlying model
weaknesses. The relationship between conversation patterns and model
vulnerabilities across harm categories remains poorly understood. We propose
Pattern Enhanced Chain of Attack (PE-CoA), a framework of five conversation
patterns to construct effective multi-turn jailbreaks through natural dialogue.
Evaluating PE-CoA on twelve LLMs spanning ten harm categories, we achieve
state-of-the-art performance, uncovering pattern-specific vulnerabilities and
LLM behavioral characteristics: models exhibit distinct weakness profiles where
robustness to one conversational pattern does not generalize to others, and
model families share similar failure modes. These findings highlight
limitations of safety training and indicate the need for pattern-aware
defenses. Code available on: https://github.com/Ragib-Amin-Nihal/PE-CoA

</details>


### [43] [Quality Estimation Reranking for Document-Level Translation](https://arxiv.org/abs/2510.08870)
*Krzysztof Mrozinski,Minji Kang,Ahmed Khota,Vincent Michael Sutanto,Giovanni Gatti De Giacomo*

Main category: cs.CL

TL;DR: 本文研究了在文档级别机器翻译中应用质量估计重排序的方法，发现使用SLIDE和GEMBA-DA等质量估计指标能显著提升翻译质量，特别是在使用更多候选翻译时效果更明显。


<details>
  <summary>Details</summary>
Motivation: 质量估计重排序在句子级别机器翻译中已被证明有效，但在日益重要的文档级别翻译领域应用不足，需要探索其在该领域的性能表现。

Method: 使用多种学习型和基于大语言模型的质量估计指标（如SLIDE和GEMBA-DA），在文档级别翻译中对生成的翻译候选进行评分和选择，评估重排序效果。

Result: 使用最佳学习型指标SLIDE时，BLEURT-20得分在2个候选翻译时提升+2.00，在32个候选时提升+5.09；使用最佳LLM指标GEMBA-DA时，分别提升+1.63和+4.30。即使在最长文档（512-1024源标记）上，32个候选重排序仍能带来+2.34（SLIDE）和+1.40（GEMBA-DA）的改进。

Conclusion: 文档级别的质量估计重排序具有实际应用价值，在合适的翻译模型和硬件条件下，能以最小的运行时开销显著提升翻译质量。

Abstract: Quality estimation (QE) reranking is a form of quality-aware decoding which
aims to improve machine translation (MT) by scoring and selecting the best
candidate from a pool of generated translations. While known to be effective at
the sentence level, its application to the increasingly prominent domain of
document-level translation remains underexplored. In this work, we evaluate QE
reranking performance on document-level (rather than the typical
sentence-level) translation, using various learned and large language model
(LLM)-based QE metrics. We find that with our best learned metric, SLIDE,
BLEURT-20 scores improve by +2.00 with only two candidates, and by +5.09 with
32, across both decoder-only LLM models and encoder-decoder neural machine
translation (NMT) models. Using the best LLM-based metric, GEMBA-DA, gains of
+1.63 and +4.30 are achieved under the same conditions. Although gains shrink
with longer inputs, reranking with 32 candidates yields improvements of +2.34
(SLIDE) and +1.40 (GEMBA-DA) on our longest documents (512-1024 source tokens).
These findings demonstrate the practical value of document-level QE, with
minimal runtime overhead given suitable translation models and hardware.

</details>


### [44] [Exploring Multi-Temperature Strategies for Token- and Rollout-Level Control in RLVR](https://arxiv.org/abs/2510.08892)
*Haomin Zhuang,Yujun Zhou,Taicheng Guo,Yue Huang,Fangxu Liu,Kai Song,Xiangliang Zhang*

Main category: cs.CL

TL;DR: 提出了一种在强化学习中为不同token类型应用不同温度设置的方法，通过为推理token使用更高温度鼓励探索，为知识token使用更低温度保持事实准确性，显著提升了LLM的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常限制更新来间接鼓励探索，但未在token生成阶段明确促进探索行为。研究发现LLM中的token在推理任务中扮演不同角色，分为高熵推理token和低熵知识token。

Method: 在采样过程中为不同token类型应用不同的温度设置：为推理token使用更高温度以主动鼓励探索，为知识token使用更低温度以保持事实正确性。系统研究了多种多温度调度策略及其在强化学习中的影响。

Result: 在多个推理基准测试上的实证评估表明，该方法显著提升了LLM的推理性能。

Conclusion: 通过明确在采样阶段促进探索的多温度方法，有效提升了LLM的推理能力，为强化学习中的token级探索提供了新的视角。

Abstract: Reinforcement Learning has demonstrated substantial improvements in the
reasoning abilities of Large Language Models (LLMs), exhibiting significant
applicability across various domains. Recent research has identified that
tokens within LLMs play distinct roles during reasoning tasks, categorizing
them into high-entropy reasoning tokens and low-entropy knowledge tokens. Prior
approaches have typically focused on restricting updates to indirectly
encourage exploration, yet they do not explicitly facilitate exploratory
behavior during the token generation stage itself. In this work, we introduce a
complementary approach that explicitly promotes exploration during sampling by
applying distinct temperature settings for different token types. Specifically,
our method employs higher temperatures for reasoning tokens to actively
encourage exploration, while retaining lower temperatures for knowledge tokens
to maintain factual correctness. Furthermore, we systematically investigate
various multi-temperature scheduling strategies and their impacts within
reinforcement learning contexts. Empirical evaluations on several reasoning
benchmarks demonstrate that our approach significantly enhances the reasoning
performance of LLMs. The code is available at
https://github.com/zhmzm/Multi_Temperature_Verl.git.

</details>


### [45] [A Unified Biomedical Named Entity Recognition Framework with Large Language Models](https://arxiv.org/abs/2510.08902)
*Tengxiao Lv,Ling Luo,Juntao Li,Yanhua Wang,Yuchen Pan,Chao Liu,Yanan Wang,Yan Jiang,Huiyi Lv,Yuanyuan Sun,Jian Wang,Hongfei Lin*

Main category: cs.CL

TL;DR: 提出基于大语言模型的统一生物医学命名实体识别框架，通过文本生成重构任务、符号标注策略处理嵌套实体，采用双语联合微调和对比学习实体选择器，在多个基准数据集上实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理嵌套实体、实体边界模糊和跨语言泛化方面存在困难，需要更强大的生物医学命名实体识别解决方案。

Method: 将BioNER重构为文本生成任务，设计符号标注策略处理平面和嵌套实体；进行中英文双语联合微调；引入基于对比学习的实体选择器过滤错误预测。

Result: 在四个基准数据集和两个未见语料上的实验结果显示，该方法实现了最先进的性能，并具有稳健的零样本跨语言泛化能力。

Conclusion: 所提出的统一BioNER框架在生物医学命名实体识别任务中表现出色，特别是在处理嵌套实体和跨语言泛化方面具有显著优势。

Abstract: Accurate recognition of biomedical named entities is critical for medical
information extraction and knowledge discovery. However, existing methods often
struggle with nested entities, entity boundary ambiguity, and cross-lingual
generalization. In this paper, we propose a unified Biomedical Named Entity
Recognition (BioNER) framework based on Large Language Models (LLMs). We first
reformulate BioNER as a text generation task and design a symbolic tagging
strategy to jointly handle both flat and nested entities with explicit boundary
annotation. To enhance multilingual and multi-task generalization, we perform
bilingual joint fine-tuning across multiple Chinese and English datasets.
Additionally, we introduce a contrastive learning-based entity selector that
filters incorrect or spurious predictions by leveraging boundary-sensitive
positive and negative samples. Experimental results on four benchmark datasets
and two unseen corpora show that our method achieves state-of-the-art
performance and robust zero-shot generalization across languages. The source
codes are freely available at https://github.com/dreamer-tx/LLMNER.

</details>


### [46] [Autoencoding-Free Context Compression for LLMs via Contextual Semantic Anchors](https://arxiv.org/abs/2510.08907)
*Xin Liu,RunSong Zhao,PengCheng Huang,XinYu Liu,JunYi Xiao,ChunYang Xiao,Tong Xiao,Shengxiang Gao,Zhengtao Yu,JingBo Zhu*

Main category: cs.CL

TL;DR: SAC提出了一种新的上下文压缩方法，通过直接选择锚点标记并聚合上下文信息到其KV表示中，避免了自编码训练，在多种压缩比下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前上下文压缩方法主要依赖自编码任务训练，但这种方法存在优化目标与下游任务不匹配的问题，削弱了对实际使用更有益的特征。

Method: SAC直接从原始上下文中选择锚点标记，通过锚点嵌入和双向注意力修改两个关键设计，将上下文信息聚合到锚点标记的KV表示中，无需自编码训练。

Result: 实验结果显示SAC在各种压缩比下都优于现有方法，在MRQA的分布外评估中，5倍压缩时比强基线提升1个EM分数，且在更高压缩比下优势更明显。

Conclusion: SAC通过直接利用锚点标记进行上下文压缩，避免了自编码训练的不匹配问题，提供了一种更有效的上下文压缩解决方案。

Abstract: Context compression presents a promising approach for accelerating large
language model (LLM) inference by compressing long contexts into compact
representations. Current context compression methods predominantly rely on
autoencoding tasks to train context-agnostic compression tokens to compress
contextual semantics. While autoencoding tasks enable compression tokens to
acquire compression capabilities, compression via autoencoding tasks creates a
fundamental mismatch: the models are optimized for reconstruction that diverge
from actual downstream tasks, thereby weakening the features more beneficial
for real-world usage. We propose Semantic-Anchor Compression (SAC), a novel
method that shifts from autoencoding task based compression to an architecture
that is equipped with this compression capability \textit{a priori}. Instead of
training models to compress contexts through autoencoding tasks, SAC directly
selects so-called anchor tokens from the original context and aggregates
contextual information into their key-value (KV) representations. By deriving
representations directly from the contextual tokens, SAC eliminates the need
for autoencoding training. To ensure compression performance while directly
leveraging anchor tokens, SAC incorporates two key designs: (1) anchor
embeddings that enable the compressor to identify critical tokens, and (2)
bidirectional attention modification that allows anchor tokens to capture
information from the entire context. Experimental results demonstrate that SAC
consistently outperforms existing context compression methods across various
compression ratios. On out-of-distribution evaluation using MRQA, SAC achieves
1 EM improvement at 5x compression over strong baselines, with increasing
advantages at higher compression ratios.

</details>


### [47] [Artificial Impressions: Evaluating Large Language Model Behavior Through the Lens of Trait Impressions](https://arxiv.org/abs/2510.08915)
*Nicholas Deas,Kathleen McKeown*

Main category: cs.CL

TL;DR: 该研究引入并分析了LLM内部表征中类似人类印象和刻板印象的人工印象模式，使用线性探针预测刻板印象内容模型的两个维度，发现人工印象可以预测模型回复的质量和模糊语使用，并研究了提示内容特征对LLM印象的影响。


<details>
  <summary>Details</summary>
Motivation: 研究LLM内部表征中是否存在类似人类印象和刻板印象的模式，以及这些人工印象如何影响模型的下游行为表现。

Method: 使用线性探针在生成的提示上拟合，根据刻板印象内容模型的两个维度预测印象，分析隐藏表征中印象的可解码性及其与模型行为的关系。

Result: 发现LLM在提示时不一致地报告印象，但印象更一致地可从隐藏表征中线性解码；人工印象可以预测模型回复的质量和模糊语使用；提示的内容、风格和方言特征会影响LLM的印象形成。

Conclusion: LLM内部存在可解码的人工印象模式，这些印象与模型的下游行为相关，提示的特定特征会影响LLM的印象形成，揭示了LLM表征中类似人类认知偏见的现象。

Abstract: We introduce and study artificial impressions--patterns in LLMs' internal
representations of prompts that resemble human impressions and stereotypes
based on language. We fit linear probes on generated prompts to predict
impressions according to the two-dimensional Stereotype Content Model (SCM).
Using these probes, we study the relationship between impressions and
downstream model behavior as well as prompt features that may inform such
impressions. We find that LLMs inconsistently report impressions when prompted,
but also that impressions are more consistently linearly decodable from their
hidden representations. Additionally, we show that artificial impressions of
prompts are predictive of the quality and use of hedging in model responses. We
also investigate how particular content, stylistic, and dialectal features in
prompts impact LLM impressions.

</details>


### [48] [SOP-Maze: Evaluating Large Language Models on Complicated Business Standard Operating Procedures](https://arxiv.org/abs/2510.08942)
*Jiaming Wang,Zhe Tang,Yilin Jin,Peng Ding,Xiaoyu Li,Xuezhi Cao*

Main category: cs.CL

TL;DR: 提出了SOP-Maze基准，包含397个任务，评估LLM在复杂标准操作程序场景中的表现，发现现有模型在程序遵循、对话处理和计算推理方面存在困难。


<details>
  <summary>Details</summary>
Motivation: 现有基准未能充分评估LLM在涉及复杂标准操作程序的真实业务场景中的能力，需要专门针对这类场景的评估工具。

Method: 从真实业务数据构建SOP-Maze基准，包含23个复杂SOP场景的397个任务，分为横向根系统（宽选项任务）和核心根系统（深度逻辑推理任务）两类。

Result: 几乎所有最先进模型在SOP-Maze上都表现不佳，识别出三个主要错误类别：路线盲区、对话脆弱性和计算错误。

Conclusion: SOP-Maze为评估LLM在复杂业务场景中的能力提供了新视角，揭示了模型在程序遵循和深度推理方面的局限性，为改进模型能力提供了重要见解。

Abstract: As large language models (LLMs) are widely deployed as domain-specific
agents, many benchmarks have been proposed to evaluate their ability to follow
instructions and make decisions in real-world scenarios. However, business
scenarios often involve complex standard operating procedures (SOPs), and the
evaluation of LLM capabilities in such contexts has not been fully explored. To
bridge this gap, we propose SOP-Maze, a benchmark constructed from real-world
business data and adapted into a collection of 397 tasks from 23 complex SOP
scenarios. We further categorize SOP tasks into two broad classes: Lateral Root
System (LRS), representing wide-option tasks that demand precise selection; and
Heart Root System (HRS), which emphasizes deep logical reasoning with complex
branches. Extensive experiments reveal that nearly all state-of-the-art models
struggle with SOP-Maze. We conduct a comprehensive analysis and identify three
key error categories: (i) route blindness: difficulty following procedures;
(ii) conversational fragility: inability to handle real dialogue nuances; and
(iii) calculation errors: mistakes in time or arithmetic reasoning under
complex contexts. The systematic study explores LLM performance across SOP
tasks that challenge both breadth and depth, offering new insights for
improving model capabilities. We have open-sourced our work on
https://github.com/ADoublLEN/SOP-Maze.

</details>


### [49] [A Human Behavioral Baseline for Collective Governance in Software Projects](https://arxiv.org/abs/2510.08956)
*Mobina Noori,Mahasweta Chakraborti,Amy X Zhang,Seth Frey*

Main category: cs.CL

TL;DR: 该研究分析了开源社区治理文档的演变，发现项目随时间推移定义了更多角色和行动，这些元素分布更均匀，而规则组成保持稳定，表明治理通过扩展和平衡参与类别而发展。


<details>
  <summary>Details</summary>
Motivation: 研究开源社区如何通过版本控制的治理文档描述参与和控制，为评估未来AI中介工作流程是否集中或重新分配权力提供可复现基线。

Method: 使用710个项目的配对快照语料库，将文本解析为参与者、规则、行动和对象，然后通过熵测量均匀性、丰富度测量多样性、Jensen Shannon散度测量漂移来分组和测量变化。

Result: 项目随时间推移定义了更多角色和更多行动，这些元素分布更均匀，而规则组成保持稳定。

Conclusion: 治理通过扩展和平衡参与类别而发展，没有发生规定性力量的重大转变，为评估未来AI中介工作流程是否集中或重新分配权力提供了可复现基线。

Abstract: We study how open source communities describe participation and control
through version controlled governance documents. Using a corpus of 710 projects
with paired snapshots, we parse text into actors, rules, actions, and objects,
then group them and measure change with entropy for evenness, richness for
diversity, and Jensen Shannon divergence for drift. Projects define more roles
and more actions over time, and these are distributed more evenly, while the
composition of rules remains stable. These findings indicate that governance
grows by expanding and balancing categories of participation without major
shifts in prescriptive force. The analysis provides a reproducible baseline for
evaluating whether future AI mediated workflows concentrate or redistribute
authority.

</details>


### [50] [Creation of the Chinese Adaptive Policy Communication Corpus](https://arxiv.org/abs/2510.08986)
*Bolun Sun,Charles Chang,Yuen Yuen Ang,Pingxu Hao,Ruotong Mu,Yuchen Xu,Zhengxin Zhang*

Main category: cs.CL

TL;DR: CAPC-CG是首个中文政策指令开放数据集，包含1949-2023年中国中央政府的法律、行政法规和部门规章，标注了清晰与模糊语言的五色分类体系，包含330万个段落单元，并提供元数据、标注框架和基准分类结果。


<details>
  <summary>Details</summary>
Motivation: 填补中文政策指令数据集的空白，支持政策沟通研究和多语言NLP应用，基于Ang的自适应政策沟通理论构建分类体系。

Method: 构建包含国家法律、行政法规和部门规章的语料库，将文档分段为段落单元，采用专家和训练有素的标注员进行两轮标注，使用Fleiss's kappa评估标注一致性。

Result: 创建了包含330万个段落单元的数据集，标注一致性达到Fleiss's kappa = 0.86，提供了多个大语言模型的基准分类结果和完整的标注手册。

Conclusion: CAPC-CG为政策沟通研究和下游任务提供了高质量的数据资源，支持监督建模和多语言NLP研究，具有重要的学术和应用价值。

Abstract: We introduce CAPC-CG, the Chinese Adaptive Policy Communication (Central
Government) Corpus, the first open dataset of Chinese policy directives
annotated with a five-color taxonomy of clear and ambiguous language
categories, building on Ang's theory of adaptive policy communication. Spanning
1949-2023, this corpus includes national laws, administrative regulations, and
ministerial rules issued by China's top authorities. Each document is segmented
into paragraphs, producing a total of 3.3 million units. Alongside the corpus,
we release comprehensive metadata, a two-round labeling framework, and a
gold-standard annotation set developed by expert and trained coders.
Inter-annotator agreement achieves a Fleiss's kappa of K = 0.86 on directive
labels, indicating high reliability for supervised modeling. We provide
baseline classification results with several large language models (LLMs),
together with our annotation codebook, and describe patterns from the dataset.
This release aims to support downstream tasks and multilingual NLP research in
policy communication.

</details>


### [51] [MASA: LLM-Driven Multi-Agent Systems for Autoformalization](https://arxiv.org/abs/2510.08988)
*Lan Zhang,Marco Valentino,André Freitas*

Main category: cs.CL

TL;DR: MASA是一个基于大语言模型的多智能体自动形式化框架，通过协作智能体将自然语言语句转换为形式化表示，具有模块化、灵活性和可扩展性特点。


<details>
  <summary>Details</summary>
Motivation: 自动形式化在连接自然语言和形式推理中起着关键作用，需要一种能够适应快速发展的领域并提高效率和可靠性的解决方案。

Method: 采用多智能体系统架构，强调模块化、灵活性和可扩展性，允许无缝集成新智能体和工具，利用大语言模型与定理证明器的交互。

Result: 在真实世界数学定义和形式数学数据集上的用例和实验证明了MASA的有效性。

Conclusion: 这项工作突显了由大语言模型和定理证明器交互驱动的多智能体系统在提高自动形式化效率和可靠性方面的潜力，为该领域的研究人员和实践者提供了宝贵的见解和支持。

Abstract: Autoformalization serves a crucial role in connecting natural language and
formal reasoning. This paper presents MASA, a novel framework for building
multi-agent systems for autoformalization driven by Large Language Models
(LLMs). MASA leverages collaborative agents to convert natural language
statements into their formal representations. The architecture of MASA is
designed with a strong emphasis on modularity, flexibility, and extensibility,
allowing seamless integration of new agents and tools to adapt to a
fast-evolving field. We showcase the effectiveness of MASA through use cases on
real-world mathematical definitions and experiments on formal mathematics
datasets. This work highlights the potential of multi-agent systems powered by
the interaction of LLMs and theorem provers in enhancing the efficiency and
reliability of autoformalization, providing valuable insights and support for
researchers and practitioners in the field.

</details>


### [52] [DARO: Difficulty-Aware Reweighting Policy Optimization](https://arxiv.org/abs/2510.09001)
*Jingyu Zhou,Lu Ma,Hao Liang,Chengyu Shen,Bin Cui,Wentao Zhang*

Main category: cs.CL

TL;DR: 本文提出了DARO方法，通过动态调整不同难度组的损失贡献来解决现有RLVR方法中损失尺度不平衡的问题，在多个数学基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于GRPO的RLVR方法存在静态或过于简化的权重分配方案，无法适应模型能力的动态变化，导致训练在不同难度级别上失衡，阻碍整体性能提升。

Method: 提出了难度感知重加权策略优化（DARO），根据模型的学习状态动态调整每个难度组的损失贡献，解决损失尺度问题。

Result: 在Qwen2.5-Math-1.5B、Qwen2.5-Math-7B和Llama3.1-8B模型上的实验表明，DARO在六个数学基准测试中优于四个领先基线方法，实现了更快的收敛速度和更优的最终性能。

Conclusion: DARO通过动态难度感知的重加权机制有效解决了现有RLVR方法的局限性，为增强语言模型的推理能力提供了更有效的训练方法。

Abstract: Recent advances in large language models (LLMs) have shown that reasoning
ability can be significantly enhanced through Reinforcement Learning with
Verifiable Rewards (RLVR). Group Relative Policy Optimization (GRPO) has
emerged as the de facto approach for RLVR, inspiring numerous variants.
However, our mathematical analysis reveals that these methods are fundamentally
weighted variations of GRPO. We provide a unified view, demonstrating that
their reliance on static or overly simplistic weighting schemes tied to sample
difficulty prevents adaptation to a model's evolving capabilities. This creates
a significant loss scale issue, where training disproportionately focuses on
certain difficulty levels at the expense of others, hindering overall
performance. To address these limitations, we introduce
\textbf{Difficulty-Aware Reweighting Policy Optimization (DARO)}, a method that
dynamically adjusts the loss contribution of each difficulty group based on the
model's learning state. Extensive experiments on Qwen2.5-Math-1.5B,
Qwen2.5-Math-7B, and Llama3.1-8B show that DARO outperforms four leading
baselines across six math benchmarks, achieving significantly faster
convergence and superior final performance.

</details>


### [53] [Decoupling Safety into Orthogonal Subspace: Cost-Efficient and Performance-Preserving Alignment for Large Language Models](https://arxiv.org/abs/2510.09004)
*Yutao Mou,Xiaoling Zhou,Yuxiao Luo,Shikun Zhang,Wei Ye*

Main category: cs.CL

TL;DR: LoRA-based refusal training enables performance-preserving safety alignment using only safety data, serving as cost-efficient safety patches that decouple safety into orthogonal low-rank subspaces.


<details>
  <summary>Details</summary>
Motivation: Safety alignment is challenging as it often degrades general performance, and current methods require expensive searches for optimal data proportions to balance safety and performance.

Method: Proposes LoRA-based refusal training that trains solely on safety data, with theoretical and experimental evidence showing LoRA decouples safety into low-rank subspaces orthogonal to the model's intrinsic capabilities.

Result: Demonstrates that LoRA enables performance-preserving safety alignment without degrading general capabilities, providing cost-efficient and plug-and-play safety patches.

Conclusion: LoRA serves as an effective method for safety alignment that preserves model performance by decoupling safety enhancements into orthogonal low-rank subspaces, avoiding interference with inherent capabilities.

Abstract: Safety alignment is essential for building trustworthy artificial
intelligence, yet it remains challenging to enhance model safety without
degrading general performance. Current approaches require computationally
expensive searches for the optimal proportion of safety-critical and
general-purpose data to balance safety and general performance, incurring high
costs with limited gains. In this work, we show that LoRA-based
Refusal-training enables performance-preserving safety alignment even when
trained solely on safety data, demonstrating that LoRA serves as
cost-efficient, performance-preserving, and plug-and-play safety patches.
Beyond empirical findings, we provide both theoretical and experimental
evidence that LoRA effectively decouples safety into a low-rank subspace
largely orthogonal to the model's intrinsic transformation space, ensuring that
safety enhancements do not interfere with inherent capabilities.

</details>


### [54] [LitE-SQL: A Lightweight and Efficient Text-to-SQL Framework with Vector-based Schema Linking and Execution-Guided Self-Correction](https://arxiv.org/abs/2510.09014)
*Shengmin Piao,Jieun Lee,Sanghyun Park*

Main category: cs.CL

TL;DR: LitE-SQL是一个轻量高效的Text-to-SQL框架，使用向量数据库进行模式链接，并通过两阶段微调实现自我修正，在保持高性能的同时大幅减少参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型的Text-to-SQL方法依赖专有模型，存在部署可行性和数据隐私问题，需要更轻量级的解决方案。

Method: 包含两个组件：(1) 模式检索器：使用预计算模式嵌入的向量数据库进行高效模式链接；(2) SQL生成器：通过监督微调和执行引导的强化学习两阶段微调，实现无需多候选生成的自我修正。

Result: 在BIRD数据集上达到72.10%执行准确率，在Spider 1.0上达到88.45%，性能与基于LLM的方法相当或更优，同时参数数量减少2-30倍。

Conclusion: 研究表明使用轻量级模型实现高质量Text-to-SQL生成是可行的，为隐私敏感和资源受限场景提供了实用解决方案。

Abstract: The Text-to-SQL task translates natural language questions into SQL queries,
enabling intuitive database interaction for non-experts. While recent methods
leveraging Large Language Models (LLMs) achieve strong performance, their
reliance on proprietary models raise concerns about deployment feasibility and
data privacy. In this work, we introduce LitE-SQL, a Lightweight and Efficient
framework with two components: (i) a Schema Retriever that performs efficient
schema linking using a vector database of pre-computed schema embeddings, and
(ii) a SQL Generator fine-tuned in two stages-supervised fine-tuning followed
by execution-guided reinforcement-enabling self-correction without costly
multi-candidate generation. On BIRD, LitE-SQL achieves 72.10% execution
accuracy, and on Spider 1.0 it reaches 88.45%, demonstrating comparable or
superior performance to LLM-based methods despite using 2x to 30x fewer
parameters. Our findings demonstrate that high-quality Text-to-SQL generation
is feasible with lightweight models, offering a practical solution for
privacy-sensitive and resource-constrained settings.

</details>


### [55] [Automated Refinement of Essay Scoring Rubrics for Language Models via Reflect-and-Revise](https://arxiv.org/abs/2510.09030)
*Keno Harada,Lui Yoshida,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.CL

TL;DR: 本研究通过迭代优化LLM使用的评分标准来提升自动作文评分性能，在TOEFL11和ASAP数据集上取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的性能对提示词高度敏感，受提示优化领域启发，本研究探索通过优化评分标准来提升自动作文评分效果。

Method: 通过让模型反思自身评分理由与人类评分差异，迭代优化评分标准，在TOEFL11和ASAP数据集上使用GPT-4.1、Gemini-2.5-Pro和Qwen-3-Next-80B-A3B-Instruct进行实验。

Result: 在TOEFL11和ASAP数据集上分别实现了高达0.19和0.47的QWK改进，即使使用简单初始标准也能达到或超过详细人工编写标准的效果。

Conclusion: 迭代评分标准优化对于基于LLM的自动作文评分至关重要，能够显著提升与人类评估的一致性。

Abstract: The performance of Large Language Models (LLMs) is highly sensitive to the
prompts they are given. Drawing inspiration from the field of prompt
optimization, this study investigates the potential for enhancing Automated
Essay Scoring (AES) by refining the scoring rubrics used by LLMs. Specifically,
our approach prompts models to iteratively refine rubrics by reflecting on
models' own scoring rationales and observed discrepancies with human scores on
sample essays. Experiments on the TOEFL11 and ASAP datasets using GPT-4.1,
Gemini-2.5-Pro, and Qwen-3-Next-80B-A3B-Instruct show Quadratic Weighted Kappa
(QWK) improvements of up to 0.19 and 0.47, respectively. Notably, even with a
simple initial rubric, our approach achieves comparable or better QWK than
using detailed human-authored rubrics. Our findings highlight the importance of
iterative rubric refinement in LLM-based AES to enhance alignment with human
evaluations.

</details>


### [56] [Exploring Cross-Lingual Knowledge Transfer via Transliteration-Based MLM Fine-Tuning for Critically Low-resource Chakma Language](https://arxiv.org/abs/2510.09032)
*Adity Khisa,Nusrat Jahan Lia,Tasnim Mahfuz Nafis,Zarif Masud,Tanzir Pial,Shebuti Rayana,Ahmedul Kabir*

Main category: cs.CL

TL;DR: 本文针对低资源语言查克玛语，构建了首个高质量的孟加拉文转写查克玛语语料库，并通过微调6种多语言Transformer模型，证明了转写方法在查克玛语迁移学习中的有效性。


<details>
  <summary>Details</summary>
Motivation: 查克玛语作为一种印度-雅利安语，可用数据有限，在语言模型中代表性不足，需要开发有效方法来支持这种低资源语言的处理。

Method: 从查克玛文学中收集语料，经母语者验证构建孟加拉文转写查克玛语数据集，然后对mBERT、XLM-RoBERTa等6种多语言Transformer模型进行掩码语言建模微调。

Result: 微调后的多语言模型在查克玛语任务上表现显著提升，最高达到73.54%的token准确率和2.90的低困惑度，证明了转写方法的有效性。

Conclusion: 孟加拉文转写查克玛语对于查克玛语的迁移学习非常有效，同时研究揭示了数据质量对模型性能的重要影响以及OCR处理形态丰富的印度文字时的局限性。

Abstract: As an Indo-Aryan language with limited available data, Chakma remains largely
underrepresented in language models. In this work, we introduce a novel corpus
of contextually coherent Bangla-transliterated Chakma, curated from Chakma
literature, and validated by native speakers. Using this dataset, we fine-tune
six encoder-based multilingual and regional transformer models (mBERT,
XLM-RoBERTa, DistilBERT, DeBERTaV3, BanglaBERT, and IndicBERT) on masked
language modeling (MLM) tasks. Our experiments show that fine-tuned
multilingual models outperform their pre-trained counterparts when adapted to
Bangla-transliterated Chakma, achieving up to 73.54% token accuracy and a
perplexity as low as 2.90. Our analysis further highlights the impact of data
quality on model performance and shows the limitations of OCR pipelines for
morphologically rich Indic scripts. Our research demonstrates that
Bangla-transliterated Chakma can be very effective for transfer learning for
Chakma language, and we release our manually validated monolingual dataset to
encourage further research on multilingual language modeling for low-resource
languages.

</details>


### [57] [Large Language Models Do NOT Really Know What They Don't Know](https://arxiv.org/abs/2510.09033)
*Chi Seng Cheang,Hou Pong Chan,Wenxuan Zhang,Yang Deng*

Main category: cs.CL

TL;DR: LLM内部状态无法可靠区分事实与幻觉，因为基于主题知识的幻觉与正确回答使用相同的内部召回过程，导致隐藏状态几何重叠且不可区分。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明LLM在内部表示中编码了事实性信号，但LLM也会产生事实错误，这引发了一个问题：内部计算是否能可靠区分事实输出和幻觉输出。

Method: 通过比较两种基于主题知识依赖程度的幻觉类型，对LLM处理事实查询的内部机制进行分析。

Result: 当幻觉与主题知识相关时，LLM使用与正确回答相同的内部召回过程，导致隐藏状态几何重叠；而与主题知识无关的幻觉产生不同的聚类表示，使其可检测。

Conclusion: LLM并未在其内部状态中编码真实性，而只是编码知识召回的模式，表明'LLM并不真正知道它们不知道什么'。

Abstract: Recent work suggests that large language models (LLMs) encode factuality
signals in their internal representations, such as hidden states, attention
weights, or token probabilities, implying that LLMs may "know what they don't
know". However, LLMs can also produce factual errors by relying on shortcuts or
spurious associations. These error are driven by the same training objective
that encourage correct predictions, raising the question of whether internal
computations can reliably distinguish between factual and hallucinated outputs.
In this work, we conduct a mechanistic analysis of how LLMs internally process
factual queries by comparing two types of hallucinations based on their
reliance on subject information. We find that when hallucinations are
associated with subject knowledge, LLMs employ the same internal recall process
as for correct responses, leading to overlapping and indistinguishable
hidden-state geometries. In contrast, hallucinations detached from subject
knowledge produce distinct, clustered representations that make them
detectable. These findings reveal a fundamental limitation: LLMs do not encode
truthfulness in their internal states but only patterns of knowledge recall,
demonstrating that "LLMs don't really know what they don't know".

</details>


### [58] [Alif: Advancing Urdu Large Language Models via Multilingual Synthetic Data Distillation](https://arxiv.org/abs/2510.09051)
*Muhammad Ali Shafique,Kanwal Mehreen,Muhammad Arham,Maaz Amjad,Sabur Butt,Hamza Farooq*

Main category: cs.CL

TL;DR: Alif-1.0-8B-Instruct是一个专门针对乌尔都语的多语言模型，通过改进的自指导技术训练，在预算低于100美元的情况下在乌尔都语任务上超越了多个主流多语言模型。


<details>
  <summary>Details</summary>
Motivation: 为乌尔都语等低资源语言开发高性能大语言模型面临高质量数据集稀缺、多语言不一致性和安全性问题等挑战，现有方法通过翻译大量数据但缺乏质量和文化细微差别。

Method: 使用改进的自指导技术创建高质量多语言合成数据集(Urdu-Instruct)，包含乌尔都语本地思维链推理、双语翻译、文化相关性和伦理安全对齐，基于预训练的Llama-3.1-8B构建模型。

Result: Alif-1.0-8B-Instruct在乌尔都语特定任务上优于Llama-3.1-8B-Instruct，并超越了Mistral-7B-Instruct-v0.3、Qwen-2.5-7B-Instruct和Cohere-Aya-Expanse-8B等领先多语言模型。

Conclusion: 通过改进的自指导方法可以高效开发高性能、低资源语言的大语言模型，同时实现文化对齐，所有数据集、模型和代码均已公开。

Abstract: Developing a high-performing large language models (LLMs) for low-resource
languages such as Urdu, present several challenges. These challenges include
the scarcity of high-quality datasets, multilingual inconsistencies, and safety
concerns. Existing multilingual LLMs often address these issues by translating
large volumes of available data. However, such translations often lack quality
and cultural nuance while also incurring significant costs for data curation
and training. To address these issues, we propose Alif-1.0-8B-Instruct, a
multilingual Urdu-English model, that tackles these challenges with a unique
approach. We train the model on a high-quality, multilingual synthetic dataset
(Urdu-Instruct), developed using a modified self-instruct technique. By using
unique prompts and seed values for each task along with a global task pool,
this dataset incorporates Urdu-native chain-of-thought based reasoning,
bilingual translation, cultural relevance, and ethical safety alignments. This
technique significantly enhances the comprehension of Alif-1.0-8B-Instruct
model for Urdu-specific tasks. As a result, Alif-1.0-8B-Instruct, built upon
the pretrained Llama-3.1-8B, demonstrates superior performance compared to
Llama-3.1-8B-Instruct for Urdu specific-tasks. It also outperformed leading
multilingual LLMs, including Mistral-7B-Instruct-v0.3, Qwen-2.5-7B-Instruct,
and Cohere-Aya-Expanse-8B, all within a training budget of under $100. Our
results demonstrate that high-performance and low-resource language LLMs can be
developed efficiently and culturally aligned using our modified self-instruct
approach. All datasets, models, and code are publicly available at:
https://github.com/traversaal-ai/alif-urdu-llm.

</details>


### [59] [ReFIne: A Framework for Trustworthy Large Reasoning Models with Reliability, Faithfulness, and Interpretability](https://arxiv.org/abs/2510.09062)
*Chung-En Sun,Ge Yan,Akshay Kulkarni,Tsui-Wei Weng*

Main category: cs.CL

TL;DR: 提出了ReFIne训练框架，通过监督微调和GRPO结合，提升推理模型的可解释性、忠实性和可靠性，而不仅仅是追求准确率和token效率。


<details>
  <summary>Details</summary>
Motivation: 当前长链思维推理研究过于关注准确率和token效率，忽视了可信赖性的关键方面。可用的推理系统必须具有可解释性、忠实性和可靠性这三个可信赖特性。

Method: ReFIne训练框架结合监督微调和GRPO，鼓励模型：(i)通过生成结构化、基于标签的推理轨迹提高可解释性；(ii)通过明确披露每个解决方案的决定性信息增强忠实性；(iii)通过提供推导过程和最终答案的自我评估来促进可靠性。

Result: 在Qwen3模型(1.7B/4B/8B)上应用ReFIne，在数学基准测试中显示：推理轨迹更清晰结构化(可解释性+44.0%)，更忠实地暴露决策过程(忠实性+18.8%)，提供信息丰富的置信度估计(可靠性+42.4%)。

Conclusion: 推理模型不仅应优化准确率，还应优化可信赖性的更广泛维度。ReFIne框架为构建可信赖的推理系统提供了重要方向。

Abstract: Recent advances in long chain-of-thought (CoT) reasoning have largely
prioritized answer accuracy and token efficiency, while overlooking aspects
critical to trustworthiness. We argue that usable reasoning systems must be
trustworthy, characterized by three properties: interpretability, faithfulness,
and reliability. To this end, we propose ReFIne, a new training framework that
integrates supervised fine-tuning with GRPO to encourage models to: (i) improve
interpretability by producing structured, tag-based traces with high-level
planning that are easier for humans to follow; (ii) enhance faithfulness by
explicitly disclosing the decisive information guiding each solution, with
consistent cross-section references; and (iii) promote reliability by providing
self-assessments of both the derivation's soundness and the confidence of the
final answer. We apply ReFIne to the Qwen3 models at multiple scales
(1.7B/4B/8B) and evaluate across mathematical benchmarks of varying difficulty.
Our experimental results show that ReFIne models generate clearer and
better-structured reasoning traces (interpretability +44.0%), more faithfully
expose their underlying decision process (faithfulness +18.8%), and offer
informative confidence estimates (reliability +42.4%). These findings highlight
an overlooked but important direction: reasoning models should be optimized not
only for accuracy, but also for broader dimensions of trustworthiness. Our code
is available at:
https://github.com/Trustworthy-ML-Lab/Training_Trustworthy_LRM_with_Refine

</details>


### [60] [FrameEOL: Semantic Frame Induction using Causal Language Models](https://arxiv.org/abs/2510.09097)
*Chihiro Yano,Kosuke Yamada,Hayato Tsukagoshi,Ryohei Sasano,Koichi Takeda*

Main category: cs.CL

TL;DR: 提出了一种基于因果语言模型（CLMs）的语义框架归纳新方法FrameEOL，通过上下文学习和深度度量学习获得更适合框架归纳的嵌入表示，在英语和日语FrameNet数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 虽然因果语言模型（如GPT、Llama系列）在广泛的语言理解任务中表现出色，并能像理解框架一样进行对话，但尚未应用于语义框架归纳任务。而掩码语言模型（MLMs）如BERT在该任务中已取得高性能。

Method: 提出FrameEOL方法：基于提示的方法获取框架嵌入，输出代表给定情境的单一框架名称标签。利用上下文学习（ICL）和深度度量学习（DML）获得更适合框架归纳的嵌入表示，然后通过聚类进行框架归纳。

Result: 在英语和日语FrameNet数据集上的实验结果表明，所提方法优于现有的框架归纳方法。特别是对于缺乏广泛框架资源的日语，仅使用5个ICL示例的CLM方法就达到了与使用DML微调的MLM方法相当的性能。

Conclusion: 基于因果语言模型的语义框架归纳方法是有效的，特别是在资源匮乏的语言中，仅需少量示例就能达到良好性能，为语义框架归纳任务提供了新的解决方案。

Abstract: Semantic frame induction is the task of clustering frame-evoking words
according to the semantic frames they evoke. In recent years, leveraging
embeddings of frame-evoking words that are obtained using masked language
models (MLMs) such as BERT has led to high-performance semantic frame
induction. Although causal language models (CLMs) such as the GPT and Llama
series succeed in a wide range of language comprehension tasks and can engage
in dialogue as if they understood frames, they have not yet been applied to
semantic frame induction. We propose a new method for semantic frame induction
based on CLMs. Specifically, we introduce FrameEOL, a prompt-based method for
obtaining Frame Embeddings that outputs One frame-name as a Label representing
the given situation. To obtain embeddings more suitable for frame induction, we
leverage in-context learning (ICL) and deep metric learning (DML). Frame
induction is then performed by clustering the resulting embeddings.
Experimental results on the English and Japanese FrameNet datasets demonstrate
that the proposed methods outperform existing frame induction methods. In
particular, for Japanese, which lacks extensive frame resources, the CLM-based
method using only 5 ICL examples achieved comparable performance to the
MLM-based method fine-tuned with DML.

</details>


### [61] [When Retrieval Succeeds and Fails: Rethinking Retrieval-Augmented Generation for LLMs](https://arxiv.org/abs/2510.09106)
*Yongjie Wang,Yue Yu,Kaisong Song,Jun Lin,Zhiqi Shen*

Main category: cs.CL

TL;DR: 本文对检索增强生成（RAG）进行了全面回顾，分析了其目标、组件、关键挑战和应用场景，指出在LLM能力不断增强的背景下需要重新思考RAG的作用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在静态语料库上训练，难以处理快速演变的信息或领域特定查询，因此需要RAG来整合外部检索机制以获取最新相关知识。

Method: 通过全面回顾RAG框架，分析其目标、核心组件、关键挑战和局限性，并展示RAG与LLM结合的应用案例。

Result: 识别了RAG的关键弱点，展示了在LLM单独表现不佳的场景中，RAG能够显著增强LLM的有效性。

Conclusion: 鼓励研究人员重新思考RAG的角色，并启发下一代RAG系统的开发，以应对LLM能力不断增强的新环境。

Abstract: Large Language Models (LLMs) have enabled a wide range of applications
through their powerful capabilities in language understanding and generation.
However, as LLMs are trained on static corpora, they face difficulties in
addressing rapidly evolving information or domain-specific queries.
Retrieval-Augmented Generation (RAG) was developed to overcome this limitation
by integrating LLMs with external retrieval mechanisms, allowing them to access
up-to-date and contextually relevant knowledge. However, as LLMs themselves
continue to advance in scale and capability, the relative advantages of
traditional RAG frameworks have become less pronounced and necessary. Here, we
present a comprehensive review of RAG, beginning with its overarching
objectives and core components. We then analyze the key challenges within RAG,
highlighting critical weakness that may limit its effectiveness. Finally, we
showcase applications where LLMs alone perform inadequately, but where RAG,
when combined with LLMs, can substantially enhance their effectiveness. We hope
this work will encourage researchers to reconsider the role of RAG and inspire
the development of next-generation RAG systems.

</details>


### [62] [DITING: A Multi-Agent Evaluation Framework for Benchmarking Web Novel Translation](https://arxiv.org/abs/2510.09116)
*Enze Zhang,Jiaying Wang,Mengxi Xiao,Jifei Liu,Ziyan Kuang,Rui Dong,Youzhong Dong,Sophia Ananiadou,Min Peng,Qianqian Xie*

Main category: cs.CL

TL;DR: 提出了DITING框架，这是首个针对网络小说翻译的全面评估框架，包含六个维度评估叙事和文化保真度，并开发了AgentEval多智能体评估框架和MetricAlign元评估数据集。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在机器翻译方面取得显著进展，但在网络小说翻译方面的有效性尚不清楚，现有基准测试依赖表面指标，无法捕捉该体裁的独特特征。

Method: 引入DITING评估框架，包含成语翻译、词汇歧义、术语本地化、时态一致性、零代词解析和文化安全六个维度，基于18K+专家标注的中英句子对；提出AgentEval推理驱动多智能体评估框架；开发MetricAlign元评估数据集。

Result: 评估14个开源、闭源和商业模型发现，中文训练的LLM优于更大的外国模型，DeepSeek-V3提供了最忠实和风格一致的翻译。

Conclusion: 为探索基于LLM的网络小说翻译建立了新范式，并提供了公共资源以推动未来研究。

Abstract: Large language models (LLMs) have substantially advanced machine translation
(MT), yet their effectiveness in translating web novels remains unclear.
Existing benchmarks rely on surface-level metrics that fail to capture the
distinctive traits of this genre. To address these gaps, we introduce DITING,
the first comprehensive evaluation framework for web novel translation,
assessing narrative and cultural fidelity across six dimensions: idiom
translation, lexical ambiguity, terminology localization, tense consistency,
zero-pronoun resolution, and cultural safety, supported by over 18K
expert-annotated Chinese-English sentence pairs. We further propose AgentEval,
a reasoning-driven multi-agent evaluation framework that simulates expert
deliberation to assess translation quality beyond lexical overlap, achieving
the highest correlation with human judgments among seven tested automatic
metrics. To enable metric comparison, we develop MetricAlign, a meta-evaluation
dataset of 300 sentence pairs annotated with error labels and scalar quality
scores. Comprehensive evaluation of fourteen open, closed, and commercial
models reveals that Chinese-trained LLMs surpass larger foreign counterparts,
and that DeepSeek-V3 delivers the most faithful and stylistically coherent
translations. Our work establishes a new paradigm for exploring LLM-based web
novel translation and provides public resources to advance future research.

</details>


### [63] [Augmenting Dialog with Think-Aloud Utterances for Modeling Individual Personality Traits by LLM](https://arxiv.org/abs/2510.09158)
*Seiya Ishikura,Hiroaki Yamada,Tatsuya Hiraoka,Hiroaki Yamada,Takenobu Tokunaga*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This study proposes augmenting dialog data with think-aloud utterances (TAUs)
for modeling individual personalities in text chat by LLM. TAU is a
verbalization of a speaker's thought before articulating the utterance. We
expect "persona LLMs" trained with TAU-augmented data can mimic the speaker's
personality trait better. We tested whether the trained persona LLMs obtain the
human personality with respect to Big Five, a framework characterizing human
personality traits from five aspects. The results showed that LLMs trained with
TAU-augmented data more closely align to the speakers' Agreeableness and
Neuroticism of Big Five than those trained with original dialog data. We also
found that the quality of TAU-augmentation impacts persona LLM's performance.

</details>


### [64] [Stronger Re-identification Attacks through Reasoning and Aggregation](https://arxiv.org/abs/2510.09184)
*Lucas Georges Gabriel Charpentier,Pierre Lison*

Main category: cs.CL

TL;DR: 本文提出了两种互补策略来构建更强的文本重识别攻击：一是通过聚合多个PII识别顺序的预测来改进结果，二是利用推理模型在拥有丰富背景知识时提升重识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有的文本去标识化技术难以有效评估其身份隐藏能力，需要通过重识别攻击来评估去标识化方法的鲁棒性。

Method: 提出两种策略：(1) 考虑PII跨度重识别顺序的重要性，通过聚合多个顺序的预测来改进结果；(2) 使用推理模型，特别是在攻击者拥有丰富背景知识时提升重识别性能。

Result: 两种策略都能显著提升重识别攻击的效果，特别是推理模型在拥有丰富背景知识时表现更佳。

Conclusion: 通过考虑重识别顺序和利用推理模型，可以构建更强的重识别攻击来更准确地评估文本去标识化方法的有效性。

Abstract: Text de-identification techniques are often used to mask personally
identifiable information (PII) from documents. Their ability to conceal the
identity of the individuals mentioned in a text is, however, hard to measure.
Recent work has shown how the robustness of de-identification methods could be
assessed by attempting the reverse process of _re-identification_, based on an
automated adversary using its background knowledge to uncover the PIIs that
have been masked. This paper presents two complementary strategies to build
stronger re-identification attacks. We first show that (1) the _order_ in which
the PII spans are re-identified matters, and that aggregating predictions
across multiple orderings leads to improved results. We also find that (2)
reasoning models can boost the re-identification performance, especially when
the adversary is assumed to have access to extensive background knowledge.

</details>


### [65] [LLaMAX2: Your Translation-Enhanced Model also Performs Well in Reasoning](https://arxiv.org/abs/2510.09189)
*Changjiang Gao,Zixian Huang,Jingyang Gong,Shujian Huang,Lei Li,Fei Yuan*

Main category: cs.CL

TL;DR: 提出了一种新的翻译增强方法，通过指令模型和层选择性调优，在保持推理能力的同时显著提升翻译性能，特别是在低资源语言上。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型擅长推理，但专门增强翻译的模型在推理任务上表现不佳，需要一种方法能同时兼顾翻译和推理能力。

Method: 使用指令模型作为基础，仅对并行数据进行层选择性调优，提出了Qwen3-XPlus模型。

Result: 在高低资源语言翻译上均有显著提升，低资源语言如斯瓦希里语达到15+ spBLEU和40+ xComet；在7个多语言任务上平均提升1+分，同时在15个流行推理数据集上保持与Qwen3指令模型相当的能力。

Conclusion: 该方法为多语言增强提供了有前景的途径，显著降低了复杂性并提高了更多语言的可访问性。

Abstract: General Large Language Models (LLMs) excel in reasoning, but those enhanced
for translation struggle with reasoning tasks. To address this, we propose a
novel translationenhanced recipe that begins with instruct models and applies
layer-selective tuning only on parallel data. Following this pipeline, we
introduce the Qwen3-XPlus models, which demonstrate significant improvements in
translation performance across both high- and lowresource languages, achieving
15+ spBLEU and 40+ xComet in low-resource languages, like Swahili.
Interestingly, training only with small parallel datasets, Qwen3-XPlus achieves
an average improvement of 1+ points on 7 multilingual tasks while maintaining
proficiency comparable to the Qwen3 instruct model in 15 popular reasoning
datasets. This work offers a promising approach to multilingual enhancement,
significantly reducing complexity and enhancing accessibility for a wider range
of languages. The code and model are publicly available.

</details>


### [66] [DICE: Structured Reasoning in LLMs through SLM-Guided Chain-of-Thought Correction](https://arxiv.org/abs/2510.09211)
*Yiqi Li,Yusheng Liao,Zhe Chen,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: DICE是一个轻量级框架，通过小语言模型(SLMs)对大型语言模型(LLMs)的输出进行思维链修正，解决LLMs在推理任务中忽视格式要求的问题，无需对LLMs进行昂贵的微调。


<details>
  <summary>Details</summary>
Motivation: LLMs在执行推理任务时经常优先考虑推理过程而忽视详细的输出格式要求，而直接微调LLMs成本高昂且参数访问受限。

Method: DICE采用两阶段方法：首先构建结构化思维链适应数据集，然后应用双重调优策略训练SLMs，以分析-回答模式生成结构化输出。

Result: 实验表明DICE将LLM输出的格式准确性和内容正确性分别提高了35.4%和29.4%，达到了最先进的性能水平。

Conclusion: DICE框架在保留LLMs广泛知识和推理能力的同时，确保输出符合用户需求，提供了一种高效解决输出格式问题的方案。

Abstract: When performing reasoning tasks with user-specific requirements, such as
strict output formats, large language models (LLMs) often prioritize reasoning
over adherence to detailed instructions. Fine-tuning LLMs on supervised
datasets to address this is impractical due to high computational costs and
limited parameter access. To tackle this, we propose DICE, a lightweight
framework that guides small language models (SLMs) to refine LLMs' outputs
through chain-of-thought (CoT) correction. DICE decouples the process by first
prompting LLMs to generate natural language responses, then using trained SLMs
to analyze and refine these outputs to meet structured output specifications.
This framework preserves LLMs' broad knowledge and reasoning capabilities while
ensuring the outputs conform to user demands. Specifically, DICE first
constructs structured CoT adaptation datasets via a two-stage method and
subsequently applies a dual-tuning strategy to fine-tune SLMs for generating
structured outputs in an analyze-then-answer pattern. Experiments demonstrate
that DICE improves the average format accuracy and content correctness of LLM
outputs by 35.4\% and 29.4\%, respectively, achieving state-of-the-art (SOTA)
performance over other competitive baselines.

</details>


### [67] [IRIS: An Iterative and Integrated Framework for Verifiable Causal Discovery in the Absence of Tabular Data](https://arxiv.org/abs/2510.09217)
*Tao Feng,Lizhen Qu,Niket Tandon,Gholamreza Haffari*

Main category: cs.CL

TL;DR: IRIS是一个创新的因果发现框架，通过结合统计算法和LLM方法，从初始变量自动收集文档、提取变量并发现已知和新的因果关系，无需预存数据集即可实现实时因果发现。


<details>
  <summary>Details</summary>
Motivation: 传统统计方法面临数据收集成本高、对已知关系重复计算、假设不现实等挑战，而现有LLM方法虽然擅长识别常见因果关系，但无法发现新的关系。

Method: IRIS框架从初始变量开始，自动收集相关文档、提取变量并发现因果关系。采用混合方法结合统计算法和LLM方法，并包含缺失变量提议组件来扩展因果图。

Result: 该框架能够发现已知和新的因果关系，通过识别和纳入缺失变量来扩展因果图，实现仅从初始变量开始的实时因果发现。

Conclusion: IRIS解决了传统因果发现方法的局限性，提供了一个无需预存数据集、能够发现新颖因果关系的实时发现框架。

Abstract: Causal discovery is fundamental to scientific research, yet traditional
statistical algorithms face significant challenges, including expensive data
collection, redundant computation for known relations, and unrealistic
assumptions. While recent LLM-based methods excel at identifying commonly known
causal relations, they fail to uncover novel relations. We introduce IRIS
(Iterative Retrieval and Integrated System for Real-Time Causal Discovery), a
novel framework that addresses these limitations. Starting with a set of
initial variables, IRIS automatically collects relevant documents, extracts
variables, and uncovers causal relations. Our hybrid causal discovery method
combines statistical algorithms and LLM-based methods to discover known and
novel causal relations. In addition to causal discovery on initial variables,
the missing variable proposal component of IRIS identifies and incorporates
missing variables to expand the causal graphs. Our approach enables real-time
causal discovery from only a set of initial variables without requiring
pre-existing datasets.

</details>


### [68] [CrisiText: A dataset of warning messages for LLM training in emergency communication](https://arxiv.org/abs/2510.09243)
*Giacomo Gonella,Gian Maria Campedelli,Stefano Menini,Marco Guerini*

Main category: cs.CL

TL;DR: CrisiText是首个用于危机预警消息生成的大规模数据集，包含13种危机场景下的40多万条预警消息，旨在通过NLG技术为危机中的人们提供及时有效的警告信息。


<details>
  <summary>Details</summary>
Motivation: 在危机情况下有效识别威胁并减轻潜在损害至关重要，但目前NLP技术在危机应对中的应用主要局限于分类任务，而使用NLG架构生成及时预警消息的潜力被大大忽视。

Method: 从现有的危机描述出发，创建与场景相关的事件链，每个事件都配有一个预警消息。生成过程遵循专家编写的指南，确保术语正确性和建议的事实性。每个消息还附带三种次优预警类型，用于研究不同的NLG方法。

Result: 进行了系列实验，比较了监督微调设置与偏好对齐、零样本和少样本方法。进一步评估了模型在分布外场景下的性能，并评估了自动后编辑器的有效性。

Conclusion: CrisiText数据集为危机预警消息生成研究提供了重要资源，展示了NLG技术在危机应对中的巨大潜力，为开发更有效的危机响应系统奠定了基础。

Abstract: Effectively identifying threats and mitigating their potential damage during
crisis situations, such as natural disasters or violent attacks, is paramount
for safeguarding endangered individuals. To tackle these challenges, AI has
been used in assisting humans in emergency situations. Still, the use of NLP
techniques remains limited and mostly focuses on classification tasks. The
significant potential of timely warning message generation using NLG
architectures, however, has been largely overlooked. In this paper we present
CrisiText, the first large-scale dataset for the generation of warning messages
across 13 different types of crisis scenarios. The dataset contains more than
400,000 warning messages (spanning almost 18,000 crisis situations) aimed at
assisting civilians during and after such events. To generate the dataset, we
started from existing crisis descriptions and created chains of events related
to the scenarios. Each event was then paired with a warning message. The
generations follow experts' written guidelines to ensure correct terminology
and factuality of their suggestions. Additionally, each message is accompanied
by three suboptimal warning types to allow for the study of different NLG
approaches. To this end, we conducted a series of experiments comparing
supervised fine-tuning setups with preference alignment, zero-shot, and
few-shot approaches. We further assessed model performance in
out-of-distribution scenarios and evaluated the effectiveness of an automatic
post-editor.

</details>


### [69] [DSPO: Stable and Efficient Policy Optimization for Agentic Search and Reasoning](https://arxiv.org/abs/2510.09255)
*Chenyang Gu,Yewen Pu,Bruce Yang,Xiaofan Li,Huan Gao*

Main category: cs.CL

TL;DR: DSPO是一种改进的强化学习算法，通过序列级优化和动态样本过滤来训练LLM进行多轮搜索和推理，无需监督演示数据，在多个QA基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 增强LLMs主动搜索外部知识的能力对于复杂现实任务至关重要，当前方法要么依赖提示激发模型内在代理能力，要么在复杂交互任务中应用RL时存在性能上限和崩溃问题。

Method: 提出了动态过滤序列级策略优化（DSPO），这是一种改进的RL算法，通过序列级优化和动态样本过滤实现稳健的代理训练，纯RL训练模型进行多轮搜索和推理交织。

Result: 在多个QA基准测试中，DSPO训练的7B模型比同类先前工作提升34.1%，在复杂多跳QA如HotpotQA中甚至比先前工作的14B模型相对提升近9%，并保持优异的训练稳定性。

Conclusion: DSPO算法有效解决了LLM在复杂交互任务中的训练稳定性问题，显著提升了模型在知识搜索和推理任务中的性能，证明了纯RL训练方法的有效性。

Abstract: Enhancing LLMs with the ability to actively search external knowledge is
crucial for complex and real-world tasks. Current approaches either rely on
prompting to elicit the model's innate agent capabilities, or suffer from
performance ceilings and collapse when applying RL to complex interactive
tasks, leaving their true agentic potential untapped. To address this, we
introduce \textbf{D}ynamic-filter \textbf{S}equence-level \textbf{P}olicy
\textbf{O}ptimization (DSPO), an improved RL algorithm designed for robust
agent training through sequence-level optimization and dynamic sample
filtering. We train our model purely through RL to interleave multi-turn search
and reasoning, obviating the need for supervised demonstration data. Across
multiple QA benchmarks, our DSPO-trained 7B model improves over a comparable
previous work by \textbf{34.1\%}, and even outperforms the 14B model from
previous work in complex multihop QA such as HotpotQA by nearly \textbf{9\%
relative}, maintaining exceptional training stability.

</details>


### [70] [Detecting Data Contamination from Reinforcement Learning Post-training for Large Language Models](https://arxiv.org/abs/2510.09259)
*Yongding Tao,Tian Wang,Yihong Dong,Huanyu Liu,Kechi Zhang,Xiaolong Hu,Ge Li*

Main category: cs.CL

TL;DR: 本文提出了首个针对RL后训练阶段数据污染检测的方法Self-Critique，通过检测模型输出熵分布崩溃来识别污染数据，在RL-MIA基准上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 随着RL后训练在提升LLM推理能力中的重要性增加，现有污染检测方法主要针对预训练和SFT阶段，缺乏专门针对RL后训练阶段的检测方法，这构成了关键漏洞。

Method: 提出Self-Critique方法，基于关键观察：RL阶段后LLM输出熵分布会崩溃到高度特定的稀疏模式。该方法通过探测底层策略崩溃（模型收敛到狭窄推理路径）来检测污染。

Result: 在多个模型和污染任务上的广泛实验表明，Self-Critique显著优于基线方法，AUC提升高达30%，而现有方法在RL阶段污染检测上接近随机猜测。

Conclusion: Self-Critique填补了RL后训练阶段污染检测的研究空白，使RL阶段污染检测成为可能，为可靠评估LLM性能提供了重要保障。

Abstract: Data contamination poses a significant threat to the reliable evaluation of
Large Language Models (LLMs). This issue arises when benchmark samples may
inadvertently appear in training sets, compromising the validity of reported
performance. While detection methods have been developed for the pre-training
and Supervised Fine-Tuning stages, a critical research gap exists for the
increasingly significant phase of Reinforcement Learning (RL) post-training. As
RL post-training becomes pivotal for advancing LLM reasoning, the absence of
specialized contamination detection methods in this paradigm presents a
critical vulnerability. To address this, we conduct the first systematic study
of data detection within RL post-training scenario and propose Self-Critique.
Our method is motivated by a key observation: after RL phase, the output
entropy distribution of LLMs tends to collapse into highly specific and sparse
modes. Self-Critique probes for the underlying policy collapse, i.e., the
model's convergence to a narrow reasoning path, which causes this entropy
reduction. To facilitate this research, we also introduce RL-MIA, a benchmark
constructed to simulate this specific contamination scenario. Extensive
experiments show that Self-Critique significantly outperforms baseline methods
across multiple models and contamination tasks, achieving an AUC improvement of
up to 30%. Whereas existing methods are close to a random guess for RL-phase
contamination, our method makes detection possible.

</details>


### [71] [CFVBench: A Comprehensive Video Benchmark for Fine-grained Multimodal Retrieval-Augmented Generation](https://arxiv.org/abs/2510.09266)
*Kaiwen Wei,Xiao Liu,Jie Zhang,Zijian Wang,Ruida Liu,Yuming Yang,Xin Xiao,Xiao Sun,Haoyang Zeng,Changzai Pan,Yidan Zhang,Jiang Zhong,Peijin Wang,Yingchao Feng*

Main category: cs.CL

TL;DR: 提出了CFVBench基准测试，评估多模态检索增强生成模型在细粒度多模态理解上的表现，并提出了自适应视觉优化框架来提升性能


<details>
  <summary>Details</summary>
Motivation: 现有的多模态检索增强生成基准测试在模态覆盖和格式多样性方面存在局限，主要关注单模态或有限模态任务，缺乏对细粒度多模态信息的处理能力评估

Method: 构建了CFVBench基准测试，包含599个公开视频和5360个开放式问答对；提出了自适应视觉优化框架，通过自适应增加帧采样密度和选择性调用外部工具来提升细粒度多模态理解

Result: 系统评估了7种检索方法和14个常用多模态大语言模型，发现当前模型在捕捉瞬态但关键的细粒度多模态细节方面存在瓶颈；自适应视觉优化框架在所有评估模型上都一致提升了细粒度多模态理解和性能

Conclusion: 细粒度多模态理解是多模态检索增强生成的关键瓶颈，提出的自适应视觉优化框架能有效缓解这一问题，为未来模型开发提供了重要方向

Abstract: Multimodal Retrieval-Augmented Generation (MRAG) enables Multimodal Large
Language Models (MLLMs) to generate responses with external multimodal
evidence, and numerous video-based MRAG benchmarks have been proposed to
evaluate model capabilities across retrieval and generation stages. However,
existing benchmarks remain limited in modality coverage and format diversity,
often focusing on single- or limited-modality tasks, or coarse-grained scene
understanding. To address these gaps, we introduce CFVBench, a large-scale,
manually verified benchmark constructed from 599 publicly available videos,
yielding 5,360 open-ended QA pairs. CFVBench spans high-density formats and
domains such as chart-heavy reports, news broadcasts, and software tutorials,
requiring models to retrieve and reason over long temporal video spans while
maintaining fine-grained multimodal information. Using CFVBench, we
systematically evaluate 7 retrieval methods and 14 widely-used MLLMs, revealing
a critical bottleneck: current models (even GPT5 or Gemini) struggle to capture
transient yet essential fine-grained multimodal details. To mitigate this, we
propose Adaptive Visual Refinement (AVR), a simple yet effective framework that
adaptively increases frame sampling density and selectively invokes external
tools when necessary. Experiments show that AVR consistently enhances
fine-grained multimodal comprehension and improves performance across all
evaluated MLLMs

</details>


### [72] [Inflated Excellence or True Performance? Rethinking Medical Diagnostic Benchmarks with Dynamic Evaluation](https://arxiv.org/abs/2510.09275)
*Xiangxu Zhang,Lei Li,Yanyun Zhou,Xiao Zhou,Yingying Zhang,Xian Wu*

Main category: cs.CL

TL;DR: DyReMe是一个动态医疗诊断基准，通过生成类似真实咨询的病例来更准确地评估大语言模型在医疗诊断中的表现，超越了传统静态基准的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前基于静态医疗考试题目的评估方法高估了大语言模型的性能，忽略了教科书案例与真实世界中模糊多变情况之间的差异，需要更贴近临床实践的评估框架。

Method: DyReMe生成新的、类似咨询的病例，引入干扰因素如鉴别诊断和常见误诊因素，并改变表达风格以模拟多样化的真实世界查询习惯。

Result: 实验表明这种动态方法提供了更具挑战性和真实性的评估，揭示了最先进大语言模型性能与真实临床实践之间的显著不匹配。

Conclusion: 迫切需要能够更好反映可信医疗诊断需求的评估框架，DyReMe为这一方向提供了有效的解决方案。

Abstract: Medical diagnostics is a high-stakes and complex domain that is critical to
patient care. However, current evaluations of large language models (LLMs) are
fundamentally misaligned with real-world clinical practice. Most of them rely
on static benchmarks derived from public medical exam items, which tend to
overestimate model performance and ignore the difference between textbook cases
and the ambiguous, varying conditions in the real world. Recent efforts toward
dynamic evaluation offer a promising alternative, but their improvements are
limited to superficial perturbations and a narrow focus on accuracy. To address
these gaps, we propose DyReMe, a dynamic benchmark for medical diagnostics that
better reflects real clinical practice. Unlike static exam-style questions,
DyReMe generates fresh, consultation-like cases that introduce distractors such
as differential diagnoses and common misdiagnosis factors. It also varies
expression styles to mimic diverse real-world query habits. Beyond accuracy,
DyReMe evaluates LLMs on three additional clinically relevant dimensions:
veracity, helpfulness, and consistency. Our experiments demonstrate that this
dynamic approach yields more challenging and realistic assessments, revealing
significant misalignments between the performance of state-of-the-art LLMs and
real clinical practice. These findings highlight the urgent need for evaluation
frameworks that better reflect the demands of trustworthy medical diagnostics.

</details>


### [73] [CLARity: Reasoning Consistency Alone Can Teach Reinforced Experts](https://arxiv.org/abs/2510.09278)
*Jiuheng Lin,Cong Jiang,Zirui Wu,Jiarui Sun,Yansong Feng*

Main category: cs.CL

TL;DR: CLARity是一个成本效益高的强化学习框架，使用小型通用LLM提升专家模型在稀缺数据领域的推理质量，通过一致性奖励机制和两阶段训练流程提高推理一致性。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺领域训练专家LLM很困难，通常依赖多选题。但标准的基于结果的强化学习存在风险，虽然可能提高准确性，但经常降低推理质量如逻辑一致性。现有监督推理的解决方案如大规模过程奖励模型成本过高。

Method: CLARity框架整合了一致性感知奖励机制与两阶段精炼-监控训练流程来增强推理一致性，以及动态数据重构策略来更好地利用有限数据。使用小型通用LLM来指导专家模型。

Result: 实验表明CLARity相比基线提高了16.5%的响应一致性和7.5%的准确性。人类评估进一步确认了连贯性和专业性的整体改进。

Conclusion: CLARity提供了一个通用解决方案，使较小模型能够通过推理一致性有效指导专家模型。

Abstract: Training expert LLMs in domains with scarce data is difficult, often relying
on multiple-choice questions (MCQs). However, standard outcome-based
reinforcement learning (RL) on MCQs is risky. While it may improve accuracy, we
observe it often degrades reasoning quality such as logical consistency.
Existing solutions to supervise reasoning, such as large-scale Process Reward
Models (PRMs), are prohibitively expensive. To address this, we propose
CLARity, a cost-effective RL framework that enhances reasoning quality using
only a small, general-purpose LLM. CLARity integrates a consistency-aware
reward mechanism with a 2-stage refine-then-monitor training pipeline to
enhance reasoning consistency, and a dynamic data reformulation strategy to to
better exploit limited data. Experiments demonstrate that CLARity improves
response consistency by 16.5% and accuracy by 7.5% over baselines. Human
evaluations further confirm holistic improvements in coherence and
professionalism. Thus, CLARity offers a generalizable solution that enables
smaller models to effectively guide expert models by reasoning consistency.Our
code is open sourced at: https://github.com/Infinite-set/CLARity

</details>


### [74] [One Sentence, Two Embeddings: Contrastive Learning of Explicit and Implicit Semantic Representations](https://arxiv.org/abs/2510.09293)
*Kohei Oda,Po-Min Chuang,Kiyoaki Shirai,Natthawut Kertkeidkachorn*

Main category: cs.CL

TL;DR: DualCSE提出了一种双向量句子嵌入方法，分别编码句子的显式和隐式语义，在共享空间中并存，可针对不同下游任务选择相应语义表示。


<details>
  <summary>Details</summary>
Motivation: 现有句子嵌入方法通常只给每个句子分配单个向量，难以有效捕捉句子中的隐式语义，存在固有局限性。

Method: 为每个句子分配两个嵌入向量：一个表示显式语义，另一个表示隐式语义，这些嵌入在共享空间中并存。

Result: 实验结果表明DualCSE能够有效编码显式和隐式语义，并提升下游任务的性能表现。

Conclusion: DualCSE通过双向量表示成功克服了传统句子嵌入方法在捕捉隐式语义方面的局限性，为不同应用场景提供了更灵活的语义选择。

Abstract: Sentence embedding methods have made remarkable progress, yet they still
struggle to capture the implicit semantics within sentences. This can be
attributed to the inherent limitations of conventional sentence embedding
methods that assign only a single vector per sentence. To overcome this
limitation, we propose DualCSE, a sentence embedding method that assigns two
embeddings to each sentence: one representing the explicit semantics and the
other representing the implicit semantics. These embeddings coexist in the
shared space, enabling the selection of the desired semantics for specific
purposes such as information retrieval and text classification. Experimental
results demonstrate that DualCSE can effectively encode both explicit and
implicit meanings and improve the performance of the downstream task.

</details>


### [75] [MaP: A Unified Framework for Reliable Evaluation of Pre-training Dynamics](https://arxiv.org/abs/2510.09295)
*Jiapeng Wang,Changxin Tian,Kunlong Chen,Ziqi Liu,Jiaxin Mao,Wayne Xin Zhao,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: 提出MaP框架，通过检查点合并和Pass@k指标来解决LLM预训练评估中的不稳定性问题，提供更可靠的性能评估方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型预训练过程中的评估存在显著不稳定性，这掩盖了真实的学习动态，阻碍了模型的可靠进展。

Method: 提出MaP双管齐下框架：1）检查点合并通过平均最近模型权重来平滑参数空间；2）Pass@k指标提供鲁棒、低方差的统计能力估计。

Result: 实验表明MaP能产生显著更平滑的性能曲线，减少运行间方差，确保更一致的模型排名。

Conclusion: MaP为观察LLM训练动态提供了更可靠和真实的视角，为LLM研究奠定了重要的实证基础。

Abstract: Reliable evaluation is fundamental to the progress of Large Language Models
(LLMs), yet the evaluation process during pre-training is plagued by
significant instability that obscures true learning dynamics. In this work, we
systematically diagnose this instability, attributing it to two distinct
sources: \textit{Parameter Instability} from training stochasticity and
\textit{Evaluation Instability} from noisy measurement protocols. To counteract
both sources of noise, we introduce \textbf{MaP}, a dual-pronged framework that
synergistically integrates checkpoint \underline{M}erging \underline{a}nd the
\underline{P}ass@k metric. Checkpoint merging smooths the parameter space by
averaging recent model weights, while Pass@k provides a robust, low-variance
statistical estimate of model capability. Extensive experiments show that MaP
yields significantly smoother performance curves, reduces inter-run variance,
and ensures more consistent model rankings. Ultimately, MaP provides a more
reliable and faithful lens for observing LLM training dynamics, laying a
crucial empirical foundation for LLM research.

</details>


### [76] [ShiZhi: A Chinese Lightweight Large Language Model for Court View Generation](https://arxiv.org/abs/2510.09297)
*Zhitian Hou,Kun Zeng*

Main category: cs.CL

TL;DR: ShiZhi是首个专门用于法院观点生成的大语言模型，在中文法院观点生成数据集CCVG上训练，在法院观点生成任务中达到58.5 BLEU-1，在罪名预测任务中达到86.1%准确率和92.5%宏F1值。


<details>
  <summary>Details</summary>
Motivation: 法院观点生成是法律人工智能中的基础任务，但由于案件事实的多样性和复杂性，直接从原始事实生成法院观点可能限制性能表现。

Method: 构建了包含11万多个案件的中文法院观点生成数据集CCVG，每个案件包含事实描述和对应的法院观点。基于此数据集开发了专门用于法院观点生成的ShiZhi大语言模型。

Result: ShiZhi在法院观点生成任务中达到58.5 BLEU-1，在罪名预测任务中达到86.1%准确率和92.5%宏F1值。

Conclusion: 实验结果表明，即使是一个小型大语言模型，在高质量领域特定数据上训练后，也能生成合理且法律连贯的法院观点。

Abstract: Criminal Court View Generation (CVG) is a fundamental task in legal
artificial intelligence, aiming to automatically generate the "Court View"
section of a legal case document. Generating court views is challenging due to
the diversity and complexity of case facts, and directly generating from raw
facts may limit performance. In this paper, we present ShiZhi, the first large
language model (LLM) specifically designed for court view generation. We
construct a Chinese Court View Generation dataset, CCVG, of more than 110K
cases, each containing fact descriptions paired with corresponding court views.
Based on this dataset, ShiZhi achieving 58.5 BLEU-1 on court view generation
and 86.1\% accuracy with 92.5\% macro F1 on charge prediction. Experimental
results demonstrate that even a small LLM can generate reasonable and legally
coherent court views when trained on high-quality domain-specific data. Our
model and dataset are available at
\href{https://github.com/ZhitianHou/ShiZhi}{https://github.com/ZhitianHou/ShiZhi}.

</details>


### [77] [Mask Tokens as Prophet: Fine-Grained Cache Eviction for Efficient dLLM Inference](https://arxiv.org/abs/2510.09309)
*Jianuo Huang,Yaojie Zhang,Yicun Yang,Benhao Huang,Biqing Qi,Dongrui Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: MaskKV是一个专为扩散大语言模型设计的无需训练的缓存淘汰框架，通过掩码查询引导的评分机制和自适应缓存预算策略，显著减少内存占用并提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型的双向注意力缓存机制需要大量内存，限制了在资源受限环境下处理长上下文的能力。现有的缓存淘汰策略为自回归模型设计，忽略了dLLMs的独特特性，导致性能不佳。

Method: 1. 掩码查询引导的评分机制：利用注意力权重识别并淘汰每个注意力头中不太重要的提示词元；2. 自适应缓存预算策略：减少中间层的缓存分配，将资源集中在偏好提示的注意力头上。

Result: 在LLaDA模型上，将KV缓存压缩到仅256对（少于5%的词元），在LongBench上保留了94%的全缓存性能，在32k提示长度下实现了高达31倍的加速。

Conclusion: MaskKV有效解决了dLLMs的缓存内存瓶颈问题，为资源受限环境下的长上下文处理提供了高效解决方案。

Abstract: Diffusion large language models (dLLMs) present a promising alternative to
dominant autoregressive models (ARMs) by the ability of parallel decoding at
the expense of substantial computation and memory costs. Specifically, the
cache mechanism for bidirectional attention in dLLMs demands large memory
footprint, restricting their ability to handle long contexts under
resource-limited settings. Existing cache eviction strategies are designed for
ARMs and ignore the unique characteristics of dLLMs, thus leading to
unsatisfactory performance. To address these challenges, we introduce MaskKV, a
training-free cache eviction framework tailored to dLLMs, focusing on the
effect of mask tokens in dLLMs. MaskKV is built on two key innovations: (1) a
mask-query guided scoring mechanism that leverages attention weights to
identify and evict less critical prompt tokens for each head; (2) an adaptive
cache budgeting strategy that improves efficiency by reducing allocation in
intermediate layers and concentrating resources on prompt-preferring heads. On
LLaDA with MaskKV, compressing the KV cache to only 256 pairs (less than 5% of
tokens) retains 94% of the full-cache performance on LongBench and achieves up
to 31x acceleration at 32k prompt length. The code is publicly available at:
https://github.com/jianuo-huang/MaskKV

</details>


### [78] [Verifying Chain-of-Thought Reasoning via Its Computational Graph](https://arxiv.org/abs/2510.09312)
*Zheng Zhao,Yeskendir Koishekenov,Xianjun Yang,Naila Murray,Nicola Cancedda*

Main category: cs.CL

TL;DR: 提出了一种基于电路推理验证的白盒方法，通过分析正确和错误推理步骤的归因图结构特征来检测推理错误，并能指导干预以修正错误推理。


<details>
  <summary>Details</summary>
Motivation: 现有的思维链验证方法基于输出或激活，无法深入了解计算失败的原因，需要一种能揭示推理错误根本原因的白盒方法。

Method: 使用电路推理验证方法，训练分类器分析正确和错误推理步骤归因图的结构特征，这些图被视为模型潜在推理电路的执行轨迹。

Result: 该方法能高度预测推理错误，发现错误特征具有领域特异性，并能通过针对性干预成功修正模型的错误推理。

Conclusion: 通过审查模型的计算过程，可以从简单的错误检测转向对LLM推理的更深层次因果理解。

Abstract: Current Chain-of-Thought (CoT) verification methods predict reasoning
correctness based on outputs (black-box) or activations (gray-box), but offer
limited insight into why a computation fails. We introduce a white-box method:
Circuit-based Reasoning Verification (CRV). We hypothesize that attribution
graphs of correct CoT steps, viewed as execution traces of the model's latent
reasoning circuits, possess distinct structural fingerprints from those of
incorrect steps. By training a classifier on structural features of these
graphs, we show that these traces contain a powerful signal of reasoning
errors. Our white-box approach yields novel scientific insights unattainable by
other methods. (1) We demonstrate that structural signatures of error are
highly predictive, establishing the viability of verifying reasoning directly
via its computational graph. (2) We find these signatures to be highly
domain-specific, revealing that failures in different reasoning tasks manifest
as distinct computational patterns. (3) We provide evidence that these
signatures are not merely correlational; by using our analysis to guide
targeted interventions on individual transcoder features, we successfully
correct the model's faulty reasoning. Our work shows that, by scrutinizing a
model's computational process, we can move from simple error detection to a
deeper, causal understanding of LLM reasoning.

</details>


### [79] [FLRC: Fine-grained Low-Rank Compressor for Efficient LLM Inference](https://arxiv.org/abs/2510.09332)
*Yu-Chen Lu,Chong-Yan Chen,Chi-Chih Chang,Yu-Fang Hu,Kai-Chiang Wu*

Main category: cs.CL

TL;DR: FLRC是一种细粒度低秩压缩方法，通过为每层分配最优秩并结合渐进低秩解码，在保持文本生成质量的同时显著提升LLM压缩效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的巨大参数量阻碍了在资源受限硬件上的部署，而现有均匀压缩方法会导致性能显著下降且在解码时表现不佳。

Method: 提出细粒度低秩压缩器(FLRC)，高效确定每层的最优秩分配，并融入渐进低秩解码机制。

Result: 在多样化基准测试中，FLRC在摘要任务上的ROUGE-L指标比现有最优低秩压缩方法提升高达17%。

Conclusion: FLRC建立了一个更稳健高效的框架来改进LLM推理，实现了更好的压缩效果和文本生成质量。

Abstract: Although large language models (LLM) have achieved remarkable performance,
their enormous parameter counts hinder deployment on resource-constrained
hardware. Low-rank compression can reduce both memory usage and computational
demand, but applying a uniform compression ratio across all layers often leads
to significant performance degradation, and previous methods perform poorly
during decoding. To address these issues, we propose the Fine-grained Low-Rank
Compressor (FLRC), which efficiently determines an optimal rank allocation for
each layer, and incorporates progressive low-rank decoding to maintain text
generation quality. Comprehensive experiments on diverse benchmarks demonstrate
the superiority of FLRC, achieving up to a 17% improvement in ROUGE-L on
summarization tasks compared to state-of-the-art low-rank compression methods,
establishing a more robust and efficient framework to improve LLM inference.

</details>


### [80] [LLP: LLM-based Product Pricing in E-commerce](https://arxiv.org/abs/2510.09347)
*Hairu Wang,Sheng You,Qiheng Zhang,Xike Xie,Shuguang Han,Yuchen Wu,Fei Huang,Jufeng Chen*

Main category: cs.CL

TL;DR: LLP是首个基于大语言模型的二手产品定价生成框架，通过检索相似产品、两阶段优化和置信度过滤，显著提升了定价准确性和采用率。


<details>
  <summary>Details</summary>
Motivation: C2C平台上的个人卖家在二手产品定价方面面临挑战，现有静态回归模型泛化性能差且无法捕捉市场动态变化。

Method: LLP框架包括：检索相似产品、利用LLM理解自由文本中的定价信息、两阶段优化（监督微调+组相对策略优化）、置信度过滤机制。

Result: 在闲鱼平台部署后，在30%产品覆盖率下，静态采用率从40%提升至72%；在90%召回率下仍保持47%的采用率。

Conclusion: LLP框架显著超越了现有定价方法，能够很好地泛化到未见过的产品类别，在实际应用中表现出色。

Abstract: Unlike Business-to-Consumer e-commerce platforms (e.g., Amazon),
inexperienced individual sellers on Consumer-to-Consumer platforms (e.g., eBay)
often face significant challenges in setting prices for their second-hand
products efficiently. Therefore, numerous studies have been proposed for
automating price prediction. However, most of them are based on static
regression models, which suffer from poor generalization performance and fail
to capture market dynamics (e.g., the price of a used iPhone decreases over
time). Inspired by recent breakthroughs in Large Language Models (LLMs), we
introduce LLP, the first LLM-based generative framework for second-hand product
pricing. LLP first retrieves similar products to better align with the dynamic
market change. Afterwards, it leverages the LLMs' nuanced understanding of key
pricing information in free-form text to generate accurate price suggestions.
To strengthen the LLMs' domain reasoning over retrieved products, we apply a
two-stage optimization, supervised fine-tuning (SFT) followed by group relative
policy optimization (GRPO), on a dataset built via bidirectional reasoning.
Moreover, LLP employs a confidence-based filtering mechanism to reject
unreliable price suggestions. Extensive experiments demonstrate that LLP
substantially surpasses existing methods while generalizing well to unseen
categories. We have successfully deployed LLP on Xianyu\footnote\{Xianyu is
China's largest second-hand e-commerce platform.\}, significantly outperforming
the previous pricing method. Under the same 30\% product coverage, it raises
the static adoption rate (SAR) from 40\% to 72\%, and maintains a strong SAR of
47\% even at 90\% recall.

</details>


### [81] [ReTraceQA: Evaluating Reasoning Traces of Small Language Models in Commonsense Question Answering](https://arxiv.org/abs/2510.09351)
*Francesco Maria Molfese,Luca Moroni,Ciro Porcaro,Simone Conia,Roberto Navigli*

Main category: cs.CL

TL;DR: ReTraceQA是一个新的基准测试，用于评估小语言模型在常识推理任务中的推理过程有效性，而不仅仅是最终答案的准确性。研究发现14-24%的情况下模型给出正确答案但推理过程有缺陷，表明现有评估方法高估了SLM的能力。


<details>
  <summary>Details</summary>
Motivation: 当前对小语言模型的评估主要关注最终答案的准确性，而忽视了推理过程的有效性，这可能导致对模型能力的错误估计。

Method: 引入ReTraceQA基准测试，使用专家标注的数据集，并采用大型语言模型作为自动评判者进行推理感知评估。

Result: 研究发现14-24%的实例中SLM给出正确答案但推理过程有缺陷；使用LLM进行推理感知评估时，SLM在所有模型和数据集上的性能显著下降，得分最多降低25%。

Conclusion: 仅依赖最终答案准确性会高估小语言模型的推理能力，需要引入过程级评估来更准确地衡量模型的实际性能。

Abstract: While Small Language Models (SLMs) have demonstrated promising performance on
an increasingly wide array of commonsense reasoning benchmarks, current
evaluation practices rely almost exclusively on the accuracy of their final
answers, neglecting the validity of the reasoning processes that lead to those
answers. To address this issue, we introduce ReTraceQA, a novel benchmark that
introduces process-level evaluation for commonsense reasoning tasks. Our
expert-annotated dataset reveals that in a substantial portion of instances
(14-24%), SLMs provide correct final answers despite flawed reasoning
processes, suggesting that the capabilities of SLMs are often overestimated by
evaluation metrics that focus only on comparing the final answer with the
ground truth. Indeed, we show that when employing strong Large Language Models
(LLMs) as automated judges for reasoning-aware evaluation rather than
answer-only metrics, SLM performance drops significantly across all models and
datasets, with scores decreasing by up to 25%.

</details>


### [82] [Logit Arithmetic Elicits Long Reasoning Capabilities Without Training](https://arxiv.org/abs/2510.09354)
*Yunxiang Zhang,Muhammad Khalifa,Lechen Zhang,Xin Liu,Ayoung Lee,Xinliang Frederick Zhang,Farima Fatahi Bayat,Lu Wang*

Main category: cs.CL

TL;DR: ThinkLogit是一种无需额外训练的解码时方法，通过logit算术使用小推理模型引导大模型进行长链推理，ThinkLogit-DPO进一步通过偏好优化训练引导模型，在五个推理基准上分别实现24.5%和29.1%的相对准确率提升。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通常需要额外训练才能实现回溯和自我修正等长链推理能力，本文研究是否可以在不进行训练的情况下激发这些行为。

Method: 提出ThinkLogit方法，利用logit算术调整目标大型非推理模型，使用小得多的推理模型作为引导器；进一步提出ThinkLogit-DPO，通过偏好优化在正确/错误推理对上训练引导模型。

Result: 使用Qwen2.5-32B模型和21倍小的R1-Distill-Qwen-1.5B引导模型，在五个推理基准上平均准确率分别提升24.5%和29.1%；当引导模型和目标模型来自不同模型家族时仍然有效，且与小型模型的后训练方法正交。

Conclusion: ThinkLogit提供了一种实用的路径，无需昂贵的后训练即可解锁大规模模型的长推理能力，改进后的引导模型可以直接插入以产生更强的大型模型。

Abstract: Large reasoning models exhibit long chain-of-thought reasoning with
strategies such as backtracking and self-correction, though recent studies
suggest that these abilities typically require additional training. We first
investigate whether such behaviors can be elicited without any training. To
this end, we propose a decoding-time approach, ThinkLogit, which utilizes logit
arithmetic to tune a target large non-reasoning model for long reasoning using
a substantially smaller reasoning model as the guider. We then show that we can
further boost its performance by training the guider model with preference
optimization over correct/incorrect reasoning pairs sampled from both the
target and guider model, a setup we refer to as ThinkLogit-DPO. Our experiments
demonstrate that ThinkLogit and ThinkLogit-DPO achieve a relative improvement
in average accuracy by 24.5% and 29.1%, respectively, over five reasoning
benchmarks using the Qwen2.5-32B guided by R1-Distill-Qwen-1.5B, a model 21x
smaller. Moreover, we find that ThinkLogit remains effective when the guider
and target come from different model families. It is also orthogonal to
post-training methods for small models, as guiders improved through supervised
distillation or reinforcement learning can be directly plugged in to yield
stronger large models, offering a practical path to unlock long reasoning in
large-scale models without costly post-training.

</details>


### [83] [NL2GenSym: Natural Language to Generative Symbolic Rules for SOAR Cognitive Architecture via Large Language Models](https://arxiv.org/abs/2510.09355)
*Fang Yuan,Junjie Zeng,Yue Hu,Zhengqiu Zhu,Quanjun Yin,Yuxiang Xie*

Main category: cs.CL

TL;DR: NL2GenSym框架通过整合大型语言模型与SOAR认知架构，从自然语言自动生成符号规则，在Water Jug Problem上实现了86%以上的成功率，并将决策周期降至最优解的1.98倍。


<details>
  <summary>Details</summary>
Motivation: SOAR认知架构在实际应用中受到手动编码规则的阻碍，而现有研究缺乏对LLM生成规则能力的实验验证。

Method: 提出执行导向的生成器-批评者机制：LLM生成器基于检索增强生成访问自演化知识库提出规则，SOAR环境立即执行验证，批评者根据执行反馈迭代优化规则。

Result: 在Water Jug Problem数据集上，框架成功率达到86%以上，生成新颖启发式规则，平均决策周期降至最优解的1.98倍，仅为基线方法的1/1000。

Conclusion: NL2GenSym框架有效解决了SOAR规则手动编码的瓶颈，证明了LLM自动生成符号规则的可行性，并发现较小参数模型也能获得更好性能。

Abstract: SOAR, a classic symbol-based cognitive architecture, has been fostering the
development of general, human-like intelligent agents. Nevertheless, its
practical adoption is hindered by the laborious manual rule coding. Emerging
Large Language Models (LLMs) present the immense potential for efficient rules
generation. However, there is a critical gap that current research
predominantly focuses on conceptual frameworks and lacks robust experimental
validation. To bridge this gap, we propose \textit{N}atural \textit{L}anguage
to \textit{Gen}erative \textit{Sym}bolic Rules (NL2GenSym), a novel framework
that integrates LLMs with SOAR to autonomously produce generative symbolic
rules from natural language. Specifically, our framework introduces a novel
Execution-Grounded Generator-Critic mechanism. The LLM-based Generator, guided
by a Retrieval-Augmented Generation-accessed self-evolving domain knowledge
base, proposes rules from natural language. Subsequently, these rules are
immediately executed within the SOAR environment to rigorously validate their
correctness. Based on this execution-grounded feedback, a reflective LLM-based
Critic drives the iterative refinement of these rules. Experiments on our
specialized Water Jug Problem (WJP) dataset, utilizing both Gemini and Qwen
series models, validate the efficacy of our framework. It achieves a success
rate over 86\% in generating rules from natural language. Crucially, the
framework also generates novel heuristic rules, reducing average decision
cycles for solving the WJP to 1.98 times the optimal solution and 1/1000 of
baseline methods. Additionally, our initial experiments show that NL2GenSym
enables smaller-parameter models to achieve better performance than larger
counterparts.

</details>


### [84] [Understanding the Effects of Domain Finetuning on LLMs](https://arxiv.org/abs/2510.09359)
*Eshaan Tanwar,Deepak Nathani,William Yang Wang,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文首次系统研究了大型医学语言模型中的领域特定微调机制，发现微调仅改变表示子空间的小部分，提出了一种新的tuning vectors框架来解释参数变化，并揭示了这些向量主要在MLP层写入新方向信息，在注意力头中放大现有方向。


<details>
  <summary>Details</summary>
Motivation: 目前对领域专业化LLM微调如何重塑参数空间的机制理解不足，先前研究主要关注自回归或通用指令模型，领域专业化LLM研究较少。

Method: 提出tuning vectors框架（受task vectors启发），明确捕捉微调引起的方向性参数变化，分析这些向量在不同模型组件中的作用。

Result: 微调仅修改表示子空间的小部分，基本保留预训练模型表示；tuning vectors对提高指令跟随和生成质量至关重要；跨领域组合tuning vectors可改善泛化能力；这些向量主要在MLP层写入新方向信息，在注意力头中放大现有方向。

Conclusion: 研究为LLM适应提供了新见解，并为分析大型语言模型专业化提供了通用、可解释的框架。

Abstract: Large Language Models (LLMs) fine-tuned for specific domains exhibit strong
performance; however, the underlying mechanisms by which this fine-tuning
reshapes their parametric space are not well understood. Prior works primarily
focus on auto-regressive or general-purpose instruct models, leaving
domain-specialised LLMs under-explored. We present the first systematic study
of domain-specific fine-tuning in large medical language models. Our analysis
reveals that fine-tuning modifies only a small subset of the representational
subspace, essentially preserving the pre-trained model's representation. To
interpret these changes in subspaces, we propose tuning vectors, a novel
framework inspired by task vectors, which explicitly capture the directional
parameter shifts induced by fine-tuning. We demonstrate that these vectors are
critical for enhancing both instruction-following and generation quality.
Furthermore, combining tuning vectors across different domains yields improved
generalisation. Upon closer inspection of directional alignment, we find these
vectors primarily write new directional information into the MLP layers of the
model, while amplifying existing directions in attention heads. Our findings
offer new insights into LLM adaptation and provide a general, interpretable
framework for analysing specialisation in large language models.

</details>


### [85] [Token-Level Policy Optimization: Linking Group-Level Rewards to Token-Level Aggregation via Markov Likelihood](https://arxiv.org/abs/2510.09369)
*Xingyu Lin,Yilin Wen,En Wang,Du Su,Wenbin Liu,Chenfu Bao,Zhonghou Lv*

Main category: cs.CL

TL;DR: TEPO是一种新颖的令牌级框架，通过马尔可夫似然将组级奖励与令牌关联，解决了GRPO等熵正则化方法在思维链稀疏令牌奖励中的挑战，显著提升了数学推理性能和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: GRPO和相关熵正则化方法在思维链的稀疏令牌奖励中面临挑战，当前方法依赖无差别的令牌级熵调整，常导致熵崩溃或模型崩溃。

Method: 提出TEPO令牌级框架，通过马尔可夫似然（序列似然）将组级奖励与令牌通过令牌级聚合关联起来。

Result: 实验显示TEPO在关键指标（包括@k和准确率）上持续优于现有基线，不仅在数学推理任务上达到新SOTA，还显著提升了训练稳定性。

Conclusion: TEPO通过创新的令牌级奖励聚合机制，有效解决了稀疏令牌奖励带来的训练不稳定问题，为LLM推理能力提升提供了更优的解决方案。

Abstract: Group Relative Policy Optimization (GRPO) has significantly advanced the
reasoning ability of large language models (LLMs), particularly by boosting
their mathematical performance. However, GRPO and related
entropy-regularization methods still face challenges rooted in the sparse token
rewards inherent to chain-of-thought (CoT). Current approaches often rely on
undifferentiated token-level entropy adjustments, which frequently lead to
entropy collapse or model collapse. In this work, we propose TEPO, a novel
token-level framework that incorporates Markov Likelihood (sequence likelihood)
links group-level rewards with tokens via token-level aggregation. Experiments
show that TEPO consistently outperforms existing baselines across key metrics
(including @k and accuracy). It not only sets a new state of the art on
mathematical reasoning tasks but also significantly enhances training
stability.

</details>


### [86] [Identifying & Interactively Refining Ambiguous User Goals for Data Visualization Code Generation](https://arxiv.org/abs/2510.09390)
*Mert İnan,Anthony Sicilia,Alex Xie,Saujas Vaduguru,Daniel Fried,Malihe Alikhani*

Main category: cs.CL

TL;DR: 本文研究了人机交互中目标共享的模糊性问题，特别是在数据可视化领域。作者开发了模糊性分类法和量化指标，证明这些指标比不确定性基线更能反映人类标注。通过多轮对话和语用模型，研究展示了如何减少模糊性并提高代码生成准确性。


<details>
  <summary>Details</summary>
Motivation: 人机通信中建立共享目标时，自然语言的模糊性会导致看似正确但不符合说话者意图的输出。在数据可视化领域，这种模糊性会影响生成可视化数据的代码质量。

Method: 1. 开发模糊性分类法和量化指标；2. 使用DS-1000数据集中的Matplotlib问题进行验证；3. 探索多轮对话减少模糊性的方法；4. 评估三种语用模型：Gricean合作原则、话语表征理论和问题讨论框架。

Result: 1. 提出的模糊性指标比不确定性基线更能与人类标注相关；2. 多轮对话能够有效减少模糊性；3. 语用对话策略提高了代码生成准确性，更好地匹配用户目标。

Conclusion: 多轮对话和语用模型能够有效解决人机交互中的模糊性问题，提高代码生成质量。在数据可视化等需要精确意图理解的领域，这种对话方法具有重要价值。

Abstract: Establishing shared goals is a fundamental step in human-AI communication.
However, ambiguities can lead to outputs that seem correct but fail to reflect
the speaker's intent. In this paper, we explore this issue with a focus on the
data visualization domain, where ambiguities in natural language impact the
generation of code that visualizes data. The availability of multiple views on
the contextual (e.g., the intended plot and the code rendering the plot) allows
for a unique and comprehensive analysis of diverse ambiguity types. We develop
a taxonomy of types of ambiguity that arise in this task and propose metrics to
quantify them. Using Matplotlib problems from the DS-1000 dataset, we
demonstrate that our ambiguity metrics better correlate with human annotations
than uncertainty baselines. Our work also explores how multi-turn dialogue can
reduce ambiguity, therefore, improve code accuracy by better matching user
goals. We evaluate three pragmatic models to inform our dialogue strategies:
Gricean Cooperativity, Discourse Representation Theory, and Questions under
Discussion. A simulated user study reveals how pragmatic dialogues reduce
ambiguity and enhance code accuracy, highlighting the value of multi-turn
exchanges in code generation.

</details>


### [87] [Beyond Single-Granularity Prompts: A Multi-Scale Chain-of-Thought Prompt Learning for Graph](https://arxiv.org/abs/2510.09394)
*Ziyu Zheng,Yaming Yang,Ziyu Guan,Wei Zhao,Xinyan Huang,Weigang Lu*

Main category: cs.CL

TL;DR: 提出了一个多尺度图思维链（MSGCOT）提示框架，通过整合多尺度结构信息来增强图提示调优的性能，特别是在少样本场景下表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有的图提示调优方法局限于单一粒度（如节点级或子图级）的提示生成，忽视了图数据固有的多尺度结构信息，这限制了提示语义的多样性。

Method: 设计了一个轻量级的低秩粗化网络来高效捕获多尺度结构特征作为层次基础向量，然后模仿人类从粗到细的认知过程，在每个推理步骤动态整合多尺度信息，形成渐进式的粗到细提示链。

Result: 在八个基准数据集上的广泛实验表明，MSGCOT优于最先进的单粒度图提示调优方法，特别是在少样本场景下展现出优越性能。

Conclusion: 将多尺度信息整合到图提示中能够显著提升图提示调优的性能，特别是在数据稀缺的情况下，证明了多尺度结构信息在图学习中的重要性。

Abstract: The "pre-train, prompt'' paradigm, designed to bridge the gap between
pre-training tasks and downstream objectives, has been extended from the NLP
domain to the graph domain and has achieved remarkable progress. Current
mainstream graph prompt-tuning methods modify input or output features using
learnable prompt vectors. However, existing approaches are confined to
single-granularity (e.g., node-level or subgraph-level) during prompt
generation, overlooking the inherently multi-scale structural information in
graph data, which limits the diversity of prompt semantics. To address this
issue, we pioneer the integration of multi-scale information into graph prompt
and propose a Multi-Scale Graph Chain-of-Thought (MSGCOT) prompting framework.
Specifically, we design a lightweight, low-rank coarsening network to
efficiently capture multi-scale structural features as hierarchical basis
vectors for prompt generation. Subsequently, mimicking human cognition from
coarse-to-fine granularity, we dynamically integrate multi-scale information at
each reasoning step, forming a progressive coarse-to-fine prompt chain.
Extensive experiments on eight benchmark datasets demonstrate that MSGCOT
outperforms the state-of-the-art single-granularity graph prompt-tuning method,
particularly in few-shot scenarios, showcasing superior performance.

</details>


### [88] [Active Model Selection for Large Language Models](https://arxiv.org/abs/2510.09418)
*Yavuz Durmazkeser,Patrik Okanovic,Andreas Kirsch,Torsten Hoefler,Nezihe Merve Gürel*

Main category: cs.CL

TL;DR: LLM SELECTOR是一个主动模型选择框架，通过自适应选择少量有信息的查询进行标注，显著降低标注成本，在6个基准测试中减少标注成本达59.62%。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型评估和基准测试方法需要完全标注的数据集，标注成本高昂，需要一种更高效的模型选择方法。

Method: LLM SELECTOR自适应选择少量信息量最大的查询进行标注，并利用基于评判的oracle标注模型进一步降低标注成本。

Result: 在6个基准测试和151个大语言模型上的实验表明，LLM SELECTOR在选择最佳和接近最佳模型时，标注成本降低了59.62%。

Conclusion: LLM SELECTOR是第一个主动的大语言模型选择框架，能够以有限的标注高效识别最佳模型，显著降低模型选择成本。

Abstract: We introduce LLM SELECTOR, the first framework for active model selection of
Large Language Models (LLMs). Unlike prior evaluation and benchmarking
approaches that rely on fully annotated datasets, LLM SELECTOR efficiently
identifies the best LLM with limited annotations. In particular, for any given
task, LLM SELECTOR adaptively selects a small set of queries to annotate that
are most informative about the best model for the task. To further reduce
annotation cost, we leverage a judge-based oracle annotation model. Through
extensive experiments on 6 benchmarks with 151 LLMs, we show that LLM SELECTOR
reduces annotation costs by up to 59.62% when selecting the best and near-best
LLM for the task.

</details>


### [89] [On the Representations of Entities in Auto-regressive Large Language Models](https://arxiv.org/abs/2510.09421)
*Victor Morand,Josiane Mothe,Benjamin Piwowarski*

Main category: cs.CL

TL;DR: 提出了一种通过实体提及重构来研究LLM内部实体表示的新框架，使用任务向量方法从隐藏状态生成多标记实体提及，发现LLM开发了实体特定机制来表示和操作多标记实体。


<details>
  <summary>Details</summary>
Motivation: 命名实体是文本知识的基础构建块，但LLM如何内部表示实体仍不清楚。先前研究主要关注显式关系，对实体表示本身知之甚少。

Method: 引入实体提及重构框架，使用任务向量方法从LLM隐藏状态的各种实体表示中一致生成多标记提及，扩展logit-lens为Entity Lens来预测多标记提及。

Result: LLM开发了实体特定机制来表示和操作任何多标记实体，包括训练期间未见过的实体。

Conclusion: 该研究为理解LLM如何编码和操作实体提供了新证据，表明LLM具有表示多标记实体的专门机制。

Abstract: Named entities are fundamental building blocks of knowledge in text,
grounding factual information and structuring relationships within language.
Despite their importance, it remains unclear how Large Language Models (LLMs)
internally represent entities. Prior research has primarily examined explicit
relationships, but little is known about entity representations themselves. We
introduce entity mention reconstruction as a novel framework for studying how
LLMs encode and manipulate entities. We investigate whether entity mentions can
be generated from internal representations, how multi-token entities are
encoded beyond last-token embeddings, and whether these representations capture
relational knowledge. Our proposed method, leveraging _task vectors_, allows to
consistently generate multi-token mentions from various entity representations
derived from the LLMs hidden states. We thus introduce the _Entity Lens_,
extending the _logit-lens_ to predict multi-token mentions. Our results bring
new evidence that LLMs develop entity-specific mechanisms to represent and
manipulate any multi-token entities, including those unseen during training.
Our code is avalable at https://github.com/VictorMorand/EntityRepresentations .

</details>


### [90] [The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue State Tracking Approach](https://arxiv.org/abs/2510.09424)
*Nizar El Ghazal,Antoine Caubrière,Valentin Vielzeuf*

Main category: cs.CL

TL;DR: 本文比较了端到端口语对话状态跟踪的上下文管理策略，发现完整口语对话历史输入性能最佳，注意力池化压缩方法在保持准确性的同时减少了上下文大小。


<details>
  <summary>Details</summary>
Motivation: 研究不同上下文管理策略对口语对话状态跟踪性能的影响，探索如何更有效地利用口语对话上下文信息。

Method: 系统评估了三种策略：传统多模态上下文（文本历史+口语当前轮）、完整口语历史、压缩口语历史，在SpokenWOZ语料库上进行实验。

Result: 完整口语对话作为输入在相似规模模型中性能最高，显著超越先前方法；注意力池化压缩方法在保持竞争力的准确性的同时减少了上下文大小。

Conclusion: 改进源于更有效的上下文利用，完整口语历史是最佳策略，注意力池化压缩提供了良好的权衡方案。

Abstract: This paper presents a comparative study of context management strategies for
end-to-end Spoken Dialog State Tracking using Speech-LLMs. We systematically
evaluate traditional multimodal context (combining text history and spoken
current turn), full spoken history, and compressed spoken history approaches.
Our experiments on the SpokenWOZ corpus demonstrate that providing the full
spoken conversation as input yields the highest performance among models of
similar size, significantly surpassing prior methods. Furthermore, we show that
attention-pooling-based compression of the spoken history offers a strong
trade-off, maintaining competitive accuracy with reduced context size. Detailed
analysis confirms that improvements stem from more effective context
utilization.

</details>


### [91] [KORMo: Korean Open Reasoning Model for Everyone](https://arxiv.org/abs/2510.09426)
*Minjun Kim,Hyeonseok Lim,Hangyeol Yoo,Inho Won,Seungwoo Song,Minkyung Cho,Junhun Yuk,Changsu Choi,Dongjae Shin,Huige Lee,Hoyun Song,Alice Oh,Kyungtae Lim*

Main category: cs.CL

TL;DR: 本文介绍了KORMo-10B，这是首个针对韩语的大规模双语开源大语言模型，主要基于合成数据训练，在多项基准测试中表现与当代多语言基线模型相当。


<details>
  <summary>Details</summary>
Motivation: 针对非英语语言（特别是韩语）缺乏大规模开源双语LLM的问题，探索在低资源环境下使用合成数据构建完全开放模型的可能性。

Method: 从零开始训练10.8B参数的KORMo-10B模型，使用韩英双语语料库，其中68.74%的韩语部分是合成数据，通过精心策划的语言平衡和多样化的指令风格来确保训练稳定性。

Result: 模型在推理、知识和指令跟随等广泛基准测试中表现与当代开源多语言基线模型相当，证明了合成数据可以可靠支持长期预训练而不会导致模型崩溃，双语指令调优使韩语推理和话语连贯性接近母语水平。

Conclusion: 这项工作为在低资源环境下开发基于合成数据的完全开放模型建立了透明框架，并为未来多语言LLM研究设定了可复现的先例。

Abstract: This work presents the first large-scale investigation into constructing a
fully open bilingual large language model (LLM) for a non-English language,
specifically Korean, trained predominantly on synthetic data. We introduce
KORMo-10B, a 10.8B-parameter model trained from scratch on a Korean-English
corpus in which 68.74% of the Korean portion is synthetic. Through systematic
experimentation, we demonstrate that synthetic data, when carefully curated
with balanced linguistic coverage and diverse instruction styles, does not
cause instability or degradation during large-scale pretraining. Furthermore,
the model achieves performance comparable to that of contemporary open-weight
multilingual baselines across a wide range of reasoning, knowledge, and
instruction-following benchmarks. Our experiments reveal two key findings: (1)
synthetic data can reliably sustain long-horizon pretraining without model
collapse, and (2) bilingual instruction tuning enables near-native reasoning
and discourse coherence in Korean. By fully releasing all components including
data, code, training recipes, and logs, this work establishes a transparent
framework for developing synthetic data-driven fully open models (FOMs) in
low-resource settings and sets a reproducible precedent for future multilingual
LLM research.

</details>


### [92] [Domain-Adapted Pre-trained Language Models for Implicit Information Extraction in Crash Narratives](https://arxiv.org/abs/2510.09434)
*Xixi Wang,Jordanka Kovaceva,Miguel Costa,Shuai Wang,Francisco Camara Pereira,Robert Thomson*

Main category: cs.CL

TL;DR: 本文研究使用紧凑的开源预训练语言模型（PLMs）来处理交通事故报告中的非结构化文本，以识别碰撞方式和车辆事故类型，通过微调技术（如LoRA）提升模型性能，在权威数据集CISS上表现优于GPT-4o等闭源大模型。


<details>
  <summary>Details</summary>
Motivation: 现实交通事故数据库中的自由文本记录对交通安全分析很重要，但大规模分析困难，因为缺乏能批量处理非标准化文本的工具。现有Transformer模型在推理密集型任务（如事故类型识别）上表现不佳，且闭源模型存在隐私问题和领域知识不足的挑战。

Method: 应用微调技术（包括LoRA和BERT）向紧凑的开源PLMs注入任务特定知识，以处理交通事故报告中的推理密集型提取任务，包括识别碰撞方式和车辆事故类型。

Result: 在权威真实数据集CISS上的实验表明，微调后的紧凑模型在性能上超过了强大的闭源LLMs（如GPT-4o），同时仅需最少的训练资源。进一步分析显示，微调后的PLMs能捕捉更丰富的叙述细节，甚至能纠正数据集中的一些错误标注。

Conclusion: 紧凑的开源PLMs通过适当的微调，能够有效支持交通事故报告中的推理密集型信息提取，在性能、隐私保护和资源效率方面优于闭源大模型，具有实际应用价值。

Abstract: Free-text crash narratives recorded in real-world crash databases have been
shown to play a significant role in improving traffic safety. However,
large-scale analyses remain difficult to implement as there are no documented
tools that can batch process the unstructured, non standardized text content
written by various authors with diverse experience and attention to detail. In
recent years, Transformer-based pre-trained language models (PLMs), such as
Bidirectional Encoder Representations from Transformers (BERT) and large
language models (LLMs), have demonstrated strong capabilities across various
natural language processing tasks. These models can extract explicit facts from
crash narratives, but their performance declines on inference-heavy tasks in,
for example, Crash Type identification, which can involve nearly 100
categories. Moreover, relying on closed LLMs through external APIs raises
privacy concerns for sensitive crash data. Additionally, these black-box tools
often underperform due to limited domain knowledge. Motivated by these
challenges, we study whether compact open-source PLMs can support
reasoning-intensive extraction from crash narratives. We target two challenging
objectives: 1) identifying the Manner of Collision for a crash, and 2) Crash
Type for each vehicle involved in the crash event from real-world crash
narratives. To bridge domain gaps, we apply fine-tuning techniques to inject
task-specific knowledge to LLMs with Low-Rank Adaption (LoRA) and BERT.
Experiments on the authoritative real-world dataset Crash Investigation
Sampling System (CISS) demonstrate that our fine-tuned compact models
outperform strong closed LLMs, such as GPT-4o, while requiring only minimal
training resources. Further analysis reveals that the fine-tuned PLMs can
capture richer narrative details and even correct some mislabeled annotations
in the dataset.

</details>


### [93] [Getting Your Indices in a Row: Full-Text Search for LLM Training Data for Real World](https://arxiv.org/abs/2510.09471)
*Ines Altemir Marinas,Anastasiia Kucherenko,Alexander Sternfeld,Andrei Kucharavy*

Main category: cs.CL

TL;DR: 本文提出了一个用于Apertus LLM训练数据的全文索引流水线，使用Elasticsearch在arm64超级集群上成功索引了8.6T token，创建了LLM安全工具和离线网络搜索引擎。


<details>
  <summary>Details</summary>
Motivation: 尽管开源LLM数量增加，但其训练数据的访问仍然受限，且数据规模庞大难以审查，可能包含从互联网抓取的关键数据。

Method: 利用Elasticsearch并行索引和Alps基础设施（先进的节能arm64超级集群），构建全文索引流水线。

Result: 成功索引了Apertus LLM家族训练数据中的8.6T token（共15.2T），证明了Elasticsearch在arm64架构上的可行性、大规模全文索引的可行性，以及此类索引在LLM安全方面的应用价值。

Conclusion: 该研究为其他团队进行大规模数据索引提供了有用参考，并促进了向更绿色计算的转型。

Abstract: The performance of Large Language Models (LLMs) is determined by their
training data. Despite the proliferation of open-weight LLMs, access to LLM
training data has remained limited. Even for fully open LLMs, the scale of the
data makes it all but inscrutable to the general scientific community, despite
potentially containing critical data scraped from the internet.
  In this paper, we present the full-text indexing pipeline for the Apertus LLM
training data. Leveraging Elasticsearch parallel indices and the Alps
infrastructure, a state-of-the-art, highly energy-efficient arm64 supercluster,
we were able to index 8.6T tokens out of 15.2T used to train the Apertus LLM
family, creating both a critical LLM safety tool and effectively an offline,
curated, open web search engine. Our contribution is threefold. First, we
demonstrate that Elasticsearch can be successfully ported onto next-generation
arm64-based infrastructure. Second, we demonstrate that full-text indexing at
the scale of modern LLM training datasets and the entire open web is feasible
and accessible. Finally, we demonstrate that such indices can be used to ensure
previously inaccessible jailbreak-agnostic LLM safety.
  We hope that our findings will be useful to other teams attempting
large-scale data indexing and facilitate the general transition towards greener
computation.

</details>


### [94] [Hybrid Models for Natural Language Reasoning: The Case of Syllogistic Logic](https://arxiv.org/abs/2510.09472)
*Manuel Vargas Guzmán,Jakub Szymanik,Maciej Malicki*

Main category: cs.CL

TL;DR: LLMs在逻辑推理中表现出递归能力但缺乏组合性，作者提出将符号推理与神经计算结合的混合架构来解决泛化问题。


<details>
  <summary>Details</summary>
Motivation: 神经模型的泛化能力（特别是逻辑推理应用）仍面临挑战，需要区分组合性和递归性这两个关键方面。

Method: 使用三段论片段作为基准评估LLMs的逻辑泛化能力，并设计结合符号推理和神经计算的混合架构。

Result: LLMs在递归性方面表现良好，但在组合性方面存在显著困难；混合架构在保持高效率的同时实现了稳健推理。

Conclusion: 混合模型有潜力有效解决神经推理系统中的关键泛化障碍，为可靠逻辑证明器提供了可行方案。

Abstract: Despite the remarkable progress in neural models, their ability to
generalize, a cornerstone for applications like logical reasoning, remains a
critical challenge. We delineate two fundamental aspects of this ability:
compositionality, the capacity to abstract atomic logical rules underlying
complex inferences, and recursiveness, the aptitude to build intricate
representations through iterative application of inference rules. In the
literature, these two aspects are often confounded together under the umbrella
term of generalization. To sharpen this distinction, we investigated the
logical generalization capabilities of pre-trained large language models (LLMs)
using the syllogistic fragment as a benchmark for natural language reasoning.
Though simple, this fragment provides a foundational yet expressive subset of
formal logic that supports controlled evaluation of essential reasoning
abilities. Our findings reveal a significant disparity: while LLMs demonstrate
reasonable proficiency in recursiveness, they struggle with compositionality.
To overcome these limitations and establish a reliable logical prover, we
propose a hybrid architecture integrating symbolic reasoning with neural
computation. This synergistic interaction enables robust and efficient
inference, neural components accelerate processing, while symbolic reasoning
ensures completeness. Our experiments show that high efficiency is preserved
even with relatively small neural components. As part of our proposed
methodology, this analysis gives a rationale and highlights the potential of
hybrid models to effectively address key generalization barriers in neural
reasoning systems.

</details>


### [95] [Multimodal Policy Internalization for Conversational Agents](https://arxiv.org/abs/2510.09474)
*Zhenhailong Wang,Jiateng Liu,Amin Fazel,Ritesh Sarkhel,Xing Fan,Xiang Li,Chenlei Guo,Heng Ji,Ruhi Sarikaya*

Main category: cs.CL

TL;DR: 提出了多模态策略内化（MPI）任务，将复杂的多模态策略内化到模型参数中，避免推理时包含冗长策略，并开发了TriMPI三阶段训练框架来提升策略遵循能力。


<details>
  <summary>Details</summary>
Motivation: 现代对话系统依赖预定义策略，但随着系统扩展，这些策略变得复杂冗长，导致遵循困难且计算成本高。多模态策略管理研究不足，现有工作主要关注文本压缩和安全规则对齐。

Method: 构建了两个数据集，提出TriMPI三阶段训练框架：1）通过持续预训练注入策略知识；2）监督微调；3）PolicyRollout强化学习扩展，在rollout中增强策略感知响应以实现有基础的探索。

Result: TriMPI在端到端准确性、泛化能力和抗遗忘鲁棒性方面取得了显著提升。

Conclusion: 这是首个关于多模态策略内化的工作，提供了数据集、训练方法和全面评估，为未来研究奠定了基础。

Abstract: Modern conversational agents like ChatGPT and Alexa+ rely on predefined
policies specifying metadata, response styles, and tool-usage rules. As these
LLM-based systems expand to support diverse business and user queries, such
policies, often implemented as in-context prompts, are becoming increasingly
complex and lengthy, making faithful adherence difficult and imposing large
fixed computational costs. With the rise of multimodal agents, policies that
govern visual and multimodal behaviors are critical but remain understudied.
Prior prompt-compression work mainly shortens task templates and
demonstrations, while existing policy-alignment studies focus only on
text-based safety rules. We introduce Multimodal Policy Internalization (MPI),
a new task that internalizes reasoning-intensive multimodal policies into model
parameters, enabling stronger policy-following without including the policy
during inference. MPI poses unique data and algorithmic challenges. We build
two datasets spanning synthetic and real-world decision-making and tool-using
tasks and propose TriMPI, a three-stage training framework. TriMPI first
injects policy knowledge via continual pretraining, then performs supervised
finetuning, and finally applies PolicyRollout, a GRPO-style reinforcement
learning extension that augments rollouts with policy-aware responses for
grounded exploration. TriMPI achieves notable gains in end-to-end accuracy,
generalization, and robustness to forgetting. As the first work on multimodal
policy internalization, we provide datasets, training recipes, and
comprehensive evaluations to foster future research. Project page:
https://mikewangwzhl.github.io/TriMPI.

</details>


### [96] [StatEval: A Comprehensive Benchmark for Large Language Models in Statistics](https://arxiv.org/abs/2510.09517)
*Yuchen Lu,Run Yang,Yichen Zhang,Shuguang Yu,Runpeng Dai,Ziwei Wang,Jiayi Xiang,Wenxin E,Siran Gao,Xinyao Ruan,Yirui Huang,Chenjing Xi,Haibo Hu,Yueming Fu,Qinglan Yu,Xiaobing Wei,Jiani Gu,Rui Sun,Jiaxuan Jia,Fan Zhou*

Main category: cs.CL

TL;DR: StatEval是首个专门针对统计学的综合基准测试，包含13,817个基础问题和2,374个研究级证明任务，揭示了当前LLM在统计推理方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试对数学和逻辑推理有较好覆盖，但统计学作为一门独特且综合的学科在基准测试中尚未得到充分探索。

Method: 设计了可扩展的多智能体流水线，结合人工验证，自动化大规模问题提取、重写和质量控制，确保学术严谨性。

Result: 实验结果显示，闭源模型如GPT5-mini在研究级问题上准确率低于57%，开源模型表现更差，突显了统计推理的独特挑战。

Conclusion: StatEval可作为推进大型语言模型统计智能的严格基准，所有数据和代码已在web平台公开。

Abstract: Large language models (LLMs) have demonstrated remarkable advances in
mathematical and logical reasoning, yet statistics, as a distinct and
integrative discipline, remains underexplored in benchmarking efforts. To
address this gap, we introduce \textbf{StatEval}, the first comprehensive
benchmark dedicated to statistics, spanning both breadth and depth across
difficulty levels. StatEval consists of 13,817 foundational problems covering
undergraduate and graduate curricula, together with 2374 research-level proof
tasks extracted from leading journals. To construct the benchmark, we design a
scalable multi-agent pipeline with human-in-the-loop validation that automates
large-scale problem extraction, rewriting, and quality control, while ensuring
academic rigor. We further propose a robust evaluation framework tailored to
both computational and proof-based tasks, enabling fine-grained assessment of
reasoning ability. Experimental results reveal that while closed-source models
such as GPT5-mini achieve below 57\% on research-level problems, with
open-source models performing significantly lower. These findings highlight the
unique challenges of statistical reasoning and the limitations of current LLMs.
We expect StatEval to serve as a rigorous benchmark for advancing statistical
intelligence in large language models. All data and code are available on our
web platform: https://stateval.github.io/.

</details>


### [97] [Can We Reliably Rank Model Performance across Domains without Labeled Data?](https://arxiv.org/abs/2510.09519)
*Veronica Rammouz,Aaron Gonzalez,Carlos Cruzportillo,Adrian Tan,Nicole Beebe,Anthony Rios*

Main category: cs.CL

TL;DR: 本文分析了无标签情况下估计NLP模型性能的方法，发现基于大语言模型的错误预测器比基于数据集漂移或零样本基线方法能产生更可靠和一致的性能排名。


<details>
  <summary>Details</summary>
Motivation: 理解NLP模型如何泛化时，无需标签估计模型性能是一个重要目标，但现有方法在跨域性能排名可靠性方面仍不明确。

Method: 使用四类基础分类器和多个大语言模型作为错误预测器，在GeoOLID和Amazon Reviews数据集的15个领域上进行两步评估实验。

Result: 实验表明，大语言模型错误预测器与真实准确率的相关性更强且更一致，排名可靠性在领域间性能差异较大且错误预测与基础模型真实失败模式一致时更高。

Conclusion: 研究结果明确了性能估计方法何时可信，为跨域模型评估提供了使用指导。

Abstract: Estimating model performance without labels is an important goal for
understanding how NLP models generalize. While prior work has proposed measures
based on dataset similarity or predicted correctness, it remains unclear when
these estimates produce reliable performance rankings across domains. In this
paper, we analyze the factors that affect ranking reliability using a two-step
evaluation setup with four base classifiers and several large language models
as error predictors. Experiments on the GeoOLID and Amazon Reviews datasets,
spanning 15 domains, show that large language model-based error predictors
produce stronger and more consistent rank correlations with true accuracy than
drift-based or zero-shot baselines. Our analysis reveals two key findings:
ranking is more reliable when performance differences across domains are
larger, and when the error model's predictions align with the base model's true
failure patterns. These results clarify when performance estimation methods can
be trusted and provide guidance for their use in cross-domain model evaluation.

</details>


### [98] [Accent-Invariant Automatic Speech Recognition via Saliency-Driven Spectrogram Masking](https://arxiv.org/abs/2510.09528)
*Mohammad Hossein Sameti,Sepehr Harfi Moridani,Ali Zarean,Hossein Sameti*

Main category: cs.CL

TL;DR: 提出了一种口音不变的ASR框架，通过口音分类器识别关键频谱区域并进行掩码增强，显著提升了Whisper模型在英语和波斯语中的口音鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 预训练Transformer模型在语音识别中表现优异，但对口音和方言变化敏感，导致在英语和波斯语等语言多样性语言中词错误率较高。

Method: 训练基于频谱图的口音分类器识别口音特征，掩码对预测影响最大的区域，使用掩码后的频谱图进行数据增强，提升ASR模型的口音鲁棒性。

Result: 在英语和波斯语实验中，该方法显著降低了Whisper模型的词错误率，并为波斯语建立了首个多区域口音数据集和系统基准。

Conclusion: 该研究推进了多语言ASR系统的发展，使其能够更好地应对口音和方言多样性，代码和数据集已公开。

Abstract: Pre-trained transformer-based models have significantly advanced automatic
speech recognition (ASR), yet they remain sensitive to accent and dialectal
variations, resulting in elevated word error rates (WER) in linguistically
diverse languages such as English and Persian. To address this challenge, we
propose an accent-invariant ASR framework that integrates accent and dialect
classification into the recognition pipeline. Our approach involves training a
spectrogram-based classifier to capture accent-specific cues, masking the
regions most influential to its predictions, and using the masked spectrograms
for data augmentation. This enhances the robustness of ASR models against
accent variability. We evaluate the method using both English and Persian
speech. For Persian, we introduce a newly collected dataset spanning multiple
regional accents, establishing the first systematic benchmark for accent
variation in Persian ASR that fills a critical gap in multilingual speech
research and provides a foundation for future studies on low-resource,
linguistically diverse languages. Experimental results with the Whisper model
demonstrate that our masking and augmentation strategy yields substantial WER
reductions in both English and Persian settings, confirming the effectiveness
of the approach. This research advances the development of multilingual ASR
systems that are resilient to accent and dialect diversity. Code and dataset
are publicly available at: https://github.com/MH-Sameti/Accent_invariant_ASR

</details>


### [99] [Mitigating Overthinking through Reasoning Shaping](https://arxiv.org/abs/2510.09535)
*Feifan Song,Shaohang Wei,Bofei Gao,Yejie Wang,Wen Luo,Wei Li,Linli Yao,Weimin Xiong,Liang Chen,Tianyu Liu,Houfeng Wang*

Main category: cs.CL

TL;DR: 提出了GRSP方法，通过段级惩罚机制来减少大型推理模型的过度思考问题，在保持精度的同时显著提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于验证器奖励的强化学习方法虽然能减少推理模型的token消耗，但往往损害模型性能，这是因为token级别的监督过于简单。

Method: 提出Group Relative Segment Penalization (GRSP)，这是一种段级推理正则化方法，基于推理段与token消耗和模型性能的强相关性，设计了跨段簇的长度感知加权机制。

Result: 广泛实验表明GRSP在不严重损害准确性的情况下实现了优越的token效率，特别是在处理更难问题时优势更明显，同时还能稳定RL训练并在不同模型规模上有效扩展。

Conclusion: 监督的粒度在平衡效率和准确性方面起着关键作用，GRSP方法通过段级惩罚机制成功解决了过度思考问题，为推理模型的效率优化提供了有效方案。

Abstract: Large reasoning models (LRMs) boosted by Reinforcement Learning from Verifier
Reward (RLVR) have shown great power in problem solving, yet they often cause
overthinking: excessive, meandering reasoning that inflates computational cost.
Prior designs of penalization in RLVR manage to reduce token consumption while
often harming model performance, which arises from the oversimplicity of
token-level supervision. In this paper, we argue that the granularity of
supervision plays a crucial role in balancing efficiency and accuracy, and
propose Group Relative Segment Penalization (GRSP), a step-level method to
regularize reasoning. Since preliminary analyses show that reasoning segments
are strongly correlated with token consumption and model performance, we design
a length-aware weighting mechanism across segment clusters. Extensive
experiments demonstrate that GRSP achieves superior token efficiency without
heavily compromising accuracy, especially the advantages with harder problems.
Moreover, GRSP stabilizes RL training and scales effectively across model
sizes.

</details>


### [100] [Evaluating Robustness of Large Language Models Against Multilingual Typographical Errors](https://arxiv.org/abs/2510.09536)
*Yihong Liu,Raoyuan Zhao,Lena Altinger,Hinrich Schütze,Michael A. Hedderich*

Main category: cs.CL

TL;DR: MulTypo是一个多语言拼写错误生成算法，用于评估LLM在包含拼写错误的输入下的鲁棒性。研究发现拼写错误会显著降低模型性能，特别是在生成式和推理任务中，且不同语言和任务类型的鲁棒性存在差异。


<details>
  <summary>Details</summary>
Motivation: 当前大多数基准测试假设输入是干净的，而现实应用中用户输入往往包含拼写错误，因此需要评估LLM在多语言环境下对拼写错误的鲁棒性。

Method: 提出了MulTypo算法，基于语言特定的键盘布局和打字行为模拟人类拼写错误，并在18个开源LLM上评估了5个下游任务。

Result: 拼写错误持续降低性能，生成式和推理任务受影响最严重；指令调优提高了干净输入性能但可能增加对噪声的脆弱性；高资源语言比低资源语言更鲁棒；从英语翻译比翻译成英语更鲁棒。

Conclusion: 需要噪声感知训练和多语言鲁棒性评估，以提高LLM在真实应用中的表现。

Abstract: Large language models (LLMs) are increasingly deployed in multilingual,
real-world applications with user inputs -- naturally introducing typographical
errors (typos). Yet most benchmarks assume clean input, leaving the robustness
of LLMs to typos across languages largely underexplored. To address this gap,
we introduce MulTypo, a multilingual typo generation algorithm that simulates
human-like errors based on language-specific keyboard layouts and typing
behavior. We evaluate 18 open-source LLMs across three model families and five
downstream tasks spanning language inference, multi-choice question answering,
mathematical reasoning, and machine translation tasks. Our results show that
typos consistently degrade performance, particularly in generative tasks and
those requiring reasoning -- while the natural language inference task is
comparatively more robust. Instruction tuning improves clean-input performance
but may increase brittleness under noise. We also observe language-dependent
robustness: high-resource languages are generally more robust than low-resource
ones, and translation from English is more robust than translation into
English. Our findings underscore the need for noise-aware training and
multilingual robustness evaluation. We make our code and data publicly
available.

</details>


### [101] [SPG: Sandwiched Policy Gradient for Masked Diffusion Language Models](https://arxiv.org/abs/2510.09541)
*Chengyu Wang,Paria Rashidinejad,DiJia Su,Song Jiang,Sid Wang,Siyan Zhao,Cai Zhou,Shannon Zejiang Shen,Feiyu Chen,Tommi Jaakkola,Yuandong Tian,Bo Liu*

Main category: cs.CL

TL;DR: 提出了Sandwiched Policy Gradient (SPG)方法，通过使用真实对数似然的上界和下界来解决扩散大语言模型(dLLMs)强化学习中的策略梯度偏差问题，显著优于基于ELBO或一步估计的基线方法。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型(dLLMs)因其并行解码能力而成为自回归模型的高效替代品，但难以通过强化学习与人类偏好或任务特定奖励对齐，因为其难处理的似然性阻碍了标准策略梯度方法的直接应用。

Method: 提出了Sandwiched Policy Gradient (SPG)方法，利用真实对数似然的上界和下界，避免了仅使用证据下界(ELBO)等单边近似引入的显著策略梯度偏差。

Result: 实验表明SPG显著优于基于ELBO或一步估计的基线方法，在GSM8K上准确率提高3.6%，MATH500提高2.6%，Countdown提高18.4%，Sudoku提高27.0%。

Conclusion: SPG方法有效解决了dLLMs强化学习中的策略梯度偏差问题，为扩散大语言模型的对齐提供了更可靠的解决方案。

Abstract: Diffusion large language models (dLLMs) are emerging as an efficient
alternative to autoregressive models due to their ability to decode multiple
tokens in parallel. However, aligning dLLMs with human preferences or
task-specific rewards via reinforcement learning (RL) is challenging because
their intractable log-likelihood precludes the direct application of standard
policy gradient methods. While prior work uses surrogates like the evidence
lower bound (ELBO), these one-sided approximations can introduce significant
policy gradient bias. To address this, we propose the Sandwiched Policy
Gradient (SPG) that leverages both an upper and a lower bound of the true
log-likelihood. Experiments show that SPG significantly outperforms baselines
based on ELBO or one-step estimation. Specifically, SPG improves the accuracy
over state-of-the-art RL methods for dLLMs by 3.6% in GSM8K, 2.6% in MATH500,
18.4% in Countdown and 27.0% in Sudoku.

</details>


### [102] [Beyond Surface Reasoning: Unveiling the True Long Chain-of-Thought Capacity of Diffusion Large Language Models](https://arxiv.org/abs/2510.09544)
*Qiguang Chen,Hanjing Li,Libo Qin,Dengyun Peng,Jinhao Liu,Jiangyi Wang,Chengyue Wu,Xie Chen,Yantao Du,Wanxiang Che*

Main category: cs.CL

TL;DR: 扩散大语言模型存在并行-顺序矛盾，导致在复杂推理任务中性能受限，需要采用并行导向提示等缓解策略。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型虽然具有高吞吐量和并行解码优势，但其并行更新与推理所需的因果顺序存在根本冲突，限制了在复杂任务中的表现。

Method: 通过行为分析识别并行-顺序矛盾，引入三个扩展维度（并行、扩散、顺序）进行实证研究，并提出并行导向提示、扩散早停等缓解策略。

Result: 实证显示并行扩展能持续改进性能，但扩散和顺序扩展受限于并行-顺序矛盾；提出的缓解策略能减少无效性和低效率。

Conclusion: 扩散大语言模型的并行-顺序矛盾是根本限制，需要针对性缓解策略来平衡并行优势与推理需求。

Abstract: Recently, Diffusion Large Language Models (DLLMs) have offered high
throughput and effective sequential reasoning, making them a competitive
alternative to autoregressive LLMs (ALLMs). However, parallel decoding, which
enables simultaneous token updates, conflicts with the causal order often
required for rigorous reasoning. We first identify this conflict as the core
Parallel-Sequential Contradiction (PSC). Behavioral analyses in both simple and
complex reasoning tasks show that DLLMs exhibit genuine parallelism only for
directly decidable outputs. As task difficulty increases, they revert to
autoregressive-like behavior, a limitation exacerbated by autoregressive
prompting, which nearly doubles the number of decoding steps with remasking
without improving quality. Moreover, PSC restricts DLLMs' self-reflection,
reasoning depth, and exploratory breadth. To further characterize PSC, we
introduce three scaling dimensions for DLLMs: parallel, diffusion, and
sequential. Empirically, while parallel scaling yields consistent improvements,
diffusion and sequential scaling are constrained by PSC. Based on these
findings, we propose several practical mitigations, parallel-oriented
prompting, diffusion early stopping, and parallel scaling, to reduce
PSC-induced ineffectiveness and inefficiencies.

</details>


### [103] [Hierarchical Indexing with Knowledge Enrichment for Multilingual Video Corpus Retrieval](https://arxiv.org/abs/2510.09553)
*Yu Wang,Tianhao Tan,Yifei Wang*

Main category: cs.CL

TL;DR: 提出一个多阶段框架，用于多语言医学视频检索，结合多语言语义、领域术语和高效长视频处理，通过分层树搜索和轻量级LLM重排序实现准确且可扩展的检索。


<details>
  <summary>Details</summary>
Motivation: 现有系统要么将小时级视频压缩为粗略嵌入，要么在细粒度匹配时产生过高成本，需要解决多语言视频语料库检索中的效率和精度问题。

Method: 将视频字幕分为语义连贯的块，用知识图谱事实丰富内容，构建分层树结构，使用语言无关的多语言编码器生成节点嵌入，采用粗到细的树搜索剪枝无关分支，仅对排名靠前的块进行轻量级LLM重评分。

Result: 在mVCR测试集上达到最先进性能，消融研究证实知识图谱丰富、分层索引和针对性LLM重排序的互补贡献。

Conclusion: 该方法为专业医学视频集合中的多语言检索提供了准确且可扩展的解决方案。

Abstract: Retrieving relevant instructional videos from multilingual medical archives
is crucial for answering complex, multi-hop questions across language
boundaries. However, existing systems either compress hour-long videos into
coarse embeddings or incur prohibitive costs for fine-grained matching. We
tackle the Multilingual Video Corpus Retrieval (mVCR) task in the NLPCC-2025
M4IVQA challenge with a multi-stage framework that integrates multilingual
semantics, domain terminology, and efficient long-form processing. Video
subtitles are divided into semantically coherent chunks, enriched with concise
knowledge-graph (KG) facts, and organized into a hierarchical tree whose node
embeddings are generated by a language-agnostic multilingual encoder. At query
time, the same encoder embeds the input question; a coarse-to-fine tree search
prunes irrelevant branches, and only the top-ranked chunks are re-scored by a
lightweight large language model (LLM). This design avoids exhaustive
cross-encoder scoring while preserving chunk-level precision. Experiments on
the mVCR test set demonstrate state-of-the-art performance, and ablation
studies confirm the complementary contributions of KG enrichment, hierarchical
indexing, and targeted LLM re-ranking. The proposed method offers an accurate
and scalable solution for multilingual retrieval in specialized medical video
collections.

</details>


### [104] [A Comprehensive Evaluation of Multilingual Chain-of-Thought Reasoning: Performance, Consistency, and Faithfulness Across Languages](https://arxiv.org/abs/2510.09555)
*Raoyuan Zhao,Yihong Liu,Hinrich Schütze,Michael A. Hedderich*

Main category: cs.CL

TL;DR: 本文首次全面研究了多语言思维链推理，评估了性能、一致性和忠实性三个维度，揭示了不同语言下思维链的质量和有效性存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 虽然大型推理模型越来越多地使用思维链推理来提高任务性能，但在多语言环境中，导致最终答案的中间步骤（即思维轨迹）仍然未被充分探索。

Method: 通过测量语言遵从性、答案准确性和答案一致性来评估模型在目标语言中思考时的表现；通过跨语言交换思维轨迹来评估思维轨迹的一致性；采用截断和错误注入等扰动技术来探究不同语言下思维轨迹的忠实性。

Result: 研究发现模型对语言有强烈偏好，不同语言下性能表现各异；思维轨迹的质量和有效性因提示语言不同而有显著差异；模型在不同语言下对思维轨迹的依赖程度各不相同。

Conclusion: 多语言思维链推理存在显著的语言依赖性，思维轨迹的质量、一致性和忠实性在不同语言间差异明显，这为未来多语言推理研究提供了重要启示。

Abstract: Large reasoning models (LRMs) increasingly rely on step-by-step
Chain-of-Thought (CoT) reasoning to improve task performance, particularly in
high-resource languages such as English. While recent work has examined
final-answer accuracy in multilingual settings, the thinking traces themselves,
i.e., the intermediate steps that lead to the final answer, remain
underexplored. In this paper, we present the first comprehensive study of
multilingual CoT reasoning, evaluating three key dimensions: performance,
consistency, and faithfulness. We begin by measuring language compliance,
answer accuracy, and answer consistency when LRMs are explicitly instructed or
prompt-hacked to think in a target language, revealing strong language
preferences and divergent performance across languages. Next, we assess
crosslingual consistency of thinking traces by interchanging them between
languages. We find that the quality and effectiveness of thinking traces vary
substantially depending on the prompt language. Finally, we adapt
perturbation-based techniques -- i.e., truncation and error injection -- to
probe the faithfulness of thinking traces across languages, showing that models
rely on traces to varying degrees. We release our code and data to support
future research.

</details>


### [105] [WUGNECTIVES: Novel Entity Inferences of Language Models from Discourse Connectives](https://arxiv.org/abs/2510.09556)
*Daniel Brubaker,William Sheffield,Junyi Jessy Li,Kanishka Misra*

Main category: cs.CL

TL;DR: 该研究翻转了传统认知，探索话语连接词是否能帮助语言模型理解世界知识，构建了WUGNECTIVES数据集来评估语言模型对新颖实体的推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统研究关注世界知识如何帮助预测话语连接词，本研究反向探索话语连接词是否能增强语言模型对世界的理解能力。

Method: 构建了包含8,880个刺激项的WUGNECTIVES数据集，评估了17个不同规模和训练方案的语言模型，特别关注连接词如何连接实体与特定属性。

Result: 发现对语言模型进行推理行为调优能显著提升大多数连接词类型的表现，但所有模型在表达让步意义的连接词上都系统性表现不佳，且不同连接词类型间存在较大性能差异。

Conclusion: 研究为更细致地探索语言线索在语言模型中的功能作用开辟了新途径，揭示了连接词在语言模型世界知识获取中的潜在价值。

Abstract: The role of world knowledge has been particularly crucial to predict the
discourse connective that marks the discourse relation between two arguments,
with language models (LMs) being generally successful at this task. We flip
this premise in our work, and instead study the inverse problem of
understanding whether discourse connectives can inform LMs about the world. To
this end, we present WUGNECTIVES, a dataset of 8,880 stimuli that evaluates
LMs' inferences about novel entities in contexts where connectives link the
entities to particular attributes. On investigating 17 different LMs at various
scales, and training regimens, we found that tuning an LM to show reasoning
behavior yields noteworthy improvements on most connectives. At the same time,
there was a large variation in LMs' overall performance across connective type,
with all models systematically struggling on connectives that express a
concessive meaning. Our findings pave the way for more nuanced investigations
into the functional role of language cues as captured by LMs. We release
WUGNECTIVES at https://github.com/sheffwb/wugnectives.

</details>


### [106] [AutoPR: Let's Automate Your Academic Promotion!](https://arxiv.org/abs/2510.09558)
*Qiguang Chen,Zheng Yan,Mingda Yang,Libo Qin,Yixin Yuan,Hanjing Li,Jinhao Liu,Yiyan Ji,Dengyun Peng,Jiannan Guan,Mengkang Hu,Yantao Du,Wanxiang Che*

Main category: cs.CL

TL;DR: AutoPR是一种将研究论文自动转化为高质量宣传内容的新任务，通过PRAgent多智能体框架实现，相比直接LLM管道显著提升观看时间604%、点赞数438%和整体参与度2.9倍以上。


<details>
  <summary>Details</summary>
Motivation: 随着同行评审研究数量激增，学者依赖社交平台发现研究，作者需投入大量精力推广工作以确保可见性和引用，需要自动化流程减少人工依赖。

Method: 提出PRAgent多智能体框架，分三个阶段：多模态内容提取、协作合成生成精炼输出、平台特定适配优化规范、语气和标签以最大化覆盖。

Result: 在PRBench基准测试中，PRAgent相比直接LLM管道实现观看时间增长604%、点赞数增长438%、整体参与度提升至少2.9倍。消融研究显示平台建模和目标推广贡献最大。

Conclusion: AutoPR是一个可处理、可衡量的研究问题，为可扩展、有影响力的自动化学术交流提供了路线图。

Abstract: As the volume of peer-reviewed research surges, scholars increasingly rely on
social platforms for discovery, while authors invest considerable effort in
promoting their work to ensure visibility and citations. To streamline this
process and reduce the reliance on human effort, we introduce Automatic
Promotion (AutoPR), a novel task that transforms research papers into accurate,
engaging, and timely public content. To enable rigorous evaluation, we release
PRBench, a multimodal benchmark that links 512 peer-reviewed articles to
high-quality promotional posts, assessing systems along three axes: Fidelity
(accuracy and tone), Engagement (audience targeting and appeal), and Alignment
(timing and channel optimization). We also introduce PRAgent, a multi-agent
framework that automates AutoPR in three stages: content extraction with
multimodal preparation, collaborative synthesis for polished outputs, and
platform-specific adaptation to optimize norms, tone, and tagging for maximum
reach. When compared to direct LLM pipelines on PRBench, PRAgent demonstrates
substantial improvements, including a 604% increase in total watch time, a 438%
rise in likes, and at least a 2.9x boost in overall engagement. Ablation
studies show that platform modeling and targeted promotion contribute the most
to these gains. Our results position AutoPR as a tractable, measurable research
problem and provide a roadmap for scalable, impactful automated scholarly
communication.

</details>


### [107] [Dyna-Mind: Learning to Simulate from Experience for Better AI Agents](https://arxiv.org/abs/2510.09577)
*Xiao Yu,Baolin Peng,Michel Galley,Hao Cheng,Qianhui Wu,Janardhan Kulkarni,Suman Nath,Zhou Yu,Jianfeng Gao*

Main category: cs.CL

TL;DR: Dyna-Mind是一个两阶段训练框架，通过在推理中整合模拟来提升AI代理在复杂交互环境中的表现，包括推理模拟(ReSim)和动态GRPO(Dyna-GRPO)两个阶段。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理在数学和编程方面表现出专家级能力，但在长视野交互任务（如网页导航和计算机/手机使用）中表现不佳，需要引入"替代性试错"能力来模拟替代未来以增强理解和性能。

Method: 第一阶段：推理模拟(ReSim)训练代理从环境交互中收集的真实经验构建的扩展搜索树生成结构化推理轨迹；第二阶段：动态GRPO(Dyna-GRPO)使用结果奖励和中间状态作为反馈来加强代理的模拟和决策能力。

Result: 在Sokoban、ALFWorld和AndroidWorld三个基准测试上的实验表明：(1) ReSim有效将模拟能力注入AI代理；(2) Dyna-GRPO利用结果和交互级信号学习更好的长视野、规划密集型任务策略。

Conclusion: 模拟在使AI代理在日益具有挑战性的环境中更有效地推理、规划和行动方面发挥着核心作用。

Abstract: Reasoning models have recently shown remarkable progress in domains such as
math and coding. However, their expert-level abilities in math and coding
contrast sharply with their performance in long-horizon, interactive tasks such
as web navigation and computer/phone-use. Inspired by literature on human
cognition, we argue that current AI agents need ''vicarious trial and error'' -
the capacity to mentally simulate alternative futures before acting - in order
to enhance their understanding and performance in complex interactive
environments. We introduce Dyna-Mind, a two-stage training framework that
explicitly teaches (V)LM agents to integrate such simulation into their
reasoning. In stage 1, we introduce Reasoning with Simulations (ReSim), which
trains the agent to generate structured reasoning traces from expanded search
trees built from real experience gathered through environment interactions.
ReSim thus grounds the agent's reasoning in faithful world dynamics and equips
it with the ability to anticipate future states in its reasoning. In stage 2,
we propose Dyna-GRPO, an online reinforcement learning method to further
strengthen the agent's simulation and decision-making ability by using both
outcome rewards and intermediate states as feedback from real rollouts.
Experiments on two synthetic benchmarks (Sokoban and ALFWorld) and one
realistic benchmark (AndroidWorld) demonstrate that (1) ReSim effectively
infuses simulation ability into AI agents, and (2) Dyna-GRPO leverages outcome
and interaction-level signals to learn better policies for long-horizon,
planning-intensive tasks. Together, these results highlight the central role of
simulation in enabling AI agents to reason, plan, and act more effectively in
the ever more challenging environments.

</details>


### [108] [Mind-Paced Speaking: A Dual-Brain Approach to Real-Time Reasoning in Spoken Language Models](https://arxiv.org/abs/2510.09592)
*Donghang Wu,Haoyang Zhang,Jun Chen,Xiangyu,Zhang,Hexin Liu,Eng Siong Chng,Fei Tian,Xuerui Yang,Xiangyu Zhang,Daxin Jiang,Gang Yu*

Main category: cs.CL

TL;DR: 提出了Mind-Paced Speaking (MPS)框架，通过双脑架构实现实时语音语言模型的链式推理，显著降低延迟同时保持推理质量。


<details>
  <summary>Details</summary>
Motivation: 现有实时语音语言模型无法有效利用链式推理，因为顺序生成整个思考过程会导致不可接受的延迟。需要像人类一样实现边说边想的能力。

Method: 采用受大脑启发的双脑方法：'构思脑'负责高层推理来引导和协调'表达脑'进行流畅语音生成，消除模式切换。

Result: 在数学推理任务Spoken-MQA上达到92.8%准确率，在语音对话任务URO-Bench上获得82.5分，推理性能接近预计算完整链式推理的模型，同时大幅降低延迟。

Conclusion: MPS框架有效弥合了高质量推理与实时交互之间的差距，为实时语音语言模型提供了可行的链式推理解决方案。

Abstract: Real-time Spoken Language Models (SLMs) struggle to leverage Chain-of-Thought
(CoT) reasoning due to the prohibitive latency of generating the entire thought
process sequentially. Enabling SLMs to think while speaking, similar to humans,
is attracting increasing attention. We present, for the first time, Mind-Paced
Speaking (MPS), a brain-inspired framework that enables high-fidelity,
real-time reasoning. Similar to how humans utilize distinct brain regions for
thinking and responding, we propose a novel dual-brain approach, employing a
"Formulation Brain" for high-level reasoning to pace and guide a separate
"Articulation Brain" for fluent speech generation. This division of labor
eliminates mode-switching, preserving the integrity of the reasoning process.
Experiments show that MPS significantly outperforms existing
think-while-speaking methods and achieves reasoning performance comparable to
models that pre-compute the full CoT before speaking, while drastically
reducing latency. Under a zero-latency configuration, the proposed method
achieves an accuracy of 92.8% on the mathematical reasoning task Spoken-MQA and
attains a score of 82.5 on the speech conversation task URO-Bench. Our work
effectively bridges the gap between high-quality reasoning and real-time
interaction.

</details>


### [109] [Prompting Test-Time Scaling Is A Strong LLM Reasoning Data Augmentation](https://arxiv.org/abs/2510.09599)
*Sondos Mahmoud Bsharat,Zhiqiang Shen*

Main category: cs.CL

TL;DR: P-TTS是一种推理时数据增强策略，仅使用90个手动选择的推理实例，通过系统化改变提示强度来合成多样化的推理轨迹，显著提升LLM在数学推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要大量人工标注的推理数据集，成本高昂且资源密集，而P-TTS旨在通过少量示例和推理时数据增强来低成本提升LLM推理能力。

Method: 使用仅90个手动选择的推理实例，通过系统化改变提示强度在推理时合成多样化推理轨迹上下文，然后对Qwen-2.5模型进行微调。

Result: 在AIME2024、AIME2025、MATH500和GPQA-Diamond等数学推理基准上，P-TTS-7B和32B模型显著优于基线，在AIME'24上分别获得+26.66%和+30.00%的绝对准确率提升，在零样本泛化任务上也表现出色。

Conclusion: 测试时缩放有效探索了推理模式的潜在空间，以最小标注成本放大LLM问题解决能力，为资源受限或快速演进领域提供了一种实用的低成本LLM推理增强方法。

Abstract: Large language models (LLMs) have demonstrated impressive reasoning
capabilities when provided with chain-of-thought exemplars, but curating large
reasoning datasets remains laborious and resource-intensive. In this work, we
introduce Prompting Test-Time Scaling (P-TTS), a simple yet effective
inference-time data augmentation strategy for enhancing LLM reasoning through
finetuning. Rather than collecting thousands or even millions of examples,
P-TTS leverages a small pool of only 90 manually selected reasoning instances
and systematically varies exemplar augmentation through principled instruction
prompting intensities at test time to synthesize diverse reasoning trajectory
contexts. Then we finetune the various sizes of Qwen-2.5 models on P-TTS data.
Across a suite of mathematical reasoning AIME2024 & 25, MATH500, and
GPQA-Diamond, our P-TTS-7B and 32B models outperform the prior competitive
baselines like S1 and S1.1 (1K-shot), achieving absolute accuracy gains of
+26.66% and +30.00% on AIME'24 (7B), and +13.34% and +6.67% on AIME'25 (7B);
P-TTS-32B yields gains of +23.33% and +16.63% on AIME'24, and +26.63% and
+3.33% on AIME'25 (vs. S1 and S1.1, respectively), with comparable or better
performance on MATH500 and GPQA-Diamond. We further show that P-TTS enhances
zero-shot generalization accuracy on out-of-domain reasoning benchmarks of
Gaokao, Kaoyan, OlympiadBench, AMC23, GradeSchoolMath, and Minerva. Our
analysis suggests that test-time scaling effectively explores the latent space
of reasoning patterns, amplifying LLM problem-solving with minimal annotation
overhead, and further unlocking the reasoning potential and capabilities of
LLMs. Prompting Test-Time Scaling offers a practical, low-cost way to elicit
LLM reasoning in resource-constrained or rapidly evolving domains.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [110] [Personalize Before Retrieve: LLM-based Personalized Query Expansion for User-Centric Retrieval](https://arxiv.org/abs/2510.08935)
*Yingyi Zhang,Pengyue Jia,Derong Xu,Yi Wen,Xianneng Li,Yichao Wang,Wenlin Zhang,Xiaopeng Li,Weinan Gan,Huifeng Guo,Yong Liu,Xiangyu Zhao*

Main category: cs.IR

TL;DR: 提出了Personalize Before Retrieve (PBR)框架，通过用户特定的查询扩展来改进检索增强生成(RAG)系统，解决了现有方法忽略用户个性化语义的问题。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统的查询扩展方法采用统一策略，忽视了用户特定的表达风格、偏好和历史上下文，导致相同的文本查询在不同用户间可能表达完全不同的意图。

Method: PBR框架包含两个组件：P-PRF（使用用户历史生成风格对齐的伪反馈来模拟用户表达风格）和P-Anchor（在用户语料库上执行基于图的结构对齐以捕捉其结构），在检索前将用户特定信号融入查询扩展。

Result: 在两个个性化基准测试上的实验表明，PBR始终优于强基线方法，在PersonaBench上相对于不同检索器实现了高达10%的性能提升。

Conclusion: 研究证明了在检索前建模个性化对于弥合用户自适应RAG系统中的语义差距具有重要价值，PBR框架能有效提升个性化检索性能。

Abstract: Retrieval-Augmented Generation (RAG) critically depends on effective query
expansion to retrieve relevant information. However, existing expansion methods
adopt uniform strategies that overlook user-specific semantics, ignoring
individual expression styles, preferences, and historical context. In practice,
identical queries in text can express vastly different intentions across users.
This representational rigidity limits the ability of current RAG systems to
generalize effectively in personalized settings. Specifically, we identify two
core challenges for personalization: 1) user expression styles are inherently
diverse, making it difficult for standard expansions to preserve personalized
intent. 2) user corpora induce heterogeneous semantic structures-varying in
topical focus and lexical organization-which hinders the effective anchoring of
expanded queries within the user's corpora space. To address these challenges,
we propose Personalize Before Retrieve (PBR), a framework that incorporates
user-specific signals into query expansion prior to retrieval. PBR consists of
two components: P-PRF, which generates stylistically aligned pseudo feedback
using user history for simulating user expression style, and P-Anchor, which
performs graph-based structure alignment over user corpora to capture its
structure. Together, they produce personalized query representations tailored
for retrieval. Experiments on two personalized benchmarks show that PBR
consistently outperforms strong baselines, with up to 10% gains on PersonaBench
across retrievers. Our findings demonstrate the value of modeling
personalization before retrieval to close the semantic gap in user-adaptive RAG
systems. Our code is available at https://github.com/Zhang-Yingyi/PBR-code.

</details>


### [111] [SHERLOCK: Towards Dynamic Knowledge Adaptation in LLM-enhanced E-commerce Risk Management](https://arxiv.org/abs/2510.08948)
*Nan Lu,Yurong Hu,Jiaquan Fang,Yan Liu,Rui Dong,Yiming Wang,Rui Lin,Shaoyi Xu*

Main category: cs.IR

TL;DR: SHERLOCK框架利用大语言模型帮助电商风险调查，通过构建知识库、智能平台和反思优化模块，显著提升分析效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 电商行业增长加剧了黑产与风控团队的对抗，大量案件分析给分析师带来沉重负担，且个体差异阻碍标准化流程建立。

Method: 1) 从多模态数据提取风控知识构建知识库；2) 基于数据飞轮构建智能平台，整合日常运营、专家标注和模型评估；3) 引入反思优化模块与知识库协作建立快速响应机制。

Result: 在京东真实交易数据集上的实验表明，该方法显著提升了LLM分析结果的事实对齐和风险定位精度，部署后大幅提高了案件调查效率。

Conclusion: SHERLOCK框架成功解决了电商风控中的工作负担和标准化问题，通过LLM推理能力有效提升了风险调查的效率和准确性。

Abstract: The growth of the e-commerce industry has intensified the adversarial
dynamics between shadow economy actors and risk management teams. Companies
often conduct risk investigations into suspicious cases to identify emerging
fraud patterns, thereby enhancing both preemptive risk prevention and post-hoc
governance. However, the sheer volume of case analyses imposes a substantial
workload on risk management analysts, as each case requires the integration of
long-term expert experience and meticulous scrutiny across multiple risk
dimensions. Additionally, individual disparities among analysts hinder the
establishment of uniform and high-standard workflows. To address these
challenges, we propose the SHERLOCK framework, which leverages the reasoning
capabilities of large language models (LLMs) to assist analysts in risk
investigations. Our approach consists of three primary components: (1)
extracting risk management knowledge from multi-modal data and constructing a
domain knowledge base (KB), (2) building an intelligent platform guided by the
data flywheel paradigm that integrates daily operations, expert annotations,
and model evaluations, with iteratively fine-tuning for preference alignment,
and (3) introducing a Reflect & Refine (R&R) module that collaborates with the
domain KB to establish a rapid response mechanism for evolving risk patterns.
Experiments conducted on the real-world transaction dataset from JD.com
demonstrate that our method significantly improves the precision of both
factual alignment and risk localization within the LLM analysis results.
Deployment of the SHERLOCK-based LLM system on JD.com has substantially
enhanced the efficiency of case investigation workflows for risk managers.

</details>


### [112] [Rethinking Reasoning in Document Ranking: Why Chain-of-Thought Falls Short](https://arxiv.org/abs/2510.08985)
*Xuan Lu,Haohang Huang,Rui Meng,Yaohui Jin,Wenjun Zeng,Xiaoyu Shen*

Main category: cs.IR

TL;DR: 本文系统研究了在文档重排序任务中使用链式思维推理的有效性，发现在点式和列表式重排序中，推理增强的重排序器不仅推理成本更高，而且性能反而低于直接预测排名的模型。


<details>
  <summary>Details</summary>
Motivation: 最近的研究受大型推理模型启发，开始将显式的链式思维推理融入基于LLM的重排序器，但推理在排序任务中的有效性尚未得到充分探索。

Method: 在点式和列表式设置下，通过监督微调和强化学习，使用多样化基准（包括推理密集型数据集BRIGHT和标准IR基准BEIR）系统研究推理在重排序中的作用。

Result: 推理增强的重排序器在点式设置中破坏校准并偏向正类，在列表式设置中提高域内拟合但增加方差且无法泛化到域外，直接微调的重排序器更稳定、有效和鲁棒。

Conclusion: 显式推理并非对重排序普遍有益，未来研究方向包括点式重排序器的校准感知评分和设计简洁、有针对性的推理策略以减轻列表式重排序中的过拟合和过度思考。

Abstract: Document reranking is a key component in information retrieval (IR), aimed at
refining initial retrieval results to improve ranking quality for downstream
tasks. Recent studies--motivated by large reasoning models (LRMs)--have begun
incorporating explicit chain-of-thought (CoT) reasoning into LLM-based
rerankers. However, the effectiveness of such reasoning for ranking tasks
remains underexplored. In this work, we present the first systematic study of
reasoning in reranking across both pointwise and listwise settings, under both
supervised fine-tuning and reinforcement learning. Using diverse benchmarks,
including reasoning-intensive datasets (BRIGHT) and standard IR benchmarks
(BEIR), we find that reasoning-augmented rerankers consistently underperform
their direct counterparts that predict rankings without CoT, despite
substantially higher inference costs. Our analysis reveals three core
limitations: (i) in pointwise rerankers, reasoning breaks calibration and
biases models toward the positive class, raising TPR but lowering TNR, which
inflates false positives and degrades ranking in negative-dominant pools; (ii)
in listwise rerankers, reasoning improves in-domain fit but increases variance
and fails to generalize out-of-domain, even when reinforcement learning
shortens rationales; and (iii) overall, directly fine-tuned rerankers remain
more stable, effective, and robust. These findings challenge the assumption
that explicit reasoning is universally beneficial for reranking. We conclude by
highlighting future directions, including calibration-aware scoring for
pointwise rerankers and the design of concise, targeted reasoning strategies to
mitigate overfitting and overthinking in listwise rerankers.

</details>


### [113] [Generative Data Augmentation in Graph Contrastive Learning for Recommendation](https://arxiv.org/abs/2510.09129)
*Yansong Wang,Qihui Lin,Junjie Huang,Tao Jia*

Main category: cs.IR

TL;DR: GDA4Rec是一个基于图对比学习的推荐系统框架，通过生成式数据增强方法创建高质量增强视图，解决传统随机数据增强方法可能破坏原始语义信息的问题。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中从稀疏的用户-物品交互中学习有效嵌入是一个基本挑战。虽然对比学习是解决这一问题的有前景方法，但现有随机数据增强方法生成的增强视图往往会改变原始语义信息。

Method: 使用噪声生成模块，利用深度生成模型近似原始数据分布进行数据增强；提取物品互补矩阵来表征物品间的潜在相关性并提供额外的自监督信号；采用整合推荐、数据增强和对比学习的联合目标函数。

Result: 在三个公共数据集上进行的广泛实验证明了该模型的优越性。

Conclusion: GDA4Rec框架能够生成高质量的增强视图，提供鲁棒的自监督信号，使模型能够学习更有效和信息丰富的嵌入。

Abstract: Recommendation systems have become indispensable in various online platforms,
from e-commerce to streaming services. A fundamental challenge in this domain
is learning effective embeddings from sparse user-item interactions. While
contrastive learning has recently emerged as a promising solution to this
issue, generating augmented views for contrastive learning through most
existing random data augmentation methods often leads to the alteration of
original semantic information. In this paper, we propose a novel framework,
GDA4Rec (Generative Data Augmentation in graph contrastive learning for
Recommendation) to generate high-quality augmented views and provide robust
self-supervised signals. Specifically, we employ a noise generation module that
leverages deep generative models to approximate the distribution of original
data for data augmentation. Additionally, GDA4Rec further extracts an item
complement matrix to characterize the latent correlations between items and
provide additional self-supervised signals. Lastly, a joint objective that
integrates recommendation, data augmentation and contrastive learning is used
to enforce the model to learn more effective and informative embeddings.
Extensive experiments are conducted on three public datasets to demonstrate the
superiority of the model. The code is available at:
https://github.com/MrYansong/GDA4Rec.

</details>


### [114] [Controlled Personalization in Legacy Media Online Services: A Case Study in News Recommendation](https://arxiv.org/abs/2510.09136)
*Marlene Holzleitner,Stephan Leitner,Hanna Lind Jorgensen,Christoph Schmitz,Jacob Welander,Dietmar Jannach*

Main category: cs.IR

TL;DR: 传统新闻媒体采用受控个性化策略，将编辑策划与算法推荐相结合，A/B测试显示即使适度个性化也能显著提升点击率、内容多样性，并减少流行偏见。


<details>
  <summary>Details</summary>
Motivation: 传统新闻媒体在采用个性化推荐技术时面临平衡技术创新与核心编辑价值观的挑战，需要找到既能提升用户体验又能维护新闻专业性的方法。

Method: 通过在挪威主要传统新闻机构网站上进行的A/B测试，评估受控个性化策略的效果，比较个性化内容与标准内容的用户行为差异。

Result: 个性化内容显著提高了点击率，减少了用户导航努力，提升了内容多样性和目录覆盖率，同时降低了流行偏见。

Conclusion: 受控个性化能够成功协调用户需求与编辑目标，为传统媒体采用个性化技术同时维护新闻价值提供了可行路径。

Abstract: Personalized news recommendations have become a standard feature of large
news aggregation services, optimizing user engagement through automated content
selection. In contrast, legacy news media often approach personalization
cautiously, striving to balance technological innovation with core editorial
values. As a result, online platforms of traditional news outlets typically
combine editorially curated content with algorithmically selected articles - a
strategy we term controlled personalization. In this industry paper, we
evaluate the effectiveness of controlled personalization through an A/B test
conducted on the website of a major Norwegian legacy news organization. Our
findings indicate that even a modest level of personalization yields
substantial benefits. Specifically, we observe that users exposed to
personalized content demonstrate higher click-through rates and reduced
navigation effort, suggesting improved discovery of relevant content. Moreover,
our analysis reveals that controlled personalization contributes to greater
content diversity and catalog coverage and in addition reduces popularity bias.
Overall, our results suggest that controlled personalization can successfully
align user needs with editorial goals, offering a viable path for legacy media
to adopt personalization technologies while upholding journalistic values.

</details>


### [115] [Hierarchical Semantic RL: Tackling the Problem of Dynamic Action Space for RL-based Recommendations](https://arxiv.org/abs/2510.09167)
*Minmao Wang,Xingchen Liu,Shijie Yi,Likang Wu,Hongke Zhao,Fei Pan,Qingpeng Cai,Peng Jiang*

Main category: cs.IR

TL;DR: HSRL通过引入固定语义动作空间和分层策略网络，解决了推荐系统中强化学习面临的高维动态动作空间问题，在公开基准和实际生产环境中均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统大多优化短期参与度，而强化学习方法面临高维动态动作空间的挑战，导致策略学习不稳定。

Method: 提出分层语义强化学习(HSRL)，使用固定语义动作空间(SAS)和语义ID(SID)进行策略学习，通过分层策略网络(HPN)和多层评论家(MLC)实现从粗到细的决策和细粒度信用分配。

Result: 在公开基准和大型短视频广告平台生产数据集上，HSRL均超越最先进基线方法。在线A/B测试7天中，转化率提升18.421%，成本仅增加1.251%。

Conclusion: HSRL为基于强化学习的推荐系统提供了一个可扩展的范式，有效解决了高维动作空间带来的训练不稳定问题。

Abstract: Recommender Systems (RS) are fundamental to modern online services. While
most existing approaches optimize for short-term engagement, recent work has
begun to explore reinforcement learning (RL) to model long-term user value.
However, these efforts face significant challenges due to the vast, dynamic
action spaces inherent in recommendation, which hinder stable policy learning.
To resolve this bottleneck, we introduce Hierarchical Semantic RL (HSRL), which
reframes RL-based recommendation over a fixed Semantic Action Space (SAS). HSRL
encodes items as Semantic IDs (SIDs) for policy learning, and maps SIDs back to
their original items via a fixed, invertible lookup during execution. To align
decision-making with SID generation, the Hierarchical Policy Network (HPN)
operates in a coarse-to-fine manner, employing hierarchical residual state
modeling to refine each level's context from the previous level's residual,
thereby stabilizing training and reducing representation-decision mismatch. In
parallel, a Multi-level Critic (MLC) provides token-level value estimates,
enabling fine-grained credit assignment. Across public benchmarks and a
large-scale production dataset from a leading Chinese short-video advertising
platform, HSRL consistently surpasses state-of-the-art baselines. In online
deployment over a seven-day A/B testing, it delivers an 18.421% CVR lift with
only a 1.251% increase in cost, supporting HSRL as a scalable paradigm for
RL-based recommendation. Our code is released at
https://github.com/MinmaoWang/HSRL.

</details>


### [116] [ChoirRec: Semantic User Grouping via LLMs for Conversion Rate Prediction of Low-Activity Users](https://arxiv.org/abs/2510.09393)
*Dakai Zhai,Jiong Gao,Boya Du,Junwei Xu,Qijie Shen,Jialin Zhu,Yuning Jiang*

Main category: cs.IR

TL;DR: ChoirRec是一个利用大语言模型构建语义用户群组来提升低活跃用户转化率预测的新框架，通过双通道架构实现跨用户知识迁移，在淘宝平台上显著提升了预测效果和订单量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在预测低活跃用户转化率时面临三个关键挑战：依赖嘈杂不可靠的行为信号、用户级信息不足、训练偏向高活跃用户而忽视低活跃用户需求。

Method: ChoirRec包含三个组件：语义群组生成模块（使用LLM形成跨活跃度用户集群）、群组感知分层表示模块（用群组级先验丰富稀疏用户嵌入）、群组感知多粒度模块（采用双通道架构和自适应融合机制）。

Result: 在淘宝平台上的离线实验显示GAUC提升1.16%，在线A/B测试显示订单量增加7.24%。

Conclusion: ChoirRec通过利用LLM的语义能力构建用户群组，有效解决了低活跃用户转化率预测的挑战，在实际应用中具有显著价值。

Abstract: Accurately predicting conversion rates (CVR) for low-activity users remains a
fundamental challenge in large-scale e-commerce recommender systems.Existing
approaches face three critical limitations: (i) reliance on noisy and
unreliable behavioral signals; (ii) insufficient user-level information due to
the lack of diverse interaction data; and (iii) a systemic training bias toward
high-activity users that overshadows the needs of low-activity users.To address
these challenges, we propose ChoirRec, a novel framework that leverages the
semantic capabilities of Large Language Models (LLMs) to construct semantic
user groups and enhance CVR prediction for low-activity users.With a
dual-channel architecture designed for robust cross-user knowledge transfer,
ChoirRec comprises three components: (i) a Semantic Group Generation module
that utilizes LLMs to form reliable, cross-activity user clusters, thereby
filtering out noisy signals; (ii) a Group-aware Hierarchical Representation
module that enriches sparse user embeddings with informative group-level priors
to mitigate data insufficiency; and (iii) a Group-aware Multi-granularity
Modual that employs a dual-channel architecture and adaptive fusion mechanism
to ensure effective learning and utilization of group knowledge. We conduct
extensive offline and online experiments on Taobao, a leading industrial-scale
e-commerce platform.ChoirRec improves GAUC by 1.16\% in offline evaluations,
while online A/B testing reveals a 7.24\% increase in order volume,
highlighting its substantial practical value in real-world applications.

</details>


### [117] [MRMR: A Realistic and Expert-Level Multidisciplinary Benchmark for Reasoning-Intensive Multimodal Retrieval](https://arxiv.org/abs/2510.09510)
*Siyue Zhang,Yuan Gao,Xiao Zhou,Yilun Zhao,Tingyu Song,Arman Cohan,Anh Tuan Luu,Chen Zhao*

Main category: cs.IR

TL;DR: MRMR是首个需要深度推理的专家级多学科多模态检索基准，包含1502个跨23个领域的查询，通过引入多领域挑战、推理密集型任务和图像-文本交错序列，显著推进了多模态检索评估。


<details>
  <summary>Details</summary>
Motivation: 现有检索基准在跨领域专业知识、深度推理能力和多模态文档处理方面存在不足，无法充分评估模型在真实复杂场景下的表现。

Method: 构建包含1502个专家验证查询的基准，涵盖23个专业领域，引入矛盾检索新任务，采用图像-文本交错序列的查询和文档结构，评估4类14个前沿多模态检索模型。

Result: 基于文本嵌入模型Qwen3-Embedding结合LLM生成图像描述的方法表现最佳，最新多模态模型在专家领域查询中表现竞争性，但在推理密集型任务上仍有不足。

Conclusion: MRMR基准为推进多模态检索在更真实和挑战性场景中的发展铺平了道路，揭示了当前模型在复杂推理任务上的改进空间。

Abstract: We introduce MRMR, the first expert-level multidisciplinary multimodal
retrieval benchmark requiring intensive reasoning. MRMR contains 1,502 queries
spanning 23 domains, with positive documents carefully verified by human
experts. Compared to prior benchmarks, MRMR introduces three key advancements.
First, it challenges retrieval systems across diverse areas of expertise,
enabling fine-grained model comparison across domains. Second, queries are
reasoning-intensive, with images requiring deeper interpretation such as
diagnosing microscopic slides. We further introduce Contradiction Retrieval, a
novel task requiring models to identify conflicting concepts. Finally, queries
and documents are constructed as image-text interleaved sequences. Unlike
earlier benchmarks restricted to single images or unimodal documents, MRMR
offers a realistic setting with multi-image queries and mixed-modality corpus
documents. We conduct an extensive evaluation of 4 categories of multimodal
retrieval systems and 14 frontier models on MRMR. The text embedding model
Qwen3-Embedding with LLM-generated image captions achieves the highest
performance, highlighting substantial room for improving multimodal retrieval
models. Although latest multimodal models such as Ops-MM-Embedding perform
competitively on expert-domain queries, they fall short on reasoning-intensive
tasks. We believe that MRMR paves the way for advancing multimodal retrieval in
more realistic and challenging scenarios.

</details>


### [118] [Doc2Query++: Topic-Coverage based Document Expansion and its Application to Dense Retrieval via Dual-Index Fusion](https://arxiv.org/abs/2510.09557)
*Tzu-Lin Kuo,Wei-Ning Chiu,Wei-Yun Ma,Pu-Jen Cheng*

Main category: cs.IR

TL;DR: Doc2Query++是一个文档扩展框架，通过无监督主题建模推断文档的潜在主题，然后使用混合关键词选择为每个文档创建多样且相关的关键词集，指导LLM生成查询，并通过双索引融合策略提升密集检索性能。


<details>
  <summary>Details</summary>
Motivation: 解决文档扩展中存在的生成不可控、跨域泛化能力差以及查询拼接带来的噪声问题。

Method: 1. 使用无监督主题建模推断文档潜在主题；2. 混合关键词选择创建多样化关键词集；3. 指导LLM生成查询；4. 双索引融合策略分离文本和查询信号。

Result: 在多个数据集上显著优于现有基线方法，在MAP、nDCG@10和Recall@100等指标上取得显著提升，适用于稀疏和密集检索。

Conclusion: Doc2Query++通过结构化查询生成和双索引融合，有效解决了文档扩展中的关键挑战，在跨域检索中表现出色。

Abstract: Document expansion (DE) via query generation tackles vocabulary mismatch in
sparse retrieval, yet faces limitations: uncontrolled generation producing
hallucinated or redundant queries with low diversity; poor generalization from
in-domain training (e.g., MS MARCO) to out-of-domain data like BEIR; and noise
from concatenation harming dense retrieval. While Large Language Models (LLMs)
enable cross-domain query generation, basic prompting lacks control, and
taxonomy-based methods rely on domain-specific structures, limiting
applicability. To address these challenges, we introduce Doc2Query++, a DE
framework that structures query generation by first inferring a document's
latent topics via unsupervised topic modeling for cross-domain applicability,
then using hybrid keyword selection to create a diverse and relevant keyword
set per document. This guides LLM not only to leverage keywords, which ensure
comprehensive topic representation, but also to reduce redundancy through
diverse, relevant terms. To prevent noise from query appending in dense
retrieval, we propose Dual-Index Fusion strategy that isolates text and query
signals, boosting performance in dense settings. Extensive experiments show
Doc2Query++ significantly outperforms state-of-the-art baselines, achieving
substantial gains in MAP, nDCG@10 and Recall@100 across diverse datasets on
both sparse and dense retrieval.

</details>
