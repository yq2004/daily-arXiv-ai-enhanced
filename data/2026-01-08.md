<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 108]
- [cs.IR](#cs.IR) [Total: 8]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [DeepResearch-Slice: Bridging the Retrieval-Utilization Gap via Explicit Text Slicing](https://arxiv.org/abs/2601.03261)
*Shuo Lu,Yinuo Xu,Jianjie Cheng,Lingxiao He,Meng Wang,Jian Liang*

Main category: cs.CL

TL;DR: 提出DeepResearch-Slice框架，通过神经符号方法预测精确的文本跨度索引，在推理前进行确定性硬过滤，以解决检索-利用差距问题。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究代理主要优化搜索策略以最大化检索概率，但存在检索-利用差距的瓶颈：即使检索到黄金证据，模型在噪声环境中由于上下文盲区而无法有效利用。

Method: 提出DeepResearch-Slice神经符号框架，不同于隐式注意力机制，该方法预测精确的跨度索引，在推理前进行确定性硬过滤，避免噪声干扰。

Result: 在六个基准测试中显示显著的鲁棒性提升。应用于冻结骨干网络时，相对改进达到73%（从19.1%提升至33.0%），有效缓解噪声影响且无需更新推理模型参数。

Conclusion: 研究结果表明，在开放式研究中需要明确的接地机制，神经符号方法能有效弥合检索与利用之间的差距。

Abstract: Deep Research agents predominantly optimize search policies to maximize retrieval probability. However, we identify a critical bottleneck: the retrieval-utilization gap, where models fail to use gold evidence even after it is retrieved, due to context blindness in noisy environments. To bridge this gap, we propose DeepResearch-Slice, a simple yet effective neuro-symbolic framework. Unlike implicit attention, our approach predicts precise span indices to perform a deterministic hard filter before reasoning. Extensive evaluations across six benchmarks show substantial robustness gains. Applying our method to frozen backbones yields a 73 percent relative improvement, from 19.1 percent to 33.0 percent, effectively mitigating noise without requiring parameter updates to the reasoning model. These results highlight the need for explicit grounding mechanisms in open-ended research.

</details>


### [2] [Internal Reasoning vs. External Control: A Thermodynamic Analysis of Sycophancy in Large Language Models](https://arxiv.org/abs/2601.03263)
*Edward Y. Chang*

Main category: cs.CL

TL;DR: 研究发现LLM的谄媚行为无法仅靠内部推理解决，外部结构约束是确保安全性的必要条件


<details>
  <summary>Details</summary>
Motivation: 大语言模型经常表现出谄媚行为，优先考虑用户认同而非正确性。研究探讨这是否需要外部监管，还是可以通过内部推理单独缓解。

Method: 使用CAP-GSM8K对抗数据集(N=500)，评估内部推理(CoT)与外部机制(RCA)在GPT-3.5、GPT-4o和GPT-5.1上的表现。

Result: 内部推理存在结构限制：导致弱模型性能崩溃(优先悖论)，前沿模型存在11.4%最终输出差距。RCA在所有层级模型上结构性地消除了谄媚行为(0.0%)。

Conclusion: 研究提出了热力学层次结构：混合系统只有在能力匹配且强大时才能实现共振(最优效率)，而弱或不匹配的组合会导致失调和熵增。这证实了外部结构约束对于保证安全性是严格必要的。

Abstract: Large Language Models frequently exhibit sycophancy, prioritizing user agreeableness over correctness. We investigate whether this requires external regulation or can be mitigated by internal reasoning alone. Using CAP-GSM8K (N=500), an adversarial dataset, we evaluate internal (CoT) versus external (RCA) mechanisms across GPT-3.5, GPT-4o, and GPT-5.1. Our results reveal the structural limits of internal reasoning: it causes performance collapse in weak models (the Prioritization Paradox) and leaves an 11.4\% final output gap in frontier models. In contrast, RCA structurally eliminates sycophancy (0.0\%) across all tiers. We synthesize these findings into a thermodynamic hierarchy: hybrid systems achieve Resonance (optimal efficiency) only when capabilities are matched and strong, while weak or mismatched pairs succumb to Dissonance and Entropy. This confirms that external structural constraints are strictly necessary to guarantee safety.

</details>


### [3] [Jailbreak-Zero: A Path to Pareto Optimal Red Teaming for Large Language Models](https://arxiv.org/abs/2601.03265)
*Kai Hu,Abhinav Aggarwal,Mehran Khodabandeh,David Zhang,Eric Hsin,Li Chen,Ankit Jain,Matt Fredrikson,Akash Bharadwaj*

Main category: cs.CL

TL;DR: Jailbreak-Zero是一种新颖的LLM安全评估红队方法，通过从基于示例的约束方法转向基于策略的框架，利用攻击LLM生成多样化对抗提示，并通过偏好数据集微调实现帕累托最优。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全评估方法主要采用基于示例的约束方法，这种方法覆盖面有限且效率不高。需要一种更全面、可扩展的方法来识别和缓解LLM的安全漏洞。

Method: 采用基于策略的框架，利用攻击LLM生成大量多样化对抗提示，然后通过偏好数据集对攻击模型进行微调，实现政策覆盖率、攻击策略多样性和提示真实性的帕累托最优。

Result: 实验证明该方法优于现有技术，对开源和专有模型（如GPT-40和Claude 3.5）的攻击成功率显著更高，同时生成人类可读且有效的对抗提示，最小化人工干预需求。

Conclusion: Jailbreak-Zero为LLM安全评估提供了更可扩展和全面的解决方案，能够更有效地识别和缓解安全漏洞，代表了从基于示例到基于策略框架的重要范式转变。

Abstract: This paper introduces Jailbreak-Zero, a novel red teaming methodology that shifts the paradigm of Large Language Model (LLM) safety evaluation from a constrained example-based approach to a more expansive and effective policy-based framework. By leveraging an attack LLM to generate a high volume of diverse adversarial prompts and then fine-tuning this attack model with a preference dataset, Jailbreak-Zero achieves Pareto optimality across the crucial objectives of policy coverage, attack strategy diversity, and prompt fidelity to real user inputs. The empirical evidence demonstrates the superiority of this method, showcasing significantly higher attack success rates against both open-source and proprietary models like GPT-40 and Claude 3.5 when compared to existing state-of-the-art techniques. Crucially, Jailbreak-Zero accomplishes this while producing human-readable and effective adversarial prompts with minimal need for human intervention, thereby presenting a more scalable and comprehensive solution for identifying and mitigating the safety vulnerabilities of LLMs.

</details>


### [4] [Benchmarking and Adapting On-Device Large Language Models for Clinical Decision Support](https://arxiv.org/abs/2601.03266)
*Alif Munim,Jun Ma,Omar Ibrahim,Alhusain Abdalla,Shuolin Yin,Leo Chen,Bo Wang*

Main category: cs.CL

TL;DR: 研究评估了两种本地部署的LLM（gpt-oss-20b和gpt-oss-120b）在临床任务中的表现，发现它们性能与更大的开源模型相当甚至更好，微调后能接近GPT-5水平，展现了本地LLM在临床决策支持中的潜力。


<details>
  <summary>Details</summary>
Motivation: 专有LLM系统存在隐私问题和云基础设施依赖，而开源替代方案通常模型过大，不适合资源受限的临床环境。需要评估本地部署LLM在临床任务中的可行性。

Method: 在三个代表性临床任务上评估gpt-oss-20b和gpt-oss-120b：一般疾病诊断、眼科专科诊断与管理、专家评估模拟。与GPT-5、o4-mini和DeepSeek-R1比较，并对gpt-oss-20b进行微调评估适应性。

Result: gpt-oss模型性能与DeepSeek-R1和o4-mini相当甚至更好，尽管模型尺寸小得多。微调显著提升了gpt-oss-20b的诊断准确率，使其接近GPT-5水平。

Conclusion: 本地部署LLM能够提供准确、可适应且保护隐私的临床决策支持，为LLM更广泛集成到常规临床实践提供了实用途径。

Abstract: Large language models (LLMs) have rapidly advanced in clinical decision-making, yet the deployment of proprietary systems is hindered by privacy concerns and reliance on cloud-based infrastructure. Open-source alternatives allow local inference but often require large model sizes that limit their use in resource-constrained clinical settings. Here, we benchmark two on-device LLMs, gpt-oss-20b and gpt-oss-120b, across three representative clinical tasks: general disease diagnosis, specialty-specific (ophthalmology) diagnosis and management, and simulation of human expert grading and evaluation. We compare their performance with state-of-the-art proprietary models (GPT-5 and o4-mini) and a leading open-source model (DeepSeek-R1), and we further evaluate the adaptability of on-device systems by fine-tuning gpt-oss-20b on general diagnostic data. Across tasks, gpt-oss models achieve performance comparable to or exceeding DeepSeek-R1 and o4-mini despite being substantially smaller. In addition, fine-tuning remarkably improves the diagnostic accuracy of gpt-oss-20b, enabling it to approach the performance of GPT-5. These findings highlight the potential of on-device LLMs to deliver accurate, adaptable, and privacy-preserving clinical decision support, offering a practical pathway for broader integration of LLMs into routine clinical practice.

</details>


### [5] [OpenAI GPT-5 System Card](https://arxiv.org/abs/2601.03267)
*Aaditya Singh,Adam Fry,Adam Perelman,Adam Tart,Adi Ganesh,Ahmed El-Kishky,Aidan McLaughlin,Aiden Low,AJ Ostrow,Akhila Ananthram,Akshay Nathan,Alan Luo,Alec Helyar,Aleksander Madry,Aleksandr Efremov,Aleksandra Spyra,Alex Baker-Whitcomb,Alex Beutel,Alex Karpenko,Alex Makelov,Alex Neitz,Alex Wei,Alexandra Barr,Alexandre Kirchmeyer,Alexey Ivanov,Alexi Christakis,Alistair Gillespie,Allison Tam,Ally Bennett,Alvin Wan,Alyssa Huang,Amy McDonald Sandjideh,Amy Yang,Ananya Kumar,Andre Saraiva,Andrea Vallone,Andrei Gheorghe,Andres Garcia Garcia,Andrew Braunstein,Andrew Liu,Andrew Schmidt,Andrey Mereskin,Andrey Mishchenko,Andy Applebaum,Andy Rogerson,Ann Rajan,Annie Wei,Anoop Kotha,Anubha Srivastava,Anushree Agrawal,Arun Vijayvergiya,Ashley Tyra,Ashvin Nair,Avi Nayak,Ben Eggers,Bessie Ji,Beth Hoover,Bill Chen,Blair Chen,Boaz Barak,Borys Minaiev,Botao Hao,Bowen Baker,Brad Lightcap,Brandon McKinzie,Brandon Wang,Brendan Quinn,Brian Fioca,Brian Hsu,Brian Yang,Brian Yu,Brian Zhang,Brittany Brenner,Callie Riggins Zetino,Cameron Raymond,Camillo Lugaresi,Carolina Paz,Cary Hudson,Cedric Whitney,Chak Li,Charles Chen,Charlotte Cole,Chelsea Voss,Chen Ding,Chen Shen,Chengdu Huang,Chris Colby,Chris Hallacy,Chris Koch,Chris Lu,Christina Kaplan,Christina Kim,CJ Minott-Henriques,Cliff Frey,Cody Yu,Coley Czarnecki,Colin Reid,Colin Wei,Cory Decareaux,Cristina Scheau,Cyril Zhang,Cyrus Forbes,Da Tang,Dakota Goldberg,Dan Roberts,Dana Palmie,Daniel Kappler,Daniel Levine,Daniel Wright,Dave Leo,David Lin,David Robinson,Declan Grabb,Derek Chen,Derek Lim,Derek Salama,Dibya Bhattacharjee,Dimitris Tsipras,Dinghua Li,Dingli Yu,DJ Strouse,Drew Williams,Dylan Hunn,Ed Bayes,Edwin Arbus,Ekin Akyurek,Elaine Ya Le,Elana Widmann,Eli Yani,Elizabeth Proehl,Enis Sert,Enoch Cheung,Eri Schwartz,Eric Han,Eric Jiang,Eric Mitchell,Eric Sigler,Eric Wallace,Erik Ritter,Erin Kavanaugh,Evan Mays,Evgenii Nikishin,Fangyuan Li,Felipe Petroski Such,Filipe de Avila Belbute Peres,Filippo Raso,Florent Bekerman,Foivos Tsimpourlas,Fotis Chantzis,Francis Song,Francis Zhang,Gaby Raila,Garrett McGrath,Gary Briggs,Gary Yang,Giambattista Parascandolo,Gildas Chabot,Grace Kim,Grace Zhao,Gregory Valiant,Guillaume Leclerc,Hadi Salman,Hanson Wang,Hao Sheng,Haoming Jiang,Haoyu Wang,Haozhun Jin,Harshit Sikchi,Heather Schmidt,Henry Aspegren,Honglin Chen,Huida Qiu,Hunter Lightman,Ian Covert,Ian Kivlichan,Ian Silber,Ian Sohl,Ibrahim Hammoud,Ignasi Clavera,Ikai Lan,Ilge Akkaya,Ilya Kostrikov,Irina Kofman,Isak Etinger,Ishaan Singal,Jackie Hehir,Jacob Huh,Jacqueline Pan,Jake Wilczynski,Jakub Pachocki,James Lee,James Quinn,Jamie Kiros,Janvi Kalra,Jasmyn Samaroo,Jason Wang,Jason Wolfe,Jay Chen,Jay Wang,Jean Harb,Jeffrey Han,Jeffrey Wang,Jennifer Zhao,Jeremy Chen,Jerene Yang,Jerry Tworek,Jesse Chand,Jessica Landon,Jessica Liang,Ji Lin,Jiancheng Liu,Jianfeng Wang,Jie Tang,Jihan Yin,Joanne Jang,Joel Morris,Joey Flynn,Johannes Ferstad,Johannes Heidecke,John Fishbein,John Hallman,Jonah Grant,Jonathan Chien,Jonathan Gordon,Jongsoo Park,Jordan Liss,Jos Kraaijeveld,Joseph Guay,Joseph Mo,Josh Lawson,Josh McGrath,Joshua Vendrow,Joy Jiao,Julian Lee,Julie Steele,Julie Wang,Junhua Mao,Kai Chen,Kai Hayashi,Kai Xiao,Kamyar Salahi,Kan Wu,Karan Sekhri,Karan Sharma,Karan Singhal,Karen Li,Kenny Nguyen,Keren Gu-Lemberg,Kevin King,Kevin Liu,Kevin Stone,Kevin Yu,Kristen Ying,Kristian Georgiev,Kristie Lim,Kushal Tirumala,Kyle Miller,Lama Ahmad,Larry Lv,Laura Clare,Laurance Fauconnet,Lauren Itow,Lauren Yang,Laurentia Romaniuk,Leah Anise,Lee Byron,Leher Pathak,Leon Maksin,Leyan Lo,Leyton Ho,Li Jing,Liang Wu,Liang Xiong,Lien Mamitsuka,Lin Yang,Lindsay McCallum,Lindsey Held,Liz Bourgeois,Logan Engstrom,Lorenz Kuhn,Louis Feuvrier,Lu Zhang,Lucas Switzer,Lukas Kondraciuk,Lukasz Kaiser,Manas Joglekar,Mandeep Singh,Mandip Shah,Manuka Stratta,Marcus Williams,Mark Chen,Mark Sun,Marselus Cayton,Martin Li,Marvin Zhang,Marwan Aljubeh,Matt Nichols,Matthew Haines,Max Schwarzer,Mayank Gupta,Meghan Shah,Melody Huang,Meng Dong,Mengqing Wang,Mia Glaese,Micah Carroll,Michael Lampe,Michael Malek,Michael Sharman,Michael Zhang,Michele Wang,Michelle Pokrass,Mihai Florian,Mikhail Pavlov,Miles Wang,Ming Chen,Mingxuan Wang,Minnia Feng,Mo Bavarian,Molly Lin,Moose Abdool,Mostafa Rohaninejad,Nacho Soto,Natalie Staudacher,Natan LaFontaine,Nathan Marwell,Nelson Liu,Nick Preston,Nick Turley,Nicklas Ansman,Nicole Blades,Nikil Pancha,Nikita Mikhaylin,Niko Felix,Nikunj Handa,Nishant Rai,Nitish Keskar,Noam Brown,Ofir Nachum,Oleg Boiko,Oleg Murk,Olivia Watkins,Oona Gleeson,Pamela Mishkin,Patryk Lesiewicz,Paul Baltescu,Pavel Belov,Peter Zhokhov,Philip Pronin,Phillip Guo,Phoebe Thacker,Qi Liu,Qiming Yuan,Qinghua Liu,Rachel Dias,Rachel Puckett,Rahul Arora,Ravi Teja Mullapudi,Raz Gaon,Reah Miyara,Rennie Song,Rishabh Aggarwal,RJ Marsan,Robel Yemiru,Robert Xiong,Rohan Kshirsagar,Rohan Nuttall,Roman Tsiupa,Ronen Eldan,Rose Wang,Roshan James,Roy Ziv,Rui Shu,Ruslan Nigmatullin,Saachi Jain,Saam Talaie,Sam Altman,Sam Arnesen,Sam Toizer,Sam Toyer,Samuel Miserendino,Sandhini Agarwal,Sarah Yoo,Savannah Heon,Scott Ethersmith,Sean Grove,Sean Taylor,Sebastien Bubeck,Sever Banesiu,Shaokyi Amdo,Shengjia Zhao,Sherwin Wu,Shibani Santurkar,Shiyu Zhao,Shraman Ray Chaudhuri,Shreyas Krishnaswamy,Shuaiqi,Xia,Shuyang Cheng,Shyamal Anadkat,Simón Posada Fishman,Simon Tobin,Siyuan Fu,Somay Jain,Song Mei,Sonya Egoian,Spencer Kim,Spug Golden,SQ Mah,Steph Lin,Stephen Imm,Steve Sharpe,Steve Yadlowsky,Sulman Choudhry,Sungwon Eum,Suvansh Sanjeev,Tabarak Khan,Tal Stramer,Tao Wang,Tao Xin,Tarun Gogineni,Taya Christianson,Ted Sanders,Tejal Patwardhan,Thomas Degry,Thomas Shadwell,Tianfu Fu,Tianshi Gao,Timur Garipov,Tina Sriskandarajah,Toki Sherbakov,Tomer Kaftan,Tomo Hiratsuka,Tongzhou Wang,Tony Song,Tony Zhao,Troy Peterson,Val Kharitonov,Victoria Chernova,Vineet Kosaraju,Vishal Kuo,Vitchyr Pong,Vivek Verma,Vlad Petrov,Wanning Jiang,Weixing Zhang,Wenda Zhou,Wenlei Xie,Wenting Zhan,Wes McCabe,Will DePue,Will Ellsworth,Wulfie Bain,Wyatt Thompson,Xiangning Chen,Xiangyu Qi,Xin Xiang,Xinwei Shi,Yann Dubois,Yaodong Yu,Yara Khakbaz,Yifan Wu,Yilei Qian,Yin Tat Lee,Yinbo Chen,Yizhen Zhang,Yizhong Xiong,Yonglong Tian,Young Cha,Yu Bai,Yu Yang,Yuan Yuan,Yuanzhi Li,Yufeng Zhang,Yuguang Yang,Yujia Jin,Yun Jiang,Yunyun Wang,Yushi Wang,Yutian Liu,Zach Stubenvoll,Zehao Dou,Zheng Wu,Zhigang Wang*

Main category: cs.CL

TL;DR: GPT-5是一个统一系统，包含智能快速模型、深度推理模型和实时路由器，通过路由决策提升效率，在减少幻觉、遵循指令和降低谄媚性方面有显著改进，并针对生物化学领域采取预防性安全措施。


<details>
  <summary>Details</summary>
Motivation: OpenAI旨在开发一个更强大、更实用的AI系统，能够处理从简单到复杂的各种查询，同时提升响应速度、减少错误输出，并在安全方面采取更严格的预防措施。

Method: GPT-5采用统一系统架构，包含三个核心组件：1）智能快速模型处理大多数问题；2）深度推理模型解决复杂问题；3）实时路由器基于对话类型、复杂度、工具需求和显式意图动态选择模型。路由器通过用户切换行为、偏好评分和正确性测量等真实信号持续训练。系统还包含安全完成机制和针对生物化学领域的特殊防护措施。

Result: GPT-5在基准测试中超越前代模型，响应速度更快，对真实世界查询更有用。在减少幻觉、改进指令遵循和降低谄媚性方面取得显著进展，在写作、编码和健康等常见应用场景表现提升。系统采用安全完成训练防止禁止内容，并对gpt-5-thinking模型在生物化学领域实施高级别防护措施。

Conclusion: GPT-5代表了AI系统设计的重大进步，通过智能路由和多模型架构实现了效率与能力的平衡，同时在安全方面采取了谨慎的预防性方法，特别是在生物化学等敏感领域，展现了负责任AI开发的原则。

Abstract: This is the system card published alongside the OpenAI GPT-5 launch, August 2025.
  GPT-5 is a unified system with a smart and fast model that answers most questions, a deeper reasoning model for harder problems, and a real-time router that quickly decides which model to use based on conversation type, complexity, tool needs, and explicit intent (for example, if you say 'think hard about this' in the prompt). The router is continuously trained on real signals, including when users switch models, preference rates for responses, and measured correctness, improving over time. Once usage limits are reached, a mini version of each model handles remaining queries.
  This system card focuses primarily on gpt-5-thinking and gpt-5-main, while evaluations for other models are available in the appendix. The GPT-5 system not only outperforms previous models on benchmarks and answers questions more quickly, but -- more importantly -- is more useful for real-world queries. We've made significant advances in reducing hallucinations, improving instruction following, and minimizing sycophancy, and have leveled up GPT-5's performance in three of ChatGPT's most common uses: writing, coding, and health. All of the GPT-5 models additionally feature safe-completions, our latest approach to safety training to prevent disallowed content.
  Similarly to ChatGPT agent, we have decided to treat gpt-5-thinking as High capability in the Biological and Chemical domain under our Preparedness Framework, activating the associated safeguards. While we do not have definitive evidence that this model could meaningfully help a novice to create severe biological harm -- our defined threshold for High capability -- we have chosen to take a precautionary approach.

</details>


### [6] [WRAVAL -- WRiting Assist eVALuation](https://arxiv.org/abs/2601.03268)
*Gabriel Benedict,Matthew Butler,Naved Merchant,Eetu Salama-Laine*

Main category: cs.CL

TL;DR: 本文提出专门针对小型语言模型(SLMs)的评估框架，展示其在非推理任务(如语气修改)中的实际应用价值，超越传统推理基准的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型评估过度依赖推理和问题解决任务来衡量通用智能，导致小型语言模型(SLMs)在这些基准上得分远低于大型语言模型(LLMs)，但这未能反映SLMs在实际工业应用中的有效性。

Method: 提出专门针对SLMs的评估框架，包含数据生成、提示调优和基于LLM的评估等新方法，重点评估非推理任务(如语气修改)，并为任务特定微调提供工具。

Result: 该框架能够更准确地评估SLMs在实际应用中的能力，特别是在边缘计算和私有计算场景中，为从业者提供了有效比较SLMs和LLMs实用性能的工具。

Conclusion: SLMs在特定非推理任务中具有显著实用价值，传统推理基准不能完全反映其实际应用能力，需要针对性的评估框架来指导工业应用中的模型选择。

Abstract: The emergence of Large Language Models (LLMs) has shifted language model evaluation toward reasoning and problem-solving tasks as measures of general intelligence. Small Language Models (SLMs) -- defined here as models under 10B parameters -- typically score 3-4 times lower than LLMs on these metrics. However, we demonstrate that these evaluations fail to capture SLMs' effectiveness in common industrial applications, such as tone modification tasks (e.g., funny, serious, professional). We propose an evaluation framework specifically designed to highlight SLMs' capabilities in non-reasoning tasks where predefined evaluation datasets don't exist. Our framework combines novel approaches in data generation, prompt-tuning, and LLM-based evaluation to demonstrate the potential of task-specific finetuning. This work provides practitioners with tools to effectively benchmark both SLMs and LLMs for practical applications, particularly in edge and private computing scenarios. Our implementation is available at: https://github.com/amazon-science/wraval.

</details>


### [7] [SegNSP: Revisiting Next Sentence Prediction for Linear Text Segmentation](https://arxiv.org/abs/2601.03474)
*José Isidro,Filipe Cunha,Purificação Silvano,Alípio Jorge,Nuno Guimarães,Sérgio Nunes,Ricardo Campos*

Main category: cs.CL

TL;DR: SegNSP将线性文本分割重构为下一句预测任务，通过标签无关的NSP方法和分割感知损失函数，在多个数据集上取得优于基线的性能。


<details>
  <summary>Details</summary>
Motivation: 线性文本分割是NLP中长期存在的问题，但由于主题边界定义复杂、话语结构多变、需要平衡局部连贯性和全局上下文等因素，该任务仍然具有挑战性。这些困难阻碍了摘要、信息检索和问答等下游应用的发展。

Method: 提出SegNSP方法，将线性文本分割重构为下一句预测任务。采用标签无关的NSP方法，预测下一句是否继续当前主题，无需显式主题标签。通过分割感知损失函数结合更难负采样来更好地捕捉话语连续性。与最近结合NSP和辅助主题分类的方法不同，该方法避免任务特定监督。

Result: 在CitiLink-Minutes数据集上建立首个分割基准，SegNSP达到B-F₁ 0.79，与人工标注的主题转换高度一致。在WikiSection数据集上达到B-F₁ 0.65，比最强可复现基线TopSeg高出0.17个绝对点。

Conclusion: 结果表明SegNSP具有竞争力和鲁棒性，证明了建模句子间连续性对于提高分割质量和支持下游NLP应用的有效性。尽管NSP在现代预训练中已被弃用，但其显式建模句子间连续性的特性使其成为检测主题边界的自然选择。

Abstract: Linear text segmentation is a long-standing problem in natural language processing (NLP), focused on dividing continuous text into coherent and semantically meaningful units. Despite its importance, the task remains challenging due to the complexity of defining topic boundaries, the variability in discourse structure, and the need to balance local coherence with global context. These difficulties hinder downstream applications such as summarization, information retrieval, and question answering. In this work, we introduce SegNSP, framing linear text segmentation as a next sentence prediction (NSP) task. Although NSP has largely been abandoned in modern pre-training, its explicit modeling of sentence-to-sentence continuity makes it a natural fit for detecting topic boundaries. We propose a label-agnostic NSP approach, which predicts whether the next sentence continues the current topic without requiring explicit topic labels, and enhance it with a segmentation-aware loss combined with harder negative sampling to better capture discourse continuity. Unlike recent proposals that leverage NSP alongside auxiliary topic classification, our approach avoids task-specific supervision. We evaluate our model against established baselines on two datasets, CitiLink-Minutes, for which we establish the first segmentation benchmark, and WikiSection. On CitiLink-Minutes, SegNSP achieves a B-$F_1$ of 0.79, closely aligning with human-annotated topic transitions, while on WikiSection it attains a B-F$_1$ of 0.65, outperforming the strongest reproducible baseline, TopSeg, by 0.17 absolute points. These results demonstrate competitive and robust performance, highlighting the effectiveness of modeling sentence-to-sentence continuity for improving segmentation quality and supporting downstream NLP applications.

</details>


### [8] [The Instruction Gap: LLMs get lost in Following Instruction](https://arxiv.org/abs/2601.03269)
*Vishesh Tripathi,Uday Allu,Biddwan Ahmed*

Main category: cs.CL

TL;DR: 对13个主流大语言模型在企业RAG场景下的指令遵循能力进行评估，发现模型间差异显著，存在"指令鸿沟"问题。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在自然语言理解和生成方面表现出色，但在企业部署中发现一个关键限制：对自定义指令的遵循不一致。企业环境需要精确的指令遵循，而现有模型在这方面存在不足。

Method: 采用系统性测试方法，对13个领先的大语言模型进行评估，涵盖指令遵循、响应准确性和性能指标。使用真实世界的RAG场景，采用企业级评估协议，通过样本测试进行分析。

Result: 不同模型在指令遵循方面存在显著差异，Claude-Sonnet-4和GPT-5表现最佳。研究揭示了"指令鸿沟"现象：模型在通用任务上表现出色，但在企业部署所需的精确指令遵循方面存在困难。

Conclusion: 这项工作为企业部署LLM解决方案提供了实用见解，并为各大模型家族的指令遵循能力建立了基准。研究强调了提升模型指令遵循能力对于企业应用的重要性。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in natural language understanding and generation, yet their deployment in enterprise environments reveals a critical limitation: inconsistent adherence to custom instructions. This study presents a comprehensive evaluation of 13 leading LLMs across instruction compliance, response accuracy, and performance metrics in realworld RAG (Retrieval-Augmented Generation) scenarios. Through systematic testing with samples and enterprise-grade evaluation protocols, we demonstrate that instruction following varies dramatically across models, with Claude-Sonnet-4 and GPT-5 achieving the highest results. Our findings reveal the "instruction gap" - a fundamental challenge where models excel at general tasks but struggle with precise instruction adherence required for enterprise deployment. This work provides practical insights for organizations deploying LLM-powered solutions and establishes benchmarks for instruction-following capabilities across major model families.

</details>


### [9] [Advances and Challenges in Semantic Textual Similarity: A Comprehensive Survey](https://arxiv.org/abs/2601.03270)
*Lokendra Kumar,Neelesh S. Upadhye,Kannan Piedy*

Main category: cs.CL

TL;DR: 这篇综述论文系统回顾了2021年以来语义文本相似性研究的最新进展，涵盖了Transformer模型、对比学习、领域适应、多模态方法、图神经网络和知识增强等六个关键方向。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer架构、对比学习和领域特定技术的快速发展，语义文本相似性研究在2021年后迅速扩展，需要对这些最新进展进行系统性梳理和总结，以指导研究者和从业者。

Method: 通过文献综述方法，系统性地回顾和分析六个关键研究领域：基于Transformer的模型、对比学习方法、领域特定解决方案、多模态方法、图神经网络方法和知识增强技术。

Result: 研究发现最近Transformer模型如FarSSiBERT和DeBERTa-v3取得了显著准确率提升，对比学习方法如AspectCSE建立了新基准，领域适应模型如CXR-BERT和Financial-STS展示了STS在专业领域的定制化应用潜力。

Conclusion: 该综述为当前STS研究方法、实际应用和剩余挑战提供了有价值的见解，旨在帮助研究者和从业者把握快速发展的技术趋势，并指出了该领域的新兴趋势和未来机会。

Abstract: Semantic Textual Similarity (STS) research has expanded rapidly since 2021, driven by advances in transformer architectures, contrastive learning, and domain-specific techniques. This survey reviews progress across six key areas: transformer-based models, contrastive learning, domain-focused solutions, multi-modal methods, graph-based approaches, and knowledge-enhanced techniques. Recent transformer models such as FarSSiBERT and DeBERTa-v3 have achieved remarkable accuracy, while contrastive methods like AspectCSE have established new benchmarks. Domain-adapted models, including CXR-BERT for medical texts and Financial-STS for finance, demonstrate how STS can be effectively customized for specialized fields. Moreover, multi-modal, graph-based, and knowledge-integrated models further enhance semantic understanding and representation. By organizing and analyzing these developments, the survey provides valuable insights into current methods, practical applications, and remaining challenges. It aims to guide researchers and practitioners alike in navigating rapid advancements, highlighting emerging trends and future opportunities in the evolving field of STS.

</details>


### [10] [Less is more: Not all samples are effective for evaluation](https://arxiv.org/abs/2601.03272)
*Wentang Song,Jinqiang Li,Kele Huang,Junhui Lin,Shengxiang Wu,Zhongshi Xie*

Main category: cs.CL

TL;DR: 提出一种无需历史模型性能数据的测试集压缩框架，通过领域适应LLM生成语义嵌入，进行任务感知聚类和数据集X射线分析，实现90%以上的评估成本降低。


<details>
  <summary>Details</summary>
Motivation: 现有专业领域LLM评估基准存在语义冗余问题，且现有压缩方法（如tinyBenchmarks）严重依赖历史模型在完整测试集上的正确性标签，无法适用于新任务、新领域或新模型的冷启动场景。

Method: 1) 在少量领域数据上微调基础LLM以内化任务相关语义；2) 仅使用原始文本内容为所有测试样本生成高级语义嵌入；3) 在领域适应嵌入空间中进行任务感知聚类；4) 引入数据集X射线机制分析聚类几何结构，根据基准的内在冗余动态校准压缩强度。

Result: 在专业领域数据集（特别是大规模3GPP通信基准）上的实验表明，该方法能有效识别和移除冗余样本，将评估成本降低90%以上，同时保持与完整基准的高度保真度。

Conclusion: 该方法为冷启动场景下的测试集压缩提供了有效解决方案，显著降低了专业领域LLM评估的计算成本，同时保持了评估结果的可靠性。

Abstract: The versatility of Large Language Models (LLMs) in vertical domains has spurred the development of numerous specialized evaluation benchmarks. However, these benchmarks often suffer from significant semantic redundancy and impose high computational costs during evaluation. Existing compression methods, such as tinyBenchmarks depend critically on correctness labels from multiple historical models evaluated on the full test set, making them inapplicable in cold-start scenarios, such as the introduction of a new task, domain, or model with no prior evaluation history.
  To address this limitation, we propose a history-free test set compression framework that requires no prior model performance data. Our method begins by fine-tuning a base LLM on a small amount of domain-specific data to internalize task-relevant semantics. It then generates high-level semantic embeddings for all original test samples using only their raw textual content. In this domain-adapted embedding space, we perform task-aware clustering and introduce a novel dataset X-ray mechanism that analyzes cluster geometry to dynamically calibrate the compression intensity based on the intrinsic redundancy of the benchmark.
  Experiments on professional-domain dataset, notably a large-scale 3GPP communications benchmark, demonstrate that our approach effectively identifies and removes redundant samples, reducing evaluation cost by over 90% while preserving high fidelity to the full benchmark.

</details>


### [11] [GuardEval: A Multi-Perspective Benchmark for Evaluating Safety, Fairness, and Robustness in LLM Moderators](https://arxiv.org/abs/2601.03273)
*Naseem Machlovi,Maryam Saleki,Ruhul Amin,Mohamed Rahouti,Shawqi Al-Maliki,Junaid Qadir,Mohamed M. Abdallah,Ala Al-Fuqaha*

Main category: cs.CL

TL;DR: GuardEval是一个统一的多视角基准数据集，包含106个细粒度类别，用于训练和评估LLM内容审核系统；GemmaGuard是基于Gemma3-12B使用QLoRA微调的模型，在细粒度内容审核任务上显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在内容审核方面存在局限：难以处理隐式冒犯性、微妙性别和种族偏见、越狱提示等复杂情况，且依赖训练数据可能强化社会偏见，导致不一致和伦理问题输出。

Method: 1) 构建GuardEval基准数据集，包含106个细粒度类别，涵盖人类情感、冒犯和仇恨言论、性别和种族偏见、安全关切等；2) 开发GemmaGuard模型，使用QLoRA技术对Gemma3-12B进行微调训练。

Result: GemmaGuard在内容审核任务上取得0.832的宏观F1分数，显著优于OpenAI Moderator（0.64）和Llama Guard（0.61）等领先模型。多视角、以人为中心的安全基准能有效减少偏见和不一致的审核决策。

Conclusion: 多样化和有代表性的数据能实质性地提高LLM在复杂边界案例上的安全性、公平性和鲁棒性。GuardEval和GemmaGuard的组合展示了多视角安全基准对改进内容审核系统的重要性。

Abstract: As large language models (LLMs) become deeply embedded in daily life, the urgent need for safer moderation systems, distinguishing between naive from harmful requests while upholding appropriate censorship boundaries, has never been greater. While existing LLMs can detect harmful or unsafe content, they often struggle with nuanced cases such as implicit offensiveness, subtle gender and racial biases, and jailbreak prompts, due to the subjective and context-dependent nature of these issues. Furthermore, their heavy reliance on training data can reinforce societal biases, resulting in inconsistent and ethically problematic outputs. To address these challenges, we introduce GuardEval, a unified multi-perspective benchmark dataset designed for both training and evaluation, containing 106 fine-grained categories spanning human emotions, offensive and hateful language, gender and racial bias, and broader safety concerns. We also present GemmaGuard (GGuard), a QLoRA fine-tuned version of Gemma3-12B trained on GuardEval, to assess content moderation with fine-grained labels. Our evaluation shows that GGuard achieves a macro F1 score of 0.832, substantially outperforming leading moderation models, including OpenAI Moderator (0.64) and Llama Guard (0.61). We show that multi-perspective, human-centered safety benchmarks are critical for reducing biased and inconsistent moderation decisions. GuardEval and GGuard together demonstrate that diverse, representative data materially improve safety, fairness, and robustness on complex, borderline cases.

</details>


### [12] [LLM_annotate: A Python package for annotating and analyzing fiction characters](https://arxiv.org/abs/2601.03274)
*Hannes Rosenbusch*

Main category: cs.CL

TL;DR: LLM_annotate是一个用于分析小说角色性格的Python包，通过大语言模型标准化角色行为标注工作流程，包含文本分块、标注、角色消歧、质量评估等功能。


<details>
  <summary>Details</summary>
Motivation: 分析小说角色性格通常需要大量人工标注，过程繁琐且难以标准化。现有方法缺乏统一的工具来支持基于大语言模型的角色分析工作流程。

Method: 开发了LLM_annotate Python包，提供标准化工作流：文本分块、基于LLM的角色行为标注、角色名称消歧、质量评分（通过人机交互GUI验证）、角色级统计和嵌入计算。支持商业、开源或自定义LLM。

Result: 通过《辛普森一家》电影和《傲慢与偏见》小说的教程示例，展示了该包能实现高效、可复现的角色分析。工具标准化了角色分析流程，支持多种LLM。

Conclusion: LLM_annotate为研究人员提供了标准化工具，简化了基于大语言模型的小说角色分析工作流程，实现了高效、可复现的角色性格分析。

Abstract: LLM_annotate is a Python package for analyzing the personality of fiction characters with large language models. It standardizes workflows for annotating character behaviors in full texts (e.g., books and movie scripts), inferring character traits, and validating annotation/inference quality via a human-in-the-loop GUI. The package includes functions for text chunking, LLM-based annotation, character name disambiguation, quality scoring, and computation of character-level statistics and embeddings. Researchers can use any LLM, commercial, open-source, or custom, within LLM_annotate. Through tutorial examples using The Simpsons Movie and the novel Pride and Prejudice, I demonstrate the usage of the package for efficient and reproducible character analyses.

</details>


### [13] [Topic Segmentation Using Generative Language Models](https://arxiv.org/abs/2601.03276)
*Pierre Mackenzie,Maya Shah,Patrick Frenett*

Main category: cs.CL

TL;DR: 本文探索使用生成式大语言模型进行主题分割，提出重叠递归提示策略，发现LLM在主题分割上优于现有方法但仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 主题分割使用生成式大语言模型的研究相对较少。现有方法主要基于句子间的语义相似性，但这些模型缺乏大语言模型所具有的长距离依赖关系和广泛知识。

Method: 提出一种基于句子枚举的重叠递归提示策略，同时支持采用边界相似性评估指标来衡量分割效果。

Result: 实验结果表明，大语言模型在主题分割任务上比现有方法更有效，但在完全可靠地用于主题分割之前仍有一些问题需要解决。

Conclusion: 大语言模型在主题分割方面展现出潜力，优于基于语义相似性的传统方法，但还需要进一步解决现有问题才能在实际应用中可靠使用。

Abstract: Topic segmentation using generative Large Language Models (LLMs) remains relatively unexplored. Previous methods use semantic similarity between sentences, but such models lack the long range dependencies and vast knowledge found in LLMs. In this work, we propose an overlapping and recursive prompting strategy using sentence enumeration. We also support the adoption of the boundary similarity evaluation metric. Results show that LLMs can be more effective segmenters than existing methods, but issues remain to be solved before they can be relied upon for topic segmentation.

</details>


### [14] [Bare-Metal Tensor Virtualization: Overcoming the Memory Wall in Edge-AI Inference on ARM64](https://arxiv.org/abs/2601.03324)
*Bugra Kilictas,Faruk Alpay*

Main category: cs.CL

TL;DR: 提出基于ARM64架构的"虚拟张量核心"软件架构，通过内存映射和手工调优的NEON SIMD内核，实现软件定义的直接内存访问，在M2硬件上为1.1亿参数模型提供超过60 tokens/秒的稳定吞吐量。


<details>
  <summary>Details</summary>
Motivation: 边缘设备部署大型语言模型受到"内存墙"的根本限制，标准推理运行时因高层抽象、动态调度和未对齐的内存访问模式而产生显著开销。

Method: 提出"虚拟张量核心"软件架构，专为ARM64微架构（Apple Silicon）优化。通过绕过标准库容器，采用直接内存映射（mmap）和手工调优的NEON SIMD内核，实现"软件定义的直接内存访问"。提出的张量虚拟化布局（TVL）保证权重矩阵100%缓存行利用率，零拷贝加载器消除初始化延迟。

Result: 在M2硬件上，对1.1亿参数模型实现了超过60 tokens/秒的稳定吞吐量。虽然专有硬件加速器（如Apple AMX）能达到更高峰值吞吐量，但该架构提供了完全开源、可移植且确定性的参考实现，在通用ARM芯片上满足200ms心理语言学延迟阈值。

Conclusion: 该工作展示了一种在通用ARM硅芯片上研究内存瓶颈的完全开放、可移植且确定性的参考实现方法，能够在没有不透明依赖的情况下满足实时处理的心理语言学延迟要求。

Abstract: The deployment of Large Language Models (LLMs) on edge devices is fundamentally constrained by the "Memory Wall" the bottleneck where data movement latency outstrips arithmetic throughput. Standard inference runtimes often incur significant overhead through high-level abstractions, dynamic dispatch, and unaligned memory access patterns. In this work, we present a novel "Virtual Tensor Core" architecture implemented in software, optimized specifically for ARM64 microarchitectures (Apple Silicon). By bypassing standard library containers in favor of direct memory mapping (mmap) and implementing hand-tuned NEON SIMD kernels, we achieve a form of "Software-Defined Direct Memory Access (DMA)." Our proposed Tensor Virtualization Layout (TVL) guarantees 100% cache line utilization for weight matrices, while our zero-copy loader eliminates initialization latency. Experimental results on a 110M parameter model demonstrate a stable throughput of >60 tokens/second on M2 hardware. While proprietary hardware accelerators (e.g., Apple AMX) can achieve higher peak throughput, our architecture provides a fully open, portable, and deterministic reference implementation for studying the memory bottleneck on general-purpose ARM silicon, meeting the 200ms psycholinguistic latency threshold without opaque dependencies.

</details>


### [15] [A path to natural language through tokenisation and transformers](https://arxiv.org/abs/2601.03368)
*David S. Berman,Alexander G. Stapleton*

Main category: cs.CL

TL;DR: 本文分析字节对编码(BPE)如何影响自然语言统计特性，发现BPE将词频推向Zipf定律分布，减少局部依赖，使模型预测熵更接近Zipf理论预测。


<details>
  <summary>Details</summary>
Motivation: 尽管自然语言展现Zipf和Heaps定律等统计规律，但这些特性如何与Transformer模型使用的现代分词方案（如BPE）相关联仍不清楚。需要理解BPE如何影响语言统计特性和信息内容。

Method: 1. 在Zipf频率分布假设下分析各种语料库的信息内容（香农熵），推导槽熵期望值的闭式表达式；2. 实证研究BPE如何转换语料库统计特性，展示递归应用BPE驱动词频趋向Zipf幂律并引起经验熵特征增长模式；3. 在不同BPE深度分词语料上训练语言模型，利用Transformer学习上下文相关词符概率分布的能力；4. 使用注意力机制诊断分析局部依赖关系。

Result: 1. BPE递归应用使词符频率趋向Zipf幂律分布；2. 诱导经验熵出现特征增长模式；3. 随着BPE深度增加，模型预测熵与Zipf推导的预测越来越一致；4. 注意力诊断表明更深的分词减少局部词符依赖，使经验分布更接近弱依赖（近似独立同分布）状态。

Conclusion: BPE不仅是一种压缩机制，也是一种统计转换方法，能够重建自然语言的关键信息特性。它通过将词频推向Zipf定律分布并减少局部依赖，使语言统计特性更符合经典语言规律。

Abstract: Natural languages exhibit striking regularities in their statistical structure, including notably the emergence of Zipf's and Heaps' laws. Despite this, it remains broadly unclear how these properties relate to the modern tokenisation schemes used in contemporary transformer models. In this note, we analyse the information content (as measured by the Shannon entropy) of various corpora under the assumption of a Zipfian frequency distribution, and derive a closed-form expression for the slot entropy expectation value. We then empirically investigate how byte--pair encoding (BPE) transforms corpus statistics, showing that recursive applications of BPE drive token frequencies toward a Zipfian power law while inducing a characteristic growth pattern in empirical entropy. Utilizing the ability of transformers to learn context dependent token probability distributions, we train language models on corpora tokenised at varying BPE depths, revealing that the model predictive entropies increasingly agree with Zipf-derived predictions as the BPE depth increases. Attention-based diagnostics further indicate that deeper tokenisation reduces local token dependencies, bringing the empirical distribution closer to the weakly dependent (near IID) regime. Together, these results clarify how BPE acts not only as a compression mechanism but also as a statistical transform that reconstructs key informational properties of natural language.

</details>


### [16] [Metaphors are a Source of Cross-Domain Misalignment of Large Reasoning Models](https://arxiv.org/abs/2601.03388)
*Zhibo Hu,Chen Wang,Yanfeng Shu,Hye-young Paik,Liming Zhu*

Main category: cs.CL

TL;DR: 研究发现隐喻会影响大语言模型的推理路径，导致跨领域未对齐问题，通过监测潜在特征可检测未对齐内容。


<details>
  <summary>Details</summary>
Motivation: 之前研究表明隐喻影响人类决策，而大语言模型的训练数据包含大量隐喻，因此研究隐喻是否也会影响LLMs的推理路径，特别是在跨领域未对齐问题上。

Method: 在预训练、微调和重新对齐阶段使用隐喻进行干预，研究隐喻与模型未对齐程度之间的因果关系，并监测全局和局部潜在特征的激活情况。

Result: 发现训练数据中的隐喻与大语言模型推理内容的未对齐程度存在强烈因果关系，通过监测潜在特征设计的检测器能够高精度预测未对齐内容。

Conclusion: 隐喻确实会影响大语言模型的推理路径，导致跨领域未对齐问题，通过监测潜在特征可以有效地检测和预测未对齐内容。

Abstract: Earlier research has shown that metaphors influence human's decision making, which raises the question of whether metaphors also influence large language models (LLMs)' reasoning pathways, considering their training data contain a large number of metaphors. In this work, we investigate the problem in the scope of the emergent misalignment problem where LLMs can generalize patterns learned from misaligned content in one domain to another domain. We discover a strong causal relationship between metaphors in training data and the misalignment degree of LLMs' reasoning contents. With interventions using metaphors in pre-training, fine-tuning and re-alignment phases, models' cross-domain misalignment degrees change significantly. As we delve deeper into the causes behind this phenomenon, we observe that there is a connection between metaphors and the activation of global and local latent features of large reasoning models. By monitoring these latent features, we design a detector that predict misaligned content with high accuracy.

</details>


### [17] [Breaking the Assistant Mold: Modeling Behavioral Variation in LLM Based Procedural Character Generation](https://arxiv.org/abs/2601.03396)
*Maan Qraitem,Kate Saenko,Bryan A. Plummer*

Main category: cs.CL

TL;DR: PersonaWeaver是一个角色生成框架，通过解构世界构建（角色、人口统计）与行为构建（道德立场、互动风格），解决现有方法中存在的正向道德偏见和乐于助人偏见，从而生成更具多样性反应和道德立场的角色。


<details>
  <summary>Details</summary>
Motivation: 现有程序化内容生成方法在角色生成方面存在两个对齐诱导的偏见：正向道德偏见（角色总是采取一致立场，如总是说撒谎是错的）和乐于助人偏见（角色总是直接回答问题，从不拒绝或回避）。虽然这些倾向适合指令跟随系统，但它们抑制了戏剧张力，产生了可预测的角色，这源于最大似然训练和助手微调。

Method: PersonaWeaver框架将世界构建（角色、人口统计）与行为构建（道德立场、互动风格）解耦，从而生成具有更多样化反应和道德立场的角色，以及在长度、语气和标点等风格标记上的二阶多样性。

Result: 该方法能够生成具有更多样化反应和道德立场的角色，以及在风格标记（如长度、语气、标点）上的二阶多样性。

Conclusion: PersonaWeaver通过解构世界构建和行为构建，有效解决了现有角色生成方法中的对齐偏见问题，为生成更具戏剧张力和不可预测性的多样化角色提供了有效框架。

Abstract: Procedural content generation has enabled vast virtual worlds through levels, maps, and quests, but large-scale character generation remains underexplored. We identify two alignment-induced biases in existing methods: a positive moral bias, where characters uniformly adopt agreeable stances (e.g. always saying lying is bad), and a helpful assistant bias, where characters invariably answer questions directly (e.g. never refusing or deflecting). While such tendencies suit instruction-following systems, they suppress dramatic tension and yield predictable characters, stemming from maximum likelihood training and assistant fine-tuning. To address this, we introduce PersonaWeaver, a framework that disentangles world-building (roles, demographics) from behavioral-building (moral stances, interactional styles), yielding characters with more diverse reactions and moral stances, as well as second-order diversity in stylistic markers like length, tone, and punctuation. Code: https://github.com/mqraitem/Persona-Weaver

</details>


### [18] [Rendering Data Unlearnable by Exploiting LLM Alignment Mechanisms](https://arxiv.org/abs/2601.03401)
*Ruihan Zhang,Jun Sun*

Main category: cs.CL

TL;DR: 本文提出Disclaimer Injection方法，通过在文本中注入精心设计的免责声明，利用LLM的对齐机制来防止模型学习受保护数据，实现了无需访问训练流程的黑盒数据保护。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在大量异构文本语料上训练，未经授权使用专有或个人数据的问题日益严重。现有的数据保护方法通常需要模型端控制或显式数据移除，在现实黑盒设置中难以实施。

Method: 提出Disclaimer Injection防御方法：1）设计触发模型对齐机制的免责声明；2）将免责声明注入到待保护的文本数据中；3）通过层级分析发现，微调这类受保护数据会持续激活对齐相关层，使对齐约束覆盖任务学习。

Result: 模型在受保护数据上训练后表现出显著且系统性的性能下降。与标准微调相比，这种防御方法能有效限制数据可学习性，而无需访问或修改训练流程。

Conclusion: 对齐行为是数据保护中一个未被探索的杠杆，Disclaimer Injection是首个无需访问训练流程即可在LLM规模上限制数据可学习性的实用方法。

Abstract: Large language models (LLMs) are increasingly trained on massive, heterogeneous text corpora, raising serious concerns about the unauthorised use of proprietary or personal data during model training. In this work, we address the problem of data protection against unwanted model learning in a realistic black-box setting. We propose Disclaimer Injection, a novel data-level defence that renders text unlearnable to LLMs. Rather than relying on model-side controls or explicit data removal, our approach exploits the models' own alignment mechanisms: by injecting carefully designed alignment-triggering disclaimers to prevent effective learning. Through layer-wise analysis, we find that fine-tuning on such protected data induces persistent activation of alignment-related layers, causing alignment constraints to override task learning even on common inputs. Consequently, models trained on such data exhibit substantial and systematic performance degradation compared to standard fine-tuning. Our results identify alignment behaviour as a previously unexplored lever for data protection and, to our knowledge, present the first practical method for restricting data learnability at LLM scale without requiring access to or modification of the training pipeline.

</details>


### [19] [Tigrinya Number Verbalization: Rules, Algorithm, and Implementation](https://arxiv.org/abs/2601.03403)
*Fitsum Gaim,Issayas Tesfamariam*

Main category: cs.CL

TL;DR: 系统性地形式化了提格里尼亚语基数和序数词的口语表达规则，填补了该语言计算资源的空白，并发布了开源实现。


<details>
  <summary>Details</summary>
Motivation: 提格里尼亚语缺乏计算资源来处理数字的口语表达，这限制了面向提格里尼亚语社区的语音合成、语言建模和可访问性应用的发展。

Method: 1. 记录提格里尼亚语数字表达的正则规则（包括连词系统、量级词和日期、时间、货币等特殊情况）
2. 提供数字到词转换的正式算法
3. 发布开源实现
4. 评估前沿大语言模型在提格里尼亚语数字表达上的能力

Result: 1. 系统性地形式化了提格里尼亚语基数和序数词的口语表达规则
2. 发现前沿大语言模型在准确表达提格里尼亚语数字方面存在显著差距
3. 强调了明确规则文档的必要性

Conclusion: 这项工作填补了提格里尼亚语计算资源的空白，为语言建模、语音合成和可访问性应用提供了重要基础，同时揭示了当前大语言模型在低资源语言处理方面的局限性。

Abstract: We present a systematic formalization of Tigrinya cardinal and ordinal number verbalization, addressing a gap in computational resources for the language. This work documents the canonical rules governing the expression of numerical values in spoken Tigrinya, including the conjunction system, scale words, and special cases for dates, times, and currency. We provide a formal algorithm for number-to-word conversion and release an open-source implementation. Evaluation of frontier large language models (LLMs) reveals significant gaps in their ability to accurately verbalize Tigrinya numbers, underscoring the need for explicit rule documentation. This work serves language modeling, speech synthesis, and accessibility applications targeting Tigrinya-speaking communities.

</details>


### [20] [Implicit Graph, Explicit Retrieval: Towards Efficient and Interpretable Long-horizon Memory for Large Language Models](https://arxiv.org/abs/2601.03417)
*Xin Zhang,Kailai Yang,Hao Li,Chenyue Li,Qiyu Wei,Sophia Ananiadou*

Main category: cs.CL

TL;DR: LatentGraphMem：结合隐式图记忆与显式子图检索的混合记忆框架，用于长上下文稀疏证据场景


<details>
  <summary>Details</summary>
Motivation: 现有记忆系统存在局限：显式结构化记忆可解释但脆弱，隐式记忆高效稳定但难以检查。长视野应用需要LLM在证据稀疏且分散的超长上下文中回答问题。

Method: 提出LatentGraphMem框架：1）在潜在空间存储图结构记忆以保证稳定性和效率；2）提供任务特定的子图检索接口，在固定预算下返回紧凑的符号子图用于下游推理和人工检查；3）训练时通过显式图视图与冻结推理器交互进行问答监督；4）推理时在潜在空间检索，仅外部化检索到的子图。

Result: 在多个模型规模的长视野基准测试中，LatentGraphMem始终优于代表性的显式图和隐式记忆基线，同时支持参数高效适应和灵活扩展到更大推理器，而不引入大型符号构件。

Conclusion: LatentGraphMem成功结合了隐式记忆的稳定性/效率与显式记忆的可解释性，为长上下文稀疏证据检索问题提供了有效的混合解决方案。

Abstract: Long-horizon applications increasingly require large language models (LLMs) to answer queries when relevant evidence is sparse and dispersed across very long contexts. Existing memory systems largely follow two paradigms: explicit structured memories offer interpretability but often become brittle under long-context overload, while latent memory mechanisms are efficient and stable yet difficult to inspect. We propose LatentGraphMem, a memory framework that combines implicit graph memory with explicit subgraph retrieval. LatentGraphMem stores a graph-structured memory in latent space for stability and efficiency, and exposes a task-specific subgraph retrieval interface that returns a compact symbolic subgraph under a fixed budget for downstream reasoning and human inspection. During training, an explicit graph view is materialized to interface with a frozen reasoner for question-answering supervision. At inference time, retrieval is performed in latent space and only the retrieved subgraph is externalized. Experiments on long-horizon benchmarks across multiple model scales show that LatentGraphMem consistently outperforms representative explicit-graph and latent-memory baselines, while enabling parameter-efficient adaptation and flexible scaling to larger reasoners without introducing large symbolic artifacts.

</details>


### [21] [PCoA: A New Benchmark for Medical Aspect-Based Summarization With Phrase-Level Context Attribution](https://arxiv.org/abs/2601.03418)
*Bohao Chu,Sameh Frihat,Tabea M. G. Pakull,Hendrik Damm,Meijie Li,Ula Muhabbek,Georg Lodde,Norbert Fuhr*

Main category: cs.CL

TL;DR: PCoA是一个医疗领域短语级上下文归因的专家标注基准，用于评估系统生成的摘要，并提出解耦评估框架分别评估摘要、引用和贡献短语的质量。


<details>
  <summary>Details</summary>
Motivation: 验证系统生成的摘要具有挑战性，特别是在高风险医疗领域，需要精确的源上下文归因。现有方法缺乏细粒度的短语级上下文归因评估。

Method: 引入PCoA基准，将每个基于方面的摘要与其支持性上下文句子和其中的贡献短语对齐。提出解耦评估框架，独立评估生成的摘要、引用和贡献短语的质量。

Result: PCoA为评估系统生成的摘要提供了可靠基准。实验表明，在摘要生成前明确识别相关句子和贡献短语可以提高整体质量。

Conclusion: PCoA解决了医疗领域摘要验证的挑战，提供了短语级上下文归因的评估基准，有助于提高系统生成摘要的可信度和质量。

Abstract: Verifying system-generated summaries remains challenging, as effective verification requires precise attribution to the source context, which is especially crucial in high-stakes medical domains. To address this challenge, we introduce PCoA, an expert-annotated benchmark for medical aspect-based summarization with phrase-level context attribution. PCoA aligns each aspect-based summary with its supporting contextual sentences and contributory phrases within them. We further propose a fine-grained, decoupled evaluation framework that independently assesses the quality of generated summaries, citations, and contributory phrases. Through extensive experiments, we validate the quality and consistency of the PCoA dataset and benchmark several large language models on the proposed task. Experimental results demonstrate that PCoA provides a reliable benchmark for evaluating system-generated summaries with phrase-level context attribution. Furthermore, comparative experiments show that explicitly identifying relevant sentences and contributory phrases before summarization can improve overall quality. The data and code are available at https://github.com/chubohao/PCoA.

</details>


### [22] [Training-Free Adaptation of New-Generation LLMs using Legacy Clinical Models](https://arxiv.org/abs/2601.03423)
*Sasha Ronaghi,Chloe Stanwyck,Asad Aali,Amir Ronaghi,Miguel Fuentes,Tina Hernandez-Boussard,Emily Alsentzer*

Main category: cs.CL

TL;DR: CAPT是一种无需训练的模型集成方法，通过对比解码将临床模型的医学知识注入到新一代通用模型中，实现医学领域适配。


<details>
  <summary>Details</summary>
Motivation: 临床领域语言模型适配通常需要针对每个新模型进行昂贵的重新预训练和微调，这限制了最新通用模型在医学领域的应用。

Method: 提出跨架构代理调优（CAPT），采用模型集成方法，利用对比解码技术，在不重新训练的情况下将现有临床模型的知识注入到新一代通用模型中，支持词汇表不重叠的模型。

Result: 在6个临床分类和文本生成任务上，CAPT使用新一代通用模型和旧一代临床模型的组合，持续优于两个模型单独使用以及现有最佳集成方法（平均比UniTE高17.6%，比代理调优高41.4%）。

Conclusion: CAPT能够有效增强临床可操作性语言，减少上下文错误，提高临床特异性，为医学领域适配提供了一种无需训练的高效解决方案。

Abstract: Adapting language models to the clinical domain through continued pretraining and fine-tuning requires costly retraining for each new model generation. We propose Cross-Architecture Proxy Tuning (CAPT), a model-ensembling approach that enables training-free adaptation of state-of-the-art general-domain models using existing clinical models. CAPT supports models with disjoint vocabularies, leveraging contrastive decoding to selectively inject clinically relevant signals while preserving the general-domain model's reasoning and fluency. On six clinical classification and text-generation tasks, CAPT with a new-generation general-domain model and an older-generation clinical model consistently outperforms both models individually and state-of-the-art ensembling approaches (average +17.6% over UniTE, +41.4% over proxy tuning across tasks). Through token-level analysis and physician case studies, we demonstrate that CAPT amplifies clinically actionable language, reduces context errors, and increases clinical specificity.

</details>


### [23] [The Critical Role of Aspects in Measuring Document Similarity](https://arxiv.org/abs/2601.03435)
*Eftekhar Hossain,Tarnika Hazra,Ahatesham Bhuiyan,Santu Karmaker*

Main category: cs.CL

TL;DR: ASPECTSIM是一个简单可解释的框架，通过显式指定方面来评估文档相似性，相比传统整体相似性方法显著提高了人类-机器一致性。


<details>
  <summary>Details</summary>
Motivation: 传统文档相似性评估采用整体方法，缺乏对特定方面的关注，导致评估不够精细和准确。需要一种能够根据显式指定方面来评估文档相似性的框架。

Method: ASPECTSIM框架要求基于明确指定的方面来评估文档相似性。使用新构建的包含26K个方面-文档对的基准进行实验，采用GPT-4o直接提示实现。还对16个小型开源LLM和9个嵌入模型进行了大规模元评估。

Result: 使用GPT-4o实现的ASPECTSIM比没有显式方面的整体相似性方法的人类-机器一致性高出约80%。对于小型开源LLM，直接提示效果不佳（20-30%一致性），但通过两阶段精炼可将一致性提高约140%。不过这些模型的表现仍远低于GPT-4o。

Conclusion: 显式考虑方面对于文档相似性评估至关重要，需要修订标准实践。虽然精炼方法能改善小型开源LLM的表现，但它们与大型专有模型（如GPT-4o）在捕捉方面条件相似性方面仍有显著差距。

Abstract: We introduce ASPECTSIM, a simple and interpretable framework that requires conditioning document similarity on an explicitly specified aspect, which is different from the traditional holistic approach in measuring document similarity. Experimenting with a newly constructed benchmark of 26K aspect-document pairs, we found that ASPECTSIM, when implemented with direct GPT-4o prompting, achieves substantially higher human-machine agreement ($\approx$80% higher) than the same for holistic similarity without explicit aspects. These findings underscore the importance of explicitly accounting for aspects when measuring document similarity and highlight the need to revise standard practice. Next, we conducted a large-scale meta-evaluation using 16 smaller open-source LLMs and 9 embedding models with a focus on making ASPECTSIM accessible and reproducible. While directly prompting LLMs to produce ASPECTSIM scores turned out be ineffective (20-30% human-machine agreement), a simple two-stage refinement improved their agreement by $\approx$140%. Nevertheless, agreement remains well below that of GPT-4o-based models, indicating that smaller open-source LLMs still lag behind large proprietary models in capturing aspect-conditioned similarity.

</details>


### [24] [Grading Scale Impact on LLM-as-a-Judge: Human-LLM Alignment Is Highest on 0-5 Grading Scale](https://arxiv.org/abs/2601.03444)
*Weiyue Li,Minda Zhao,Weixuan Dong,Jiahui Cai,Yuze Wei,Michael Pocress,Yi Li,Wanyan Yuan,Xiaoyue Wang,Ruoyu Hou,Kaiyuan Lou,Wenqi Zeng,Yutong Yang,Yilun Du,Mengyu Wang*

Main category: cs.CL

TL;DR: 研究发现LLM评分受评分尺度影响，0-5分制能获得最佳人机一致性，且总体可靠性可能掩盖不同任务间的异质性


<details>
  <summary>Details</summary>
Motivation: 尽管LLM越来越多地被用作自动评估器，但先前研究表明LLM评分在提示改变时缺乏一致性，而评分尺度本身的影响尚未得到充分探索

Method: 比较人类和LLM评分者，收集两组在三种评分尺度上的评分数据，涵盖六个基准（包括客观、主观开放和混合任务），使用组内相关系数(ICC)测量绝对一致性

Result: LLM评分在主观基准上不同尺度间不一致；评分尺度选择显著影响人机一致性（即使组内可靠性高）；0-5分制在任务聚合上获得最强人机对齐；总体可靠性可能掩盖基准异质性；发现性别群体间存在系统性差异

Conclusion: 评分尺度设计和子级别诊断是LLM作为评分者协议的重要组成部分，需要更细致的评估方法

Abstract: Large language models (LLMs) are increasingly used as automated evaluators, yet prior works demonstrate that these LLM judges often lack consistency in scoring when the prompt is altered. However, the effect of the grading scale itself remains underexplored. We study the LLM-as-a-judge problem by comparing two kinds of raters: humans and LLMs. We collect ratings from both groups on three scales and across six benchmarks that include objective, open-ended subjective, and mixed tasks. Using intraclass correlation coefficients (ICC) to measure absolute agreement, we find that LLM judgments are not perfectly consistent across scales on subjective benchmarks, and that the choice of scale substantially shifts human-LLM agreement, even when within-group panel reliability is high. Aggregated over tasks, the grading scale of 0-5 yields the strongest human-LLM alignment. We further demonstrate that pooled reliability can mask benchmark heterogeneity and reveal systematic subgroup differences in alignment across gender groups, strengthening the importance of scale design and sub-level diagnostics as essential components of LLM-as-a-judge protocols.

</details>


### [25] [Enhancing Linguistic Competence of Language Models through Pre-training with Language Learning Tasks](https://arxiv.org/abs/2601.03448)
*Atsuki Yamaguchi,Maggie Mi,Nikolaos Aletras*

Main category: cs.CL

TL;DR: L2T是一个新的预训练框架，在标准的下一个token预测基础上加入了语言学习任务，模仿人类语言习得过程，提高了语言模型的linguistic competence。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型预训练只关注下一个token预测，虽然能学习世界知识和推理能力，但没有明确优化linguistic competence（语言能力）。

Method: 提出L2T框架，将原始文本转化为结构化输入-输出对，提供明确的语言刺激。在预训练中混合使用原始文本和L2T数据。

Result: 在linguistic competence基准测试上表现更好，加速了语言能力的获取，同时在一般推理任务上保持竞争力。

Conclusion: 将语言学习任务整合到预训练中能有效提升语言模型的linguistic competence，同时不影响其通用推理能力。

Abstract: Language models (LMs) are pre-trained on raw text datasets to generate text sequences token-by-token. While this approach facilitates the learning of world knowledge and reasoning, it does not explicitly optimize for linguistic competence. To bridge this gap, we propose L2T, a pre-training framework integrating Language Learning Tasks alongside standard next-token prediction. Inspired by human language acquisition, L2T transforms raw text into structured input-output pairs to provide explicit linguistic stimulation. Pre-training LMs on a mixture of raw text and L2T data not only improves overall performance on linguistic competence benchmarks but accelerates its acquisition, while maintaining competitive performance on general reasoning tasks.

</details>


### [26] [Prompting Underestimates LLM Capability for Time Series Classification](https://arxiv.org/abs/2601.03464)
*Dan Schumacher,Erfan Nourbakhsh,Rocky Slavin,Anthony Rios*

Main category: cs.CL

TL;DR: LLMs在时间序列分类中表现不佳主要是由于基于提示的评估方法的局限性，而非模型缺乏时间结构表征能力。通过线性探针直接分析内部表征，发现LLMs实际具备很强的时间序列理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于提示的评估显示LLMs在时间序列分类上表现很差，这引发了对其是否真正理解时间序列结构的质疑。作者希望探究这种差表现是由于模型表征能力不足，还是评估方法的问题。

Method: 1. 直接比较基于提示的输出和线性探针对相同内部表征的分析
2. 使用零样本提示和线性探针两种方法评估LLMs的时间序列分类能力
3. 进行层次分析，研究时间序列信息在不同Transformer层中的表征模式
4. 分析视觉和多模态输入对时间序列理解的影响

Result: 1. 零样本提示表现接近随机水平（F1 0.15-0.26）
2. 线性探针显著提升性能（F1 0.61-0.67），达到或超过专门的时间序列模型
3. 类判别性时间序列信息在早期Transformer层中就已出现
4. 视觉和多模态输入能够增强时间序列信息的表征

Conclusion: LLMs内部实际编码了有意义的时间序列信息，但基于提示的评估方法存在系统性不匹配问题，导致严重低估了LLMs的时间序列理解能力。线性探针分析揭示了模型强大的表征能力，表明当前评估方法需要改进。

Abstract: Prompt-based evaluations suggest that large language models (LLMs) perform poorly on time series classification, raising doubts about whether they encode meaningful temporal structure. We show that this conclusion reflects limitations of prompt-based generation rather than the model's representational capacity by directly comparing prompt outputs with linear probes over the same internal representations. While zero-shot prompting performs near chance, linear probes improve average F1 from 0.15-0.26 to 0.61-0.67, often matching or exceeding specialized time series models. Layer-wise analyses further show that class-discriminative time series information emerges in early transformer layers and is amplified by visual and multimodal inputs. Together, these results demonstrate a systematic mismatch between what LLMs internally represent and what prompt-based evaluation reveals, leading current evaluations to underestimate their time series understanding.

</details>


### [27] [EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning](https://arxiv.org/abs/2601.03471)
*Mingyang Wei,Dehai Min,Zewen Liu,Yuzhang Xie,Guanchen Wu,Carl Yang,Max S. Y. Lau,Qi He,Lu Cheng,Wei Jin*

Main category: cs.CL

TL;DR: EpiQAL是首个用于流行病学问答的诊断基准，包含三个子集，评估文本事实回忆、多步推理和结论重构能力，实验显示当前LLMs在流行病学推理方面表现有限。


<details>
  <summary>Details</summary>
Motivation: 现有医学问答基准主要关注临床知识或患者层面推理，缺乏系统评估证据基础的流行病学推理能力，而可靠的流行病学推理需要综合研究证据来推断疾病负担、传播动态和干预效果。

Method: 构建EpiQAL基准，包含三个基于开放获取文献的子集：评估文本事实回忆、多步推理（链接文档证据与流行病学原理）、结论重构（隐藏讨论部分）。构建方法结合专家设计的分类指导、多模型验证和基于检索的难度控制。

Result: 在十个开放模型上的实验显示：当前LLMs在流行病学推理方面表现有限，多步推理最具挑战性；模型排名在不同子集间变化；规模大小不能单独预测成功；思维链提示对多步推理有益但在其他地方效果不一。

Conclusion: EpiQAL为证据基础、推理能力和结论重构提供了细粒度的诊断信号，有助于评估和改进模型在流行病学推理方面的能力。

Abstract: Reliable epidemiological reasoning requires synthesizing study evidence to infer disease burden, transmission dynamics, and intervention effects at the population level. Existing medical question answering benchmarks primarily emphasize clinical knowledge or patient-level reasoning, yet few systematically evaluate evidence-grounded epidemiological inference. We present EpiQAL, the first diagnostic benchmark for epidemiological question answering across diverse diseases, comprising three subsets built from open-access literature. The subsets respectively evaluate text-grounded factual recall, multi-step inference linking document evidence with epidemiological principles, and conclusion reconstruction with the Discussion section withheld. Construction combines expert-designed taxonomy guidance, multi-model verification, and retrieval-based difficulty control. Experiments on ten open models reveal that current LLMs show limited performance on epidemiological reasoning, with multi-step inference posing the greatest challenge. Model rankings shift across subsets, and scale alone does not predict success. Chain-of-Thought prompting benefits multi-step inference but yields mixed results elsewhere. EpiQAL provides fine-grained diagnostic signals for evidence grounding, inferential reasoning, and conclusion reconstruction.

</details>


### [28] [Self-Explaining Hate Speech Detection with Moral Rationales](https://arxiv.org/abs/2601.03481)
*Francielle Vargas,Jackson Trager,Diego Alves,Surendrabikram Thapa,Matteo Guida,Berk Atil,Daryna Dementieva,Andrew Smart,Ameeta Agrawal*

Main category: cs.CL

TL;DR: SMRA是首个整合道德理论基础作为注意力对齐监督的自解释仇恨言论检测框架，通过HateBRMoralXplain数据集验证，在性能和解释忠实性上均有提升


<details>
  <summary>Details</summary>
Motivation: 现有仇恨言论检测模型过度依赖表层词汇特征，容易产生虚假相关性，且缺乏鲁棒性、文化情境化和可解释性

Method: 提出监督道德理论基础注意力（SMRA）框架，基于道德基础理论，将token级注意力与专家标注的道德理论基础对齐，引导模型关注道德显著片段而非虚假词汇模式

Result: 在二元仇恨言论检测和多标签道德情感分类任务中，SMRA一致提升性能（分别提升0.9和1.5 F1），同时显著增强解释忠实性（IoU F1提升7.4%，Token F1提升5.0%），解释更简洁但充分性提升2.3%，公平性保持稳定

Conclusion: SMRA通过直接整合道德理论基础监督，实现了更忠实、可解释且无需在性能或偏差上妥协的仇恨言论检测框架，并发布了巴西葡萄牙语基准数据集HateBRMoralXplain

Abstract: Hate speech detection models rely on surface-level lexical features, increasing vulnerability to spurious correlations and limiting robustness, cultural contextualization, and interpretability. We propose Supervised Moral Rationale Attention (SMRA), the first self-explaining hate speech detection framework to incorporate moral rationales as direct supervision for attention alignment. Based on Moral Foundations Theory, SMRA aligns token-level attention with expert-annotated moral rationales, guiding models to attend to morally salient spans rather than spurious lexical patterns. Unlike prior rationale-supervised or post-hoc approaches, SMRA integrates moral rationale supervision directly into the training objective, producing inherently interpretable and contextualized explanations. To support our framework, we also introduce HateBRMoralXplain, a Brazilian Portuguese benchmark dataset annotated with hate labels, moral categories, token-level moral rationales, and socio-political metadata. Across binary hate speech detection and multi-label moral sentiment classification, SMRA consistently improves performance (e.g., +0.9 and +1.5 F1, respectively) while substantially enhancing explanation faithfulness, increasing IoU F1 (+7.4 pp) and Token F1 (+5.0 pp). Although explanations become more concise, sufficiency improves (+2.3 pp) and fairness remains stable, indicating more faithful rationales without performance or bias trade-offs

</details>


### [29] [CALM: Culturally Self-Aware Language Models](https://arxiv.org/abs/2601.03483)
*Lingzhi Shen,Xiaohao Cai,Yunfei Long,Imran Razzak,Guanming Chen,Shoaib Jameel*

Main category: cs.CL

TL;DR: CALM框架通过解耦任务语义与文化概念，利用对比学习构建文化聚类，并通过混合专家机制和自提示反思学习，赋予语言模型文化自我意识，在跨文化基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型处理文化的方式过于静态，将文化视为背景知识而非动态演变的概念，这限制了它们在需要真正文化敏感性的下游任务中的可靠性。

Method: CALM框架解耦任务语义与显性文化概念及潜在文化信号，通过对比学习形成结构化文化聚类，利用交叉注意力对齐建立细粒度文化特征交互，采用混合专家机制沿文化特定维度自适应整合，构建文化基础内部身份状态，并通过自提示反思学习实现持续适应和自我修正。

Result: 在多个跨文化基准数据集上的广泛实验表明，CALM框架持续优于最先进的方法。

Conclusion: CALM框架成功赋予语言模型文化自我意识，能够理解和适应动态演变的文化语境，在需要文化敏感性的任务中展现出卓越性能。

Abstract: Cultural awareness in language models is the capacity to understand and adapt to diverse cultural contexts. However, most existing approaches treat culture as static background knowledge, overlooking its dynamic and evolving nature. This limitation reduces their reliability in downstream tasks that demand genuine cultural sensitivity. In this work, we introduce CALM, a novel framework designed to endow language models with cultural self-awareness. CALM disentangles task semantics from explicit cultural concepts and latent cultural signals, shaping them into structured cultural clusters through contrastive learning. These clusters are then aligned via cross-attention to establish fine-grained interactions among related cultural features and are adaptively integrated through a Mixture-of-Experts mechanism along culture-specific dimensions. The resulting unified representation is fused with the model's original knowledge to construct a culturally grounded internal identity state, which is further enhanced through self-prompted reflective learning, enabling continual adaptation and self-correction. Extensive experiments conducted on multiple cross-cultural benchmark datasets demonstrate that CALM consistently outperforms state-of-the-art methods.

</details>


### [30] [Submodular Evaluation Subset Selection in Automatic Prompt Optimization](https://arxiv.org/abs/2601.03493)
*Jinming Nian,Zhiyuan Peng,Hongwei Shang,Dae Hoon Park,Yi Fang*

Main category: cs.CL

TL;DR: 提出SESS方法，使用次模评估子集选择优化自动提示优化过程，相比随机或启发式选择能获得更好的优化提示。


<details>
  <summary>Details</summary>
Motivation: 自动提示优化依赖小规模评估子集的性能反馈，但如何选择这个评估子集通常被视为实现细节。论文研究从原则性角度出发，探索评估子集选择对提示优化的影响。

Method: 提出SESS方法，将评估子集选择形式化为最大化目标集函数，证明在温和条件下该函数具有单调性和次模性，从而可以使用贪心算法选择并具有理论保证。

Result: 在GSM8K、MATH和GPQA-Diamond数据集上的实验表明，次模选择的评估子集相比随机或启发式基线方法，能够产生更好的优化提示。

Conclusion: 评估子集选择对自动提示优化至关重要，次模选择方法能够提供更有效的反馈信号，从而提升优化效果。

Abstract: Automatic prompt optimization reduces manual prompt engineering, but relies on task performance measured on a small, often randomly sampled evaluation subset as its main source of feedback signal. Despite this, how to select that evaluation subset is usually treated as an implementation detail. We study evaluation subset selection for prompt optimization from a principled perspective and propose SESS, a submodular evaluation subset selection method. We frame selection as maximizing an objective set function and show that, under mild conditions, it is monotone and submodular, enabling greedy selection with theoretical guarantees. Across GSM8K, MATH, and GPQA-Diamond, submodularly selected evaluation subsets can yield better optimized prompts than random or heuristic baselines.

</details>


### [31] [Beyond Perplexity: A Lightweight Benchmark for Knowledge Retention in Supervised Fine-Tuning](https://arxiv.org/abs/2601.03505)
*Soheil Zibakhsh Shabgahi,Pedram Aghazadeh,Farinaz Koushanfar*

Main category: cs.CL

TL;DR: 提出KR-Test评估框架，用于区分SFT中事实学习与语言风格模仿，通过对比示例测量正确与错误续写的似然偏好。


<details>
  <summary>Details</summary>
Motivation: 传统SFT依赖验证困惑度监控训练不足，因为它混淆了风格模仿与事实内化，需要能区分两者的评估方法。

Method: 引入KR-Test框架，使用自动生成的对比示例，测量正确与错误续写的似然偏好，无需指令调优或生成解码。

Result: 验证了框架完整性（通过"盲测vs先知"基线分析），并展示了KR-Test在分析LoRA训练动态中的诊断能力。

Conclusion: KR-Test通过揭示语言收敛与知识保留之间的细粒度分离，增强了微调动态的可解释性。

Abstract: Supervised Fine-Tuning (SFT) is a standard approach for injecting domain knowledge into Large Language Models (LLMs). However, relying on validation perplexity to monitor training is often insufficient, as it confounds stylistic mimicry with genuine factual internalization. To address this, we introduce the Knowledge Retention (KR) Test , a lightweight, corpus-grounded evaluation framework designed to distinguish factual learning from linguistics. KR-Test utilizes automatically generated contrastive examples to measure likelihood preferences for correct versus incorrect continuations, requiring no instruction tuning or generative decoding. We validate the framework's integrity through a "blind vs. oracle" baseline analysis. Furthermore, we demonstrate the diagnostic capabilities of KR-Test by analyzing the training dynamics of Low-Rank Adaptation (LoRA). By exposing the fine-grained dissociation between linguistic convergence and knowledge retention, KR-Test enhances the interpretability of fine-tuning dynamics.

</details>


### [32] [Reasoning Pattern Alignment Merging for Adaptive Reasoning](https://arxiv.org/abs/2601.03506)
*Zhaofeng Zhong,Wei Yuan,Tong Chen,Xiangyu Zhao,Quoc Viet Hung Nguyen,Hongzhi Yin*

Main category: cs.CL

TL;DR: RPAM通过模型合并实现自适应推理，无需重新训练或大量数据，在保持性能的同时显著降低推理成本


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型生成冗长推理路径导致计算开销大、延迟高，而现有加速方法要么需要重新训练成本高昂，要么对输入和提示词设计敏感

Method: 提出推理模式对齐合并(RPAM)：通过合并长思维链模型和短思维链模型，构建小规模模式标注校准集为查询分配推理模式，基于特征对齐优化分层合并系数，同时使用对比目标将中间表示推离非选定模型

Result: 在七个广泛使用的推理基准测试中，RPAM显著降低了推理成本同时保持了强大的性能

Conclusion: RPAM提供了一种轻量级且有效的模型合并方法，实现了查询自适应推理，为高效推理提供了实用解决方案

Abstract: Recent large reasoning models (LRMs) have made substantial progress in complex reasoning tasks, yet they often generate lengthy reasoning paths for every query, incurring unnecessary computation and latency. Existing speed-up approaches typically rely on retraining the model or designing sophisticated prompting, which are either prohibitively expensive or highly sensitive to the input and prompt formulation. In this work, we study model merging as a lightweight alternative for efficient reasoning: by combining a long chain-of-thought (Long-CoT) reasoning model with a Short-CoT instruction model, we obtain an adaptive reasoner without training from scratch or requiring large-scale additional data. Building on this idea, we propose Reasoning Pattern Alignment Merging (RPAM), a layer-wise model merging framework based on feature alignment to facilitate query-adaptive reasoning. RPAM first constructs a small pattern-labeled calibration set that assigns each query an appropriate reasoning pattern. It then optimizes layer-wise merging coefficients by aligning the merged model's intermediate representations with those of the selected model, while a contrastive objective explicitly pushes them away from the non-selected model. Experiments on seven widely used reasoning benchmarks show that RPAM substantially reduces inference cost while maintaining strong performance. Upon article acceptance, we will provide open-source code to reproduce experiments for RPAM.

</details>


### [33] [IntroLM: Introspective Language Models via Prefilling-Time Self-Evaluation](https://arxiv.org/abs/2601.03511)
*Hossein Hosseini Kasnavieh,Gholamreza Haffari,Chris Leckie,Adel N. Toosi*

Main category: cs.CL

TL;DR: IntroLM让大语言模型在预填充阶段通过内省标记预测自身输出质量，无需外部评估器，提高路由效率


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖外部分类器（如BERT模型）来预测LLM输出质量，但这些方法存在上下文窗口有限、表示能力受限和额外计算开销的问题。需要一种更高效的方法让LLM能够自我评估输出质量。

Method: 提出IntroLM方法，通过引入内省标记让因果语言模型在预填充阶段预测自身输出质量。使用标记条件LoRA技术，仅在内省标记激活时学习预测输出质量，保持原始骨干模型行为不变，避免使用外部评估器。

Result: 在问答基准测试中，应用于Qwen3 8B的IntroLM在成功预测方面达到90%的ROC AUC，比DeBERTa分类器提高14%。集成到多模型路由系统后，在保持相同可靠性的情况下，延迟降低33%，大模型使用量减少50%。

Conclusion: IntroLM是一种有效的方法，使语言模型能够自我评估输出质量，无需外部分类器，显著提高了多模型路由系统的效率和成本效益。

Abstract: A major challenge for the operation of large language models (LLMs) is how to predict whether a specific LLM will produce sufficiently high-quality output for a given query. Existing approaches rely on external classifiers, most commonly BERT based models, which suffer from limited context windows, constrained representational capacity, and additional computational overhead. We propose IntroLM, a method that enables causal language models to predict their own output quality during the prefilling phase without affecting generation using introspective tokens. By introducing token conditional LoRA that activates only for the introspective token, the model learns to predict the output quality for a given query while preserving the original backbone behavior and avoiding external evaluators. On question answering benchmarks, IntroLM applied to Qwen3 8B achieves a ROC AUC of 90 precent for success prediction, outperforming a DeBERTa classifier by 14 precent. When integrated into multi model routing systems, IntroLM achieves superior cost performance tradeoffs, reducing latency by up to 33 precent and large model usage by up to 50 precent at matched reliability.

</details>


### [34] [Mem-Gallery: Benchmarking Multimodal Long-Term Conversational Memory for MLLM Agents](https://arxiv.org/abs/2601.03515)
*Yuanchen Bei,Tianxin Wei,Xuying Ning,Yanjun Zhao,Zhining Liu,Xiao Lin,Yada Zhu,Hendrik Hamann,Jingrui He,Hanghang Tong*

Main category: cs.CL

TL;DR: Mem-Gallery是一个评估多模态大语言模型在长期对话中记忆能力的新基准，包含高质量多会话对话，并提出了系统性的三维评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有基准要么评估纯文本对话中的多会话记忆，要么评估局部上下文中的多模态理解，无法评估多模态记忆如何在长期对话轨迹中被保留、组织和演化。

Method: 引入Mem-Gallery基准，包含基于视觉和文本信息的高质量多会话对话，具有长交互视野和丰富的多模态依赖。基于此数据集提出了系统性的三维评估框架：记忆提取与测试时适应、记忆推理、记忆知识管理。

Result: 对13个记忆系统进行广泛基准测试，揭示了几个关键发现：需要显式的多模态信息保留和记忆组织、记忆推理和知识管理方面存在持续限制、当前模型存在效率瓶颈。

Conclusion: Mem-Gallery为评估多模态长期对话记忆提供了新基准，揭示了当前MLLM代理在记忆能力方面的关键局限，强调了显式多模态记忆保留和组织的重要性。

Abstract: Long-term memory is a critical capability for multimodal large language model (MLLM) agents, particularly in conversational settings where information accumulates and evolves over time. However, existing benchmarks either evaluate multi-session memory in text-only conversations or assess multimodal understanding within localized contexts, failing to evaluate how multimodal memory is preserved, organized, and evolved across long-term conversational trajectories. Thus, we introduce Mem-Gallery, a new benchmark for evaluating multimodal long-term conversational memory in MLLM agents. Mem-Gallery features high-quality multi-session conversations grounded in both visual and textual information, with long interaction horizons and rich multimodal dependencies. Building on this dataset, we propose a systematic evaluation framework that assesses key memory capabilities along three functional dimensions: memory extraction and test-time adaptation, memory reasoning, and memory knowledge management. Extensive benchmarking across thirteen memory systems reveals several key findings, highlighting the necessity of explicit multimodal information retention and memory organization, the persistent limitations in memory reasoning and knowledge management, as well as the efficiency bottleneck of current models.

</details>


### [35] [PALM-Bench: A Comprehensive Benchmark for Personalized Audio-Language Models](https://arxiv.org/abs/2601.03531)
*Yuwen Wang,Xinyuan Qian,Tian-Hao Zhang,Jiaran Gao,Yuchen Pan,Xin Wang,Zhou Pan,Chen Wei,Yiming Wang*

Main category: cs.CL

TL;DR: 本文提出了个性化音频语言模型（PALM）的任务，创建了首个基准测试PALM-Bench，并发现现有方法在建模个性化知识和跨任务迁移方面存在局限。


<details>
  <summary>Details</summary>
Motivation: 现有大型音频语言模型虽然在音频理解和生成方面表现出色，但主要处理通用任务（如内容总结），无法充分支持个性化问答（如总结特定朋友的话语）。人类决策依赖个人背景，而现有模型缺乏这种个性化能力。

Method: 1. 形式化定义个性化音频语言模型（PALM）任务；2. 创建首个基准测试PALM-Bench，包含多说话者场景下的多个任务；3. 对代表性开源LALM进行广泛实验，评估现有训练自由提示和监督微调策略。

Result: 实验表明，现有方法（训练自由提示和监督微调）虽然有所改进，但在建模个性化知识和跨任务鲁棒迁移方面仍存在明显局限。

Conclusion: 个性化音频语言模型是一个重要但尚未充分探索的研究方向，需要新的方法来有效建模个性化知识并在任务间鲁棒迁移。发布的基准测试将促进该领域的方法发展。

Abstract: Large Audio-Language Models (LALMs) have demonstrated strong performance in audio understanding and generation. Yet, our extensive benchmarking reveals that their behavior is largely generic (e.g., summarizing spoken content) and fails to adequately support personalized question answering (e.g., summarizing what my best friend says). In contrast, human conditions their interpretation and decision-making on each individual's personal context. To bridge this gap, we formalize the task of Personalized LALMs (PALM) for recognizing personal concepts and reasoning within personal context. Moreover, we create the first benchmark (PALM-Bench) to foster the methodological advances in PALM and enable structured evaluation on several tasks across multi-speaker scenarios. Our extensive experiments on representative open-source LALMs, show that existing training-free prompting and supervised fine-tuning strategies, while yield improvements, remains limited in modeling personalized knowledge and transferring them across tasks robustly. Data and code will be released.

</details>


### [36] [Persona-aware and Explainable Bikeability Assessment: A Vision-Language Model Approach](https://arxiv.org/abs/2601.03534)
*Yilong Dai,Ziyi Wang,Chenguang Wang,Kexin Zhou,Yiheng Qian,Susu Xu,Xiang Yan*

Main category: cs.CL

TL;DR: 本文提出了一种基于人物感知的视觉语言模型框架，用于自行车友好性评估，通过理论驱动的人物设定、多粒度监督微调和AI数据增强，实现了可解释的评估和评分预测。


<details>
  <summary>Details</summary>
Motivation: 现有的基于感知的自行车友好性评估方法存在两个主要局限：一是难以捕捉道路环境的复杂性，二是未能充分考虑用户主观感知的异质性。需要一种能够更好地整合用户安全感和舒适度感知的评估方法。

Method: 提出了一个包含三个创新贡献的人物感知视觉语言模型框架：1)基于成熟骑车者类型学的理论驱动人物设定，通过思维链推理生成特定人物的解释；2)多粒度监督微调，结合稀缺的专家标注推理和丰富的用户评分进行联合预测和可解释评估；3)AI驱动的数据增强，创建受控配对数据以分离基础设施变量的影响。

Result: 开发了基于全景图像的众包系统，收集了427名骑车者的12,400条人物设定评估。实验结果表明，该框架在自行车友好性评分预测方面具有竞争力，同时能够实现可解释的因素归因。

Conclusion: 该人物感知视觉语言模型框架能够有效解决现有自行车友好性评估方法的局限性，不仅提高了评分预测的准确性，还提供了可解释的评估结果，有助于创建更友好的自行车城市环境。

Abstract: Bikeability assessment is essential for advancing sustainable urban transportation and creating cyclist-friendly cities, and it requires incorporating users' perceptions of safety and comfort. Yet existing perception-based bikeability assessment approaches face key limitations in capturing the complexity of road environments and adequately accounting for heterogeneity in subjective user perceptions. This paper proposes a persona-aware Vision-Language Model framework for bikeability assessment with three novel contributions: (i) theory-grounded persona conditioning based on established cyclist typology that generates persona-specific explanations via chain-of-thought reasoning; (ii) multi-granularity supervised fine-tuning that combines scarce expert-annotated reasoning with abundant user ratings for joint prediction and explainable assessment; and (iii) AI-enabled data augmentation that creates controlled paired data to isolate infrastructure variable impacts. To test and validate this framework, we developed a panoramic image-based crowdsourcing system and collected 12,400 persona-conditioned assessments from 427 cyclists. Experiment results show that the proposed framework offers competitive bikeability rating prediction while uniquely enabling explainable factor attribution.

</details>


### [37] [DeepSynth-Eval: Objectively Evaluating Information Consolidation in Deep Survey Writing](https://arxiv.org/abs/2601.03540)
*Hongzhi Zhang,Yuanze Hu,Tinghai Zhang,Jia Fu,Tao Wang,Junwei Jing,Zhaoxin Fan,Qi Wang,Ruiming Tang,Han Li,Guorui Zhou,Kun Gai*

Main category: cs.CL

TL;DR: DeepSynth-Eval：评估LLM自主代理在信息整合能力的新基准，通过逆向工程高质量综述论文构建黄金标准，发现多步规划写作流程显著优于单轮生成


<details>
  <summary>Details</summary>
Motivation: LLM向自主代理发展推动了深度研究的进展，但检索后的信息整合阶段（需要消化大量上下文并将碎片化证据整合为连贯的长篇报告）因开放式写作的主观性而缺乏客观评估方法

Method: 1) 利用高质量综述论文作为黄金标准，逆向工程研究请求并基于其参考文献构建"Oracle Contexts"以隔离合成与检索噪声；2) 提出细粒度评估协议，使用通用检查清单（用于事实覆盖）和约束检查清单（用于结构组织），将主观判断转化为可验证指标

Result: 在96个任务上的实验表明，从数百篇参考文献中整合信息仍然是重大挑战。代理式的规划-写作工作流程显著优于单轮生成，有效减少幻觉并提高对复杂结构约束的遵守

Conclusion: DeepSynth-Eval填补了LLM自主代理信息整合能力评估的空白，证明了多步规划写作流程的重要性，为未来自主研究代理的发展提供了有价值的评估框架

Abstract: The evolution of Large Language Models (LLMs) towards autonomous agents has catalyzed progress in Deep Research. While retrieval capabilities are well-benchmarked, the post-retrieval synthesis stage--where agents must digest massive amounts of context and consolidate fragmented evidence into coherent, long-form reports--remains under-evaluated due to the subjectivity of open-ended writing. To bridge this gap, we introduce DeepSynth-Eval, a benchmark designed to objectively evaluate information consolidation capabilities. We leverage high-quality survey papers as gold standards, reverse-engineering research requests and constructing "Oracle Contexts" from their bibliographies to isolate synthesis from retrieval noise. We propose a fine-grained evaluation protocol using General Checklists (for factual coverage) and Constraint Checklists (for structural organization), transforming subjective judgment into verifiable metrics. Experiments across 96 tasks reveal that synthesizing information from hundreds of references remains a significant challenge. Our results demonstrate that agentic plan-and-write workflows significantly outperform single-turn generation, effectively reducing hallucinations and improving adherence to complex structural constraints.

</details>


### [38] [Layer-Order Inversion: Rethinking Latent Multi-Hop Reasoning in Large Language Models](https://arxiv.org/abs/2601.03542)
*Xukai Liu,Ye Liu,Jipeng Zhang,Yanghai Zhang,Kai Zhang,Qi Liu*

Main category: cs.CL

TL;DR: 该论文发现大语言模型在多跳推理中存在"层序反转"现象，即后跳答案实体可能比桥接实体更早可解码，这挑战了传统的跳对齐电路假设。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在多跳推理中表现良好，但其内部如何组合多个事实的机制仍不清楚。现有研究提出的"跳对齐电路假设"认为桥接实体在不同层中顺序计算，但这一假设是否具有普遍性需要验证。

Method: 通过系统分析真实世界的多跳查询，研究大语言模型在多跳推理中的内部机制。提出"概率性回忆-提取"框架，将多跳推理建模为浅层MLP层中的广泛概率性回忆和深层注意力层中的选择性提取。

Result: 发现"层序反转"现象：后跳答案实体比桥接实体更早可解码，且这种现象随着总跳数增加而增强。提出的"概率性回忆-提取"框架得到实证验证，能解释先前层解码证据、思维链收益，并诊断多跳失败原因。

Conclusion: 传统的跳对齐电路假设不能普遍适用，大语言模型的多跳推理机制更符合概率性回忆-提取框架。这一发现为理解大语言模型内部推理机制提供了新视角，有助于诊断和改善多跳推理能力。

Abstract: Large language models (LLMs) perform well on multi-hop reasoning, yet how they internally compose multiple facts remains unclear. Recent work proposes \emph{hop-aligned circuit hypothesis}, suggesting that bridge entities are computed sequentially across layers before later-hop answers. Through systematic analyses on real-world multi-hop queries, we show that this hop-aligned assumption does not generalize: later-hop answer entities can become decodable earlier than bridge entities, a phenomenon we call \emph{layer-order inversion}, which strengthens with total hops. To explain this behavior, we propose a \emph{probabilistic recall-and-extract} framework that models multi-hop reasoning as broad probabilistic recall in shallow MLP layers followed by selective extraction in deeper attention layers. This framework is empirically validated through systematic probing analyses, reinterpreting prior layer-wise decoding evidence, explaining chain-of-thought gains, and providing a mechanistic diagnosis of multi-hop failures despite correct single-hop knowledge. Code is available at https://github.com/laquabe/Layer-Order-Inversion.

</details>


### [39] [EvolMem: A Cognitive-Driven Benchmark for Multi-Session Dialogue Memory](https://arxiv.org/abs/2601.03543)
*Ye Shen,Dun Pei,Yiqiu Guo,Junying Wang,Yijin Guo,Zicheng Zhang,Qi Jia,Jun Zhou,Guangtao Zhai*

Main category: cs.CL

TL;DR: EvolMem是一个评估LLMs和智能体系统多会话记忆能力的新基准，基于认知心理学构建，包含陈述性和非陈述性记忆的细粒度评估维度。


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏对大型语言模型在多会话环境下不同记忆维度的系统评估，特别是在多会话设置中的记忆能力评估不足。

Method: 提出混合数据合成框架，包括话题启动生成和叙事启发转换，用于可扩展地生成具有可控复杂度的多会话对话，并提供样本特定的评估指南。

Result: 评估显示没有LLM在所有记忆维度上始终优于其他模型，智能体记忆机制不一定能增强LLMs能力且通常存在显著的效率限制。

Conclusion: EvolMem基准为评估LLMs和智能体系统的多会话记忆能力提供了系统框架，揭示了当前模型的局限性，并为未来改进指明了方向。

Abstract: Despite recent advances in understanding and leveraging long-range conversational memory, existing benchmarks still lack systematic evaluation of large language models(LLMs) across diverse memory dimensions, particularly in multi-session settings. In this work, we propose EvolMem, a new benchmark for assessing multi-session memory capabilities of LLMs and agent systems. EvolMem is grounded in cognitive psychology and encompasses both declarative and non-declarative memory, further decomposed into multiple fine-grained abilities. To construct the benchmark, we introduce a hybrid data synthesis framework that consists of topic-initiated generation and narrative-inspired transformations. This framework enables scalable generation of multi-session conversations with controllable complexity, accompanied by sample-specific evaluation guidelines. Extensive evaluation reveals that no LLM consistently outperforms others across all memory dimensions. Moreover, agent memory mechanisms do not necessarily enhance LLMs' capabilities and often exhibit notable efficiency limitations. Data and code will be released at https://github.com/shenye7436/EvolMem.

</details>


### [40] [Value-Action Alignment in Large Language Models under Privacy-Prosocial Conflict](https://arxiv.org/abs/2601.03546)
*Guanyu Chen,Chenxiao Yu,Xiyang Hu*

Main category: cs.CL

TL;DR: 论文提出了一种基于情境的评估协议，通过标准化问卷顺序测量隐私态度、亲社会性和数据共享接受度，并使用多组结构方程模型和价值-行动对齐率来评估大语言模型在数据共享决策中价值与行动的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法往往单独测量隐私相关态度或共享意图，难以确定模型表达的价值是否像真实人类行为一样共同预测其下游数据共享行动。需要一种方法来评估大语言模型在涉及隐私担忧和亲社会动机的数据共享决策任务中价值与行动的一致性。

Method: 引入基于情境的评估协议，在有限的历史保持会话中顺序管理隐私态度、亲社会性和数据共享接受度的标准化问卷。使用多组结构方程模型识别从隐私担忧和亲社会性到数据共享的关系。提出价值-行动对齐率作为人类参考的方向一致性指标，聚合路径层面的预期符号证据。

Result: 在多个大语言模型中观察到稳定但模型特定的隐私-PSA-数据共享接受度特征谱，以及价值-行动对齐方面的显著异质性。

Conclusion: 该方法能够有效评估大语言模型在数据共享决策任务中价值与行动的一致性，揭示了不同模型在隐私担忧和亲社会动机影响下的不同行为模式，为理解模型决策机制提供了新的评估框架。

Abstract: Large language models (LLMs) are increasingly used to simulate decision-making tasks involving personal data sharing, where privacy concerns and prosocial motivations can push choices in opposite directions. Existing evaluations often measure privacy-related attitudes or sharing intentions in isolation, which makes it difficult to determine whether a model's expressed values jointly predict its downstream data-sharing actions as in real human behaviors. We introduce a context-based assessment protocol that sequentially administers standardized questionnaires for privacy attitudes, prosocialness, and acceptance of data sharing within a bounded, history-carrying session. To evaluate value-action alignments under competing attitudes, we use multi-group structural equation modeling (MGSEM) to identify relations from privacy concerns and prosocialness to data sharing. We propose Value-Action Alignment Rate (VAAR), a human-referenced directional agreement metric that aggregates path-level evidence for expected signs. Across multiple LLMs, we observe stable but model-specific Privacy-PSA-AoDS profiles, and substantial heterogeneity in value-action alignment.

</details>


### [41] [Evaluating LLMs for Police Decision-Making: A Framework Based on Police Action Scenarios](https://arxiv.org/abs/2601.03553)
*Sangyub Lee,Heedou Kim,Hyeoncheol Kim*

Main category: cs.CL

TL;DR: 提出了PAS（警察行动场景）评估框架，用于评估LLM在警务操作中的表现，构建了包含8000多份官方文档的新QA数据集，实验显示商用LLM在警务相关任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: LLM在警务操作中的应用日益增长，但缺乏专门针对警务操作的评估框架。LLM的响应可能不总是法律上错误，但未经核实的使用仍可能导致非法逮捕和不当证据收集等严重问题。

Method: 提出PAS（Police Action Scenarios）系统评估框架，涵盖整个评估过程。从8000多份官方文档构建新颖的QA数据集，并通过统计分析建立关键指标，由警务专家验证。

Result: 实验结果显示商用LLM在新的警务相关任务上表现不佳，特别是在提供基于事实的建议方面存在困难。

Conclusion: 研究表明需要可扩展的评估框架来确保AI驱动的警务操作可靠性，并发布了数据和提示模板。

Abstract: The use of Large Language Models (LLMs) in police operations is growing, yet an evaluation framework tailored to police operations remains absent. While LLM's responses may not always be legally incorrect, their unverified use still can lead to severe issues such as unlawful arrests and improper evidence collection. To address this, we propose PAS (Police Action Scenarios), a systematic framework covering the entire evaluation process. Applying this framework, we constructed a novel QA dataset from over 8,000 official documents and established key metrics validated through statistical analysis with police expert judgements. Experimental results show that commercial LLMs struggle with our new police-related tasks, particularly in providing fact-based recommendations. This study highlights the necessity of an expandable evaluation framework to ensure reliable AI-driven police operations. We release our data and prompt template.

</details>


### [42] [DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs](https://arxiv.org/abs/2601.03559)
*Shidong Cao,Hongzhan Lin,Yuxuan Gu,Ziyang Luo,Jing Ma*

Main category: cs.CL

TL;DR: DiffCoT：一种将扩散模型原理应用于思维链推理的框架，通过迭代去噪过程生成和修正推理步骤，提高多步数学问题求解的鲁棒性和纠错能力。


<details>
  <summary>Details</summary>
Motivation: 传统的思维链推理方法存在暴露偏差和错误累积问题，早期的错误会在自回归解码过程中不可逆地传播，影响后续推理步骤的准确性。

Method: 提出DiffCoT框架，将思维链推理重构为迭代去噪过程。采用滑动窗口机制在推理步骤级别集成扩散原理，实现中间步骤的统一生成和回顾性修正，同时保持token级别的自回归。引入因果扩散噪声调度来尊重推理链的时间结构，保持因果一致性。

Result: 在三个多步思维链推理基准测试和多种模型主干上的实验表明，DiffCoT始终优于现有的思维链偏好优化方法，在思维链推理中展现出更好的鲁棒性和纠错能力。

Conclusion: DiffCoT通过将扩散风格推理应用于思维链，有效解决了传统自回归推理中的错误传播问题，为多步推理任务提供了一种更鲁棒和可修正的解决方案。

Abstract: Chain-of-Thought (CoT) reasoning improves multi-step mathematical problem solving in large language models but remains vulnerable to exposure bias and error accumulation, as early mistakes propagate irreversibly through autoregressive decoding. In this work, we propose DiffCoT, a diffusion-styled CoT framework that reformulates CoT reasoning as an iterative denoising process. DiffCoT integrates diffusion principles at the reasoning-step level via a sliding-window mechanism, enabling unified generation and retrospective correction of intermediate steps while preserving token-level autoregression. To maintain causal consistency, we further introduce a causal diffusion noise schedule that respects the temporal structure of reasoning chains. Extensive experiments on three multi-step CoT reasoning benchmarks across diverse model backbones demonstrate that DiffCoT consistently outperforms existing CoT preference optimization methods, yielding improved robustness and error-correction capability in CoT reasoning.

</details>


### [43] [How Do Large Language Models Learn Concepts During Continual Pre-Training?](https://arxiv.org/abs/2601.03570)
*Barry Menglong Yao,Sha Li,Yunzhi Yao,Minqian Liu,Zaishuo Xia,Qifan Wang,Lifu Huang*

Main category: cs.CL

TL;DR: 该论文研究了大语言模型在持续预训练中如何获取、保留和遗忘概念，通过概念电路分析揭示了概念学习与遗忘的动态模式及其相互作用。


<details>
  <summary>Details</summary>
Motivation: 人类主要通过概念理解世界，但大语言模型在持续预训练过程中如何获取、保留和遗忘概念仍不清楚。研究者希望从概念电路的视角理解这些动态过程。

Method: 通过分析LLMs内部的概念电路（与特定概念相关的计算子图），结合图指标来表征电路结构，研究概念获取、遗忘以及多个概念之间的干扰和协同作用。

Result: 发现：(1)概念电路能显著反映概念学习和遗忘；(2)概念电路在持续预训练中呈现先增后减再稳定的阶段性模式；(3)学习增益大的概念后续遗忘更明显；(4)语义相似概念干扰更强；(5)不同概念知识可迁移性存在差异。

Conclusion: 研究提供了概念学习动态的电路级视角，为设计更可解释和鲁棒的概念感知训练策略提供了依据。

Abstract: Human beings primarily understand the world through concepts (e.g., dog), abstract mental representations that structure perception, reasoning, and learning. However, how large language models (LLMs) acquire, retain, and forget such concepts during continual pretraining remains poorly understood. In this work, we study how individual concepts are acquired and forgotten, as well as how multiple concepts interact through interference and synergy. We link these behavioral dynamics to LLMs' internal Concept Circuits, computational subgraphs associated with specific concepts, and incorporate Graph Metrics to characterize circuit structure. Our analysis reveals: (1) LLMs concept circuits provide a non-trivial, statistically significant signal of concept learning and forgetting; (2) Concept circuits exhibit a stage-wise temporal pattern during continual pretraining, with an early increase followed by gradual decrease and stabilization; (3) concepts with larger learning gains tend to exhibit greater forgetting under subsequent training; (4) semantically similar concepts induce stronger interference than weakly related ones; (5) conceptual knowledge differs in their transferability, with some significantly facilitating the learning of others. Together, our findings offer a circuit-level view of concept learning dynamics and inform the design of more interpretable and robust concept-aware training strategies for LLMs.

</details>


### [44] [PsychEthicsBench: Evaluating Large Language Models Against Australian Mental Health Ethics](https://arxiv.org/abs/2601.03578)
*Yaling Shen,Stephanie Fong,Yiwen Jiang,Zimu Wang,Feilong Tang,Qingyang Xu,Xiangyu Zhao,Zhongxing Xu,Jiahe Liu,Jinpeng Hu,Dominic Dwyer,Zongyuan Ge*

Main category: cs.CL

TL;DR: PsychEthicsBench：首个基于澳大利亚心理学和精神病学指南的原则性基准，用于评估LLM在心理健康领域的伦理知识和行为响应，揭示拒绝率并非伦理行为的有效指标。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在心理健康应用中的评估主要依赖拒绝性安全信号，这种方法对临床实践所需的细微行为洞察有限。临床不恰当的拒绝可能被视为缺乏同理心并阻碍求助行为，因此需要超越拒绝中心的评估框架。

Method: 基于澳大利亚心理学和精神病学指南，开发了PsychEthicsBench基准，包含多项选择和开放式任务，并配有细粒度伦理注释。在14个模型上进行实证评估。

Result: 拒绝率是伦理行为的差指标，安全触发与临床适当性存在显著分歧。领域特定的微调可能降低伦理鲁棒性，多个专业模型在伦理对齐上表现不如基础模型。

Conclusion: PsychEthicsBench为心理健康领域LLM的系统性、司法管辖区感知评估奠定了基础，促进了该领域更负责任的发展。

Abstract: The increasing integration of large language models (LLMs) into mental health applications necessitates robust frameworks for evaluating professional safety alignment. Current evaluative approaches primarily rely on refusal-based safety signals, which offer limited insight into the nuanced behaviors required in clinical practice. In mental health, clinically inadequate refusals can be perceived as unempathetic and discourage help-seeking. To address this gap, we move beyond refusal-centric metrics and introduce \texttt{PsychEthicsBench}, the first principle-grounded benchmark based on Australian psychology and psychiatry guidelines, designed to evaluate LLMs' ethical knowledge and behavioral responses through multiple-choice and open-ended tasks with fine-grained ethicality annotations. Empirical results across 14 models reveal that refusal rates are poor indicators of ethical behavior, revealing a significant divergence between safety triggers and clinical appropriateness. Notably, we find that domain-specific fine-tuning can degrade ethical robustness, as several specialized models underperform their base backbones in ethical alignment. PsychEthicsBench provides a foundation for systematic, jurisdiction-aware evaluation of LLMs in mental health, encouraging more responsible development in this domain.

</details>


### [45] [OLA: Output Language Alignment in Code-Switched LLM Interactions](https://arxiv.org/abs/2601.03589)
*Juhyun Oh,Haneul Yoo,Faiz Ghifari Haznitrama,Alice Oh*

Main category: cs.CL

TL;DR: 当前LLM在代码切换交互中输出语言对齐能力不足，存在明显的非英语响应偏好，通过少量数据微调可显著改善。


<details>
  <summary>Details</summary>
Motivation: 代码切换是多语言用户的自然行为，但LLM难以根据上下文推断用户期望的输出语言，经常返回错误语言的响应，这影响了多语言交互的实际效果。

Method: 提出了OLA基准来评估LLM在韩英代码切换中的输出语言对齐能力，涵盖从简单句子内混合到指令内容不匹配的各种场景。使用前沿模型进行测试，并扩展到中英和印尼语对。还尝试了思维链提示和代码切换感知DPO微调。

Result: 前沿模型经常误解隐含的语言期望，表现出明显的非英语响应偏好（韩英、中英、印尼语对均如此）。模型还表现出不稳定性，如中途切换语言和语言侵入。思维链提示无法解决这些错误，但仅用约1K示例的代码切换感知DPO微调就能显著减少对齐错误。

Conclusion: LLM在代码切换交互中的输出语言对齐失败源于对齐不足而非根本限制，少量针对性微调即可改善。这凸显了将多语言LLM与用户真实世界代码切换交互中的隐含期望对齐的重要性。

Abstract: Code-switching, alternating between languages within a conversation, is natural for multilingual users, yet poses fundamental challenges for large language models (LLMs). When a user code-switches in their prompt to an LLM, they typically do not specify the expected language of the LLM response, and thus LLMs must infer the output language from contextual and pragmatic cues. We find that current LLMs systematically fail to align with this expectation, responding in undesired languages even when cues are clear to humans. We introduce OLA, a benchmark to evaluate LLMs' Output Language Alignment in code-switched interactions. OLA focuses on Korean--English code-switching and spans simple intra-sentential mixing to instruction-content mismatches. Even frontier models frequently misinterpret implicit language expectation, exhibiting a bias toward non-English responses. We further show this bias generalizes beyond Korean to Chinese and Indonesian pairs. Models also show instability through mid-response switching and language intrusions. Chain-of-Thought prompting fails to resolve these errors, indicating weak pragmatic reasoning about output language. However, Code-Switching Aware DPO with minimal data (about 1K examples) substantially reduces misalignment, suggesting these failures stem from insufficient alignment rather than fundamental limitations. Our results highlight the need to align multilingual LLMs with users' implicit expectations in real-world code-switched interactions.

</details>


### [46] [From Chains to Graphs: Self-Structured Reasoning for General-Domain LLMs](https://arxiv.org/abs/2601.03597)
*Yingjian Chen,Haoran Liu,Yinhong Liu,Sherry T. Tong,Aosong Feng,Jinghui Lu,Juntao Zhang,Yusuke Iwasawa,Yutaka Matsuo,Irene Li*

Main category: cs.CL

TL;DR: 论文提出了一种名为SGR的图结构推理框架，使大语言模型能够在回答开放域问题时构建和使用自己的图结构推理过程，显著提升了推理一致性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的推理过程通常是线性的且逻辑不一致，而真实世界的推理需要整合多个前提并并行解决子问题。现有方法如思维链虽然以线性文本形式表达推理，但经常导致不一致的结论。现有方法依赖外部提供的图结构，未探索大语言模型如何构建和使用自己的图结构推理。

Method: 提出自我图推理（SGR）框架，使大语言模型能够在生成最终答案之前，将推理过程明确表示为结构化图。构建了图结构推理数据集，将多个候选推理图合并为精炼的图结构用于模型训练。

Result: 在五个通用和专门领域的QA基准测试中，SGR持续提升了推理一致性，相比基础模型获得了17.74%的性能提升。使用SGR微调的LLaMA-3.3-70B模型表现与GPT-4o相当，并超越了Claude-3.5-Haiku。

Conclusion: 图结构推理能够显著提升大语言模型的推理一致性和性能，SGR框架为开放域问答中的结构化推理提供了有效解决方案，使模型能够构建和使用自己的图结构推理过程。

Abstract: Large Language Models (LLMs) show strong reasoning ability in open-domain question answering, yet their reasoning processes are typically linear and often logically inconsistent. In contrast, real-world reasoning requires integrating multiple premises and solving subproblems in parallel. Existing methods, such as Chain-of-Thought (CoT), express reasoning in a linear textual form, which may appear coherent but frequently leads to inconsistent conclusions. Recent approaches rely on externally provided graphs and do not explore how LLMs can construct and use their own graph-structured reasoning, particularly in open-domain QA. To fill this gap, we novelly explore graph-structured reasoning of LLMs in general-domain question answering. We propose Self-Graph Reasoning (SGR), a framework that enables LLMs to explicitly represent their reasoning process as a structured graph before producing the final answer. We further construct a graph-structured reasoning dataset that merges multiple candidate reasoning graphs into refined graph structures for model training. Experiments on five QA benchmarks across both general and specialized domains show that SGR consistently improves reasoning consistency and yields a 17.74% gain over the base model. The LLaMA-3.3-70B model fine-tuned with SGR performs comparably to GPT-4o and surpasses Claude-3.5-Haiku, demonstrating the effectiveness of graph-structured reasoning.

</details>


### [47] [DiVA: Fine-grained Factuality Verification with Agentic-Discriminative Verifier](https://arxiv.org/abs/2601.03605)
*Hui Huang,Muyun Yang,Yuki Arase*

Main category: cs.CL

TL;DR: 提出DiVA框架，结合生成模型的搜索能力和判别模型的精确评分，用于细粒度事实性验证，并构建FGVeriBench基准。


<details>
  <summary>Details</summary>
Motivation: 现有事实性验证研究主要进行二元判断（正确/错误），无法区分不同程度的错误严重性，这限制了其在细粒度评估和偏好优化等应用中的实用性。

Method: 提出Agentic Discriminative Verifier (DiVA)混合框架，结合生成模型的代理搜索能力和判别模型的精确评分能力。同时构建了新的基准FGVeriBench作为细粒度事实性验证的测试平台。

Result: 在FGVeriBench上的实验结果表明，DiVA在通用问题和多跳问题的事实性验证上都显著优于现有方法。

Conclusion: DiVA框架通过结合生成和判别模型的优势，有效解决了细粒度事实性验证问题，为LLM事实性评估提供了更精细的工具。

Abstract: Despite the significant advancements of Large Language Models (LLMs), their factuality remains a critical challenge, fueling growing interest in factuality verification. Existing research on factuality verification primarily conducts binary judgments (e.g., correct or incorrect), which fails to distinguish varying degrees of error severity. This limits its utility for applications such as fine-grained evaluation and preference optimization. To bridge this gap, we propose the Agentic Discriminative Verifier (DiVA), a hybrid framework that synergizes the agentic search capabilities of generative models with the precise scoring aptitude of discriminative models. We also construct a new benchmark, FGVeriBench, as a robust testbed for fine-grained factuality verification. Experimental results on FGVeriBench demonstrate that our DiVA significantly outperforms existing methods on factuality verification for both general and multi-hop questions.

</details>


### [48] [Analyzing Reasoning Shifts in Audio Deepfake Detection under Adversarial Attacks: The Reasoning Tax versus Shield Bifurcation](https://arxiv.org/abs/2601.03615)
*Binh Nguyen,Thai Le*

Main category: cs.CL

TL;DR: 本研究评估了音频语言模型在对抗攻击下的推理鲁棒性，发现显式推理并非总能增强鲁棒性，而是呈现分化：对于具有稳健声学感知的模型，推理起到防御作用；对于其他模型，推理反而会降低性能，特别是在语言攻击下。


<details>
  <summary>Details</summary>
Motivation: 音频语言模型为可解释的音频深度伪造检测提供了新方向，但现有研究主要关注最终预测的变化，缺乏对模型推理过程在对抗攻击下鲁棒性的系统分析。

Method: 提出了一个法证审计框架，从三个相互关联的维度评估ALMs在对抗攻击下的推理鲁棒性：声学感知、认知一致性和认知失调。

Result: 研究揭示了显式推理并不普遍增强鲁棒性。对于具有稳健声学感知的模型，推理起到防御"盾牌"作用；对于其他模型，推理反而会带来性能"税"，特别是在语言攻击下会降低认知一致性并提高攻击成功率。即使在分类失败时，高认知失调也能作为"无声警报"指示潜在操纵。

Conclusion: 本研究对推理在法证音频深度伪造分析中的作用及其脆弱性进行了关键评估，强调了分析推理鲁棒性的重要性，为未来可解释音频检测系统的开发提供了重要见解。

Abstract: Audio Language Models (ALMs) offer a promising shift towards explainable audio deepfake detections (ADDs), moving beyond \textit{black-box} classifiers by providing some level of transparency into their predictions via reasoning traces. This necessitates a new class of model robustness analysis: robustness of the predictive reasoning under adversarial attacks, which goes beyond existing paradigm that mainly focuses on the shifts of the final predictions (e.g., fake v.s. real). To analyze such reasoning shifts, we introduce a forensic auditing framework to evaluate the robustness of ALMs' reasoning under adversarial attacks in three inter-connected dimensions: acoustic perception, cognitive coherence, and cognitive dissonance. Our systematic analysis reveals that explicit reasoning does not universally enhance robustness. Instead, we observe a bifurcation: for models exhibiting robust acoustic perception, reasoning acts as a defensive \textit{``shield''}, protecting them from adversarial attacks. However, for others, it imposes a performance \textit{``tax''}, particularly under linguistic attacks which reduce cognitive coherence and increase attack success rate. Crucially, even when classification fails, high cognitive dissonance can serve as a \textit{silent alarm}, flagging potential manipulation. Overall, this work provides a critical evaluation of the role of reasoning in forensic audio deepfake analysis and its vulnerabilities.

</details>


### [49] [Evaluating the Pre-Consultation Ability of LLMs using Diagnostic Guidelines](https://arxiv.org/abs/2601.03627)
*Jean Seo,Gibaeg Kim,Kihun Shin,Seungseop Lim,Hyunkyung Lee,Wooseok Han,Jongwon Lee,Eunho Yang*

Main category: cs.CL

TL;DR: EPAG是一个评估LLMs在预诊咨询中应用能力的基准数据集和框架，通过直接比较病史与诊断指南以及间接疾病诊断来评估模型表现。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在临床环境中的应用日益增多，需要评估它们在预诊咨询中的实际能力，特别是在遵循诊断指南和准确诊断方面的表现。

Method: 提出了EPAG评估框架，包含基准数据集和评估管道，通过两种方式评估LLMs：1）直接比较病史与诊断指南的匹配度；2）间接评估疾病诊断准确性。

Result: 实验发现：1）使用精心策划的任务特定数据集微调的小型开源模型在预诊咨询中可能优于前沿LLMs；2）增加病史信息量不一定能提高诊断性能；3）预诊咨询的语言会影响对话特征。

Conclusion: 通过开源数据集和评估管道，EPAG为评估和进一步发展LLMs在真实临床环境中的应用提供了重要工具，强调了特定领域微调的重要性。

Abstract: We introduce EPAG, a benchmark dataset and framework designed for Evaluating the Pre-consultation Ability of LLMs using diagnostic Guidelines. LLMs are evaluated directly through HPI-diagnostic guideline comparison and indirectly through disease diagnosis. In our experiments, we observe that small open-source models fine-tuned with a well-curated, task-specific dataset can outperform frontier LLMs in pre-consultation. Additionally, we find that increased amount of HPI (History of Present Illness) does not necessarily lead to improved diagnostic performance. Further experiments reveal that the language of pre-consultation influences the characteristics of the dialogue. By open-sourcing our dataset and evaluation pipeline on https://github.com/seemdog/EPAG, we aim to contribute to the evaluation and further development of LLM applications in real-world clinical settings.

</details>


### [50] [Reasoning Model Is Superior LLM-Judge, Yet Suffers from Biases](https://arxiv.org/abs/2601.03630)
*Hui Huang,Xuanxin Wu,Muyun Yang,Yuki Arase*

Main category: cs.CL

TL;DR: 本文首次系统比较了大型推理模型(LRMs)与非推理大语言模型在评估任务上的表现，发现LRMs在判断准确性、指令遵循和对抗攻击鲁棒性方面更优，但仍存在表面质量偏见，并提出PlanJudge策略显著减轻了这种偏见。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对大型推理模型(LRMs)作为评估者与非推理LLMs的系统比较研究，需要了解LRMs在判断任务中的优势和局限性，特别是它们在推理密集型任务中的表现、指令遵循能力、对抗攻击鲁棒性以及潜在的偏见问题。

Method: 1) 进行系统实证分析比较LRMs和非推理LLMs在评估任务中的表现；2) 提出PlanJudge评估策略，要求模型在执行前生成明确的评估计划；3) 通过实验验证PlanJudge在减轻模型偏见方面的有效性。

Result: 1) LRMs在判断准确性上优于非推理LLMs，特别是在推理密集型任务中；2) LRMs在评估情境中展现出更好的指令遵循能力；3) LRMs对针对判断任务的对抗攻击具有更强的鲁棒性；4) LRMs仍存在强烈的表面质量偏见；5) PlanJudge策略显著减轻了LRMs和标准LLMs中的偏见。

Conclusion: LRMs作为评估者在多个维度上优于非推理LLMs，但仍存在偏见问题。PlanJudge这一简单而有效的策略能够显著减轻这些偏见，为改进模型评估提供了实用方法。该研究为理解LRMs在评估任务中的表现提供了系统见解。

Abstract: This paper presents the first systematic comparison investigating whether Large Reasoning Models (LRMs) are superior judge to non-reasoning LLMs. Our empirical analysis yields four key findings: 1) LRMs outperform non-reasoning LLMs in terms of judgment accuracy, particularly on reasoning-intensive tasks; 2) LRMs demonstrate superior instruction-following capabilities in evaluation contexts; 3) LRMs exhibit enhanced robustness against adversarial attacks targeting judgment tasks; 4) However, LRMs still exhibit strong biases in superficial quality. To improve the robustness against biases, we propose PlanJudge, an evaluation strategy that prompts the model to generate an explicit evaluation plan before execution. Despite its simplicity, our experiments demonstrate that PlanJudge significantly mitigates biases in both LRMs and standard LLMs.

</details>


### [51] [Agent-Dice: Disentangling Knowledge Updates via Geometric Consensus for Agent Continual Learning](https://arxiv.org/abs/2601.03641)
*Zheng Wu,Xingyu Lou,Xinbei Ma,Yansi Li,Weiwen Liu,Weinan Zhang,Jun Wang,Zhuosheng Zhang*

Main category: cs.CL

TL;DR: Agent-Dice是一个基于方向共识评估的参数融合框架，通过几何共识过滤和曲率重要性加权来解决LLM智能体持续学习中的稳定性-可塑性困境。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的智能体在与动态环境交互时面临持续学习新任务而不会灾难性遗忘的关键挑战，即稳定性-可塑性困境。作者认为这一困境源于未能明确区分跨任务共享的通用知识和任务特定干扰引入的冲突知识。

Method: 提出Agent-Dice参数融合框架，采用两阶段过程解耦知识更新：1)几何共识过滤来修剪冲突梯度；2)基于曲率的重要性加权来放大共享语义。该框架基于方向共识评估，提供了理论分析验证融合方案的有效性。

Result: 在GUI智能体和工具使用智能体领域的广泛实验表明，Agent-Dice表现出出色的持续学习性能，同时计算开销和参数更新量最小。

Conclusion: Agent-Dice通过显式区分通用知识和冲突知识，有效解决了LLM智能体持续学习中的稳定性-可塑性困境，为智能体的持续学习提供了理论洞察和实践解决方案。

Abstract: Large Language Model (LLM)-based agents significantly extend the utility of LLMs by interacting with dynamic environments. However, enabling agents to continually learn new tasks without catastrophic forgetting remains a critical challenge, known as the stability-plasticity dilemma. In this work, we argue that this dilemma fundamentally arises from the failure to explicitly distinguish between common knowledge shared across tasks and conflicting knowledge introduced by task-specific interference. To address this, we propose Agent-Dice, a parameter fusion framework based on directional consensus evaluation. Concretely, Agent-Dice disentangles knowledge updates through a two-stage process: geometric consensus filtering to prune conflicting gradients, and curvature-based importance weighting to amplify shared semantics. We provide a rigorous theoretical analysis that establishes the validity of the proposed fusion scheme and offers insight into the origins of the stability-plasticity dilemma. Extensive experiments on GUI agents and tool-use agent domains demonstrate that Agent-Dice exhibits outstanding continual learning performance with minimal computational overhead and parameter updates.

</details>


### [52] [LLM-MC-Affect: LLM-Based Monte Carlo Modeling of Affective Trajectories and Latent Ambiguity for Interpersonal Dynamic Insight](https://arxiv.org/abs/2601.03645)
*Yu-Zheng Lin,Bono Po-Jen Shih,John Paul Martin Encinas,Elizabeth Victoria Abraham Achom,Karan Himanshu Patel,Jesus Horacio Pacheco,Sicong Shao,Jyotikrishna Dass,Soheil Salehi,Pratik Satam*

Main category: cs.CL

TL;DR: 提出LLM-MC-Affect概率框架，将情绪建模为情感空间中的连续潜在概率分布，通过蒙特卡洛估计生成情感轨迹，分析人际情感协调的动态耦合。


<details>
  <summary>Details</summary>
Motivation: 现有文本情感分析方法通常将情感视为确定性的点估计，忽略了情感的主观性、潜在模糊性以及对话中的序列耦合特性，无法捕捉人际情感协调的动态本质。

Method: 引入LLM-MC-Affect概率框架，利用随机LLM解码和蒙特卡洛估计来近似情感分布，生成高保真的情感轨迹，通过序列互相关和基于斜率的指标分析人际耦合模式。

Result: 在师生教学对话案例研究中，该方法的定量指标成功提取了高层交互洞察（如有效支架教学），验证了框架的解释能力。

Conclusion: 该工作为理解人际动态建立了一个可扩展且可部署的途径，提供了一个可推广的解决方案，不仅适用于教育领域，还可扩展到更广泛的社会和行为研究。

Abstract: Emotional coordination is a core property of human interaction that shapes how relational meaning is constructed in real time. While text-based affect inference has become increasingly feasible, prior approaches often treat sentiment as a deterministic point estimate for individual speakers, failing to capture the inherent subjectivity, latent ambiguity, and sequential coupling found in mutual exchanges. We introduce LLM-MC-Affect, a probabilistic framework that characterizes emotion not as a static label, but as a continuous latent probability distribution defined over an affective space. By leveraging stochastic LLM decoding and Monte Carlo estimation, the methodology approximates these distributions to derive high-fidelity sentiment trajectories that explicitly quantify both central affective tendencies and perceptual ambiguity. These trajectories enable a structured analysis of interpersonal coupling through sequential cross-correlation and slope-based indicators, identifying leading or lagging influences between interlocutors. To validate the interpretive capacity of this approach, we utilize teacher-student instructional dialogues as a representative case study, where our quantitative indicators successfully distill high-level interaction insights such as effective scaffolding. This work establishes a scalable and deployable pathway for understanding interpersonal dynamics, offering a generalizable solution that extends beyond education to broader social and behavioral research.

</details>


### [53] [ELO: Efficient Layer-Specific Optimization for Continual Pretraining of Multilingual LLMs](https://arxiv.org/abs/2601.03648)
*HanGyeol Yoo,ChangSu Choi,Minjun Kim,Seohyun Song,SeungWoo Song,Inho Won,Jongyoul Park,Cheoneum Park,KyungTae Lim*

Main category: cs.CL

TL;DR: ELO方法通过分层优化和层对齐，显著提升多语言大模型在特定语言上的持续预训练效率，同时保持源语言能力。


<details>
  <summary>Details</summary>
Motivation: 传统持续预训练方法在针对特定语言优化多语言大模型时存在两个主要问题：1) 计算成本高昂；2) 容易导致源语言性能退化。需要一种更高效的优化方法。

Method: ELO方法包含两个阶段：1) ELO预训练阶段：识别并分离出关键的第一层和最后一层，仅对这些层用目标语言数据进行训练，大幅减少可训练参数和计算量；2) 层对齐阶段：将训练好的层重新整合到原始模型中，并进行简短的完整微调以实现参数对齐。

Result: 实验结果显示，ELO方法相比现有方法实现了高达6.46倍的训练加速，在定性基准测试中目标语言性能提升达6.2%，同时有效保持了源语言（英语）的能力。

Conclusion: ELO方法为多语言大模型的特定语言优化提供了一种高效且有效的解决方案，在显著降低计算成本的同时，既提升了目标语言性能，又保持了源语言能力。

Abstract: We propose an efficient layer-specific optimization (ELO) method designed to enhance continual pretraining (CP) for specific languages in multilingual large language models (MLLMs). This approach addresses the common challenges of high computational cost and degradation of source language performance associated with traditional CP. The ELO method consists of two main stages: (1) ELO Pretraining, where a small subset of specific layers, identified in our experiments as the critically important first and last layers, are detached from the original MLLM and trained with the target language. This significantly reduces not only the number of trainable parameters but also the total parameters computed during the forward pass, minimizing GPU memory consumption and accelerating the training process. (2) Layer Alignment, where the newly trained layers are reintegrated into the original model, followed by a brief full fine-tuning step on a small dataset to align the parameters. Experimental results demonstrate that the ELO method achieves a training speedup of up to 6.46 times compared to existing methods, while improving target language performance by up to 6.2\% on qualitative benchmarks and effectively preserving source language (English) capabilities.

</details>


### [54] [SyncThink: A Training-Free Strategy to Align Inference Termination with Reasoning Saturation](https://arxiv.org/abs/2601.03649)
*Gengyang Li,Wang Cai,Yifeng Gao,Yunfang Wu*

Main category: cs.CL

TL;DR: SyncThink是一种无需训练、即插即用的解码方法，通过监控模型的推理转换信号来提前终止推理，减少CoT提示的推理开销，在保持准确率的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought（CoT）提示虽然能提升推理能力，但通常会产生冗长的推理轨迹，大幅增加推理成本。需要一种方法来减少CoT开销而不修改模型权重。

Method: 基于发现答案token对早期推理关注较弱，主要关注特殊token "/think"这一信息瓶颈现象，SyncThink监控模型自身的推理转换信号，在适当时机终止推理过程。

Result: 在GSM8K、MMLU、GPQA和BBH等基准测试中，SyncThink在三个DeepSeek-R1蒸馏模型上实现了62.00%的平均Top-1准确率，仅使用656个生成token和28.68秒延迟，而完整CoT解码需要2141个token和92.01秒延迟。在GPQA等长视野任务中，SyncThink通过防止过度思考可进一步提升8.1个绝对准确率。

Conclusion: SyncThink通过利用模型自身的推理信号来提前终止推理，在保持甚至提升准确率的同时，显著降低了CoT方法的计算开销，为高效推理提供了一种有效的训练免费解决方案。

Abstract: Chain-of-Thought (CoT) prompting improves reasoning but often produces long and redundant traces that substantially increase inference cost. We present SyncThink, a training-free and plug-and-play decoding method that reduces CoT overhead without modifying model weights. We find that answer tokens attend weakly to early reasoning and instead focus on the special token "/think", indicating an information bottleneck. Building on this observation, SyncThink monitors the model's own reasoning-transition signal and terminates reasoning. Experiments on GSM8K, MMLU, GPQA, and BBH across three DeepSeek-R1 distilled models show that SyncThink achieves 62.00 percent average Top-1 accuracy using 656 generated tokens and 28.68 s latency, compared to 61.22 percent, 2141 tokens, and 92.01 s for full CoT decoding. On long-horizon tasks such as GPQA, SyncThink can further yield up to +8.1 absolute accuracy by preventing over-thinking.

</details>


### [55] [e5-omni: Explicit Cross-modal Alignment for Omni-modal Embeddings](https://arxiv.org/abs/2601.03666)
*Haonan Chen,Sicheng Gao,Radu Timofte,Tetsuya Sakai,Zhicheng Dou*

Main category: cs.CL

TL;DR: e5-omni提出了一种轻量级显式对齐方法，将现成的视觉语言模型转化为鲁棒的全模态嵌入模型，解决了跨模态相似性尺度不一致、负样本效率低下和统计特性不匹配等问题。


<details>
  <summary>Details</summary>
Motivation: 当前全模态嵌入模型主要依赖预训练视觉语言模型的隐式对齐，这导致三个常见问题：1) 相似度分数具有模态依赖性，尺度不一致；2) 混合模态批次导致负样本硬度分布不平衡，许多负样本很快变得无关紧要；3) 跨模态嵌入的一阶和二阶统计量不匹配，影响排序稳定性。

Method: 提出e5-omni方法，包含三个组件：1) 模态感知温度校准，对齐相似度尺度；2) 可控负样本课程学习与去偏，聚焦于混淆负样本同时减少假负样本的影响；3) 批量白化与协方差正则化，更好地匹配共享嵌入空间中的跨模态几何结构。

Result: 在MMEB-V2和AudioCaps数据集上的实验显示，该方法在强双模态和全模态基线上获得了一致的性能提升，且该配方能很好地迁移到其他视觉语言模型骨干网络。

Conclusion: e5-omni提供了一种简单有效的显式对齐方法，能够将现成的视觉语言模型转化为鲁棒的全模态嵌入模型，解决了跨模态对齐中的关键问题，并在多个基准上取得了显著改进。

Abstract: Modern information systems often involve different types of items, e.g., a text query, an image, a video clip, or an audio segment. This motivates omni-modal embedding models that map heterogeneous modalities into a shared space for direct comparison. However, most recent omni-modal embeddings still rely heavily on implicit alignment inherited from pretrained vision-language model (VLM) backbones. In practice, this causes three common issues: (i) similarity logits have modality-dependent sharpness, so scores are not on a consistent scale; (ii) in-batch negatives become less effective over time because mixed-modality batches create an imbalanced hardness distribution; as a result, many negatives quickly become trivial and contribute little gradient; and (iii) embeddings across modalities show mismatched first- and second-order statistics, which makes rankings less stable. To tackle these problems, we propose e5-omni, a lightweight explicit alignment recipe that adapts off-the-shelf VLMs into robust omni-modal embedding models. e5-omni combines three simple components: (1) modality-aware temperature calibration to align similarity scales, (2) a controllable negative curriculum with debiasing to focus on confusing negatives while reducing the impact of false negatives, and (3) batch whitening with covariance regularization to better match cross-modal geometry in the shared embedding space. Experiments on MMEB-V2 and AudioCaps show consistent gains over strong bi-modal and omni-modal baselines, and the same recipe also transfers well to other VLM backbones. We release our model checkpoint at https://huggingface.co/Haon-Chen/e5-omni-7B.

</details>


### [56] [eTracer: Towards Traceable Text Generation via Claim-Level Grounding](https://arxiv.org/abs/2601.03669)
*Bohao Chu,Qianli Wang,Hendrik Damm,Hui Wang,Ula Muhabbek,Elisabeth Livingstone,Christoph M. Friedrich,Norbert Fuhr*

Main category: cs.CL

TL;DR: eTracer是一个可追溯文本生成框架，通过将生成声明与上下文证据对齐，实现响应可验证性和可信度。


<details>
  <summary>Details</summary>
Motivation: 在生物医学等高风险领域，如何有效验证系统生成的响应是一个重要挑战。现有方法在将生成声明与上下文句子级证据对齐方面存在局限。

Method: 提出eTracer框架，采用事后追溯方法，将每个响应声明与上下文证据对齐（支持或反驳）。基于声明级对齐结果，不仅让用户能够精确追溯响应到上下文来源，还能量化响应忠实度。

Result: 实验表明，声明级对齐方法缓解了传统对齐方法的局限，显著提高了整体对齐质量和用户验证效率。

Conclusion: eTracer通过声明级证据对齐实现了可追溯文本生成，提高了生成响应的可验证性和可信度，特别是在生物医学等高风险领域具有重要应用价值。

Abstract: How can system-generated responses be efficiently verified, especially in the high-stakes biomedical domain? To address this challenge, we introduce eTracer, a plug-and-play framework that enables traceable text generation by grounding claims against contextual evidence. Through post-hoc grounding, each response claim is aligned with contextual evidence that either supports or contradicts it. Building on claim-level grounding results, eTracer not only enables users to precisely trace responses back to their contextual source but also quantifies response faithfulness, thereby enabling the verifiability and trustworthiness of generated responses. Experiments show that our claim-level grounding approach alleviates the limitations of conventional grounding methods in aligning generated statements with contextual sentence-level evidence, resulting in substantial improvements in overall grounding quality and user verification efficiency. The code and data are available at https://github.com/chubohao/eTracer.

</details>


### [57] [DisastQA: A Comprehensive Benchmark for Evaluating Question Answering in Disaster Management](https://arxiv.org/abs/2601.03670)
*Zhitong Chen,Kai Yin,Xiangjue Dong,Chengkai Liu,Xiangpeng Li,Yiming Xiao,Bo Li,Junwei Ma,Ali Mostafavi,James Caverlee*

Main category: cs.CL

TL;DR: DisastQA是一个包含3000个经过严格验证问题的大规模灾难管理问答基准，涵盖8种灾难类型，旨在评估模型在不确定和冲突信息下的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有问答基准建立在干净证据上，无法捕捉灾难管理中不确定和冲突信息的真实推理场景，因此需要专门针对灾难管理的问答基准。

Method: 通过人机协作流水线构建基准，采用分层抽样确保平衡覆盖，包含2000个多项选择题和1000个开放式问题。模型在不同证据条件下评估，从闭卷到噪声证据整合，并提出了基于关键点的人类验证评估协议。

Result: 对20个模型的实验显示，与MMLU-Pro等通用排行榜存在显著差异。在干净设置下，最近的开源模型接近专有系统，但在现实噪声下性能急剧下降，暴露了灾难响应的关键可靠性差距。

Conclusion: DisastQA基准揭示了现有模型在灾难管理现实场景中的局限性，强调了在噪声和不确定信息下可靠推理的重要性，为改进灾难响应AI系统提供了重要基准。

Abstract: Accurate question answering (QA) in disaster management requires reasoning over uncertain and conflicting information, a setting poorly captured by existing benchmarks built on clean evidence. We introduce DisastQA, a large-scale benchmark of 3,000 rigorously verified questions (2,000 multiple-choice and 1,000 open-ended) spanning eight disaster types. The benchmark is constructed via a human-LLM collaboration pipeline with stratified sampling to ensure balanced coverage. Models are evaluated under varying evidence conditions, from closed-book to noisy evidence integration, enabling separation of internal knowledge from reasoning under imperfect information. For open-ended QA, we propose a human-verified keypoint-based evaluation protocol emphasizing factual completeness over verbosity. Experiments with 20 models reveal substantial divergences from general-purpose leaderboards such as MMLU-Pro. While recent open-weight models approach proprietary systems in clean settings, performance degrades sharply under realistic noise, exposing critical reliability gaps for disaster response. All code, data, and evaluation resources are available at https://github.com/TamuChen18/DisastQA_open.

</details>


### [58] [NeuronScope: A Multi-Agent Framework for Explaining Polysemantic Neurons in Language Models](https://arxiv.org/abs/2601.03671)
*Weiqi Liu,Yongliang Miao,Haiyan Zhao,Yanguang Liu,Mengnan Du*

Main category: cs.CL

TL;DR: NeuronScope：一个多智能体框架，通过迭代、激活引导的过程解决LLM神经元多义性解释问题


<details>
  <summary>Details</summary>
Motivation: 大语言模型中的神经元级解释面临多义性的根本挑战，即单个神经元对多个不同语义概念产生响应。现有的单次解释方法难以准确捕捉这种多概念行为。

Method: NeuronScope将神经元解释重新构建为迭代、激活引导的过程。该方法明确地将神经元激活分解为原子语义组件，将它们聚类为不同的语义模式，并使用神经元激活反馈迭代地优化每个解释。

Result: 实验表明，NeuronScope能够揭示隐藏的多义性，并产生与单次基线方法相比显著更高的激活相关性的解释。

Conclusion: NeuronScope通过多智能体框架有效解决了LLM神经元多义性解释的挑战，提供了一种更准确、更忠实地捕捉神经元多概念行为的方法。

Abstract: Neuron-level interpretation in large language models (LLMs) is fundamentally challenged by widespread polysemanticity, where individual neurons respond to multiple distinct semantic concepts. Existing single-pass interpretation methods struggle to faithfully capture such multi-concept behavior. In this work, we propose NeuronScope, a multi-agent framework that reformulates neuron interpretation as an iterative, activation-guided process. NeuronScope explicitly deconstructs neuron activations into atomic semantic components, clusters them into distinct semantic modes, and iteratively refines each explanation using neuron activation feedback. Experiments demonstrate that NeuronScope uncovers hidden polysemanticity and produces explanations with significantly higher activation correlation compared to single-pass baselines.

</details>


### [59] [Towards Compositional Generalization of LLMs via Skill Taxonomy Guided Data Synthesis](https://arxiv.org/abs/2601.03676)
*Yifan Wei,Li Du,Xiaoyan Yu,Yang Feng,Angsheng Li*

Main category: cs.CL

TL;DR: STEPS框架通过构建技能分类学指导的熵基后训练数据合成，解决LLM在组合泛化上的数据瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和基于代理的系统在组合泛化方面存在困难，主要原因是复杂技能组合遵循长尾幂律分布，形成了数据瓶颈，限制了指令跟随性能和代理中心任务的泛化能力。

Method: STEPS框架首先通过结构信息理论揭示技能间的潜在关系，构建可解释的层次化技能分类学；然后基于该分类学，将数据合成构建为约束信息最大化问题，选择在层次结构中最大化边际结构信息同时保持语义一致性的技能组合。

Result: 在具有挑战性的指令跟随基准测试中，STEPS优于现有的数据合成基线方法，同时在下游基于代理的评估中也展现出改进的组合泛化能力。

Conclusion: STEPS框架通过技能分类学指导的熵基数据合成，有效解决了LLM组合泛化的数据瓶颈问题，在指令跟随和代理任务中均展现出优越性能。

Abstract: Large Language Models (LLMs) and agent-based systems often struggle with compositional generalization due to a data bottleneck in which complex skill combinations follow a long-tailed, power-law distribution, limiting both instruction-following performance and generalization in agent-centric tasks. To address this challenge, we propose STEPS, a Skill Taxonomy guided Entropy-based Post-training data Synthesis framework for generating compositionally challenging data. STEPS explicitly targets compositional generalization by uncovering latent relationships among skills and organizing them into an interpretable, hierarchical skill taxonomy using structural information theory. Building on this taxonomy, we formulate data synthesis as a constrained information maximization problem, selecting skill combinations that maximize marginal structural information within the hierarchy while preserving semantic coherence. Experiments on challenging instruction-following benchmarks show that STEPS outperforms existing data synthesis baselines, while also yielding improved compositional generalization in downstream agent-based evaluations.

</details>


### [60] [From Implicit to Explicit: Token-Efficient Logical Supervision for Mathematical Reasoning in LLMs](https://arxiv.org/abs/2601.03682)
*Shaojie Wang,Liang Zhang*

Main category: cs.CL

TL;DR: 提出FSLR训练框架，专注于提升LLMs的逻辑关系理解能力，通过训练模型仅进行问题解决的第一步规划，显著提高数学推理性能，同时大幅降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在数学问题解决中逻辑推理能力有限，主要依赖模式匹配和记忆。研究发现超过90%的错误与逻辑关系理解能力不足有关，而现有的CoT-SFT方法无法有效减少这类错误。

Method: 提出First-Step Logical Reasoning (FSLR)框架，专注于训练模型进行问题解决的第一步规划：识别要使用的变量和要应用的操作。这种方法为逻辑关系理解提供显式监督，而不是像CoT-SFT那样将逻辑关系隐含在完整的解题轨迹中。

Result: FSLR在多个模型和数据集上一致优于CoT-SFT，在分布内和分布外设置下平均分别提升3.2%和4.6%。同时训练速度提高4-6倍，训练token消耗减少超过80%。

Conclusion: FSLR通过专注于逻辑关系理解的第一步规划，有效解决了LLMs数学推理中的关键瓶颈，提供了一种高效且有效的训练方法，显著提升性能同时大幅降低计算成本。

Abstract: Recent studies reveal that large language models (LLMs) exhibit limited logical reasoning abilities in mathematical problem-solving, instead often relying on pattern-matching and memorization. We systematically analyze this limitation, focusing on logical relationship understanding, which is a core capability underlying genuine logical reasoning, and reveal that errors related to this capability account for over 90\% of incorrect predictions, with Chain-of-Thought Supervised Fine-Tuning (CoT-SFT) failing to substantially reduce these errors. To address this bottleneck, we propose First-Step Logical Reasoning (FSLR), a lightweight training framework targeting logical relationship understanding. Our key insight is that the first planning step-identifying which variables to use and which operation to apply-encourages the model to derive logical relationships directly from the problem statement. By training models on this isolated step, FSLR provides explicit supervision for logical relationship understanding, unlike CoT-SFT which implicitly embeds such relationships within complete solution trajectories. Extensive experiments across multiple models and datasets demonstrate that FSLR consistently outperforms CoT-SFT under both in-distribution and out-of-distribution settings, with average improvements of 3.2\% and 4.6\%, respectively. Moreover, FSLR achieves 4-6x faster training and reduces training token consumption by over 80\%.

</details>


### [61] [Evaluation Framework for AI Creativity: A Case Study Based on Story Generation](https://arxiv.org/abs/2601.03698)
*Pharath Sathya,Yin Jou Huang,Fei Cheng*

Main category: cs.CL

TL;DR: 提出结构化评估框架，通过四个维度（新颖性、价值、一致性、共鸣）和11个子维度评估AI故事生成的创意性，发现创意评估是层次性的而非累积性的。


<details>
  <summary>Details</summary>
Motivation: 现有基于参考的评估指标无法捕捉创意的主观性，评估创意文本生成仍具挑战性。

Method: 提出包含四个组件（新颖性、价值、一致性、共鸣）和11个子组件的结构化评估框架；使用"Spike Prompting"控制故事生成；通过115名读者的众包研究，考察不同创意组件如何影响即时和反思性的人类创意判断。

Result: 创意评估是层次性的而非累积性的，不同维度在不同判断阶段变得突出；反思性评估显著改变评分和评分者间一致性；框架能揭示被基于参考的评估所掩盖的创意维度。

Conclusion: 该结构化评估框架能有效揭示创意文本生成的多个维度，为AI故事生成的创意评估提供了更全面和细致的分析工具。

Abstract: Evaluating creative text generation remains a challenge because existing reference-based metrics fail to capture the subjective nature of creativity. We propose a structured evaluation framework for AI story generation comprising four components (Novelty, Value, Adherence, and Resonance) and eleven sub-components. Using controlled story generation via ``Spike Prompting'' and a crowdsourced study of 115 readers, we examine how different creative components shape both immediate and reflective human creativity judgments. Our findings show that creativity is evaluated hierarchically rather than cumulatively, with different dimensions becoming salient at different stages of judgment, and that reflective evaluation substantially alters both ratings and inter-rater agreement. Together, these results support the effectiveness of our framework in revealing dimensions of creativity that are obscured by reference-based evaluation.

</details>


### [62] [RedBench: A Universal Dataset for Comprehensive Red Teaming of Large Language Models](https://arxiv.org/abs/2601.03699)
*Quy-Anh Dang,Chris Ngo,Truong-Son Hy*

Main category: cs.CL

TL;DR: RedBench是一个统一的对抗性提示数据集，整合了37个现有基准数据集，包含29,362个攻击和拒绝提示样本，采用标准化风险分类和领域划分，用于系统评估LLM的漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有红队数据集存在风险分类不一致、领域覆盖有限、评估方法过时等问题，阻碍了对LLM漏洞的系统性评估。随着LLM在安全关键应用中的普及，确保其对抗性提示的鲁棒性至关重要。

Method: 整合37个来自顶级会议和代码库的基准数据集，建立包含22个风险类别和19个领域的标准化分类体系，创建包含29,362个攻击和拒绝提示样本的统一数据集。

Result: 建立了现代LLM的基线评估，开源了数据集和评估代码，为LLM漏洞评估提供了标准化框架，促进了不同模型间的公平比较。

Conclusion: RedBench解决了现有红队数据集的局限性，为系统评估LLM对抗性提示的鲁棒性提供了统一标准，推动了安全可靠LLM的研究和实际部署。

Abstract: As large language models (LLMs) become integral to safety-critical applications, ensuring their robustness against adversarial prompts is paramount. However, existing red teaming datasets suffer from inconsistent risk categorizations, limited domain coverage, and outdated evaluations, hindering systematic vulnerability assessments. To address these challenges, we introduce RedBench, a universal dataset aggregating 37 benchmark datasets from leading conferences and repositories, comprising 29,362 samples across attack and refusal prompts. RedBench employs a standardized taxonomy with 22 risk categories and 19 domains, enabling consistent and comprehensive evaluations of LLM vulnerabilities. We provide a detailed analysis of existing datasets, establish baselines for modern LLMs, and open-source the dataset and evaluation code. Our contributions facilitate robust comparisons, foster future research, and promote the development of secure and reliable LLMs for real-world deployment. Code: https://github.com/knoveleng/redeval

</details>


### [63] [ADEPT: Adaptive Dynamic Early-Exit Process for Transformers](https://arxiv.org/abs/2601.03700)
*Sangmin Yoo,Srikanth Malla,Chiho Choi,Wei D. Lu,Joon Hee Choi*

Main category: cs.CL

TL;DR: ADEPT提出了一种自适应动态早退机制，可在Transformer模型的预填充和生成阶段实现动态早退，通过解耦跳过层的KV缓存依赖，显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 当前早退策略存在局限性：生成阶段只适用于第一个token，预填充阶段只在提示级别，跳过层的KV缓存成为后续token生成的瓶颈，限制了早退的效益。

Method: 提出ADEPT方法，包含自适应token级早退机制，根据token复杂度动态调整计算；通过解耦跳过层的顺序依赖来优化KV生成过程，使token级早退更实用。

Result: 实验结果显示：ADEPT在语言生成任务中效率提升高达25%，在下游分类任务中实现4倍加速，性能提升高达45%。

Conclusion: ADEPT通过自适应动态早退机制有效解决了现有早退策略的KV缓存瓶颈问题，在保持性能的同时显著提升了Transformer模型的推理效率。

Abstract: The inference of large language models imposes significant computational workloads, often requiring the processing of billions of parameters. Although early-exit strategies have proven effective in reducing computational demands by halting inference earlier, they apply either to only the first token in the generation phase or at the prompt level in the prefill phase. Thus, the Key-Value (KV) cache for skipped layers remains a bottleneck for subsequent token generation, limiting the benefits of early exit. We introduce ADEPT (Adaptive Dynamic Early-exit Process for Transformers), a novel approach designed to overcome this issue and enable dynamic early exit in both the prefill and generation phases. The proposed adaptive token-level early-exit mechanism adjusts computation dynamically based on token complexity, optimizing efficiency without compromising performance. ADEPT further enhances KV generation procedure by decoupling sequential dependencies in skipped layers, making token-level early exit more practical. Experimental results demonstrate that ADEPT improves efficiency by up to 25% in language generation tasks and achieves a 4x speed-up in downstream classification tasks, with up to a 45% improvement in performance.

</details>


### [64] [AirNav: A Large-Scale Real-World UAV Vision-and-Language Navigation Dataset with Natural and Diverse Instructions](https://arxiv.org/abs/2601.03707)
*Hengxing Cai,Yijie Rao,Ligang Huang,Zanyang Zhong,Jinhan Dong,Jingjun Tan,Wenhao Lu,Renxin Zhong*

Main category: cs.CL

TL;DR: 提出了AirNav大规模无人机视觉语言导航基准和AirVLN-R1模型，解决了现有数据集依赖虚拟环境、指令不自然、规模有限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有无人机视觉语言导航数据集存在三个主要问题：依赖虚拟环境、指令不够自然、数据集规模有限，需要构建基于真实城市空中数据的大规模基准。

Method: 1. 构建AirNav基准：使用真实城市空中数据而非合成环境，包含自然多样的指令；2. 提出AirVLN-R1模型：结合监督微调（SFT）和强化学习微调（RFT）提升性能和泛化能力；3. 通过真实世界测试初步验证可行性。

Result: 成功构建了基于真实城市空中数据的大规模无人机VLN基准AirNav，并开发了AirVLN-R1模型，通过真实世界测试验证了可行性。数据集和代码已公开可用。

Conclusion: AirNav基准和AirVLN-R1模型为解决无人机VLN中的关键挑战提供了有效方案，通过真实数据和混合训练方法提升了导航系统的自然性和泛化能力，为未来研究提供了有价值的资源。

Abstract: Existing Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) datasets face issues such as dependence on virtual environments, lack of naturalness in instructions, and limited scale. To address these challenges, we propose AirNav, a large-scale UAV VLN benchmark constructed from real urban aerial data, rather than synthetic environments, with natural and diverse instructions. Additionally, we introduce the AirVLN-R1, which combines Supervised Fine-Tuning and Reinforcement Fine-Tuning to enhance performance and generalization. The feasibility of the model is preliminarily evaluated through real-world tests. Our dataset and code are publicly available.

</details>


### [65] [Visual Merit or Linguistic Crutch? A Close Look at DeepSeek-OCR](https://arxiv.org/abs/2601.03714)
*Yunhao Liang,Ruixuan Ying,Bo Li,Hong Li,Kai Yan,Qingwen Li,Min Yang,Okamoto Satoshi,Zhe Cui,Shiwen Ni*

Main category: cs.CL

TL;DR: 本文通过语义破坏实验发现DeepSeek-OCR的性能主要依赖语言先验而非真正的视觉识别能力，在无语言支持时性能从90%骤降至20%，且与传统OCR方法相比鲁棒性较差。


<details>
  <summary>Details</summary>
Motivation: DeepSeek-OCR声称能实现高比例视觉-文本压缩，可能解决LLM长上下文瓶颈。但需要探究其性能究竟来自真正的视觉识别能力还是语言先验的依赖。

Method: 使用句子级和单词级语义破坏来隔离模型的固有OCR能力与语言先验，对比13个基线模型，分析视觉token数量与先验依赖的关系，并进行上下文压力测试。

Result: 1) 无语言支持时，DeepSeek-OCR性能从约90%降至20%；2) 传统管道OCR方法比端到端方法对语义扰动更鲁棒；3) 视觉token越少，对先验依赖越强，幻觉风险越高；4) 约10,000文本token时模型完全崩溃。

Conclusion: 当前的光学压缩技术可能反而加剧长上下文瓶颈问题，需要明确DeepSeek-OCR的能力边界，为未来视觉-文本压缩范式优化提供重要见解。

Abstract: DeepSeek-OCR utilizes an optical 2D mapping approach to achieve high-ratio vision-text compression, claiming to decode text tokens exceeding ten times the input visual tokens. While this suggests a promising solution for the LLM long-context bottleneck, we investigate a critical question: "Visual merit or linguistic crutch - which drives DeepSeek-OCR's performance?" By employing sentence-level and word-level semantic corruption, we isolate the model's intrinsic OCR capabilities from its language priors. Results demonstrate that without linguistic support, DeepSeek-OCR's performance plummets from approximately 90% to 20%. Comparative benchmarking against 13 baseline models reveals that traditional pipeline OCR methods exhibit significantly higher robustness to such semantic perturbations than end-to-end methods. Furthermore, we find that lower visual token counts correlate with increased reliance on priors, exacerbating hallucination risks. Context stress testing also reveals a total model collapse around 10,000 text tokens, suggesting that current optical compression techniques may paradoxically aggravate the long-context bottleneck. This study empirically defines DeepSeek-OCR's capability boundaries and offers essential insights for future optimizations of the vision-text compression paradigm. We release all data, results and scripts used in this study at https://github.com/dududuck00/DeepSeekOCR.

</details>


### [66] [MIND: From Passive Mimicry to Active Reasoning through Capability-Aware Multi-Perspective CoT Distillation](https://arxiv.org/abs/2601.03717)
*Jin Cui,Jiaqi Guo,Jiepeng Zhou,Ruixuan Yang,Jiayi Lu,Jiajun Xu,Jiangcheng Song,Boran Zhao,Pengju Ren*

Main category: cs.CL

TL;DR: MIND是一个能力自适应的知识蒸馏框架，通过教学助理网络和反馈驱动的惯性校准机制，帮助小模型从大模型学习推理能力，实现领域内和跨领域的优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有的知识蒸馏方法通常要求学生模型严格遵循教师的单一"最优"推理路径，但由于师生模型不同的归纳偏置和内在偏好，加上学生模型在训练过程中不断变化的能力和推理偏好，教师的推理路径可能成为分布外噪声，导致学生模型潜在推理分布退化，性能不佳。

Method: MIND框架包含两个核心组件：1）教学助理网络，通过综合多样的教师视角生成多样化的监督信号；2）反馈驱动的惯性校准机制，利用惯性过滤的训练损失使监督与学生当前的适应能力对齐，有效提升性能同时减轻灾难性遗忘。

Result: 大量实验表明，MIND在领域内和领域外基准测试中都达到了最先进的性能，精密的潜在空间分析进一步证实了推理能力内化的机制。

Conclusion: MIND成功地将知识蒸馏从被动模仿转变为主动认知构建，通过能力自适应框架有效解决了现有方法中学生模型与教师模型推理路径不匹配的问题，实现了更好的领域性能和跨领域泛化能力。

Abstract: While Large Language Models (LLMs) have emerged with remarkable capabilities in complex tasks through Chain-of-Thought reasoning, practical resource constraints have sparked interest in transferring these abilities to smaller models. However, achieving both domain performance and cross-domain generalization remains challenging. Existing approaches typically restrict students to following a single golden rationale and treat different reasoning paths independently. Due to distinct inductive biases and intrinsic preferences, alongside the student's evolving capacity and reasoning preferences during training, a teacher's "optimal" rationale could act as out-of-distribution noise. This misalignment leads to a degeneration of the student's latent reasoning distribution, causing suboptimal performance. To bridge this gap, we propose MIND, a capability-adaptive framework that transitions distillation from passive mimicry to active cognitive construction. We synthesize diverse teacher perspectives through a novel "Teaching Assistant" network. By employing a Feedback-Driven Inertia Calibration mechanism, this network utilizes inertia-filtered training loss to align supervision with the student's current adaptability, effectively enhancing performance while mitigating catastrophic forgetting. Extensive experiments demonstrate that MIND achieves state-of-the-art performance on both in-distribution and out-of-distribution benchmarks, and our sophisticated latent space analysis further confirms the mechanism of reasoning ability internalization.

</details>


### [67] [Stuttering-Aware Automatic Speech Recognition for Indonesian Language](https://arxiv.org/abs/2601.03727)
*Fadhil Muhammad,Alwin Djuliansah,Adrian Aryaputra Hamzah,Kurniawati Azizah*

Main category: cs.CL

TL;DR: 提出一个数据增强框架，通过将口吃特征注入流利文本生成合成口吃音频，用于微调印尼语Whisper模型，提升对口吃语音的识别能力。


<details>
  <summary>Details</summary>
Motivation: 自动语音识别系统在处理口吃语音时性能显著下降，特别是在印尼语等低资源语言中，专门的口吃数据集几乎不存在，限制了系统对非典型语音模式的适应性。

Method: 1. 提出数据增强框架，通过规则转换和大语言模型将重复和延长等口吃特征注入流利文本；2. 使用文本转语音合成生成合成口吃音频；3. 应用合成数据通过迁移学习微调预训练的印尼语Whisper模型。

Result: 实验表明，这种针对性的合成数据暴露能持续减少对口吃语音的识别错误，同时保持对流利片段的识别性能，验证了合成数据管道在开发更具包容性语音技术中的有效性。

Conclusion: 合成数据增强框架是解决低资源语言口吃语音识别问题的有效方法，无需大规模真实录音即可使模型适应非流利声学模式，为在代表性不足的语言中开发更具包容性的语音技术提供了实用解决方案。

Abstract: Automatic speech recognition systems have achieved remarkable performance on fluent speech but continue to degrade significantly when processing stuttered speech, a limitation that is particularly acute for low-resource languages like Indonesian where specialized datasets are virtually non-existent. To overcome this scarcity, we propose a data augmentation framework that generates synthetic stuttered audio by injecting repetitions and prolongations into fluent text through a combination of rule-based transformations and large language models followed by text-to-speech synthesis. We apply this synthetic data to fine-tune a pre-trained Indonesian Whisper model using transfer learning, enabling the architecture to adapt to dysfluent acoustic patterns without requiring large-scale real-world recordings. Our experiments demonstrate that this targeted synthetic exposure consistently reduces recognition errors on stuttered speech while maintaining performance on fluent segments, validating the utility of synthetic data pipelines for developing more inclusive speech technologies in under-represented languages.

</details>


### [68] [O-Researcher: An Open Ended Deep Research Model via Multi-Agent Distillation and Agentic RL](https://arxiv.org/abs/2601.03743)
*Yi Yao,He Zhu,Piaohong Wang,Jincheng Ren,Xinlong Yang,Qianben Chen,Xiaowan Li,Dingfeng Shi,Jiaxian Li,Qiexiang Wang,Sinuo Wang,Xinpeng Liu,Jiaqi Wu,Minghao Liu,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 提出自动合成高质量研究级指令数据的多智能体框架，通过两阶段训练显著提升开源大模型性能，在深度研究基准上达到新SOTA。


<details>
  <summary>Details</summary>
Motivation: 开源与闭源大语言模型之间的性能差距主要源于高质量训练数据的获取不平等。为了弥合这一差距，需要一种无需依赖专有数据或模型的解决方案。

Method: 1. 多智能体工作流：协作的AI智能体模拟复杂的工具集成推理，端到端生成多样且高保真的研究级指令数据
2. 两阶段训练策略：结合监督微调和新颖的强化学习方法，最大化模型对齐和能力

Result: 该框架显著提升了多个规模的开源模型性能，在主要深度研究基准上实现了新的最先进性能。

Conclusion: 本工作为推进开源大语言模型提供了一条可扩展且有效的途径，无需依赖专有数据或模型，有助于缩小开源与闭源模型之间的差距。

Abstract: The performance gap between closed-source and open-source large language models (LLMs) is largely attributed to disparities in access to high-quality training data. To bridge this gap, we introduce a novel framework for the automated synthesis of sophisticated, research-grade instructional data. Our approach centers on a multi-agent workflow where collaborative AI agents simulate complex tool-integrated reasoning to generate diverse and high-fidelity data end-to-end. Leveraging this synthesized data, we develop a two-stage training strategy that integrates supervised fine-tuning with a novel reinforcement learning method, designed to maximize model alignment and capability. Extensive experiments demonstrate that our framework empowers open-source models across multiple scales, enabling them to achieve new state-of-the-art performance on the major deep research benchmark. This work provides a scalable and effective pathway for advancing open-source LLMs without relying on proprietary data or models.

</details>


### [69] [Whose Facts Win? LLM Source Preferences under Knowledge Conflicts](https://arxiv.org/abs/2601.03746)
*Jakob Schuster,Vagrant Gautam,Katja Markert*

Main category: cs.CL

TL;DR: 大型语言模型在检索增强生成中面临知识冲突时，会受到信息来源可信度偏好的影响，但重复效应会逆转这种偏好。论文提出了一种新方法能有效减少重复偏差。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在检索增强生成管道中应用增多，研究其在知识冲突下的行为变得日益重要。目前信息来源的偏好作用尚未被充分研究，论文旨在填补这一空白，基于跨学科的可信度研究来探讨LLM如何解决跨上下文知识冲突。

Method: 提出了一个新颖框架来研究来源偏好如何影响LLM解决英语语境中的知识冲突。对13个开源权重LLM进行了全面、严格控制评估，并开发了一种新方法来减少重复偏差。

Result: LLM偏好机构证实的信息（如政府或报纸来源），而非个人和社交媒体信息。然而，这种来源偏好可以通过简单重复低可信度来源的信息而被逆转。提出的新方法能将重复偏差减少高达99.8%，同时保持至少88.8%的原始偏好。

Conclusion: 信息来源的偏好显著影响LLM的知识冲突解决，但重复效应会破坏这种偏好。论文提出的方法能有效减轻重复偏差，为知识密集型NLP中的可信度和来源偏好研究提供了有价值的方向。作者公开了所有数据和代码以促进未来研究。

Abstract: As large language models (LLMs) are more frequently used in retrieval-augmented generation pipelines, it is increasingly relevant to study their behavior under knowledge conflicts. Thus far, the role of the source of the retrieved information has gone unexamined. We address this gap with a novel framework to investigate how source preferences affect LLM resolution of inter-context knowledge conflicts in English, motivated by interdisciplinary research on credibility. With a comprehensive, tightly-controlled evaluation of 13 open-weight LLMs, we find that LLMs prefer institutionally-corroborated information (e.g., government or newspaper sources) over information from people and social media. However, these source preferences can be reversed by simply repeating information from less credible sources. To mitigate repetition effects and maintain consistent preferences, we propose a novel method that reduces repetition bias by up to 99.8%, while also maintaining at least 88.8% of original preferences. We release all data and code to encourage future work on credibility and source preferences in knowledge-intensive NLP.

</details>


### [70] [Evaluation of Multilingual LLMs Personalized Text Generation Capabilities Targeting Groups and Social-Media Platforms](https://arxiv.org/abs/2601.03752)
*Dominik Macko*

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型在多语言环境中个性化文本生成的能力及其对机器生成文本可检测性的影响，覆盖10种语言、1080种个性化组合和16个模型，发现个性化质量存在语言差异且影响检测效果。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在多语言连贯文本生成能力上的提升，引发了对其潜在滥用的担忧。先前研究表明它们可用于生成多语言的个性化虚假信息，且个性化会降低机器生成文本的可检测性，但这仅在英语中被研究。本研究旨在跨10种语言探讨这一现象，不仅关注个性化能力的潜在滥用，也关注其潜在益处。

Method: 研究覆盖了10种语言，设计了1080种不同的个性化提示组合（包括针对不同人口统计群体和社交媒体平台的个性化），使用16个不同的语言模型生成文本，共计17,280个文本样本。

Result: 结果显示，针对人口统计群体和针对社交媒体平台的个性化文本生成质量在不同语言间存在差异。平台个性化对生成文本的可检测性影响更大，尤其是在英语中，其个性化质量最高。

Conclusion: 个性化文本生成的质量和其对检测性的影响存在语言间的差异，平台个性化尤其影响可检测性。这强调了在多语言环境中评估个性化生成文本检测方法的重要性，并提示需考虑个性化能力带来的潜在风险与益处。

Abstract: Capabilities of large language models to generate multilingual coherent text have continuously enhanced in recent years, which opens concerns about their potential misuse. Previous research has shown that they can be misused for generation of personalized disinformation in multiple languages. It has also been observed that personalization negatively affects detectability of machine-generated texts; however, this has been studied in the English language only. In this work, we examine this phenomenon across 10 languages, while we focus not only on potential misuse of personalization capabilities, but also on potential benefits they offer. Overall, we cover 1080 combinations of various personalization aspects in the prompts, for which the texts are generated by 16 distinct language models (17,280 texts in total). Our results indicate that there are differences in personalization quality of the generated texts when targeting demographic groups and when targeting social-media platforms across languages. Personalization towards platforms affects detectability of the generated texts in a higher scale, especially in English, where the personalization quality is the highest.

</details>


### [71] [Do LLM Self-Explanations Help Users Predict Model Behavior? Evaluating Counterfactual Simulatability with Pragmatic Perturbations](https://arxiv.org/abs/2601.03775)
*Pingjun Hong,Benjamin Roth*

Main category: cs.CL

TL;DR: 论文研究LLM自我解释是否帮助用户预测模型行为，通过反事实可模拟性评估，发现自我解释能提升预测准确性，但效果受扰动策略和评估者能力影响。


<details>
  <summary>Details</summary>
Motivation: LLM生成的自我解释可能不能真实反映模型的决策过程，但研究者想知道这些解释是否能帮助用户预测模型行为，即反事实可模拟性。

Method: 使用StrategyQA数据集，评估人类和LLM评估者在有无模型思维链或事后解释的情况下，预测模型对反事实后续问题答案的能力。比较LLM生成的反事实与基于语用学扰动的测试用例构建方法。

Result: 自我解释一致提高了LLM评估者和人类的模拟准确性，但提升程度和稳定性强烈依赖于扰动策略和评估者能力。人类自由文本理由的定性分析表明，访问解释帮助人类对扰动问题形成更准确的预测。

Conclusion: 尽管LLM自我解释可能不能真实反映内部决策过程，但它们确实能帮助用户更好地预测模型行为，特别是在反事实情境下，这对提高AI系统的可解释性和用户信任有重要意义。

Abstract: Large Language Models (LLMs) can produce verbalized self-explanations, yet prior studies suggest that such rationales may not reliably reflect the model's true decision process. We ask whether these explanations nevertheless help users predict model behavior, operationalized as counterfactual simulatability. Using StrategyQA, we evaluate how well humans and LLM judges can predict a model's answers to counterfactual follow-up questions, with and without access to the model's chain-of-thought or post-hoc explanations. We compare LLM-generated counterfactuals with pragmatics-based perturbations as alternative ways to construct test cases for assessing the potential usefulness of explanations. Our results show that self-explanations consistently improve simulation accuracy for both LLM judges and humans, but the degree and stability of gains depend strongly on the perturbation strategy and judge strength. We also conduct a qualitative analysis of free-text justifications written by human users when predicting the model's behavior, which provides evidence that access to explanations helps humans form more accurate predictions on the perturbed questions.

</details>


### [72] [Tracing the complexity profiles of different linguistic phenomena through the intrinsic dimension of LLM representations](https://arxiv.org/abs/2601.03779)
*Marco Baroni,Emily Cheng,Iria deDios-Flores,Francesca Franzon*

Main category: cs.CL

TL;DR: LLM表征的内在维度（ID）可作为语言复杂度的标记，能区分不同类型复杂度并揭示不同LLM中相似的语处理阶段。


<details>
  <summary>Details</summary>
Motivation: 探索LLM表征的内在维度（ID）是否能够作为语言复杂度的标记，特别是研究不同LLM层中的ID分布是否能够区分形式复杂度和功能复杂度。

Method: 通过分析LLM不同层中的内在维度（ID）来表征语言复杂度，使用表示相似性和层消融实验验证趋势，对比形式复杂度（多重并列或从属从句）和功能复杂度（右分支vs中心嵌入、明确vs模糊关系从句附着）的ID差异。

Result: 形式复杂度（多重并列或从属从句）在ID差异中表现明显，其起始时间与先前研究中确定的更抽象语言处理阶段对齐；功能复杂度（右分支vs中心嵌入、明确vs模糊关系从句附着）也能被ID捕捉，但不够显著，且不与相同处理阶段相关。

Conclusion: ID是LLM中语言复杂度的有用标记，能够区分不同类型的复杂度，并指向不同LLM中相似的语言处理阶段。

Abstract: We explore the intrinsic dimension (ID) of LLM representations as a marker of linguistic complexity, asking if different ID profiles across LLM layers differentially characterize formal and functional complexity. We find the formal contrast between sentences with multiple coordinated or subordinated clauses to be reflected in ID differences whose onset aligns with a phase of more abstract linguistic processing independently identified in earlier work. The functional contrasts between sentences characterized by right branching vs. center embedding or unambiguous vs. ambiguous relative clause attachment are also picked up by ID, but in a less marked way, and they do not correlate with the same processing phase. Further experiments using representational similarity and layer ablation confirm the same trends. We conclude that ID is a useful marker of linguistic complexity in LLMs, that it allows to differentiate between different types of complexity, and that it points to similar stages of linguistic processing across disparate LLMs.

</details>


### [73] [HearSay Benchmark: Do Audio LLMs Leak What They Hear?](https://arxiv.org/abs/2601.03783)
*Jin Wang,Liang Lin,Kaiwen Luo,Weiliu Wang,Yitian Chen,Moayad Aloqaily,Xuehai Tang,Zhenhong Zhou,Kun Wang,Li Sun,Qingsong Wen*

Main category: cs.CL

TL;DR: HearSay是首个系统评估音频大语言模型隐私泄露风险的基准，发现ALLMs能通过声纹高精度提取用户隐私属性，现有安全机制严重不足，思维链推理会进一步放大隐私风险。


<details>
  <summary>Details</summary>
Motivation: 音频大语言模型在理解和生成方面取得显著进展，但其潜在的隐私影响尚未得到充分研究。本文旨在首次系统性地探究ALLMs是否仅通过声学声纹就会无意中泄露用户隐私。

Method: 构建了HearSay基准数据集，包含超过22,000个真实世界音频片段，通过自动化分析和人工验证的严格流程确保数据质量。在多个ALLMs上进行广泛的实验评估。

Result: 1. 显著隐私泄露：ALLMs能高精度地从声纹中提取隐私属性，性别识别准确率达92.89%，并能有效分析社会属性。
2. 安全机制不足：现有防护措施严重不足，大多数模型对隐私侵犯请求的拒绝率接近零。
3. 推理放大风险：思维链推理在能力较强的模型中会进一步加剧隐私风险，揭示更深层次的声学相关性。

Conclusion: 研究揭示了ALLMs中存在的关键隐私漏洞，强调了针对性的隐私对齐的紧迫需求。提供了公开的代码和数据集以促进进一步研究。

Abstract: While Audio Large Language Models (ALLMs) have achieved remarkable progress in understanding and generation, their potential privacy implications remain largely unexplored. This paper takes the first step to investigate whether ALLMs inadvertently leak user privacy solely through acoustic voiceprints and introduces $\textit{HearSay}$, a comprehensive benchmark constructed from over 22,000 real-world audio clips. To ensure data quality, the benchmark is meticulously curated through a rigorous pipeline involving automated profiling and human verification, guaranteeing that all privacy labels are grounded in factual records. Extensive experiments on $\textit{HearSay}$ yield three critical findings: $\textbf{Significant Privacy Leakage}$: ALLMs inherently extract private attributes from voiceprints, reaching 92.89% accuracy on gender and effectively profiling social attributes. $\textbf{Insufficient Safety Mechanisms}$: Alarmingly, existing safeguards are severely inadequate; most models fail to refuse privacy-intruding requests, exhibiting near-zero refusal rates for physiological traits. $\textbf{Reasoning Amplifies Risk}$: Chain-of-Thought (CoT) reasoning exacerbates privacy risks in capable models by uncovering deeper acoustic correlations. These findings expose critical vulnerabilities in ALLMs, underscoring the urgent need for targeted privacy alignment. The codes and dataset are available at https://github.com/JinWang79/HearSay_Benchmark

</details>


### [74] [Membox: Weaving Topic Continuity into Long-Range Memory for LLM Agents](https://arxiv.org/abs/2601.03785)
*Dehao Tao,Guoliang Ma,Yongfeng Huang,Minghu Jiang*

Main category: cs.CL

TL;DR: Membox是一个分层记忆架构，通过Topic Loom监控对话并将相同主题的轮次分组为连贯的"记忆盒子"，再通过Trace Weaver链接形成长期事件时间线，显著提升LLM代理的时序推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理记忆系统遵循"碎片化-补偿"范式：先将对话流分解为孤立话语存储，然后通过基于嵌入的检索恢复连贯性。这种方法不可逆地破坏了叙述和因果流程，同时使检索偏向词汇相似性，无法有效保持对话中的主题连续性。

Method: 提出membox分层记忆架构，包含：1) Topic Loom：以滑动窗口方式持续监控对话，将连续相同主题的轮次分组为连贯的"记忆盒子"进行存储；2) Trace Weaver：将密封的盒子链接成长期事件时间线轨迹，恢复跨越不连续性的宏观主题重现。

Result: 在LoCoMo基准测试中，Membox在时序推理任务上实现了高达68%的F1改进，优于Mem0、A-MEM等基线方法。值得注意的是，Membox仅使用现有方法所需上下文令牌的一小部分就获得了这些收益，在效率和效果之间实现了优越平衡。

Conclusion: 通过显式建模主题连续性，Membox提供了一种认知激励的机制，用于增强LLM代理的连贯性和效率。该方法不仅显著提升了时序推理性能，还以更少的计算资源实现了这一目标，为LLM记忆系统设计提供了新方向。

Abstract: Human-agent dialogues often exhibit topic continuity-a stable thematic frame that evolves through temporally adjacent exchanges-yet most large language model (LLM) agent memory systems fail to preserve it. Existing designs follow a fragmentation-compensation paradigm: they first break dialogue streams into isolated utterances for storage, then attempt to restore coherence via embedding-based retrieval. This process irreversibly damages narrative and causal flow, while biasing retrieval towards lexical similarity. We introduce membox, a hierarchical memory architecture centered on a Topic Loom that continuously monitors dialogue in a sliding-window fashion, grouping consecutive same-topic turns into coherent "memory boxes" at storage time. Sealed boxes are then linked by a Trace Weaver into long-range event-timeline traces, recovering macro-topic recurrences across discontinuities. Experiments on LoCoMo demonstrate that Membox achieves up to 68% F1 improvement on temporal reasoning tasks, outperforming competitive baselines (e.g., Mem0, A-MEM). Notably, Membox attains these gains while using only a fraction of the context tokens required by existing methods, highlighting a superior balance between efficiency and effectiveness. By explicitly modeling topic continuity, Membox offers a cognitively motivated mechanism for enhancing both coherence and efficiency in LLM agents.

</details>


### [75] [Compact Example-Based Explanations for Language Models](https://arxiv.org/abs/2601.03786)
*Loris Schoenegger,Benjamin Roth*

Main category: cs.CL

TL;DR: 该论文提出了一种新的选择相关性评分方法，用于评估训练数据影响估计中选择策略的有效性，并发现常见选择策略常不如随机选择，进而提出了一种平衡影响力和代表性的新策略。


<details>
  <summary>Details</summary>
Motivation: 训练数据影响估计方法能够量化训练文档对模型输出的贡献，但由于人类无法解读数千个文档，只能选择少量文档作为解释。然而现有系统在评估时大多忽略了选择策略的重要性，而选择哪些文档会直接影响解释质量。

Method: 提出了一种新的选择相关性评分方法，这是一种无需重新训练的度量标准，用于量化一组示例对解释模型输出的有用性。通过微调实验验证该评分，并基于此提出了平衡影响力和代表性的选择策略。

Result: 验证了选择相关性评分能够预测一组示例是否支持或削弱模型的预测。发现常见选择策略通常表现不如随机选择。提出的平衡策略能够比简单选择排名最高的示例更好地利用选择预算。

Conclusion: 选择策略对训练数据影响估计的解释质量至关重要，提出的选择相关性评分和新策略能够有效提升解释效果，为基于示例的解释系统提供了更好的文档选择方法。

Abstract: Training data influence estimation methods quantify the contribution of training documents to a model's output, making them a promising source of information for example-based explanations. As humans cannot interpret thousands of documents, only a small subset of the training data can be presented as an explanation. Although the choice of which documents to include directly affects explanation quality, previous evaluations of such systems have largely ignored any selection strategies. To address this, we propose a novel selection relevance score, a retraining-free metric that quantifies how useful a set of examples is for explaining a model's output. We validate this score through fine-tuning experiments, confirming that it can predict whether a set of examples supports or undermines the model's predictions. Using this metric, we further show that common selection strategies often underperform random selection. Motivated by this finding, we propose a strategy that balances influence and representativeness, enabling better use of selection budgets than naively selecting the highest-ranking examples.

</details>


### [76] [NeoAMT: Neologism-Aware Agentic Machine Translation with Reinforcement Learning](https://arxiv.org/abs/2601.03790)
*Zhongtao Miao,Kaiyan Zhao,Masaaki Nagata,Yoshimasa Tsuruoka*

Main category: cs.CL

TL;DR: NeoAMT：一种基于Wiktionary搜索工具的智能框架，用于新词感知机器翻译，包含新数据集构建、强化学习训练和自适应生成方法。


<details>
  <summary>Details</summary>
Motivation: 新词感知机器翻译旨在翻译包含新词的源语句，但该领域相比通用机器翻译仍探索不足，需要专门的方法来处理新词翻译问题。

Method: 1. 创建覆盖16种语言、75个翻译方向的新词感知机器翻译数据集；2. 基于Wiktionary构建搜索工具；3. 使用强化学习训练翻译代理，包含新颖的奖励设计和基于"翻译难度"的自适应生成方法。

Result: 构建了包含约1000万条记录的Wiktionary数据集和300万条记录的检索语料库，提出了NeoAMT框架，通过强化学习训练翻译代理并评估新词感知机器翻译的准确性。

Conclusion: NeoAMT框架为处理新词翻译提供了一种有效的解决方案，通过结合Wiktionary知识和强化学习训练，能够提高翻译代理在新词感知机器翻译任务中的表现。

Abstract: Neologism-aware machine translation aims to translate source sentences containing neologisms into target languages. This field remains underexplored compared with general machine translation (MT). In this paper, we propose an agentic framework, NeoAMT, for neologism-aware machine translation using a Wiktionary search tool. Specifically, we first create a new dataset for neologism-aware machine translation and develop a search tool based on Wiktionary. The new dataset covers 16 languages and 75 translation directions and is derived from approximately 10 million records of an English Wiktionary dump. The retrieval corpus of the search tool is also constructed from around 3 million cleaned records of the Wiktionary dump. We then use it for training the translation agent with reinforcement learning (RL) and evaluating the accuracy of neologism-aware machine translation. Based on this, we also propose an RL training framework that contains a novel reward design and an adaptive rollout generation approach by leveraging "translation difficulty" to further improve the translation quality of translation agents using our search tool.

</details>


### [77] [Do LLMs Really Memorize Personally Identifiable Information? Revisiting PII Leakage with a Cue-Controlled Memorization Framework](https://arxiv.org/abs/2601.03791)
*Xiaoyu Luo,Yiyi Chen,Qiongxiu Li,Johannes Bjerva*

Main category: cs.CL

TL;DR: 论文提出CRM评估框架，通过控制提示词与目标PII的重叠线索，重新评估LLM中的PII泄露问题，发现先前报告的泄露主要源于线索驱动行为而非真实记忆。


<details>
  <summary>Details</summary>
Motivation: 现有研究常将LLM的PII重建成功解释为记忆证据，但作者认为这种评估可能受到提示诱导的泛化或模式完成影响，需要更严谨的评估方法来区分真实记忆与线索驱动行为。

Method: 提出"线索抵抗记忆"（CRM）评估框架，明确控制提示-目标重叠线索。在32种语言上进行大规模多语言重新评估，涵盖多种记忆范式：重建基设置（逐字前缀-后缀完成和关联重建）、无线索生成和成员推理。

Result: 当控制表面形式线索后，重建成功率显著下降；无线索生成和成员推理的真阳性率极低。先前报告的PII泄露主要源于直接表面形式线索驱动，而非真实记忆。

Conclusion: LLM中的PII泄露更多是线索驱动行为而非真实记忆，强调线索控制评估对可靠量化隐私相关记忆的重要性，为未来隐私评估提供了更严谨的方法论基础。

Abstract: Large Language Models (LLMs) have been reported to "leak" Personally Identifiable Information (PII), with successful PII reconstruction often interpreted as evidence of memorization. We propose a principled revision of memorization evaluation for LLMs, arguing that PII leakage should be evaluated under low lexical cue conditions, where target PII cannot be reconstructed through prompt-induced generalization or pattern completion. We formalize Cue-Resistant Memorization (CRM) as a cue-controlled evaluation framework and a necessary condition for valid memorization evaluation, explicitly conditioning on prompt-target overlap cues. Using CRM, we conduct a large-scale multilingual re-evaluation of PII leakage across 32 languages and multiple memorization paradigms. Revisiting reconstruction-based settings, including verbatim prefix-suffix completion and associative reconstruction, we find that their apparent effectiveness is driven primarily by direct surface-form cues rather than by true memorization. When such cues are controlled for, reconstruction success diminishes substantially. We further examine cue-free generation and membership inference, both of which exhibit extremely low true positive rates. Overall, our results suggest that previously reported PII leakage is better explained by cue-driven behavior than by genuine memorization, highlighting the importance of cue-controlled evaluation for reliably quantifying privacy-relevant memorization in LLMs.

</details>


### [78] [VietMed-MCQ: A Consistency-Filtered Data Synthesis Framework for Vietnamese Traditional Medicine Evaluation](https://arxiv.org/abs/2601.03792)
*Huynh Trung Kiet,Dao Sy Duy Minh,Nguyen Dinh Ha Duong,Le Hoang Minh Huy,Long Nguyen,Dien Dinh*

Main category: cs.CL

TL;DR: 研究者创建了越南传统医学多选题数据集VietMed-MCQ，用于评估大语言模型在专业医学领域的表现，发现具有中文先验知识的通用模型优于越南中心模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在通用医学领域表现出色，但在越南传统医学等专业文化特定领域表现显著下降，主要原因是缺乏高质量结构化基准数据集。

Method: 采用检索增强生成(RAG)管道生成多选题数据集，包含双重模型验证机制确保推理一致性，通过子串匹配进行证据检查，数据集包含3,190个问题覆盖三个难度级别，由一名医学专家和四名学生验证。

Result: 数据集获得94.2%的专家批准率，评分者间一致性高(Fleiss' kappa = 0.82)。对七个开源模型的测试显示，具有中文先验知识的通用模型优于越南中心模型，所有模型在复杂诊断推理方面仍有困难。

Conclusion: VietMed-MCQ填补了低资源医学领域基准的空白，展示了跨语言概念迁移的重要性，为越南传统医学的NLP研究提供了宝贵资源。

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in general medical domains. However, their performance significantly degrades in specialized, culturally specific domains such as Vietnamese Traditional Medicine (VTM), primarily due to the scarcity of high-quality, structured benchmarks. In this paper, we introduce VietMed-MCQ, a novel multiple-choice question dataset generated via a Retrieval-Augmented Generation (RAG) pipeline with an automated consistency check mechanism. Unlike previous synthetic datasets, our framework incorporates a dual-model validation approach to ensure reasoning consistency through independent answer verification, though the substring-based evidence checking has known limitations. The complete dataset of 3,190 questions spans three difficulty levels and underwent validation by one medical expert and four students, achieving 94.2 percent approval with substantial inter-rater agreement (Fleiss' kappa = 0.82). We benchmark seven open-source models on VietMed-MCQ. Results reveal that general-purpose models with strong Chinese priors outperform Vietnamese-centric models, highlighting cross-lingual conceptual transfer, while all models still struggle with complex diagnostic reasoning. Our code and dataset are publicly available to foster research in low-resource medical domains.

</details>


### [79] [Where meaning lives: Layer-wise accessibility of psycholinguistic features in encoder and decoder language models](https://arxiv.org/abs/2601.03798)
*Taisiia Tikhomirova,Dirk U. Wulff*

Main category: cs.CL

TL;DR: 对10个Transformer模型进行分层探测研究，发现意义表征的位置强烈依赖于方法选择，最终层很少是最佳表征层，但所有模型都共享词汇属性早于体验和情感维度的深度排序模式。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer语言模型在何处编码心理学上有意义的内容，对理论和实践都至关重要。需要系统地探究不同模型架构和方法如何影响意义的表征位置。

Method: 对10个Transformer模型（包括编码器专用和解码器专用架构）进行分层探测研究，涵盖58个心理语言学特征，比较三种嵌入提取方法：上下文嵌入和孤立嵌入。

Result: 意义表观定位强烈依赖于方法：上下文嵌入比孤立嵌入具有更高的特征选择性和不同的分层分布。所有模型和方法中，最终层很少是线性探测恢复心理语言学信息的最佳层。尽管存在差异，模型共享意义维度的深度排序模式：词汇属性在较早层达到峰值，而体验和情感维度在较晚层达到峰值。

Conclusion: Transformer模型中意义"存在"的位置反映了方法选择与架构约束之间的相互作用。研究结果强调了在解释模型内部表征时考虑方法因素的重要性。

Abstract: Understanding where transformer language models encode psychologically meaningful aspects of meaning is essential for both theory and practice. We conduct a systematic layer-wise probing study of 58 psycholinguistic features across 10 transformer models, spanning encoder-only and decoder-only architectures, and compare three embedding extraction methods. We find that apparent localization of meaning is strongly method-dependent: contextualized embeddings yield higher feature-specific selectivity and different layer-wise profiles than isolated embeddings. Across models and methods, final-layer representations are rarely optimal for recovering psycholinguistic information with linear probes. Despite these differences, models exhibit a shared depth ordering of meaning dimensions, with lexical properties peaking earlier and experiential and affective dimensions peaking later. Together, these results show that where meaning "lives" in transformer models reflects an interaction between methodological choices and architectural constraints.

</details>


### [80] [AI Generated Text Detection](https://arxiv.org/abs/2601.03812)
*Adilkhan Alikhanov,Aidar Amangeldi,Diar Demeubay,Dilnaz Akhmetzhan,Nurbek Moldakhmetov,Omar Polat,Galymzhan Zharas*

Main category: cs.CL

TL;DR: 该论文评估了AI文本检测方法，发现深度学习模型（特别是DistilBERT）在检测AI生成文本方面优于传统机器学习方法，强调了上下文语义建模的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速发展，学生越来越多地将AI生成的内容作为自己的作业提交，这违反了学术诚信。因此需要有效的AI文本检测方法来应对这一问题。

Method: 使用HC3和DAIGT v2两个数据集构建统一基准，采用基于主题的数据分割以防止信息泄露。评估了TF-IDF逻辑回归、BiLSTM分类器和DistilBERT等模型。

Result: TF-IDF逻辑回归准确率为82.87%，BiLSTM达到88.86%，DistilBERT达到88.11%但ROC-AUC最高为0.96，表现最佳。深度学习模型明显优于传统方法。

Conclusion: 上下文语义建模显著优于词汇特征，适当的评估协议对防止主题记忆很重要。未来计划扩展数据集多样性，使用LoRA等参数高效微调方法，并探索更小模型和硬件感知优化。

Abstract: The rapid development of large language models has led to an increase in AI-generated text, with students increasingly using LLM-generated content as their own work, which violates academic integrity. This paper presents an evaluation of AI text detection methods, including both traditional machine learning models and transformer-based architectures. We utilize two datasets, HC3 and DAIGT v2, to build a unified benchmark and apply a topic-based data split to prevent information leakage. This approach ensures robust generalization across unseen domains. Our experiments show that TF-IDF logistic regression achieves a reasonable baseline accuracy of 82.87%. However, deep learning models outperform it. The BiLSTM classifier achieves an accuracy of 88.86%, while DistilBERT achieves a similar accuracy of 88.11% with the highest ROC-AUC score of 0.96, demonstrating the strongest overall performance. The results indicate that contextual semantic modeling is significantly superior to lexical features and highlight the importance of mitigating topic memorization through appropriate evaluation protocols. The limitations of this work are primarily related to dataset diversity and computational constraints. In future work, we plan to expand dataset diversity and utilize parameter-efficient fine-tuning methods such as LoRA. We also plan to explore smaller or distilled models and employ more efficient batching strategies and hardware-aware optimization.

</details>


### [81] [Step Potential Advantage Estimation: Harnessing Intermediate Confidence and Correctness for Efficient Mathematical Reasoning](https://arxiv.org/abs/2601.03823)
*Fei Wu,Zhenrong Zhang,Qikai Chang,Jianshu Zhang,Quan Liu,Jun Du*

Main category: cs.CL

TL;DR: 提出SPAE方法，通过步骤级潜在信号改进RLVR中的优势估计，提高推理准确率并减少响应长度。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法缺乏语义基础的步骤级推理进展度量，导致LLMs无法区分必要推理和冗余验证，甚至可能将正确轨迹变为错误答案。

Method: 引入无训练探测机制提取中间置信度和正确性，组合成步骤潜在信号来估计每个步骤的推理状态。基于此提出步骤潜在优势估计（SPAE），进行细粒度信用分配。

Result: 在多个基准测试中，SPAE在提高准确率的同时显著减少响应长度，优于强RL基线和近期的高效推理及token级优势估计方法。

Conclusion: SPAE通过步骤级潜在信号改进了RLVR中的优势估计，实现了更精确的推理过程监督，解决了现有方法在区分必要推理和冗余验证方面的不足。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) elicits long chain-of-thought reasoning in large language models (LLMs), but outcome-based rewards lead to coarse-grained advantage estimation. While existing approaches improve RLVR via token-level entropy or sequence-level length control, they lack a semantically grounded, step-level measure of reasoning progress. As a result, LLMs fail to distinguish necessary deduction from redundant verification: they may continue checking after reaching a correct solution and, in extreme cases, overturn a correct trajectory into an incorrect final answer. To remedy the lack of process supervision, we introduce a training-free probing mechanism that extracts intermediate confidence and correctness and combines them into a Step Potential signal that explicitly estimates the reasoning state at each step. Building on this signal, we propose Step Potential Advantage Estimation (SPAE), a fine-grained credit assignment method that amplifies potential gains, penalizes potential drops, and applies penalty after potential saturates to encourage timely termination. Experiments across multiple benchmarks show SPAE consistently improves accuracy while substantially reducing response length, outperforming strong RL baselines and recent efficient reasoning and token-level advantage estimation methods. The code is available at https://github.com/cii030/SPAE-RL.

</details>


### [82] [Rethinking Table Pruning in TableQA: From Sequential Revisions to Gold Trajectory-Supervised Parallel Search](https://arxiv.org/abs/2601.03851)
*Yu Guo,Shenghao Ye,Shuangwu Chen,Zijian Wen,Tao Zhang,Qirui Bai,Dong Jin,Yunpeng Hou,Huasen He,Jian Yang,Xiaobin Tan*

Main category: cs.CL

TL;DR: TabTrim是一个新颖的表格剪枝框架，将表格剪枝从顺序修订转变为基于黄金轨迹监督的并行搜索，显著提升了表格问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有表格剪枝方法依赖顺序修订和不可靠的批判信号，往往无法检测答案关键数据的丢失，限制了表格问答性能。

Method: TabTrim通过黄金SQL查询执行过程中的中间子表生成黄金剪枝轨迹，训练剪枝器和验证器使逐步剪枝结果与黄金轨迹对齐，推理时采用并行搜索探索多个候选剪枝轨迹。

Result: TabTrim-8B达到73.5%的平均准确率，比最强基线高出3.2%，在WikiTQ上达到79.4%，在TableBench上达到61.2%，在多样化表格推理任务上实现了最先进性能。

Conclusion: TabTrim通过黄金轨迹监督的并行搜索方法有效解决了现有表格剪枝方法的局限性，显著提升了表格问答性能，为表格推理任务提供了更可靠的剪枝框架。

Abstract: Table Question Answering (TableQA) benefits significantly from table pruning, which extracts compact sub-tables by eliminating redundant cells to streamline downstream reasoning. However, existing pruning methods typically rely on sequential revisions driven by unreliable critique signals, often failing to detect the loss of answer-critical data. To address this limitation, we propose TabTrim, a novel table pruning framework which transforms table pruning from sequential revisions to gold trajectory-supervised parallel search. TabTrim derives a gold pruning trajectory using the intermediate sub-tables in the execution process of gold SQL queries, and trains a pruner and a verifier to make the step-wise pruning result align with the gold pruning trajectory. During inference, TabTrim performs parallel search to explore multiple candidate pruning trajectories and identify the optimal sub-table. Extensive experiments demonstrate that TabTrim achieves state-of-the-art performance across diverse tabular reasoning tasks: TabTrim-8B reaches 73.5% average accuracy, outperforming the strongest baseline by 3.2%, including 79.4% on WikiTQ and 61.2% on TableBench.

</details>


### [83] [What Does Loss Optimization Actually Teach, If Anything? Knowledge Dynamics in Continual Pre-training of LLMs](https://arxiv.org/abs/2601.03858)
*Seyed Mahed Mousavi,Simone Alghisi,Giuseppe Riccardi*

Main category: cs.CL

TL;DR: 持续预训练（CPT）中的损失优化与知识学习存在系统性偏离：损失单调下降，但事实学习不稳定且非单调，导致知识获取窗口窄且系统遗忘。


<details>
  <summary>Details</summary>
Motivation: 当前CPT实践将损失作为知识学习的代理指标，但缺乏对训练过程中知识如何变化的深入理解。研究者希望将CPT视为知识学习过程而非单纯的优化问题进行研究。

Method: 构建了分布匹配的事实文档基准，在CPT循环中直接插入诊断探针，实现epoch级别的知识获取动态和OOD通用技能变化的测量，并分析CPT如何重塑知识电路。

Result: 在三个指令调优LLM和多种CPT策略中，优化与学习系统性地偏离：损失单调下降，但事实学习不稳定、非单调；获取的事实很少被巩固；学习强烈依赖于先验暴露；OOD性能从早期epoch就开始退化。电路分析显示知识路径在epoch间快速重构。

Conclusion: 损失优化与CPT中的学习进展存在错位，需要基于任务级学习动态的停止标准来评估CPT训练。

Abstract: Continual Pre-Training (CPT) is widely used for acquiring and updating factual knowledge in LLMs. This practice treats loss as a proxy for knowledge learning, while offering no grounding into how it changes during training. We study CPT as a knowledge learning process rather than a solely optimization problem. We construct a controlled, distribution-matched benchmark of factual documents and interleave diagnostic probes directly into the CPT loop, enabling epoch-level measurement of knowledge acquisition dynamics and changes in Out-Of-Domain (OOD) general skills (e.g., math). We further analyze how CPT reshapes knowledge circuits during training. Across three instruction-tuned LLMs and multiple CPT strategies, optimization and learning systematically diverge as loss decreases monotonically while factual learning is unstable and non-monotonic. Acquired facts are rarely consolidated, learning is strongly conditioned on prior exposure, and OOD performance degrades from early epochs. Circuit analysis reveals rapid reconfiguration of knowledge pathways across epochs, providing an explanation for narrow acquisition windows and systematic forgetting. These results show that loss optimization is misaligned with learning progress in CPT and motivate evaluation of stopping criteria based on task-level learning dynamics.

</details>


### [84] [PartisanLens: A Multilingual Dataset of Hyperpartisan and Conspiratorial Immigration Narratives in European Media](https://arxiv.org/abs/2601.03860)
*Michele Joshua Maggini,Paloma Piot,Anxo Pérez,Erik Bran Marino,Lúa Santamaría Montesinos,Ana Lisboa,Marta Vázquez Abuín,Javier Parapar,Pablo Gamallo*

Main category: cs.CL

TL;DR: PartisanLens是一个多语言数据集，包含西班牙语、意大利语和葡萄牙语的1617条超党派新闻标题，标注了多个政治话语维度，用于检测超党派叙事和人口替代阴谋论。


<details>
  <summary>Details</summary>
Motivation: 超党派叙事和人口替代阴谋论对政治极化和公共安全构成严重威胁，但现有资源稀缺、英语中心化，且通常孤立分析不同政治话语维度而非作为相互关联的整体。

Method: 1) 创建PartisanLens多语言数据集；2) 评估主流LLM在分类任务上的表现；3) 评估LLM作为自动标注器的可行性；4) 探索LLM在模拟社会经济和意识形态视角下是否能复制人类标注模式。

Result: 建立了检测超党派和PRCT叙事的稳健基线，展示了LLM作为自动标注器的潜力和当前局限，并探索了LLM模拟人类标注模式的可能性。

Conclusion: PartisanLens为欧洲语境下检测党派和阴谋论叙事提供了重要资源，支持未来研究，并揭示了LLM在该领域的应用前景和局限性。

Abstract: Detecting hyperpartisan narratives and Population Replacement Conspiracy Theories (PRCT) is essential to addressing the spread of misinformation. These complex narratives pose a significant threat, as hyperpartisanship drives political polarisation and institutional distrust, while PRCTs directly motivate real-world extremist violence, making their identification critical for social cohesion and public safety. However, existing resources are scarce, predominantly English-centric, and often analyse hyperpartisanship, stance, and rhetorical bias in isolation rather than as interrelated aspects of political discourse. To bridge this gap, we introduce \textsc{PartisanLens}, the first multilingual dataset of \num{1617} hyperpartisan news headlines in Spanish, Italian, and Portuguese, annotated in multiple political discourse aspects. We first evaluate the classification performance of widely used Large Language Models (LLMs) on this dataset, establishing robust baselines for the classification of hyperpartisan and PRCT narratives. In addition, we assess the viability of using LLMs as automatic annotators for this task, analysing their ability to approximate human annotation. Results highlight both their potential and current limitations. Next, moving beyond standard judgments, we explore whether LLMs can emulate human annotation patterns by conditioning them on socio-economic and ideological profiles that simulate annotator perspectives. At last, we provide our resources and evaluation, \textsc{PartisanLens} supports future research on detecting partisan and conspiratorial narratives in European contexts.

</details>


### [85] [What Matters For Safety Alignment?](https://arxiv.org/abs/2601.03868)
*Xing Li,Hui-Ling Zhen,Lihao Yin,Xianzhi Yu,Zhenhua Dong,Mingxuan Yuan*

Main category: cs.CL

TL;DR: 对32个大型语言模型和语言推理模型的安全对齐能力进行大规模实证研究，评估内在特性和外部攻击技术的影响，发现推理和自反思机制对安全性的关键作用，同时揭示后训练和知识蒸馏可能降低安全性，以及文本补全接口的严重安全风险。


<details>
  <summary>Details</summary>
Motivation: 研究动机是系统评估LLMs和LRMs的安全对齐能力，为开发更安全可靠的AI系统提供关键见解。当前需要理解哪些因素影响模型的安全性，以及模型在面对各种攻击时的脆弱性。

Method: 采用大规模实证研究方法，评估32个近期流行的LLMs和LRMs，涵盖13个不同模型家族，参数规模从3B到235B。使用5个已建立的安全数据集，通过56种越狱技术和4种思维链攻击策略进行漏洞探测，共进行460万次API调用。系统研究6个关键内在模型特性和3种外部攻击技术的影响。

Result: 1. GPT-OSS-20B、Qwen3-Next-80B-A3B-Thinking和GPT-OSS-120B是最安全的三个模型，表明集成推理和自反思机制对安全对齐有显著优势。2. 后训练和知识蒸馏可能导致安全对齐系统性退化。3. 使用响应前缀的思维链攻击可将攻击成功率平均提升3.34倍，Seed-OSS-36B-Instruct的攻击成功率从0.6%升至96.3%，揭示文本补全接口的严重安全风险。4. 角色扮演、提示注入和基于梯度的对抗提示搜索是现代模型中引发未对齐行为的主要方法。

Conclusion: 安全必须作为后训练和知识蒸馏阶段的显式约束或核心优化目标，不能仅仅从属于通用能力追求。文本补全接口和允许用户定义响应前缀的功能存在严重安全风险，急需架构和部署层面的保护措施。推理和自反思机制对模型安全对齐至关重要。

Abstract: This paper presents a comprehensive empirical study on the safety alignment capabilities. We evaluate what matters for safety alignment in LLMs and LRMs to provide essential insights for developing more secure and reliable AI systems. We systematically investigate and compare the influence of six critical intrinsic model characteristics and three external attack techniques. Our large-scale evaluation is conducted using 32 recent, popular LLMs and LRMs across thirteen distinct model families, spanning a parameter scale from 3B to 235B. The assessment leverages five established safety datasets and probes model vulnerabilities with 56 jailbreak techniques and four CoT attack strategies, resulting in 4.6M API calls. Our key empirical findings are fourfold. First, we identify the LRMs GPT-OSS-20B, Qwen3-Next-80B-A3B-Thinking, and GPT-OSS-120B as the top-three safest models, which substantiates the significant advantage of integrated reasoning and self-reflection mechanisms for robust safety alignment. Second, post-training and knowledge distillation may lead to a systematic degradation of safety alignment. We thus argue that safety must be treated as an explicit constraint or a core optimization objective during these stages, not merely subordinated to the pursuit of general capability. Third, we reveal a pronounced vulnerability: employing a CoT attack via a response prefix can elevate the attack success rate by 3.34x on average and from 0.6% to 96.3% for Seed-OSS-36B-Instruct. This critical finding underscores the safety risks inherent in text-completion interfaces and features that allow user-defined response prefixes in LLM services, highlighting an urgent need for architectural and deployment safeguards. Fourth, roleplay, prompt injection, and gradient-based search for adversarial prompts are the predominant methodologies for eliciting unaligned behaviors in modern models.

</details>


### [86] [Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning](https://arxiv.org/abs/2601.03872)
*Jinyang Wu,Guocheng Zhai,Ruihan Jin,Jiahao Yuan,Yuhao Shen,Shuai Zhang,Zhengqi Wen,Jianhua Tao*

Main category: cs.CL

TL;DR: ATLAS提出了一种双路径框架，用于动态选择大型语言模型与工具的最佳组合，通过训练免费的集群路由和基于强化学习的多步路由，显著提升了跨领域复杂推理的性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLM和外部工具的多样性增加，选择最优的模型-工具组合成为高维优化挑战。现有方法通常依赖单一模型或固定工具调用逻辑，无法充分利用异构模型-工具对的性能差异。

Method: ATLAS采用双路径方法：1) 训练免费的基于集群的路由，利用经验先验实现领域特定对齐；2) 基于强化学习的多步路由，探索自主轨迹以实现分布外泛化。

Result: 在15个基准测试上的实验表明，该方法优于GPT-4o等闭源模型，在分布内任务上提升10.1%，在分布外任务上提升13.1%。在视觉推理方面也通过编排专业多模态工具取得了显著增益。

Conclusion: ATLAS框架通过动态工具使用和模型-工具对齐，有效解决了高维优化挑战，为跨领域复杂推理提供了有效的解决方案，并在多个基准测试中展现出优越性能。

Abstract: The integration of large language models (LLMs) with external tools has significantly expanded the capabilities of AI agents. However, as the diversity of both LLMs and tools increases, selecting the optimal model-tool combination becomes a high-dimensional optimization challenge. Existing approaches often rely on a single model or fixed tool-calling logic, failing to exploit the performance variations across heterogeneous model-tool pairs. In this paper, we present ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation), a dual-path framework for dynamic tool usage in cross-domain complex reasoning. ATLAS operates via a dual-path approach: (1) \textbf{training-free cluster-based routing} that exploits empirical priors for domain-specific alignment, and (2) \textbf{RL-based multi-step routing} that explores autonomous trajectories for out-of-distribution generalization. Extensive experiments across 15 benchmarks demonstrate that our method outperforms closed-source models like GPT-4o, surpassing existing routing methods on both in-distribution (+10.1%) and out-of-distribution (+13.1%) tasks. Furthermore, our framework shows significant gains in visual reasoning by orchestrating specialized multi-modal tools.

</details>


### [87] [Evaluating Small Decoder-Only Language Models for Grammar Correction and Text Simplification](https://arxiv.org/abs/2601.03874)
*Anthony Lamelas*

Main category: cs.CL

TL;DR: 小型语言模型在语法纠错和文本简化任务上效率高但性能不及大型语言模型，存在语义保持困难和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然性能强大，但规模大、计算成本高，难以在许多场景中访问、部署和保障安全。研究旨在探索小型解码器语言模型是否能成为语法纠错和文本简化任务的高效替代方案。

Method: 在JFLEG和ASSET数据集上测试小型语言模型的三种配置：开箱即用、微调后使用、以及顺序运行，使用已建立的评估指标进行性能评估。

Result: 小型语言模型虽然能学习某些行为，但性能仍低于强基线模型和当前的大型语言模型。它们在保持语义一致性和避免幻觉方面存在困难。

Conclusion: 尽管小型语言模型具有效率优势，但在改写任务上尚无法与现代大型语言模型竞争。需要通过进一步训练来缩小性能差距。

Abstract: Large language models have become extremely popular recently due to their ability to achieve strong performance on a variety of tasks, such as text generation and rewriting, but their size and computation cost make them difficult to access, deploy, and secure in many settings. This paper investigates whether small, decoder-only language models can provide an efficient alternative for the tasks of grammar correction and text simplification. The experiments in this paper focus on testing small language models out of the box, fine-tuned, and run sequentially on the JFLEG and ASSET datasets using established metrics. The results show that while SLMs may learn certain behaviors well, their performance remains below strong baselines and current LLMs. The results also show that SLMs struggle with retaining meaning and hallucinations. These findings suggest that despite their efficiency advantages, current SLMs are not yet competitive enough with modern LLMs for rewriting, and further advances in training are required for SLMs to close the performance gap between them and today's LLMs.

</details>


### [88] [Decide Then Retrieve: A Training-Free Framework with Uncertainty-Guided Triggering and Dual-Path Retrieval](https://arxiv.org/abs/2601.03908)
*Wang Chen,Guanqiang Qi,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.CL

TL;DR: DTR是一个无需训练的自适应检索增强生成框架，通过生成不确定性触发检索，采用双路径机制和自适应信息选择，减少不必要检索的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成方法存在两个主要问题：1) 不加区分地触发检索，引入噪声；2) 依赖单一路径证据构建，限制了性能提升。

Method: 提出DTR框架：1) 利用生成不确定性自适应决定何时需要检索；2) 引入双路径检索机制，结合自适应信息选择，更好地处理稀疏和模糊查询。

Result: 在五个开放域QA基准测试、多种模型规模和不同检索器上的实验表明，DTR在EM和F1指标上持续优于标准RAG和强基线方法，同时减少了不必要的检索。

Conclusion: DTR提供了一个无需训练的有效框架，能够自适应地管理检索过程，在减少计算开销的同时提升大语言模型在知识密集型任务中的性能。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating external knowledge, but existing approaches indiscriminately trigger retrieval and rely on single-path evidence construction, often introducing noise and limiting performance gains. In this work, we propose Decide Then Retrieve (DTR), a training-free framework that adaptively determines when retrieval is necessary and how external information should be selected. DTR leverages generation uncertainty to guide retrieval triggering and introduces a dual-path retrieval mechanism with adaptive information selection to better handle sparse and ambiguous queries. Extensive experiments across five open-domain QA benchmarks, multiple model scales, and different retrievers demonstrate that DTR consistently improves EM and F1 over standard RAG and strong retrieval-enhanced baselines, while reducing unnecessary retrievals. The code and data used in this paper are available at https://github.com/ChenWangHKU/DTR.

</details>


### [89] [When Models Decide and When They Bind: A Two-Stage Computation for Multiple-Choice Question-Answering](https://arxiv.org/abs/2601.03914)
*Hugh Mee Wong,Rick Nouwen,Albert Gatt*

Main category: cs.CL

TL;DR: 语言模型在多项选择题作答中采用两阶段机制：首先在内容空间选择正确答案，然后将胜出内容绑定到相应输出符号


<details>
  <summary>Details</summary>
Motivation: 多项选择题（MCQA）容易评估，但引入了元任务：模型既要解决问题，又要输出代表答案的符号，这混淆了推理错误和符号绑定失败。需要研究语言模型如何在内部实现MCQA

Method: 使用表征分析（PCA、线性探针）和因果干预来研究语言模型实现MCQA的内部机制。分析选项边界（换行）残差状态，进行胜出身份探测

Result: 发现选项边界残差状态包含与每个选项正确性相关的强线性可解码信号。胜出身份探测显示两阶段进展：胜出内容位置在最终选项处理后立即可解码，而输出符号在答案发射位置附近表示。符号和内容排列测试支持两阶段机制

Conclusion: 语言模型在MCQA中采用两阶段机制：首先在内容空间选择胜出者，然后将该胜出者绑定或路由到适当的符号进行输出

Abstract: Multiple-choice question answering (MCQA) is easy to evaluate but adds a meta-task: models must both solve the problem and output the symbol that *represents* the answer, conflating reasoning errors with symbol-binding failures. We study how language models implement MCQA internally using representational analyses (PCA, linear probes) as well as causal interventions. We find that option-boundary (newline) residual states often contain strong linearly decodable signals related to per-option correctness. Winner-identity probing reveals a two-stage progression: the winning *content position* becomes decodable immediately after the final option is processed, while the *output symbol* is represented closer to the answer emission position. Tests under symbol and content permutations support a two-stage mechanism in which models first select a winner in content space and then bind or route that winner to the appropriate symbol to emit.

</details>


### [90] [Doc-PP: Document Policy Preservation Benchmark for Large Vision-Language Models](https://arxiv.org/abs/2601.03926)
*Haeun Jang,Hwan Chang,Hwanhee Lee*

Main category: cs.CL

TL;DR: Doc-PP基准揭示LVLM在文档问答中存在推理诱导的安全漏洞，DVA框架通过解耦推理与策略验证显著提升政策合规性


<details>
  <summary>Details</summary>
Motivation: 现实中文档问答需要遵守动态的用户定义信息披露政策，但现有安全研究主要关注隐式社会规范或纯文本场景，忽略了多模态文档的复杂性

Method: 1) 构建Doc-PP基准，基于真实报告并要求在严格非披露政策下进行跨视觉和文本元素的推理；2) 提出DVA框架，将推理过程分解为分解、验证、聚合三个步骤，解耦推理与策略验证

Result: 发现系统性"推理诱导安全漏洞"：模型在需要复杂合成或跨模态聚合推理时频繁泄露敏感信息；提供提取文本虽改善感知但无意中促进泄漏；DVA框架显著优于标准提示防御方法

Conclusion: 多模态文档的政策合规理解需要专门的安全机制，DVA框架为政策合规的文档理解提供了稳健的基准解决方案，解耦推理与验证是关键

Abstract: The deployment of Large Vision-Language Models (LVLMs) for real-world document question answering is often constrained by dynamic, user-defined policies that dictate information disclosure based on context. While ensuring adherence to these explicit constraints is critical, existing safety research primarily focuses on implicit social norms or text-only settings, overlooking the complexities of multimodal documents. In this paper, we introduce Doc-PP (Document Policy Preservation Benchmark), a novel benchmark constructed from real-world reports requiring reasoning across heterogeneous visual and textual elements under strict non-disclosure policies. Our evaluation highlights a systemic Reasoning-Induced Safety Gap: models frequently leak sensitive information when answers must be inferred through complex synthesis or aggregated across modalities, effectively circumventing existing safety constraints. Furthermore, we identify that providing extracted text improves perception but inadvertently facilitates leakage. To address these vulnerabilities, we propose DVA (Decompose-Verify-Aggregation), a structural inference framework that decouples reasoning from policy verification. Experimental results demonstrate that DVA significantly outperforms standard prompting defenses, offering a robust baseline for policy-compliant document understanding

</details>


### [91] [Large-Scale Aspect-Based Sentiment Analysis with Reasoning-Infused LLMs](https://arxiv.org/abs/2601.03940)
*Paweł Liskowski,Krzysztof Jankowski*

Main category: cs.CL

TL;DR: Arctic-ABSA是用于现实场景情感分析的强大模型集合，通过扩展情感类别、联合预测整体情感、支持多语言和推理注入技术，在性能和泛化能力上超越GPT-4o和Claude 3.5 Sonnet，并发布大规模基准数据集ABSA-mix。


<details>
  <summary>Details</summary>
Motivation: 现有ABSA模型通常只处理三种基本情感类别，无法满足商业应用中更复杂的情感分析需求，且在跨语言和泛化能力上存在局限。需要开发能处理混合情感、未知情感，并能联合分析整体文本情感的多语言模型。

Method: 1) 使用大规模公开数据和精心生成的合成数据训练，数据集规模是SemEval14的20倍；2) 将情感类别从传统的3类扩展到5类（正、负、中、混合、未知）；3) 联合预测整体文本情感；4) 支持多语言处理；5) 通过CoT微调和新型推理预训练技术提升模型推理能力；6) 开发395M参数编码器和8B参数解码器架构。

Result: 1) 在SemEval14基准上达到新的SOTA结果；2) 比GPT-4o和Claude 3.5 Sonnet准确率高出最多10个百分点；3) 单个多语言模型在6种语言上保持87-91%的准确率，且不降低英语性能；4) 发布ABSA-mix基准，聚合了17个公开ABSA数据集，涵盖92个领域。

Conclusion: Arctic-ABSA通过扩展情感类别、多语言支持、推理注入和大规模数据训练，显著提升了ABSA任务的性能。模型在准确率、泛化能力和多语言处理方面均超越现有先进模型，为商业应用提供了更实用的解决方案，同时发布的ABSA-mix基准将促进该领域的研究发展。

Abstract: We introduce Arctic-ABSA, a collection of powerful models for real-life aspect-based sentiment analysis (ABSA). Our models are tailored to commercial needs, trained on a large corpus of public data alongside carefully generated synthetic data, resulting in a dataset 20 times larger than SemEval14. We extend typical ABSA models by expanding the number of sentiment classes from the standard three (positive, negative, neutral) to five, adding mixed and unknown classes, while also jointly predicting overall text sentiment and supporting multiple languages. We experiment with reasoning injection by fine-tuning on Chain-of-Thought (CoT) examples and introduce a novel reasoning pretraining technique for encoder-only models that significantly improves downstream fine-tuning and generalization. Our 395M-parameter encoder and 8B-parameter decoder achieve up to 10 percentage points higher accuracy than GPT-4o and Claude 3.5 Sonnet, while setting new state-of-the-art results on the SemEval14 benchmark. A single multilingual model maintains 87-91% accuracy across six languages without degrading English performance. We release ABSA-mix, a large-scale benchmark aggregating 17 public ABSA datasets across 92 domains.

</details>


### [92] [RADAR: Retrieval-Augmented Detector with Adversarial Refinement for Robust Fake News Detection](https://arxiv.org/abs/2601.03981)
*Song-Duo Ma,Yi-Hung Liu,Hsin-Yu Lin,Pin-Yu Chen,Hong-Yan Huang,Shau-Yung Hsu,Yun-Nung Chen*

Main category: cs.CL

TL;DR: RADAR：一个用于虚假新闻检测的检索增强对抗性检测器，通过生成器创建对抗性样本，检测器利用检索验证，结合自然语言对抗反馈实现协同进化。


<details>
  <summary>Details</summary>
Motivation: 有效应对LLM生成的虚假信息传播，需要更鲁棒的检测方法。现有方法在面对不断进化的虚假信息生成技术时存在局限性。

Method: 1. 生成器：通过对真实文章进行事实扰动来改写，创建对抗性样本
2. 检测器：轻量级检测器，利用密集段落检索验证声明
3. 语言对抗反馈：提供结构化自然语言批评而非标量奖励，引导生成器进行更复杂的规避尝试，迫使检测器适应和改进

Result: 在虚假新闻检测基准测试中，RADAR达到86.98% ROC-AUC，显著优于基于检索的通用LLM。消融研究表明：检测器端检索带来最大增益，语言对抗反馈和少样本演示为鲁棒训练提供关键信号。

Conclusion: RADAR通过生成器-检测器协同进化和语言对抗反馈，实现了对LLM生成虚假信息的有效检测，为对抗性虚假新闻检测提供了新框架。

Abstract: To efficiently combat the spread of LLM-generated misinformation, we present RADAR, a retrieval-augmented detector with adversarial refinement for robust fake news detection. Our approach employs a generator that rewrites real articles with factual perturbations, paired with a lightweight detector that verifies claims using dense passage retrieval. To enable effective co-evolution, we introduce verbal adversarial feedback (VAF). Rather than relying on scalar rewards, VAF issues structured natural-language critiques; these guide the generator toward more sophisticated evasion attempts, compelling the detector to adapt and improve. On a fake news detection benchmark, RADAR achieves 86.98% ROC-AUC, significantly outperforming general-purpose LLMs with retrieval. Ablation studies confirm that detector-side retrieval yields the largest gains, while VAF and few-shot demonstrations provide critical signals for robust training.

</details>


### [93] [Benchmark^2: Systematic Evaluation of LLM Benchmarks](https://arxiv.org/abs/2601.03986)
*Qi Qian,Chengsong Huang,Jingwen Xu,Changze Lv,Muling Wu,Wenhao Liu,Xiaohua Wang,Zhenghua Wang,Zisu Huang,Muzhao Tian,Jianhan Xu,Kun Hu,He-Da Wang,Yao Hu,Xuanjing Huang,Xiaoqing Zheng*

Main category: cs.CL

TL;DR: 提出了Benchmark^2框架，包含三个互补指标来系统评估LLM基准质量，发现现有基准存在显著质量差异，并证明基于这些指标的筛选构建方法可以用更少测试集实现可比评估性能。


<details>
  <summary>Details</summary>
Motivation: 随着评估大语言模型（LLM）的基准测试快速增加，迫切需要系统方法来评估基准本身的质量。

Method: 提出了Benchmark^2框架，包含三个互补指标：1) 跨基准排名一致性，衡量基准产生的模型排名是否与同类基准一致；2) 可区分性分数，量化基准区分模型的能力；3) 能力对齐偏差，识别同一模型家族中更强模型失败而较弱模型成功的异常实例。

Result: 在涵盖数学、推理和知识领域的15个基准上对11个LLM（跨越四个模型家族）进行广泛实验，发现现有基准存在显著质量差异，并证明基于这些指标的筛选构建方法可以用更少测试集实现可比评估性能。

Conclusion: Benchmark^2为系统评估LLM基准质量提供了有效框架，能够识别基准质量问题并指导更高效的基准构建。

Abstract: The rapid proliferation of benchmarks for evaluating large language models (LLMs) has created an urgent need for systematic methods to assess benchmark quality itself. We propose Benchmark^2, a comprehensive framework comprising three complementary metrics: (1) Cross-Benchmark Ranking Consistency, measuring whether a benchmark produces model rankings aligned with peer benchmarks; (2) Discriminability Score, quantifying a benchmark's ability to differentiate between models; and (3) Capability Alignment Deviation, identifying problematic instances where stronger models fail but weaker models succeed within the same model family. We conduct extensive experiments across 15 benchmarks spanning mathematics, reasoning, and knowledge domains, evaluating 11 LLMs across four model families. Our analysis reveals significant quality variations among existing benchmarks and demonstrates that selective benchmark construction based on our metrics can achieve comparable evaluation performance with substantially reduced test sets.

</details>


### [94] [VotIE: Information Extraction from Meeting Minutes](https://arxiv.org/abs/2601.03997)
*José Pedro Evans,Luís Filipe Cunha,Purificação Silvano,Alípio Jorge,Nuno Guimarães,Sérgio Nunes,Ricardo Campos*

Main category: cs.CL

TL;DR: 本文介绍了VotIE任务，用于从市政会议纪要中提取结构化投票事件，建立了葡萄牙市政纪要基准，比较了微调编码器与生成式LLM的性能差异。


<details>
  <summary>Details</summary>
Motivation: 市政会议纪要记录地方民主决策过程，但与标准化格式的议会记录不同，它们以高度异质、自由叙述的文本形式编码投票结果，各市政间差异巨大，给自动化提取带来重大挑战。

Method: 提出VotIE（投票信息提取）新任务，基于CitiLink语料库建立葡萄牙市政纪要基准。实验比较微调编码器（XLM-R-CRF）与生成式大语言模型的性能，分别在领域内评估和跨市政迁移设置下进行测试。

Result: 1) 领域内评估中，微调编码器表现最佳，XLM-R-CRF达到93.2%宏F1，优于生成式方法；2) 跨市政迁移设置中，微调模型性能大幅下降，而少样本LLM表现出更强鲁棒性，性能下降较小。但生成式模型计算成本高，限制了实用性。

Conclusion: 尽管生成式LLM在泛化能力上有优势，但其高计算成本限制了实际应用。轻量级微调编码器在大规模实际部署中仍是更实用的选择。作者公开了基准、训练模型和评估框架以支持可复现研究。

Abstract: Municipal meeting minutes record key decisions in local democratic processes. Unlike parliamentary proceedings, which typically adhere to standardized formats, they encode voting outcomes in highly heterogeneous, free-form narrative text that varies widely across municipalities, posing significant challenges for automated extraction. In this paper, we introduce VotIE (Voting Information Extraction), a new information extraction task aimed at identifying structured voting events in narrative deliberative records, and establish the first benchmark for this task using Portuguese municipal minutes, building on the recently introduced CitiLink corpus. Our experiments yield two key findings. First, under standard in-domain evaluation, fine-tuned encoders, specifically XLM-R-CRF, achieve the strongest performance, reaching 93.2\% macro F1, outperforming generative approaches. Second, in a cross-municipality setting that evaluates transfer to unseen administrative contexts, these models suffer substantial performance degradation, whereas few-shot LLMs demonstrate greater robustness, with significantly smaller declines in performance. Despite this generalization advantage, the high computational cost of generative models currently constrains their practicality. As a result, lightweight fine-tuned encoders remain a more practical option for large-scale, real-world deployment. To support reproducible research in administrative NLP, we publicly release our benchmark, trained models, and evaluation framework.

</details>


### [95] [Simulated Students in Tutoring Dialogues: Substance or Illusion?](https://arxiv.org/abs/2601.04025)
*Alexander Scarlatos,Jaewook Lee,Simon Woodhead,Andrew Lan*

Main category: cs.CL

TL;DR: 该论文研究了LLM模拟学生的质量评估问题，提出了全面的评估指标，并测试了多种模拟方法，发现现有方法效果有限。


<details>
  <summary>Details</summary>
Motivation: LLM在教育领域的应用需要真实学生进行评估，但这耗时且难以规模化。虽然许多研究使用模拟学生进行训练和评估，但缺乏对模拟学生质量的系统评估和测量。

Method: 1. 正式定义学生模拟任务；2. 提出涵盖语言、行为和认知维度的评估指标；3. 在真实数学辅导对话数据集上对多种学生模拟方法进行基准测试，包括提示策略、监督微调和偏好优化等方法。

Result: 自动评估和人工评估结果显示：提示策略效果较差；监督微调和偏好优化方法表现更好，但仍然有限。这表明确实需要改进模拟学生的质量。

Conclusion: 学生模拟是一个具有挑战性的任务，现有方法效果有限，需要未来进一步研究改进LLM模拟学生的质量。

Abstract: Advances in large language models (LLMs) enable many new innovations in education. However, evaluating the effectiveness of new technology requires real students, which is time-consuming and hard to scale up. Therefore, many recent works on LLM-powered tutoring solutions have used simulated students for both training and evaluation, often via simple prompting. Surprisingly, little work has been done to ensure or even measure the quality of simulated students. In this work, we formally define the student simulation task, propose a set of evaluation metrics that span linguistic, behavioral, and cognitive aspects, and benchmark a wide range of student simulation methods on these metrics. We experiment on a real-world math tutoring dialogue dataset, where both automated and human evaluation results show that prompting strategies for student simulation perform poorly; supervised fine-tuning and preference optimization yield much better but still limited performance, motivating future work on this challenging task.

</details>


### [96] [SpeakerSleuth: Evaluating Large Audio-Language Models as Judges for Multi-turn Speaker Consistency](https://arxiv.org/abs/2601.04029)
*Jonggeun Lee,Junseong Pyo,Gyuhyeon Seo,Yohan Jo*

Main category: cs.CL

TL;DR: 研究发现大型音频语言模型在评估多轮对话中的说话人一致性方面存在严重缺陷，主要倾向于优先考虑文本而非声学特征。


<details>
  <summary>Details</summary>
Motivation: 尽管大型音频语言模型已被广泛用作语音生成质量的评估工具，但其在多轮对话中评估说话人一致性的能力尚未被探索。研究者希望了解这些模型是否能可靠地判断多轮对话中的说话人一致性。

Method: 提出了SpeakerSleuth基准测试，包含三个反映真实需求的任务。构建了1,818个人工验证的评估实例，涵盖四个不同数据集（合成和真实语音），并控制声学难度。评估了九个广泛使用的大型音频语言模型。

Result: 模型在检测声学不一致性方面表现不佳：有些模型过度预测不一致，有些则过于宽松；难以精确定位有问题的对话轮次；当提供其他对话者的轮次时，性能显著下降，模型优先考虑文本连贯性而非声学线索，甚至无法检测明显的性别切换。但在从多个声学变体中选择最佳匹配说话人的音频时表现较好，显示出固有的声学辨别能力。

Conclusion: 大型音频语言模型存在显著偏见：倾向于优先考虑文本而非声学特征，揭示了需要解决的基本模态不平衡问题，以构建可靠的音频语言评估模型。

Abstract: Large Audio-Language Models (LALMs) as judges have emerged as a prominent approach for evaluating speech generation quality, yet their ability to assess speaker consistency across multi-turn conversations remains unexplored. We present SpeakerSleuth, a benchmark evaluating whether LALMs can reliably judge speaker consistency in multi-turn dialogues through three tasks reflecting real-world requirements. We construct 1,818 human-verified evaluation instances across four diverse datasets spanning synthetic and real speech, with controlled acoustic difficulty. Evaluating nine widely-used LALMs, we find that models struggle to reliably detect acoustic inconsistencies. For instance, given audio samples of the same speaker's turns, some models overpredict inconsistency, whereas others are overly lenient. Models further struggle to identify the exact turns that are problematic. When other interlocutors' turns are provided together, performance degrades dramatically as models prioritize textual coherence over acoustic cues, failing to detect even obvious gender switches for a speaker. On the other hand, models perform substantially better in choosing the audio that best matches the speaker among several acoustic variants, demonstrating inherent acoustic discrimination capabilities. These findings expose a significant bias in LALMs: they tend to prioritize text over acoustics, revealing fundamental modality imbalances that need to be addressed to build reliable audio-language judges.

</details>


### [97] [Analyzing and Improving Cross-lingual Knowledge Transfer for Machine Translation](https://arxiv.org/abs/2601.04036)
*David Stap*

Main category: cs.CL

TL;DR: 该论文研究多语言神经模型中的跨语言知识迁移，提出多种方法提升低资源语言翻译的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 多语言机器翻译系统面临跨语言表示学习的挑战，特别是低资源语言由于平行数据有限，限制了泛化和迁移能力。需要理解多语言模型如何跨语言共享知识，以及表示、数据可用性和训练策略之间的相互作用。

Method: 1) 分析语言相似性对迁移的影响；2) 使用检索和辅助监督增强低资源翻译；3) 研究大规模语言模型在平行数据微调中引入的意外权衡；4) 分析训练中语言多样性的作用；5) 增加翻译覆盖范围以改善泛化并减少离目标行为。

Result: 研究揭示了建模选择和数据组成如何塑造多语言学习，表明增加翻译覆盖范围可以改善泛化并减少离目标行为，为更包容和鲁棒的多语言NLP系统提供了见解。

Conclusion: 该论文强调了建模选择和数据组成对多语言学习的重要影响，为提高多语言NLP系统的包容性和鲁棒性提供了理论基础和方法指导。

Abstract: Multilingual machine translation systems aim to make knowledge accessible across languages, yet learning effective cross-lingual representations remains challenging. These challenges are especially pronounced for low-resource languages, where limited parallel data constrains generalization and transfer. Understanding how multilingual models share knowledge across languages requires examining the interaction between representations, data availability, and training strategies. In this thesis, we study cross-lingual knowledge transfer in neural models and develop methods to improve robustness and generalization in multilingual settings, using machine translation as a central testbed. We analyze how similarity between languages influences transfer, how retrieval and auxiliary supervision can strengthen low-resource translation, and how fine-tuning on parallel data can introduce unintended trade-offs in large language models. We further examine the role of language diversity during training and show that increasing translation coverage improves generalization and reduces off-target behavior. Together, this work highlights how modeling choices and data composition shape multilingual learning and offers insights toward more inclusive and resilient multilingual NLP systems.

</details>


### [98] [When Helpers Become Hazards: A Benchmark for Analyzing Multimodal LLM-Powered Safety in Daily Life](https://arxiv.org/abs/2601.04043)
*Xinyue Lou,Jinan Xu,Jingyi Yin,Xiaolong Wang,Zhaolu Kang,Youwei Liao,Yixuan Wang,Xiangyu Shi,Fengran Mo,Su Yao,Kaiyu Huang*

Main category: cs.CL

TL;DR: SaLAD是一个多模态安全基准，包含2,013个真实世界图像-文本样本，涵盖10个常见类别，用于评估MLLMs对日常行为安全的影响。研究表明当前顶尖模型在危险查询上的安全响应率仅为57.2%。


<details>
  <summary>Details</summary>
Motivation: 随着MLLMs成为人类生活中不可或缺的助手，它们生成的不安全内容对人类行为构成威胁。目前缺乏评估MLLMs对日常行为安全影响的基准，特别是需要真实视觉输入和细粒度跨模态推理的安全风险场景。

Method: 提出了SaLAD多模态安全基准，包含2,013个真实世界图像-文本样本，涵盖10个常见类别，平衡设计了不安全场景和过度敏感案例。采用基于安全警告的评估框架，鼓励模型提供清晰的信息性安全警告而非通用拒绝。

Result: 在18个MLLMs上的评估显示，表现最佳的模型在不安全查询上的安全响应率仅为57.2%。流行的安全对齐方法在SaLAD场景下效果有限，揭示了当前MLLMs在识别日常危险行为方面的脆弱性。

Conclusion: SaLAD基准揭示了当前MLLMs在识别日常危险行为方面存在严重不足，即使经过安全对齐的模型也表现不佳。这表明需要更强大的多模态安全评估和改进方法。

Abstract: As Multimodal Large Language Models (MLLMs) become an indispensable assistant in human life, the unsafe content generated by MLLMs poses a danger to human behavior, perpetually overhanging human society like a sword of Damocles. To investigate and evaluate the safety impact of MLLMs responses on human behavior in daily life, we introduce SaLAD, a multimodal safety benchmark which contains 2,013 real-world image-text samples across 10 common categories, with a balanced design covering both unsafe scenarios and cases of oversensitivity. It emphasizes realistic risk exposure, authentic visual inputs, and fine-grained cross-modal reasoning, ensuring that safety risks cannot be inferred from text alone. We further propose a safety-warning-based evaluation framework that encourages models to provide clear and informative safety warnings, rather than generic refusals. Results on 18 MLLMs demonstrate that the top-performing models achieve a safe response rate of only 57.2% on unsafe queries. Moreover, even popular safety alignment methods limit effectiveness of the models in our scenario, revealing the vulnerabilities of current MLLMs in identifying dangerous behaviors in daily life. Our dataset is available at https://github.com/xinyuelou/SaLAD.

</details>


### [99] [Modular Prompt Optimization: Optimizing Structured Prompts with Section-Local Textual Gradients](https://arxiv.org/abs/2601.04055)
*Prith Sharma,Austin Z. Henley*

Main category: cs.CL

TL;DR: MPO是一种基于模式的提示优化框架，将提示视为结构化对象，通过局部文本梯度独立优化各个语义部分，提升小型开源语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法将提示视为单一文本块，难以定位错误、保留关键指令或控制提示长度增长。需要一种能够保持提示结构完整性的优化方法。

Method: 提出模块化提示优化(MPO)框架，将提示分解为固定语义部分（系统角色、上下文、任务描述、约束、输出格式），对每个部分独立应用局部文本梯度优化，通过去重整合更新，保持整体模式固定。

Result: 在ARC-Challenge和MMLU基准测试中，使用LLaMA-3 8B-Instruct和Mistral-7B-Instruct模型，MPO始终优于未优化的结构化提示和TextGrad基线，在不修改模型参数或改变提示结构的情况下显著提升准确率。

Conclusion: 保持固定提示模式的同时应用局部、分部分的优化是提升小型开源语言模型推理性能的有效实用方法，MPO框架提供了可解释且鲁棒的优化过程。

Abstract: Prompt quality plays a central role in controlling the behavior, reliability, and reasoning performance of large language models (LLMs), particularly for smaller open-source instruction-tuned models that depend heavily on explicit structure. While recent work has explored automatic prompt optimization using textual gradients and self-refinement, most existing methods treat prompts as monolithic blocks of text, making it difficult to localize errors, preserve critical instructions, or prevent uncontrolled prompt growth. We introduce Modular Prompt Optimization (MPO), a schema-based prompt optimization framework that treats prompts as structured objects composed of fixed semantic sections, including system role, context, task description, constraints, and output format. MPO applies section-local textual gradients, generated by a critic language model, to refine each section independently while keeping the overall prompt schema fixed. Section updates are consolidated through de-duplication to reduce redundancy and interference between components, yielding an interpretable and robust optimization process. We evaluate MPO on two reasoning benchmarks, ARC-Challenge and MMLU, using LLaMA-3 8B-Instruct and Mistral-7B-Instruct as solver models. Across both benchmarks and models, MPO consistently outperforms an untuned structured prompt and the TextGrad baseline, achieving substantial accuracy gains without modifying model parameters or altering prompt structure. These results demonstrate that maintaining a fixed prompt schema while applying localized, section-wise optimization is an effective and practical approach for improving reasoning performance in small open-source LMs.

</details>


### [100] [Bridging the Discrete-Continuous Gap: Unified Multimodal Generation via Coupled Manifold Discrete Absorbing Diffusion](https://arxiv.org/abs/2601.04056)
*Yuanfeng Xu,Yuhao Chen,Liang Lin,Guangrun Wang*

Main category: cs.CL

TL;DR: CoM-DAD提出了一种分层双过程的概率框架，将多模态生成解耦为高层语义规划和低层标记合成，通过连续潜在扩散和离散吸收扩散过程实现统一文本-图像生成。


<details>
  <summary>Details</summary>
Motivation: 生成建模在离散数据（文本）的自回归方法和连续数据（图像）的扩散方法之间存在分歧，阻碍了真正统一的多模态系统发展。掩码语言模型虽然提供高效双向上下文，但缺乏自回归模型的生成保真度和扩散模型的语义连续性，且在多模态设置中存在严重对齐挑战和训练不稳定问题。

Method: CoM-DAD将多模态生成建模为分层双过程：1）通过连续潜在扩散过程建模语义流形；2）将标记生成作为离散吸收扩散过程，由可变速率噪声调度调节，并以这些演化的语义先验为条件。引入随机混合模态传输策略，无需重型对比双编码器即可对齐不同模态。

Result: 该方法相比标准掩码建模展现出优越的稳定性，为可扩展的统一文本-图像生成建立了新范式。

Conclusion: CoM-DAD通过解耦语义规划和标记合成，解决了多模态生成中的对齐和稳定性问题，为统一生成建模提供了创新框架。

Abstract: The bifurcation of generative modeling into autoregressive approaches for discrete data (text) and diffusion approaches for continuous data (images) hinders the development of truly unified multimodal systems. While Masked Language Models (MLMs) offer efficient bidirectional context, they traditionally lack the generative fidelity of autoregressive models and the semantic continuity of diffusion models. Furthermore, extending masked generation to multimodal settings introduces severe alignment challenges and training instability. In this work, we propose \textbf{CoM-DAD} (\textbf{Co}upled \textbf{M}anifold \textbf{D}iscrete \textbf{A}bsorbing \textbf{D}iffusion), a novel probabilistic framework that reformulates multimodal generation as a hierarchical dual-process. CoM-DAD decouples high-level semantic planning from low-level token synthesis. First, we model the semantic manifold via a continuous latent diffusion process; second, we treat token generation as a discrete absorbing diffusion process, regulated by a \textbf{Variable-Rate Noise Schedule}, conditioned on these evolving semantic priors. Crucially, we introduce a \textbf{Stochastic Mixed-Modal Transport} strategy that aligns disparate modalities without requiring heavy contrastive dual-encoders. Our method demonstrates superior stability over standard masked modeling, establishing a new paradigm for scalable, unified text-image generation.

</details>


### [101] [KDCM: Reducing Hallucination in LLMs through Explicit Reasoning Structures](https://arxiv.org/abs/2601.04086)
*Jinbo Hao,Kai Yang,Qingzhen Su,Yifan Li,Chao Jiang*

Main category: cs.CL

TL;DR: 提出一个代码引导的推理框架，通过编程模块指导知识图谱探索，减少LLM提示引发的幻觉


<details>
  <summary>Details</summary>
Motivation: 大语言模型容易产生幻觉，特别是由提示引发的错误，需要一种方法来缓解这个问题

Method: 扩展链式知识蒸馏方法，嵌入可执行代码模块到推理提示中，指导知识图谱探索，明确调控中间推理步骤

Result: 在多个公开基准测试中，代码引导推理显著改善上下文建模并减少提示引发的幻觉，HIT@1/3/5分别提升15.64%、13.38%、13.28%，多个评估设置下得分超过95%

Conclusion: 该方法有效约束错误推理，同时提高准确性和可解释性，为缓解LLM幻觉提供了有效解决方案

Abstract: To mitigate hallucinations in large language models (LLMs), we propose a framework that focuses on errors induced by prompts. Our method extends a chain-style knowledge distillation approach by incorporating a programmable module that guides knowledge graph exploration. This module is embedded as executable code within the reasoning prompt, allowing the model to leverage external structured knowledge during inference. Based on this design, we develop an enhanced distillation-based reasoning framework that explicitly regulates intermediate reasoning steps, resulting in more reliable predictions. We evaluate the proposed approach on multiple public benchmarks using GPT-4 and LLaMA-3.3. Experimental results show that code-guided reasoning significantly improves contextual modeling and reduces prompt-induced hallucinations. Specifically, HIT@1, HIT@3, and HIT@5 increase by 15.64%, 13.38%, and 13.28%, respectively, with scores exceeding 95% across several evaluation settings. These findings indicate that the proposed method effectively constrains erroneous reasoning while improving both accuracy and interpretability.

</details>


### [102] [SearchAttack: Red-Teaming LLMs against Real-World Threats via Framing Unsafe Web Information-Seeking Tasks](https://arxiv.org/abs/2601.04093)
*Yu Yan,Sheng Sun,Mingfeng Li,Zheming Yang,Chiwei Zhu,Fei Ma,Benfeng Xu,Min Liu*

Main category: cs.CL

TL;DR: SearchAttack：一种针对检索增强LLM的红队攻击方法，通过将有害语义外包给网络搜索，仅保留查询骨架和碎片线索，引导LLM重构检索内容来实现恶意目标。


<details>
  <summary>Details</summary>
Motivation: 针对检索增强LLM在开放和知识密集型任务中的不可靠性，当搜索引擎被触发处理有害任务时，LLM的安全防护无法撤回包含直接有害内容的搜索结果，这种暴露风险构成了关键的攻击面。

Method: SearchAttack将有害语义外包给网络搜索，只保留查询的骨架结构和碎片化线索，然后通过结构化指导引导LLM重构检索到的内容，从而实现恶意目标。

Result: 大量实验表明，SearchAttack在攻击检索增强LLM系统方面表现出很强的有效性，成功实现了对这些系统的红队测试和漏洞评估。

Conclusion: 网络搜索是检索增强LLM的关键攻击面，SearchAttack方法能够有效利用这一弱点，为负责任的安全漏洞评估提供了有效的红队测试工具。

Abstract: Recently, people have suffered and become increasingly aware of the unreliability gap in LLMs for open and knowledge-intensive tasks, and thus turn to search-augmented LLMs to mitigate this issue. However, when the search engine is triggered for harmful tasks, the outcome is no longer under the LLM's control. Once the returned content directly contains targeted, ready-to-use harmful takeaways, the LLM's safeguards cannot withdraw that exposure. Motivated by this dilemma, we identify web search as a critical attack surface and propose \textbf{\textit{SearchAttack}} for red-teaming. SearchAttack outsources the harmful semantics to web search, retaining only the query's skeleton and fragmented clues, and further steers LLMs to reconstruct the retrieved content via structural rubrics to achieve malicious goals. Extensive experiments are conducted to red-team the search-augmented LLMs for responsible vulnerability assessment. Empirically, SearchAttack demonstrates strong effectiveness in attacking these systems.

</details>


### [103] [Layer-wise Positional Bias in Short-Context Language Modeling](https://arxiv.org/abs/2601.04098)
*Maryam Rahimi,Mahdi Nouri,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: 该研究提出基于归因的框架分析语言模型中的位置偏差，发现位置重要性模式具有架构特异性、输入稳定性，并揭示了深度相关的近因偏差和逐渐减弱的首因偏差。


<details>
  <summary>Details</summary>
Motivation: 语言模型往往偏好使用输入中特定位置的信息，而不考虑语义相关性。虽然位置偏差在各种背景下已有研究，但先前工作未能阐明这些偏差如何在各层和输入位置间演变，或如何独立于任务复杂性而变化。

Method: 引入基于归因的框架分析短上下文语言建模中的位置效应。采用滑动窗口方法的层传导性技术，量化每层如何跨输入位置分配重要性，得到层间位置重要性剖面。

Result: 位置重要性剖面具有架构特异性，在不同输入间保持稳定，且对词汇重排具有不变性。研究发现明显的近因偏差随深度增加而增强，而微弱的首因偏差随模型深度而减弱。此外，早期层在所有位置上优先加权内容词而非功能词，而后期层则失去这种词类区分。

Conclusion: 该研究为理解语言模型中位置偏差的层间动态提供了系统框架，揭示了模型架构如何影响位置信息处理模式，对优化模型设计和解释模型行为具有重要意义。

Abstract: Language models often show a preference for using information from specific positions in the input regardless of semantic relevance. While positional bias has been studied in various contexts, from attention sinks to task performance degradation in long-context settings, prior work has not established how these biases evolve across individual layers and input positions, or how they vary independent of task complexity. We introduce an attribution-based framework to analyze positional effects in short-context language modeling. Using layer conductance with a sliding-window approach, we quantify how each layer distributes importance across input positions, yielding layer-wise positional importance profiles. We find that these profiles are architecture-specific, stable across inputs, and invariant to lexical scrambling. Characterizing these profiles, we find prominent recency bias that increases with depth and subtle primacy bias that diminishes through model depth. Beyond positional structure, we also show that early layers preferentially weight content words over function words across all positions, while later layers lose this word-type differentiation.

</details>


### [104] [InfiniteWeb: Scalable Web Environment Synthesis for GUI Agent Training](https://arxiv.org/abs/2601.04126)
*Ziyun Zhang,Zezhou Wang,Xiaoyi Zhang,Zongyu Guo,Jiahao Li,Bin Li,Yan Lu*

Main category: cs.CL

TL;DR: InfiniteWeb系统自动生成大规模功能性网页环境用于GUI智能体训练，通过统一规范、任务驱动开发和多样性保障，显著提升智能体在现实网页任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 训练GUI智能体面临合适环境稀缺的挑战，而现有方法难以构建具有多个互联页面的真实功能性网站。

Method: 采用统一规范、任务中心测试驱动开发，结合网站种子和参考设计图像确保多样性，并生成可验证的任务评估器为强化学习提供密集奖励信号。

Result: InfiniteWeb在构建真实网站方面超越商业编码智能体，基于该环境训练的GUI智能体在OSWorld和Online-Mind2Web基准上取得显著性能提升。

Conclusion: InfiniteWeb系统能有效生成大规模功能性网页环境，解决了GUI智能体训练的环境稀缺问题，为实用AI助手发展提供了有力支持。

Abstract: GUI agents that interact with graphical interfaces on behalf of users represent a promising direction for practical AI assistants. However, training such agents is hindered by the scarcity of suitable environments. We present InfiniteWeb, a system that automatically generates functional web environments at scale for GUI agent training. While LLMs perform well on generating a single webpage, building a realistic and functional website with many interconnected pages faces challenges. We address these challenges through unified specification, task-centric test-driven development, and a combination of website seed with reference design image to ensure diversity. Our system also generates verifiable task evaluators enabling dense reward signals for reinforcement learning. Experiments show that InfiniteWeb surpasses commercial coding agents at realistic website construction, and GUI agents trained on our generated environments achieve significant performance improvements on OSWorld and Online-Mind2Web, demonstrating the effectiveness of proposed system.

</details>


### [105] [ContextFocus: Activation Steering for Contextual Faithfulness in Large Language Models](https://arxiv.org/abs/2601.04131)
*Nikhil Anand,Shwetha Somasundaram,Anirudh Phukan,Apoorv Saxena,Koyel Mukherjee*

Main category: cs.CL

TL;DR: ContextFocus是一种轻量级的激活引导方法，用于解决LLM在检索上下文与内部知识冲突时的不忠实输出问题，无需微调且推理开销小。


<details>
  <summary>Details</summary>
Motivation: 随着世界知识不断更新，LLM部署越来越依赖其遵循外部检索上下文的能力。但当检索证据与模型内部知识冲突时，LLM往往倾向于依赖记忆的事实，产生不忠实的输出。

Method: 提出ContextFocus方法，这是一种轻量级的激活引导方法，通过引导模型激活来改善知识冲突情况下的上下文忠实性。该方法无需模型微调，推理时开销最小。

Result: 在ConFiQA基准测试中，ContextFocus显著优于包括ContextDPO、COIECD和基于提示的方法在内的强基线。实验表明该方法与提示策略互补，在更大模型上仍然有效。

Conclusion: ContextFocus在提高LLM输出的上下文忠实性方面展现出高效性、鲁棒性和效率，为解决知识冲突问题提供了一种有效的轻量级解决方案。

Abstract: Large Language Models (LLMs) encode vast amounts of parametric knowledge during pre-training. As world knowledge evolves, effective deployment increasingly depends on their ability to faithfully follow externally retrieved context. When such evidence conflicts with the model's internal knowledge, LLMs often default to memorized facts, producing unfaithful outputs. In this work, we introduce ContextFocus, a lightweight activation steering approach that improves context faithfulness in such knowledge-conflict settings while preserving fluency and efficiency. Unlike prior approaches, our solution requires no model finetuning and incurs minimal inference-time overhead, making it highly efficient. We evaluate ContextFocus on the ConFiQA benchmark, comparing it against strong baselines including ContextDPO, COIECD, and prompting-based methods. Furthermore, we show that our method is complementary to prompting strategies and remains effective on larger models. Extensive experiments show that ContextFocus significantly improves contextual-faithfulness. Our results highlight the effectiveness, robustness, and efficiency of ContextFocus in improving contextual-faithfulness of LLM outputs.

</details>


### [106] [LLMberjack: Guided Trimming of Debate Trees for Multi-Party Conversation Creation](https://arxiv.org/abs/2601.04135)
*Leonardo Bottona,Nicolò Penzo,Bruno Lepri,Marco Guerini,Sara Tonelli*

Main category: cs.CL

TL;DR: LLMberjack：一个从辩论回复树构建多方对话的平台，提供可视化界面和LLM辅助编辑功能


<details>
  <summary>Details</summary>
Motivation: 目前缺乏从现有辩论结构（回复树）创建多方对话的工具资源，需要支持透明可重复的工作流程

Method: 开发交互式平台，可视化讨论树，支持用户构建连贯的线性化对话序列，保持参与者身份和话语关系，集成LLM辅助自动编辑消息和说话者描述

Result: 树状可视化有助于创建连贯有意义的对话线程，LLM支持提高输出质量并减少人工工作量，平台开源可用

Conclusion: LLMberjack平台有效解决了从辩论结构创建多方对话的资源缺乏问题，通过可视化界面和LLM辅助实现了透明可重复的工作流程

Abstract: We present LLMberjack, a platform for creating multi-party conversations starting from existing debates, originally structured as reply trees. The system offers an interactive interface that visualizes discussion trees and enables users to construct coherent linearized dialogue sequences while preserving participant identity and discourse relations. It integrates optional large language model (LLM) assistance to support automatic editing of the messages and speakers' descriptions. We demonstrate the platform's utility by showing how tree visualization facilitates the creation of coherent, meaningful conversation threads and how LLM support enhances output quality while reducing human effort. The tool is open-source and designed to promote transparent and reproducible workflows to create multi-party conversations, addressing a lack of resources of this type.

</details>


### [107] [FLEx: Language Modeling with Few-shot Language Explanations](https://arxiv.org/abs/2601.04157)
*Adar Avsian,Christopher Richardson,Anirudh Sundar,Larry Heck*

Main category: cs.CL

TL;DR: FLEx是一种通过少量解释性示例改进语言模型行为的方法，使用嵌入聚类选择代表性错误，验证解释有效性，并总结为提示前缀，在推理时引导模型避免类似错误。


<details>
  <summary>Details</summary>
Motivation: 语言模型虽然在多种任务上表现良好，但仍会犯错且错误会在相关查询中重复出现。自然语言解释有助于纠正这些错误，但在需要专家标注的领域中大规模收集解释不可行。

Method: FLEx方法包括三个步骤：1）使用嵌入聚类选择代表性的模型错误；2）验证相关解释是否能纠正这些错误；3）将解释总结为提示前缀，在推理时预置到输入前，引导模型避免类似错误而不修改模型权重。

Result: 在CounterBench、GSM8K和ReasonIF三个数据集上的评估显示，FLEx始终优于链式思维提示，最多能减少83%的CoT剩余错误。

Conclusion: FLEx提供了一种有效的方法，通过少量解释性示例改进语言模型行为，无需大规模人工标注或模型权重更新，在多个基准测试中显著提升了模型性能。

Abstract: Language models have become effective at a wide range of tasks, from math problem solving to open-domain question answering. However, they still make mistakes, and these mistakes are often repeated across related queries. Natural language explanations can help correct these errors, but collecting them at scale may be infeasible, particularly in domains where expert annotators are required. To address this issue, we introduce FLEx ($\textbf{F}$ew-shot $\textbf{L}$anguage $\textbf{Ex}$planations), a method for improving model behavior using a small number of explanatory examples. FLEx selects representative model errors using embedding-based clustering, verifies that the associated explanations correct those errors, and summarizes them into a prompt prefix that is prepended at inference-time. This summary guides the model to avoid similar errors on new inputs, without modifying model weights. We evaluate FLEx on CounterBench, GSM8K, and ReasonIF. We find that FLEx consistently outperforms chain-of-thought (CoT) prompting across all three datasets and reduces up to 83\% of CoT's remaining errors.

</details>


### [108] [All That Glisters Is Not Gold: A Benchmark for Reference-Free Counterfactual Financial Misinformation Detection](https://arxiv.org/abs/2601.04160)
*Yuechen Jiang,Zhiwei Liu,Yupeng Cao,Yueru He,Ziyang Xu,Chen Xu,Zhiyang Deng,Prayag Tiwari,Xi Chen,Alejandro Lopez-Lira,Jimin Huang,Junichi Tsujii,Sophia Ananiadou*

Main category: cs.CL

TL;DR: RFC Bench是一个评估大语言模型在现实金融新闻中检测金融虚假信息的基准，包含无参考检测和基于比较的诊断两个任务，结果显示当前模型在无参考设置下表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有金融虚假信息检测面临挑战，因为金融新闻具有上下文复杂性，信息往往分散在多个线索中。需要建立更贴近现实场景的评估基准来研究模型在无参考环境下的推理能力。

Method: 开发了RFC Bench基准，在段落级别操作，捕捉金融新闻的上下文复杂性。设计了两个互补任务：1) 无参考虚假信息检测；2) 基于比较的诊断（使用原始-扰动配对输入）。

Result: 实验显示一致模式：当有比较上下文时，模型性能显著更强；而无参考设置暴露了显著弱点，包括不稳定的预测和升高的无效输出。这表明当前模型难以在没有外部基础的情况下保持一致的信念状态。

Conclusion: RFC Bench通过揭示当前模型在无参考推理方面的不足，为研究无参考推理和推进更可靠的现实世界金融虚假信息检测提供了结构化测试平台。

Abstract: We introduce RFC Bench, a benchmark for evaluating large language models on financial misinformation under realistic news. RFC Bench operates at the paragraph level and captures the contextual complexity of financial news where meaning emerges from dispersed cues. The benchmark defines two complementary tasks: reference free misinformation detection and comparison based diagnosis using paired original perturbed inputs. Experiments reveal a consistent pattern: performance is substantially stronger when comparative context is available, while reference free settings expose significant weaknesses, including unstable predictions and elevated invalid outputs. These results indicate that current models struggle to maintain coherent belief states without external grounding. By highlighting this gap, RFC Bench provides a structured testbed for studying reference free reasoning and advancing more reliable financial misinformation detection in real world settings.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [109] [Enhancing Retrieval-Augmented Generation with Two-Stage Retrieval: FlashRank Reranking and Query Expansion](https://arxiv.org/abs/2601.03258)
*Sherine George*

Main category: cs.IR

TL;DR: 提出两阶段检索管道：LLM驱动的查询扩展提高召回率，FlashRank重排器在token预算下动态选择最优证据子集，平衡检索召回与LLM上下文限制。


<details>
  <summary>Details</summary>
Motivation: RAG框架在提升事实性和领域适应性时面临关键瓶颈：平衡检索召回率与有限的LLM上下文窗口。检索太少段落可能遗漏关键上下文，检索太多则会淹没提示窗口，稀释相关性并增加成本。

Method: 1. LLM驱动的查询扩展模块：提高候选检索的召回率
2. FlashRank重排器：基于边际效用的快速重排器，在token预算下动态选择最优证据子集，将文档效用建模为相关性、新颖性、简洁性和交叉编码器证据的加权组合

Result: 该解决方案提高了答案准确性、忠实性和计算效率，形成了一个可推广的解决方案，有效平衡了检索召回与LLM上下文限制的问题。

Conclusion: 提出的两阶段检索管道通过查询扩展和动态重排，解决了RAG框架中检索召回与LLM上下文窗口的平衡问题，实现了更高效、准确的检索增强生成。

Abstract: Retrieval-Augmented Generation (RAG) couples a retriever with a large language model (LLM) to ground generated responses in external evidence. While this framework enhances factuality and domain adaptability, it faces a key bottleneck: balancing retrieval recall with limited LLM context. Retrieving too few passages risks missing critical context, while retrieving too many overwhelms the prompt window, diluting relevance and increasing cost.
  We propose a two-stage retrieval pipeline that integrates LLM-driven query expansion to improve candidate recall and FlashRank, a fast marginal-utility reranker that dynamically selects an optimal subset of evidence under a token budget. FlashRank models document utility as a weighted combination of relevance, novelty, brevity, and cross-encoder evidence. Together, these modules form a generalizable solution that increases answer accuracy, faithfulness, and computational efficiency.

</details>


### [110] [LLMDiRec: LLM-Enhanced Intent Diffusion for Sequential Recommendation](https://arxiv.org/abs/2601.03259)
*Bo-Chian Chen,Manel Slokom*

Main category: cs.IR

TL;DR: LLMDiRec通过整合大语言模型与意图感知扩散模型，结合协同信号和语义表示，显著提升了序列推荐性能，特别是在捕捉复杂用户意图和长尾物品推荐方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有序列推荐模型（包括先进的基于扩散的方法）往往难以捕捉用户行为背后丰富的语义意图，特别是对新用户和长尾物品。这种局限性源于它们依赖缺乏语义基础的ID嵌入。

Method: 提出LLMDiRec方法，将大语言模型整合到意图感知扩散模型中。结合ID嵌入的协同信号和LLM的丰富语义表示，使用动态融合机制和多任务目标来对齐两种视角。

Result: 在五个公开数据集上的大量实验表明，LLMDiRec优于最先进的算法，特别是在捕捉复杂用户意图和提升长尾物品推荐性能方面表现出显著改进。

Conclusion: LLMDiRec通过整合LLM语义表示与扩散模型，有效解决了序列推荐中语义意图捕捉不足的问题，特别是在处理新用户和长尾物品方面表现出色，为推荐系统研究提供了新方向。

Abstract: Existing sequential recommendation models, even advanced diffusion-based approaches, often struggle to capture the rich semantic intent underlying user behavior, especially for new users or long-tail items. This limitation stems from their reliance on ID-based embeddings, which lack semantic grounding. We introduce LLMDiRec, a new approach that addresses this gap by integrating Large Language Models (LLMs) into an intent-aware diffusion model. Our approach combines collaborative signals from ID embeddings with rich semantic representations from LLMs, using a dynamic fusion mechanism and a multi-task objective to align both views. We run extensive experiments on five public datasets. We run extensive experiments on five public datasets. We demonstrate that \modelname outperforms state-of-the-art algorithms, with particularly strong improvements in capturing complex user intents and enhancing recommendation performance for long-tail items.

</details>


### [111] [Roles of MLLMs in Visually Rich Document Retrieval for RAG: A Survey](https://arxiv.org/abs/2601.03262)
*Xiantao Zhang*

Main category: cs.IR

TL;DR: 本综述探讨了多模态大语言模型如何改进视觉丰富文档的检索增强生成，系统分析了MLLMs在VRD检索中的三种角色及其权衡，并提供了实践指导和研究方向。


<details>
  <summary>Details</summary>
Motivation: 视觉丰富文档（VRD）对检索增强生成（RAG）提出了特殊挑战，包括布局依赖的语义、脆弱的OCR识别以及信息分散在复杂图表和结构化表格中。传统方法难以有效处理这些多模态特征，需要更先进的解决方案。

Method: 本综述通过系统文献调研，将MLLMs在VRD检索中的应用组织为三种角色：模态统一描述器（将视觉内容转换为文本描述）、多模态嵌入器（生成跨模态表示）、端到端表示器（直接生成检索友好表示）。从检索粒度、信息保真度、延迟与索引大小、重排与接地兼容性等维度进行比较分析。

Result: 提出了MLLMs在VRD检索中的三种角色框架，分析了各自的优缺点和适用场景。模态统一描述器适合语义搜索但可能丢失布局信息；多模态嵌入器能保留更多视觉特征但计算成本较高；端到端表示器在平衡性能和效率方面有优势。提供了根据具体需求选择合适角色的实践指导。

Conclusion: MLLMs为VRD检索提供了有前景的解决方案，但不同角色存在显著权衡。未来研究方向包括自适应检索单元、模型压缩、评估方法开发等，需要继续推进以提升VRD检索的实用性和效率。

Abstract: Visually rich documents (VRDs) challenge retrieval-augmented generation (RAG) with layout-dependent semantics, brittle OCR, and evidence spread across complex figures and structured tables. This survey examines how Multimodal Large Language Models (MLLMs) are being used to make VRD retrieval practical for RAG. We organize the literature into three roles: Modality-Unifying Captioners, Multimodal Embedders, and End-to-End Representers. We compare these roles along retrieval granularity, information fidelity, latency and index size, and compatibility with reranking and grounding. We also outline key trade-offs and offer some practical guidance on when to favor each role. Finally, we identify promising directions for future research, including adaptive retrieval units, model size reduction, and the development of evaluation methods.

</details>


### [112] [STELLA: Self-Reflective Terminology-Aware Framework for Building an Aerospace Information Retrieval Benchmark](https://arxiv.org/abs/2601.03496)
*Bongmin Kim*

Main category: cs.IR

TL;DR: STELLA框架构建航空航天领域信息检索基准，包含术语一致查询和术语无关查询，分别评估词法匹配和语义匹配能力。


<details>
  <summary>Details</summary>
Motivation: 航空航天领域严重依赖技术文档检索，但缺乏反映该领域术语和查询意图特点的公开信息检索基准。

Method: 提出STELLA框架，从NASA技术报告构建基准，包含文档布局检测、段落分块、术语词典构建、合成查询生成和跨语言扩展等步骤。生成术语一致查询（TCQ）和术语无关查询（TAQ）两种查询类型。

Result: 评估7个嵌入模型发现，大型解码器嵌入模型在语义理解方面表现最强，而BM25等词法匹配方法在需要精确术语匹配的领域仍具竞争力。

Conclusion: STELLA基准为航空航天领域信息检索任务提供了可复现的评估基础，有助于可靠评估和改进嵌入模型性能。

Abstract: Tasks in the aerospace industry heavily rely on searching and reusing large volumes of technical documents, yet there is no public information retrieval (IR) benchmark that reflects the terminology- and query-intent characteristics of this domain. To address this gap, this paper proposes the STELLA (Self-Reflective TErminoLogy-Aware Framework for BuiLding an Aerospace Information Retrieval Benchmark) framework. Using this framework, we introduce the STELLA benchmark, an aerospace-specific IR evaluation set constructed from NASA Technical Reports Server (NTRS) documents via a systematic pipeline that comprises document layout detection, passage chunking, terminology dictionary construction, synthetic query generation, and cross-lingual extension. The framework generates two types of queries: the Terminology Concordant Query (TCQ), which includes the terminology verbatim to evaluate lexical matching, and the Terminology Agnostic Query (TAQ), which utilizes the terminology's description to assess semantic matching. This enables a disentangled evaluation of the lexical and semantic matching capabilities of embedding models. In addition, we combine Chain-of-Density (CoD) and the Self-Reflection method with query generation to improve quality and implement a hybrid cross-lingual extension that reflects real user querying practices. Evaluation of seven embedding models on the STELLA benchmark shows that large decoder-based embedding models exhibit the strongest semantic understanding, while lexical matching methods such as BM25 remain highly competitive in domains where exact lexical matching technical term is crucial. The STELLA benchmark provides a reproducible foundation for reliable performance evaluation and improvement of embedding models in aerospace-domain IR tasks. The STELLA benchmark can be found in https://huggingface.co/datasets/telepix/STELLA.

</details>


### [113] [Shielded RecRL: Explanation Generation for Recommender Systems without Ranking Degradation](https://arxiv.org/abs/2601.03608)
*Ansh Tiwari,Ayush Chauhan*

Main category: cs.IR

TL;DR: Shielded RecRL使用强化学习方法为推荐系统生成个性化解释，同时保持原有推荐排序性能不变。通过双塔架构和梯度屏蔽策略，仅训练LLM的0.4%参数，在亚马逊图书数据集上提升点击率22.5%。


<details>
  <summary>Details</summary>
Motivation: 现有基于RLHF的推荐方法直接优化物品排序，这会改变推荐系统的核心排序性能。需要一种方法能够生成个性化解释，同时保持原始推荐排序模型不变。

Method: 采用双塔架构：保持推荐排序模型不变，使用语言模型学习生成解释。设计复合奖励信号（解释长度、内容相关性、连贯性），使用PPO和KL散度约束，通过LoRA适配器仅训练LLM的0.4%参数。

Result: 在亚马逊图书数据集（约5万条幻想和浪漫题材交互）上，将相对点击率提升22.5%（基线1.225倍），同时保持推荐物品排序行为基本不变。消融研究证实梯度屏蔽策略和奖励设计有效平衡解释质量和策略漂移。

Conclusion: Shielded RecRL通过丰富的个性化解释增强了推荐系统的用户面向方面，同时不损害核心推荐准确性。证明了在保持推荐排序性能不变的情况下，通过强化学习优化解释生成是可行的。

Abstract: We introduce Shielded RecRL, a reinforcement learning approach to generate personalized explanations for recommender systems without sacrificing the system's original ranking performance. Unlike prior RLHF-based recommender methods that directly optimize item rankings, our two-tower architecture keeps the recommender's ranking model intact while a language model learns to produce helpful explanations. We design a composite reward signal combining explanation length, content relevance, and coherence, and apply proximal policy optimization (PPO) with a KL-divergence constraint to fine-tune a large language model with only 0.4% of its parameters trainable via LoRA adapters. In experiments on an Amazon Books dataset (approximately 50K interactions in the fantasy and romance genres), Shielded RecRL improved the relative click-through rate (CTR) by 22.5% (1.225x over baseline) while keeping the recommender's item-ranking behavior virtually unchanged. An extensive ablation study confirms that our gradient shielding strategy and reward design effectively balance explanation quality and policy drift. Our results demonstrate that Shielded RecRL enhances user-facing aspects of recommendations through rich, personalized explanations without degrading core recommendation accuracy.

</details>


### [114] [Perception-Aware Bias Detection for Query Suggestions](https://arxiv.org/abs/2601.03730)
*Fabian Haak,Philipp Schaer*

Main category: cs.IR

TL;DR: 该论文扩展了针对人物相关搜索查询建议的偏见检测流程，通过引入感知感知指标来更好地检测系统性主题偏见。


<details>
  <summary>Details</summary>
Motivation: 虽然网络搜索偏见已受到广泛关注，但查询建议中的偏见问题研究较少。查询建议具有稀疏性和缺乏上下文元数据的特性，使其成为偏见检测的难点。此外，用户对查询建议的感知往往是短暂和潜意识的，因此需要开发能够反映用户实际感知的偏见检测方法。

Method: 在Bonart等人开发的人物相关搜索查询建议偏见检测流程基础上，引入了感知感知指标来克服查询建议稀疏性和缺乏上下文的问题。这些指标考虑了用户对查询建议的短暂和潜意识感知特点。

Result: 增强后的流程能够更好地检测搜索引擎人物相关搜索查询建议中的系统性主题偏见。分析结果证实了这一假设，由于采用了感知感知偏见检测指标，流程产生的结果可以反映用户能够察觉的偏见。

Conclusion: 该研究通过引入感知感知指标，成功改进了查询建议偏见检测流程，使其能够更准确地识别用户在实际使用中可能察觉的系统性偏见，为自动偏见检测方法的发展做出了贡献。

Abstract: Bias in web search has been in the spotlight of bias detection research for quite a while. At the same time, little attention has been paid to query suggestions in this regard. Awareness of the problem of biased query suggestions has been raised. Likewise, there is a rising need for automatic bias detection approaches. This paper adds on the bias detection pipeline for bias detection in query suggestions of person-related search developed by Bonart et al. \cite{Bonart_2019a}. The sparseness and lack of contextual metadata of query suggestions make them a difficult subject for bias detection. Furthermore, query suggestions are perceived very briefly and subliminally. To overcome these issues, perception-aware metrics are introduced. Consequently, the enhanced pipeline is able to better detect systematic topical bias in search engine query suggestions for person-related searches. The results of an analysis performed with the developed pipeline confirm this assumption. Due to the perception-aware bias detection metrics, findings produced by the pipeline can be assumed to reflect bias that users would discern.

</details>


### [115] [Bridging OLAP and RAG: A Multidimensional Approach to the Design of Corpus Partitioning](https://arxiv.org/abs/2601.03748)
*Dario Maio,Stefano Rizzi*

Main category: cs.IR

TL;DR: 提出维度事实模型（DFM）作为指导大规模RAG系统多维分区的概念框架，将语义聚类与基于时间、组织等概念维度的分区相结合，实现可解释、可治理的检索流程。


<details>
  <summary>Details</summary>
Motivation: 当前工业级RAG系统依赖水平分片、近似最近邻搜索等技术实现可扩展性，但这些基于相似性的自底向上组织缺乏概念化的语料分区依据，现有维度（如时间、组织上下文）使用方式随意且结构不良。

Method: 提出维度事实模型（DFM）作为概念框架，为RAG语料的多维分区设计提供原则性指导。DFM能够推理事实、维度、层次结构和粒度，支持分层路由和可控回退策略。

Result: DFM框架将检索过程从"黑盒"相似性匹配转变为可治理、确定性的工作流程，确保在元数据不完整时检索依然稳健，同时支持分层路由和可控回退策略。

Conclusion: 该论文旨在弥合OLAP风格的多维建模与现代RAG架构之间的差距，促进大规模、原则性、可解释且可治理的检索策略研究，为RAG系统设计提供概念性指导。

Abstract: Retrieval-Augmented Generation (RAG) systems are increasingly deployed on large-scale document collections, often comprising millions of documents and tens of millions of text chunks. In industrial-scale retrieval platforms, scalability is typically addressed through horizontal sharding and a combination of Approximate Nearest-Neighbor search, hybrid indexing, and optimized metadata filtering. Although effective from an efficiency perspective, these mechanisms rely on bottom-up, similarity-driven organization and lack a conceptual rationale for corpus partitioning. In this paper, we claim that the design of large-scale RAG systems may benefit from the combination of two orthogonal strategies: semantic clustering, which optimizes locality in embedding space, and multidimensional partitioning, which governs where retrieval should occur based on conceptual dimensions such as time and organizational context. Although such dimensions are already implicitly present in current systems, they are used in an ad hoc and poorly structured manner. We propose the Dimensional Fact Model (DFM) as a conceptual framework to guide the design of multidimensional partitions for RAG corpora. The DFM provides a principled way to reason about facts, dimensions, hierarchies, and granularity in retrieval-oriented settings. This framework naturally supports hierarchical routing and controlled fallback strategies, ensuring that retrieval remains robust even in the presence of incomplete metadata, while transforming the search process from a 'black-box' similarity matching into a governable and deterministic workflow. This work is intended as a position paper; its goal is to bridge the gap between OLAP-style multidimensional modeling and modern RAG architectures, and to stimulate further research on principled, explainable, and governable retrieval strategies at scale.

</details>


### [116] [Unleashing the Potential of Neighbors: Diffusion-based Latent Neighbor Generation for Session-based Recommendation](https://arxiv.org/abs/2601.03903)
*Yuhan Yang,Jie Zou,Guojia An,Jiwei Wei,Yang Yang,Heng Tao Shen*

Main category: cs.IR

TL;DR: DiffSBR：基于扩散的潜在邻居生成模型，用于会话推荐，通过生成未直接观察到的潜在邻居来增强会话表示，提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有会话推荐方法通常依赖显式观察到的邻居会话数据，忽略了兴趣空间中潜在相关但未直接观察到的邻居，未能充分利用邻居会话的潜力。

Method: 提出DiffSBR模型，包含两个扩散模块：检索增强扩散模块（利用检索到的邻居作为指导信号）和自增强扩散模块（通过对比学习注入当前会话的多模态信号）。采用训练策略使检索器能从生成器的反馈中学习。

Result: 在四个公开数据集上的实验表明，DiffSBR能生成有效的潜在邻居，并相比最先进的基线方法提升了推荐性能。

Conclusion: DiffSBR通过扩散生成潜在邻居，有效缓解了数据稀疏问题，充分利用了邻居会话的潜力，提升了会话推荐的效果。

Abstract: Session-based recommendation aims to predict the next item that anonymous users may be interested in, based on their current session interactions. Recent studies have demonstrated that retrieving neighbor sessions to augment the current session can effectively alleviate the data sparsity issue and improve recommendation performance. However, existing methods typically rely on explicitly observed session data, neglecting latent neighbors - not directly observed but potentially relevant within the interest space - thereby failing to fully exploit the potential of neighbor sessions in recommendation. To address the above limitation, we propose a novel model of diffusion-based latent neighbor generation for session-based recommendation, named DiffSBR. Specifically, DiffSBR leverages two diffusion modules, including retrieval-augmented diffusion and self-augmented diffusion, to generate high-quality latent neighbors. In the retrieval-augmented diffusion module, we leverage retrieved neighbors as guiding signals to constrain and reconstruct the distribution of latent neighbors. Meanwhile, we adopt a training strategy that enables the retriever to learn from the feedback provided by the generator. In the self-augmented diffusion module, we explicitly guide the generation of latent neighbors by injecting the current session's multi-modal signals through contrastive learning. After obtaining the generated latent neighbors, we utilize them to enhance session representations for improving session-based recommendation. Extensive experiments on four public datasets show that DiffSBR generates effective latent neighbors and improves recommendation performance against state-of-the-art baselines.

</details>
