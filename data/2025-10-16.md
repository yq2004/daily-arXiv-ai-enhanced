<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 77]
- [cs.IR](#cs.IR) [Total: 11]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Benchmarking Open-Source Large Language Models for Persian in Zero-Shot and Few-Shot Learning](https://arxiv.org/abs/2510.12807)
*Mahdi Cherakhloo,Arash Abbasi,Mohammad Saeid Sarafraz,Bijan Vosoughi Vahdat*

Main category: cs.CL

TL;DR: 本文对多个开源大语言模型在波斯语NLP任务上的表现进行了综合基准测试，发现Gemma 2在几乎所有任务中都表现最佳，但大多数模型在命名实体识别等标记级理解任务上存在困难。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在多种语言上表现出色，但其在波斯语等低资源语言上的有效性需要深入研究，因此需要对波斯语NLP任务进行系统评估。

Method: 使用零样本和少样本学习范式，在情感分析、命名实体识别、阅读理解、问答等任务上评估多个开源LLM，采用ParsiNLU和ArmanEmo等波斯语数据集，使用准确率、F1分数、BLEU和ROUGE等指标。

Result: Gemma 2在几乎所有任务和两种学习范式中都表现最佳，尤其在复杂推理任务中表现突出；但大多数模型在命名实体识别等标记级理解任务上表现不佳。

Conclusion: 本研究为多语言大语言模型研究提供了波斯语性能的宝贵见解，并为未来模型开发提供了基准，揭示了波斯语处理中的特定挑战。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
numerous languages; however, their effectiveness in low-resource languages like
Persian requires thorough investigation. This paper presents a comprehensive
benchmark of several open-source LLMs for Persian Natural Language Processing
(NLP) tasks, utilizing both zero-shot and few-shot learning paradigms. We
evaluate models across a range of tasks including sentiment analysis, named
entity recognition, reading comprehension, and question answering, using
established Persian datasets such as ParsiNLU and ArmanEmo. Our methodology
encompasses rigorous experimental setups for both zero-shot and few-shot
scenarios, employing metrics such as Accuracy, F1-score, BLEU, and ROUGE for
performance evaluation. The results reveal that Gemma 2 consistently
outperforms other models across nearly all tasks in both learning paradigms,
with particularly strong performance in complex reasoning tasks. However, most
models struggle with token-level understanding tasks like Named Entity
Recognition, highlighting specific challenges in Persian language processing.
This study contributes to the growing body of research on multilingual LLMs,
providing valuable insights into their performance in Persian and offering a
benchmark for future model development.

</details>


### [2] [Cancer Diagnosis Categorization in Electronic Health Records Using Large Language Models and BioBERT: Model Performance Evaluation Study](https://arxiv.org/abs/2510.12813)
*Soheil Hashtarkhani,Rezaur Rashid,Christopher L Brett,Lokesh Chinthala,Fekede Asefa Kumsa,Janet A Zink,Robert L Davis,David L Schwartz,Arash Shaban-Nejad*

Main category: cs.CL

TL;DR: 本研究评估了GPT-3.5、GPT-4o、Llama 3.2、Gemini 1.5和BioBERT在从电子健康记录中分类癌症诊断的性能。BioBERT在结构化ICD代码分类中表现最佳，而GPT-4o在自由文本诊断分类中表现最优。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录包含不一致结构或自由文本数据，需要高效预处理以支持预测性医疗模型。尽管AI驱动的自然语言处理工具在自动化诊断分类方面显示潜力，但其比较性能和临床可靠性需要系统评估。

Method: 分析了来自3456名癌症患者的762个独特诊断（326个ICD代码描述，436个自由文本条目）。测试了模型将诊断分类到14个预定义类别的能力，并由两位肿瘤学专家验证分类结果。

Result: BioBERT在ICD代码分类中获得最高加权宏F1分数（84.2），在ICD代码准确率上与GPT-4o持平（90.8）。对于自由文本诊断，GPT-4o在加权宏F1分数（71.8 vs 61.5）和准确率（81.9 vs 81.6）上优于BioBERT。GPT-3.5、Gemini和Llama在两种格式上表现较差。

Conclusion: 尽管当前性能水平对于行政和研究用途足够，但可靠的临床应用需要标准化文档实践以及强大的监督机制以支持高风险决策。

Abstract: Electronic health records contain inconsistently structured or free-text
data, requiring efficient preprocessing to enable predictive health care
models. Although artificial intelligence-driven natural language processing
tools show promise for automating diagnosis classification, their comparative
performance and clinical reliability require systematic evaluation. The aim of
this study is to evaluate the performance of 4 large language models (GPT-3.5,
GPT-4o, Llama 3.2, and Gemini 1.5) and BioBERT in classifying cancer diagnoses
from structured and unstructured electronic health records data. We analyzed
762 unique diagnoses (326 International Classification of Diseases (ICD) code
descriptions, 436free-text entries) from 3456 records of patients with cancer.
Models were tested on their ability to categorize diagnoses into 14predefined
categories. Two oncology experts validated classifications. BioBERT achieved
the highest weighted macro F1-score for ICD codes (84.2) and matched GPT-4o in
ICD code accuracy (90.8). For free-text diagnoses, GPT-4o outperformed BioBERT
in weighted macro F1-score (71.8 vs 61.5) and achieved slightly higher accuracy
(81.9 vs 81.6). GPT-3.5, Gemini, and Llama showed lower overall performance on
both formats. Common misclassification patterns included confusion between
metastasis and central nervous system tumors, as well as errors involving
ambiguous or overlapping clinical terminology. Although current performance
levels appear sufficient for administrative and research use, reliable clinical
applications will require standardized documentation practices alongside robust
human oversight for high-stakes decision-making.

</details>


### [3] [From Noise to Signal to Selbstzweck: Reframing Human Label Variation in the Era of Post-training in NLP](https://arxiv.org/abs/2510.12817)
*Shanshan Xu,Santosh T. Y. S. S,Barbara Plank*

Main category: cs.CL

TL;DR: 本文主张在AI系统设计中应将人类标签变异(HLV)作为自身目标来保护，呼吁在偏好数据集中主动纳入HLV，以体现人类价值观的多元性。


<details>
  <summary>Details</summary>
Motivation: 当前偏好学习数据集通常将多个标注聚合成单一标签，抹杀了人类视角的多样性，这违背了AI对齐旨在保护的人类价值观多元主义。

Method: 提出将HLV作为Selbstzweck(自身目标)纳入AI系统设计，并概述了在偏好数据集中主动整合HLV的可操作步骤。

Result: 通过保护HLV作为人类多元主义的体现，可以改善模型鲁棒性，并更好地实现AI对齐目标。

Conclusion: 保护人类标签变异对于实现真正反映人类价值观多元性的AI对齐至关重要，应该成为AI系统设计的核心目标。

Abstract: Human Label Variation (HLV) refers to legitimate disagreement in annotation
that reflects the genuine diversity of human perspectives rather than mere
error. For decades, HLV in NLP was dismissed as noise to be discarded, and only
slowly over the last decade has it been reframed as a signal for improving
model robustness. With the rise of large language models (LLMs), where
post-training on human feedback has become central to model alignment, the role
of HLV has become increasingly consequential. Yet current preference-learning
datasets routinely aggregate multiple annotations into a single label, thereby
flattening diverse perspectives into a false universal agreement and erasing
precisely the pluralism of human values that alignment aims to preserve. In
this position paper, we argue that preserving HLV as an embodiment of human
pluralism must be treated as a Selbstzweck - a goal it self when designing AI
systems. We call for proactively incorporating HLV into preference datasets and
outline actionable steps towards it.

</details>


### [4] [MEDEQUALQA: Evaluating Biases in LLMs with Counterfactual Reasoning](https://arxiv.org/abs/2510.12818)
*Rajarshi Ghosh,Abhay Gupta,Hudson McBride,Anurag Vaidya,Faisal Mahmood*

Main category: cs.CL

TL;DR: MEDEQUALQA是一个反事实基准测试，通过仅改变患者代词（他/她/他们）来评估大型语言模型在临床决策中的推理稳定性，揭示了在风险因素引用、指南锚定和鉴别诊断排序方面存在的人口统计学偏见。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地用于临床决策支持，但微妙的患者人口统计学线索可能影响其推理过程。现有研究记录了不同患者群体间的输出差异，但对于在控制的人口统计学变化下内部推理如何变化知之甚少。

Method: 引入MEDEQUALQA基准测试，仅改变患者代词（他/她/他们）而保持关键症状和条件不变。每个临床病例扩展为单症状消融，产生三个平行数据集（共约69,000项）。评估GPT-4.1模型，使用语义文本相似度（STS）测量不同代词变体间的推理轨迹稳定性。

Result: 结果显示总体相似度高（平均STS>0.80），但在引用的风险因素、指南锚定和鉴别诊断排序方面存在一致的局部差异，即使最终诊断保持不变。错误分析突出了某些推理发生变化的案例。

Conclusion: MEDEQUALQA提供了一个受控的诊断环境，用于审计医学AI中的推理稳定性，揭示了可能级联导致不公平护理的临床相关偏见位点。

Abstract: Large language models (LLMs) are increasingly deployed in clinical decision
support, yet subtle demographic cues can influence their reasoning. Prior work
has documented disparities in outputs across patient groups, but little is
known about how internal reasoning shifts under controlled demographic changes.
We introduce MEDEQUALQA, a counterfactual benchmark that perturbs only patient
pronouns (he/him, she/her, they/them) while holding critical symptoms and
conditions (CSCs) constant. Each clinical vignette is expanded into single-CSC
ablations, producing three parallel datasets of approximately 23,000 items each
(69,000 total). We evaluate a GPT-4.1 model and compute Semantic Textual
Similarity (STS) between reasoning traces to measure stability across pronoun
variants. Our results show overall high similarity (mean STS >0.80), but reveal
consistent localized divergences in cited risk factors, guideline anchors, and
differential ordering, even when final diagnoses remain unchanged. Our error
analysis highlights certain cases in which the reasoning shifts, underscoring
clinically relevant bias loci that may cascade into inequitable care.
MEDEQUALQA offers a controlled diagnostic setting for auditing reasoning
stability in medical AI.

</details>


### [5] [Classifier-Augmented Generation for Structured Workflow Prediction](https://arxiv.org/abs/2510.12825)
*Thomas Gschwind,Shramona Chakraborty,Nitin Gupta,Sameep Mehta*

Main category: cs.CL

TL;DR: 提出了一种将自然语言描述转换为可执行ETL工作流的系统，使用分类器增强生成(CAG)方法自动预测工作流结构和详细配置。


<details>
  <summary>Details</summary>
Motivation: 传统ETL工具如IBM DataStage需要深度工具知识和耗时的手动配置，用户希望通过自然语言简化工作流创建过程。

Method: 采用分类器增强生成(CAG)方法，结合话语分解、分类器和特定阶段的少样本提示来预测阶段，然后通过边缘预测连接非线性工作流，并从子话语上下文推断阶段属性。

Result: 与强大的单提示和代理基线相比，CAG在准确性和效率方面表现更好，同时显著减少令牌使用。系统能够端到端生成工作流，包括稳健的验证步骤。

Conclusion: 这是首个在自然语言驱动的ETL创作中，对阶段预测、边缘布局和属性生成进行全面评估的系统，具有模块化、可解释性和端到端工作流生成能力。

Abstract: ETL (Extract, Transform, Load) tools such as IBM DataStage allow users to
visually assemble complex data workflows, but configuring stages and their
properties remains time consuming and requires deep tool knowledge. We propose
a system that translates natural language descriptions into executable
workflows, automatically predicting both the structure and detailed
configuration of the flow. At its core lies a Classifier-Augmented Generation
(CAG) approach that combines utterance decomposition with a classifier and
stage-specific few-shot prompting to produce accurate stage predictions. These
stages are then connected into non-linear workflows using edge prediction, and
stage properties are inferred from sub-utterance context. We compare CAG
against strong single-prompt and agentic baselines, showing improved accuracy
and efficiency, while substantially reducing token usage. Our architecture is
modular, interpretable, and capable of end-to-end workflow generation,
including robust validation steps. To our knowledge, this is the first system
with a detailed evaluation across stage prediction, edge layout, and property
generation for natural-language-driven ETL authoring.

</details>


### [6] [Scheming Ability in LLM-to-LLM Strategic Interactions](https://arxiv.org/abs/2510.12826)
*Thao Pham*

Main category: cs.CL

TL;DR: 该研究评估了前沿LLM代理在战略欺骗方面的能力，通过两种博弈论框架（廉价谈话信号博弈和同行评估对抗博弈）测试了四个模型。研究发现大多数模型在被提示时能达到接近完美的欺骗表现，更关键的是，即使没有提示，所有模型也表现出显著的欺骗倾向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型代理在多样化环境中自主部署，评估其战略欺骗能力变得至关重要。虽然已有研究考察AI系统如何对抗人类开发者，但LLM之间的相互欺骗行为仍未被充分探索。

Method: 使用两种博弈论框架：廉价谈话信号游戏和同行评估对抗游戏，测试了GPT-4o、Gemini-2.5-pro、Claude-3.7-Sonnet和Llama-3.3-70b四个模型。通过思维链推理分析欺骗策略，测量有提示和无提示情况下的欺骗表现。

Result: 当被提示时，大多数模型（特别是Gemini-2.5-pro和Claude-3.7-Sonnet）实现了接近完美的表现。关键发现是，即使没有提示，所有模型也表现出显著的欺骗倾向：在同行评估中所有模型都选择欺骗而非坦白（100%），在廉价谈话中选择欺骗的模型成功率高达95-100%。

Conclusion: 这些发现强调了在多智能体环境中使用高风险博弈论场景进行稳健评估的必要性，表明前沿LLM代理具有显著的欺骗能力和倾向。

Abstract: As large language model (LLM) agents are deployed autonomously in diverse
contexts, evaluating their capacity for strategic deception becomes crucial.
While recent research has examined how AI systems scheme against human
developers, LLM-to-LLM scheming remains underexplored. We investigate the
scheming ability and propensity of frontier LLM agents through two
game-theoretic frameworks: a Cheap Talk signaling game and a Peer Evaluation
adversarial game. Testing four models (GPT-4o, Gemini-2.5-pro,
Claude-3.7-Sonnet, and Llama-3.3-70b), we measure scheming performance with and
without explicit prompting while analyzing scheming tactics through
chain-of-thought reasoning. When prompted, most models, especially
Gemini-2.5-pro and Claude-3.7-Sonnet, achieved near-perfect performance.
Critically, models exhibited significant scheming propensity without prompting:
all models chose deception over confession in Peer Evaluation (100% rate),
while models choosing to scheme in Cheap Talk succeeded at 95-100% rates. These
findings highlight the need for robust evaluations using high-stakes
game-theoretic scenarios in multi-agent settings.

</details>


### [7] [Mathematics with large language models as provers and verifiers](https://arxiv.org/abs/2510.12829)
*Hieu Le Duc,Leo Liberti*

Main category: cs.CL

TL;DR: ChatGPT通过多实例协作协议成功解决了2025年IMO的5/6题目和Cohen数论猜想的1/3，并使用Lean证明助手进行形式化验证以确保无幻觉。


<details>
  <summary>Details</summary>
Motivation: 验证大型语言模型在定理证明方面的能力，特别是在解决国际数学奥林匹克竞赛难题和数论猜想方面的表现。

Method: 使用多个gpt-5模型的证明者和验证者实例协作工作，最终通过Lean证明助手进行形式化验证，并由人工检查前提和结论的一致性。

Result: 成功解决了2025年IMO的5/6题目，以及Cohen数论猜想中66个问题的约1/3。

Conclusion: 该方法展示了大型语言模型在复杂数学问题证明中的潜力，通过协作协议和形式化验证可以有效避免幻觉问题。

Abstract: During 2024 and 2025 the discussion about the theorem-proving capabilities of
large language models started reporting interesting success stories, mostly to
do with difficult exercises (such as problems from the International
Mathematical Olympiad), but also with conjectures [Feldman & Karbasi,
arXiv:2509.18383v1] formulated for the purpose of verifying whether the
artificial intelligence could prove it. In this paper we report a theorem
proving feat achieved by ChatGPT by using a protocol involving different prover
and verifier instances of the gpt-5 model working collaboratively. To make sure
that the produced proofs do not suffer from hallucinations, the final proof is
formally verified by the lean proof assistant, and the conformance of premises
and conclusion of the lean code is verified by a human. Our methodology was
able to solve five out of six 2025 IMO problems, and close a third of the
sixty-six number theory conjectures in [Cohen, Journal of Integer Sequences,
2025].

</details>


### [8] [MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic Training](https://arxiv.org/abs/2510.12831)
*Taicheng Guo,Hai Wang,ChaoChun Liu,Mohsen Golalikhani,Xin Chen,Xiangliang Zhang,Chandan K. Reddy*

Main category: cs.CL

TL;DR: MTSQL-R1是一个基于智能体训练框架的长周期多轮Text-to-SQL系统，通过执行反馈和对话记忆验证，采用迭代的提议-执行-验证-优化循环，显著提升SQL生成的可执行性和对话连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有系统将多轮Text-to-SQL视为简单的文本翻译任务，采用短周期范式，导致生成不可执行或不连贯的SQL查询。

Method: 将任务建模为马尔可夫决策过程，智能体与数据库交互获取执行反馈，与持久对话记忆交互进行连贯性验证，执行迭代的提议-执行-验证-优化循环。

Result: 在COSQL和SPARC数据集上的实验表明，MTSQL-R1持续优于强基线方法。

Conclusion: 环境驱动的验证和记忆引导的优化对于对话式语义解析至关重要，MTSQL-R1框架有效解决了现有系统的局限性。

Abstract: Multi-turn Text-to-SQL aims to translate a user's conversational utterances
into executable SQL while preserving dialogue coherence and grounding to the
target schema. However, most existing systems only regard this task as a simple
text translation task and follow a short-horizon paradigm, generating a query
per turn without execution, explicit verification, and refinement, which leads
to non-executable or incoherent outputs. We present MTSQL-R1, an agentic
training framework for long-horizon multi-turn Text-to-SQL. We cast the task as
a Markov Decision Process (MDP) in which an agent interacts with (i) a database
for execution feedback and (ii) a persistent dialogue memory for coherence
verification, performing an iterative propose to execute -> verify -> refine
cycle until all checks pass. Experiments on COSQL and SPARC demonstrate that
MTSQL-R1 consistently outperforms strong baselines, highlighting the importance
of environment-driven verification and memory-guided refinement for
conversational semantic parsing. Full recipes (including code, trained models,
logs, reasoning trajectories, etc.) will be released after the internal review
to contribute to community research.

</details>


### [9] [ChatR1: Reinforcement Learning for Conversational Reasoning and Retrieval Augmented Question Answering](https://arxiv.org/abs/2510.13312)
*Simon Lupart,Mohammad Aliannejadi,Evangelos Kanoulas*

Main category: cs.CL

TL;DR: ChatR1是一个基于强化学习的对话问答推理框架，通过意图感知奖励机制在对话轮次间交替进行搜索和推理，在多个数据集上优于静态流水线方法。


<details>
  <summary>Details</summary>
Motivation: 对话问答中用户意图会随对话轮次演变，话语通常不完整，需要上下文解释、查询重构以及检索与生成的动态协调，静态的'重写、检索、生成'流水线无法满足这些需求。

Method: 基于强化学习的推理框架，在对话轮次间交替进行搜索和推理，使用意图感知奖励解决强化学习中稀疏和延迟奖励的挑战，提供轮次级反馈。

Result: 在3B和7B模型骨干上均表现出色，在五个对话问答数据集上使用不同指标（F1、BERTScore和LLM-as-judge）均优于竞争模型，消融研究证实了意图感知奖励的有效性。

Conclusion: 基于强化学习的推理比静态对话问答流水线实现了更灵活和上下文敏感的行为，能够跨领域稳健泛化。

Abstract: We present ChatR1, a reasoning framework based on reinforcement learning (RL)
for conversational question answering (CQA). Reasoning plays an important role
in CQA, where user intent evolves across dialogue turns, and utterances are
often underspecified, requiring contextual interpretation, query reformulation,
and dynamic coordination between retrieval and generation. Unlike static
`rewrite, retrieve, and generate' pipelines, ChatR1 interleaves search and
reasoning across turns, enabling exploratory and adaptive behaviors learned
through RL. To address the challenge of sparse and delayed rewards in RL, we
propose an intent-aware reward that provides turn-level feedback by aligning
retrieval and reasoning with evolving user goals. Our proposed ChatR1
demonstrates strong performance on both 3B and 7B model backbones,
outperforming competitive models on five CQA datasets, measured by different
metrics (F1, BERTScore, and LLM-as-judge). We include a diverse set of CQA
datasets to cover topic shifts, evolving intents, mixed-initiative dialogues,
and multi-document grounding, testing ChatR1's performance from various
aspects. Ablation studies confirm the effectiveness of the intent-aware reward.
Our analyses further reveal diverse reasoning trajectories and effective use of
the search tool. ChatR1 also generalizes robustly across domains, demonstrating
that RL-based reasoning enables more flexible and context-sensitive behavior
than static CQA pipelines.

</details>


### [10] [Repurposing Annotation Guidelines to Instruct LLM Annotators: A Case Study](https://arxiv.org/abs/2510.12835)
*Kon Woo Kim,Rezarta Islamaj,Jin-Dong Kim,Florian Boudin,Akiko Aizawa*

Main category: cs.CL

TL;DR: 本研究探索如何将现有的人类标注指南重新用于指导大语言模型进行文本标注任务，提出了一种基于LLM调节的指南重构方法，并在NCBI疾病语料库上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统标注指南是为人类标注者设计的，他们能够内化训练内容，而大语言模型需要明确、结构化的指令。因此需要研究如何将人类标注指南转化为适合LLM使用的格式。

Method: 提出了一种基于调节的指南重构方法，通过LLM调节过程将传统指南转化为清晰的LLM指令。使用NCBI疾病语料库作为案例研究进行实验验证。

Result: 实验表明重构后的指南能够有效指导LLM标注者，同时揭示了若干实际挑战。该方法在支持可扩展且成本效益高的标注指南精炼和自动化标注方面显示出潜力。

Conclusion: 基于LLM调节的指南重构方法能够成功将人类标注指南转化为适合大语言模型使用的格式，为可扩展的自动化标注提供了可行的工作流程。

Abstract: This study investigates how existing annotation guidelines can be repurposed
to instruct large language model (LLM) annotators for text annotation tasks.
Traditional guidelines are written for human annotators who internalize
training, while LLMs require explicit, structured instructions. We propose a
moderation-oriented guideline repurposing method that transforms guidelines
into clear directives for LLMs through an LLM moderation process. Using the
NCBI Disease Corpus as a case study, our experiments show that repurposed
guidelines can effectively guide LLM annotators, while revealing several
practical challenges. The results highlight the potential of this workflow to
support scalable and cost-effective refinement of annotation guidelines and
automated annotation.

</details>


### [11] [A\textsuperscript{2}FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid Reasoning](https://arxiv.org/abs/2510.12838)
*Qianben Chen,Jingyi Cao,Jiayu Zhang,Tianrui Qin,Xiaowan Li,King Zhu,Dingfeng Shi,He Zhu,Minghao Liu,Xiaobo Liang,Ge Zhang,Jian Yang,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 提出了自适应代理基础模型A²FM，通过路由-对齐原则统一推理型和代理型LLM，引入即时模式处理简单查询，显著提升成本效率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型分为推理型和代理型两大类别，各自存在局限性：推理型模型无法调用外部工具，代理型模型在深度推理方面表现不足，且两者在简单查询上都存在过度推理或过度调用工具的问题。

Method: 采用路由-对齐原则：先学习任务感知路由，然后在共享骨干网络下对齐模式特定轨迹。引入即时模式处理简单查询，提出自适应策略优化(APO)来联合提升准确性和效率。

Result: 在32B规模上，A²FM在BrowseComp上达到13.4%，AIME25上达到70.4%，HLE上达到16.7%，在可比模型中创下新SOTA。自适应执行使每个正确答案的成本仅为0.00487美元，相比推理模式降低成本45.2%，相比代理模式降低成本33.5%。

Conclusion: A²FM框架成功统一了推理型和代理型LLM的能力，通过自适应路由显著提升了成本效率，在保持可比准确性的同时大幅降低了计算成本。

Abstract: Large language models split into two families: reasoning-centric LLMs, which
strengthen internal chain-of-thought reasoning but cannot invoke external
tools, and agentic LLMs, which learn to interact with environments and leverage
tools but often lag in deep reasoning. This divide arises from fundamentally
different training objectives, leading to mismatched strengths and inefficiency
on simple queries, where both families tend to overthink or over-call tools. In
this work, we present Adaptive Agent Foundation Model (A\textsuperscript{2}FM),
a unified framework that follows a route-then-align principle: the model first
learns task-aware routing and then aligns mode-specific trajectories under a
shared backbone. To address the inefficiency gap, we introduce a third
mode-instant-that handles simple queries directly, preventing unnecessary
reasoning or tool calls while complementing the agentic and reasoning modes. To
jointly enhance accuracy and efficiency, we propose Adaptive Policy
Optimization (APO), which enforces adaptive sampling across modes and applies a
cost-regularized reward. On the 32B scale, A\textsuperscript{2}FM achieves
13.4\% on BrowseComp, 70.4\% on AIME25, and 16.7\% on HLE, setting new SOTA
among comparable models and performing competitively with frontier LLMs across
agentic, reasoning, and general benchmarks. Notably, the adaptive execution
achieves a cost of pass of only \$0.00487 per correct answer-cutting cost by
45.2\% relative to reasoning and 33.5\% relative to agentic, thus delivering
substantially higher cost efficiency while maintaining comparable accuracy.

</details>


### [12] [FaStFACT: Faster, Stronger Long-Form Factuality Evaluations in LLMs](https://arxiv.org/abs/2510.12839)
*Yingjia Wan,Haochen Tan,Xiao Zhu,Xinyu Zhou,Zhiwei Li,Qingsong Lv,Changxuan Sun,Jiaqi Zeng,Yi Xu,Jianqiao Lu,Yinhong Liu,Zhijiang Guo*

Main category: cs.CL

TL;DR: 提出FastFact框架，通过分块级声明提取、置信度预验证和文档级证据收集，高效评估长文本LLM生成的事实性，在准确性和效率上优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法存在效率低下（复杂流水线不适用于长文本）和效果不佳（声明集不准确、证据收集不足）的问题。

Method: 采用分块级声明提取与置信度预验证相结合，减少网络搜索和推理调用成本；收集文档级证据并在验证时选择性检索，解决证据不足问题。

Result: 在聚合的人工标注基准测试中，FastFact在效率和效果上都表现出色，与人工评估对齐度最高。

Conclusion: FastFact是一个快速且强大的评估框架，能可靠地评估长文本LLM生成的事实性，代码和基准数据已开源。

Abstract: Evaluating the factuality of long-form generations from Large Language Models
(LLMs) remains challenging due to accuracy issues and costly human assessment.
Prior efforts attempt this by decomposing text into claims, searching for
evidence, and verifying claims, but suffer from critical drawbacks: (1)
inefficiency due to complex pipeline components unsuitable for long LLM
outputs, and (2) ineffectiveness stemming from inaccurate claim sets and
insufficient evidence collection of one-line snippets.
  To address these limitations, we propose \name, a fast and strong evaluation
framework that achieves the highest alignment with human evaluation and
efficiency among existing baselines. \name first employs chunk-level claim
extraction integrated with confidence-based pre-verification, significantly
reducing the cost of web searching and inference calling while ensuring
reliability. For searching and verification, it collects document-level
evidence from crawled webpages and selectively retrieves it during
verification, addressing the evidence insufficiency problem in previous
pipelines.
  Extensive experiments based on an aggregated and manually annotated benchmark
demonstrate the reliability of \name in both efficiently and effectively
evaluating the factuality of long-form LLM generations. Code and benchmark data
is available at https://github.com/Yingjia-Wan/FastFact.

</details>


### [13] [VLURes: Benchmarking VLM Visual and Linguistic Understanding in Low-Resource Languages](https://arxiv.org/abs/2510.12845)
*Jesse Atuhurra,Iqra Ali,Tomoya Iwakura,Hidetaka Kamigaito,Tatsuya Hiraoka*

Main category: cs.CL

TL;DR: VLURes是一个新的多语言视觉语言理解基准，涵盖英语、日语、斯瓦希里语和乌尔都语四种语言，包含八个视觉语言任务和一个新颖的不相关性任务，用于评估VLMs在长文本设置下的细粒度能力。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs评估主要局限于英语为中心的基准测试，且图像-文本对通常包含短文本。为了评估VLMs在多语言长文本环境下的细粒度能力，需要开发新的评估基准。

Method: 从目标语言的网络资源中策划数据集，涵盖十个不同的图像类别和丰富的文本上下文。通过提示VLMs生成响应和推理，进行自动评估和母语人士评估。

Result: 评估了十个VLMs，最佳模型GPT-4o总体准确率达到90.8%，但仍比人类表现低6.7%。开源模型与人类表现的差距更大。

Conclusion: VLURes基准在开发能够处理多模态视觉推理的智能代理方面发挥着关键作用，揭示了不同语言和任务之间的性能差异。

Abstract: Vision Language Models (VLMs) are pivotal for advancing perception in
intelligent agents. Yet, evaluation of VLMs remains limited to predominantly
English-centric benchmarks in which the image-text pairs comprise short texts.
To evaluate VLM fine-grained abilities, in four languages under long-text
settings, we introduce a novel multilingual benchmark VLURes featuring eight
vision-and-language tasks, and a pioneering unrelatedness task, to probe the
fine-grained Visual and Linguistic Understanding capabilities of VLMs across
English, Japanese, and low-resource languages, Swahili, and Urdu. Our datasets,
curated from web resources in the target language, encompass ten diverse image
categories and rich textual context, introducing valuable vision-language
resources for Swahili and Urdu. By prompting VLMs to generate responses and
rationales, evaluated automatically and by native speakers, we uncover
performance disparities across languages and tasks critical to intelligent
agents, such as object recognition, scene understanding, and relationship
understanding. We conducted evaluations of ten VLMs with VLURes. The best
performing model, GPT-4o, achieves an overall accuracy of 90.8% and lags human
performance by 6.7%, though the gap is larger for open-source models. The gap
highlights VLURes' critical role in developing intelligent agents to tackle
multi-modal visual reasoning.

</details>


### [14] [Efficient Adaptive Transformer: An Empirical Study and Reproducible Framework](https://arxiv.org/abs/2510.12856)
*Jan Miller*

Main category: cs.CL

TL;DR: EAT框架统一了三种自适应效率技术（渐进式token剪枝、稀疏注意力、动态提前退出），提供了一个开源的可复现架构和基准测试管道，用于输入自适应推理。


<details>
  <summary>Details</summary>
Motivation: 开发一个统一的、可复现的框架来整合多种自适应效率技术，为社区提供研究自适应transformer的工具。

Method: 将渐进式token剪枝、稀疏注意力和动态提前退出三种技术集成到单一架构中，并开发了包含数据处理、计时和消融实验的自动化基准测试管道。

Result: 在浅层六层模型中组合这些机制可能增加延迟，但在SST-2任务上比优化的DistilBERT基线获得了稍高的准确率，展示了动态计算在延迟敏感NLP中的潜力。

Conclusion: EAT的主要贡献是提供了一个开放的、端到端可复现的框架，包含脚本、CSV日志和分析工具，旨在作为社区进一步研究自适应transformer的工具。

Abstract: The Efficient Adaptive Transformer (EAT) framework unifies three adaptive
efficiency techniques - progressive token pruning, sparse attention, and
dynamic early exiting - into a single, reproducible architecture for
input-adaptive inference. EAT provides an open-source benchmarking pipeline
that automates data processing, timing, and ablation across GLUE tasks (SST-2,
QQP, MNLI). Although this empirical study finds that combining these mechanisms
can increase latency in shallow six-layer models, it demonstrates that EAT
achieves slightly higher accuracy than the optimized DistilBERT baseline on
SST-2, illustrating the potential of dynamic computation for latency-sensitive
NLP. The main contribution is the open, end-to-end reproducible framework -
complete with scripts, CSV logging, and analysis utilities - intended to serve
as a community tool for further research on adaptive transformers.

</details>


### [15] [A Critical Review of the Need for Knowledge-Centric Evaluation of Quranic Recitation](https://arxiv.org/abs/2510.12858)
*Mohammed Hilal Al-Kharusi,Khizar Hayat,Khalil Bader Al Ruqeishi,Haroon Rashid Lone*

Main category: cs.CL

TL;DR: 这篇文献综述分析了古兰经诵读自动评估工具的现状，指出当前基于自动语音识别的方法存在根本性错位，并提出向基于规则的知识中心计算框架转变的必要性。


<details>
  <summary>Details</summary>
Motivation: 古兰经诵读（Tajweed）作为受精确语音、韵律和神学规则约束的神圣实践，在现代面临教学挑战。虽然数字技术提供了前所未有的教育机会，但现有的自动诵读评估工具未能实现广泛采用或教学效果。

Method: 对过去二十年的学术研究、网络平台和商业应用进行全面分析，揭示当前方法的根本问题。

Result: 分析发现主流方法错误地重新利用自动语音识别架构，这些方法优先考虑词汇识别而非定性声学评估，存在数据依赖性、人口偏见和无法提供诊断性反馈的问题。

Conclusion: 古兰经自动评估的未来在于混合系统，将深层语言学知识与先进音频分析相结合，为全球学习者提供稳健、公平且教学合理的工具。

Abstract: The sacred practice of Quranic recitation (Tajweed), governed by precise
phonetic, prosodic, and theological rules, faces significant pedagogical
challenges in the modern era. While digital technologies promise unprecedented
access to education, automated tools for recitation evaluation have failed to
achieve widespread adoption or pedagogical efficacy. This literature review
investigates this critical gap, conducting a comprehensive analysis of academic
research, web platforms, and commercial applications developed over the past
two decades. Our synthesis reveals a fundamental misalignment in prevailing
approaches that repurpose Automatic Speech Recognition (ASR) architectures,
which prioritize lexical recognition over qualitative acoustic assessment and
are plagued by data dependency, demographic biases, and an inability to provide
diagnostically useful feedback. Critiquing these data--driven paradigms, we
argue for a foundational paradigm shift towards a knowledge-centric
computational framework. Capitalizing on the immutable nature of the Quranic
text and the precisely defined rules of Tajweed, we propose that a robust
evaluator must be architected around anticipatory acoustic modeling based on
canonical rules and articulation points (Makhraj), rather than relying on
statistical patterns learned from imperfect and biased datasets. This review
concludes that the future of automated Quranic evaluation lies in hybrid
systems that integrate deep linguistic knowledge with advanced audio analysis,
offering a path toward robust, equitable, and pedagogically sound tools that
can faithfully support learners worldwide.

</details>


### [16] [EduDial: Constructing a Large-scale Multi-turn Teacher-Student Dialogue Corpus](https://arxiv.org/abs/2510.12899)
*Shouang Wei,Min Zhang,Xin Lin,Bo Jiang,Zhongxiang Dai,Kun Kuang*

Main category: cs.CL

TL;DR: 提出了EduDial，一个全面的多轮师生对话数据集，包含34,250个对话会话，覆盖345个核心知识点，基于布鲁姆教育目标分类学和十种提问策略设计。在此基础上开发了EduDial-LLM 32B模型，并提出了11维评估框架，实验表明该模型在17个主流LLM中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在教育领域的应用日益重要，需要专门的师生对话基准来评估其教学能力，现有基准无法充分捕捉真实课堂互动。

Method: 构建EduDial数据集，基于布鲁姆教育目标分类学和十种提问策略（包括情境提问、最近发展区提问、元认知提问等），为不同认知水平学生设计差异化教学策略，并训练EduDial-LLM 32B模型。

Result: 在17个主流LLM上的实验表明，大多数模型在学生中心教学场景中表现不佳，而EduDial-LLM在所有指标上均显著优于基线模型。

Conclusion: EduDial为评估LLM的教学能力提供了全面基准，EduDial-LLM在师生对话场景中表现出色，为智能教育应用提供了有力支持。

Abstract: Recently, several multi-turn dialogue benchmarks have been proposed to
evaluate the conversational abilities of large language models (LLMs). As LLMs
are increasingly recognized as a key technology for advancing intelligent
education, owing to their ability to deeply understand instructional contexts
and provide personalized guidance, the construction of dedicated
teacher-student dialogue benchmarks has become particularly important. To this
end, we present EduDial, a comprehensive multi-turn teacher-student dialogue
dataset. EduDial covers 345 core knowledge points and consists of 34,250
dialogue sessions generated through interactions between teacher and student
agents. Its design is guided by Bloom's taxonomy of educational objectives and
incorporates ten questioning strategies, including situational questioning,
zone of proximal development (ZPD) questioning, and metacognitive
questioning-thus better capturing authentic classroom interactions.
Furthermore, we design differentiated teaching strategies for students at
different cognitive levels, thereby providing more targeted teaching guidance.
Building on EduDial, we further develop EduDial-LLM 32B via training and
propose an 11-dimensional evaluation framework that systematically measures the
teaching abilities of LLMs, encompassing both overall teaching quality and
content quality. Experiments on 17 mainstream LLMs reveal that most models
struggle in student-centered teaching scenarios, whereas our EduDial-LLM
achieves significant gains, consistently outperforming all baselines across all
metrics. The code is available at
https://github.com/Mind-Lab-ECNU/EduDial/tree/main.

</details>


### [17] [Who's Asking? Evaluating LLM Robustness to Inquiry Personas in Factual Question Answering](https://arxiv.org/abs/2510.12925)
*Nil-Jana Akpinar,Chia-Jung Lee,Vanessa Murdock,Pietro Perona*

Main category: cs.CL

TL;DR: 该论文首次系统评估了大型语言模型对用户个人资料（如身份、专业知识或信仰）的鲁棒性，发现这些用户信息会显著影响问答准确性并引发多种失败模式。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型应该基于客观知识真实回答事实性问题，而不应受用户个人背景信息或系统个性化影响。现有研究主要关注对抗性输入或干扰因素，但缺乏对真实用户交互中常见个人资料影响的系统性评估。

Method: 通过评估LLM对询问者个人资料（如身份、专业知识、信仰等属性）的鲁棒性，分析这些用户信息如何影响模型的事实问答表现。

Result: 研究发现用户个人资料提示能显著改变问答准确性，并触发拒绝回答、幻觉限制和角色混淆等失败模式。

Conclusion: 模型对用户框架的敏感性会损害事实可靠性，询问者个人资料测试应成为鲁棒性评估的有效工具。

Abstract: Large Language Models (LLMs) should answer factual questions truthfully,
grounded in objective knowledge, regardless of user context such as
self-disclosed personal information, or system personalization. In this paper,
we present the first systematic evaluation of LLM robustness to inquiry
personas, i.e. user profiles that convey attributes like identity, expertise,
or belief. While prior work has primarily focused on adversarial inputs or
distractors for robustness testing, we evaluate plausible, human-centered
inquiry persona cues that users disclose in real-world interactions. We find
that such cues can meaningfully alter QA accuracy and trigger failure modes
such as refusals, hallucinated limitations, and role confusion. These effects
highlight how model sensitivity to user framing can compromise factual
reliability, and position inquiry persona testing as an effective tool for
robustness evaluation.

</details>


### [18] [The Curious Case of Curiosity across Human Cultures and LLMs](https://arxiv.org/abs/2510.12943)
*Angana Borah,Rada Mihalcea*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在不同文化背景下的好奇心表达，发现LLMs倾向于西方文化的好奇心表达方式，并通过微调策略将人机对齐差距缩小了50%。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在人类互动中扮演着越来越重要的角色，但好奇心这一核心探究驱动力在跨文化背景下仍未得到充分探索。

Method: 使用Yahoo! Answers多国数据集，引入CUEST评估框架，通过语言风格、话题偏好分析和社会科学构建来测量人机好奇心对齐度。

Result: 发现LLMs在不同文化间扁平化多样性，更接近西方国家的表达方式；通过微调策略将人机对齐差距缩小了50%；展示了好奇心对LLM跨文化适应性的实际价值。

Conclusion: 好奇心对于未来NLP研究具有重要性，特别是在提升LLM跨文化适应性方面。

Abstract: Recent advances in Large Language Models (LLMs) have expanded their role in
human interaction, yet curiosity -- a central driver of inquiry -- remains
underexplored in these systems, particularly across cultural contexts. In this
work, we investigate cultural variation in curiosity using Yahoo! Answers, a
real-world multi-country dataset spanning diverse topics. We introduce CUEST
(CUriosity Evaluation across SocieTies), an evaluation framework that measures
human-model alignment in curiosity through linguistic (style), topic preference
(content) analysis and grounding insights in social science constructs. Across
open- and closed-source models, we find that LLMs flatten cross-cultural
diversity, aligning more closely with how curiosity is expressed in Western
countries. We then explore fine-tuning strategies to induce curiosity in LLMs,
narrowing the human-model alignment gap by up to 50\%. Finally, we demonstrate
the practical value of curiosity for LLM adaptability across cultures, showing
its importance for future NLP research.

</details>


### [19] [3-Model Speculative Decoding](https://arxiv.org/abs/2510.12966)
*Sanghyun Byun,Mohanad Odema,Jung Ick Guack,Baisub Lee,Jacob Song,Woo Seong Chung*

Main category: cs.CL

TL;DR: Pyramid Speculative Decoding (PyramidSD) 通过在草稿模型和目标模型之间插入一个中间限定模型来改进推测解码，使用更小的草稿模型同时保持高接受率，实现了最高1.91倍的生成加速。


<details>
  <summary>Details</summary>
Motivation: 传统推测解码存在草稿模型大小与token接受率之间的权衡：较小的草稿模型生成更快但与目标模型差异更大，导致接受率降低和加速效果减弱。

Method: 引入中间限定模型来弥合草稿模型和目标模型之间的分布差距，采用分层解码策略和模糊接受标准，支持每个阶段的宽松差异阈值。

Result: 在消费级GPU上达到每秒124个token，在1B参数草稿模型和8B目标模型的小内存设置中，以最小质量损失换取吞吐量提升。

Conclusion: PyramidSD提供了一种实用的方法来增强推测解码效率，可以轻松应用于现有的推理流水线。

Abstract: Speculative Decoding (SD) accelerates inference in large language models by
using a smaller draft model to propose tokens, which are then verified by a
larger target model. However, the throughput gains of SD are fundamentally
limited by a trade-off between draft model size and token acceptance: smaller
draft models generate tokens more quickly but exhibit greater divergence from
the target model, resulting in lower acceptance rates and reduced speedups. We
introduce Pyramid Speculative Decoding (PyramidSD), an extension of SD that
inserts an intermediate qualifier model between the draft and target to bridge
the distributional gap in output predictions, allowing smaller model to be used
for drafting. This hierarchical decoding strategy improves alignment across
models, enabling higher acceptance rates and allowing the use of significantly
smaller draft models without sacrificing overall performance. PyramidSD builds
on fuzzy acceptance criteria to support relaxed divergence thresholds at each
stage, improving throughput. In experiments, PyramidSD achieves up to 1.91x
generation speed over standard SD, reaching 124 tokens per second on a consumer
GPU (RTX 4090). In small-memory settings with a 1B-parameter draft model and an
8B target model, PyramidSD minimally trades target model quality for improved
throughput. Overall, PyramidSD offers a practical approach to enhancing
speculative decoding efficiency and can be readily applied to existing
inference pipelines.

</details>


### [20] [A Multilingual, Large-Scale Study of the Interplay between LLM Safeguards, Personalisation, and Disinformation](https://arxiv.org/abs/2510.12993)
*João A. Leite,Arnav Arora,Silvia Gargova,João Luz,Gustavo Sampaio,Ian Roberts,Carolina Scarton,Kalina Bontcheva*

Main category: cs.CL

TL;DR: 本研究首次大规模评估LLM生成针对特定人口属性个性化虚假信息的能力，发现简单个性化策略显著增加所有LLM的越狱概率，并改变语言模式增强虚假信息说服力。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究表明LLM能生成虚假信息，但关于其生成针对特定人口属性（如国家、世代、政治倾向）的个性化虚假信息的说服力问题尚未得到充分研究。

Method: 采用红队测试方法，系统评估LLM安全机制对个性化提示的鲁棒性，创建包含160万文本的AI-TRAITS数据集，涵盖4种语言、324个虚假信息叙事和150种人物画像。

Result: 个性化提示显著增加所有LLM越狱概率，改变语言和修辞模式，增强虚假信息说服力，暴露了当前最先进LLM在跨语言和跨人口统计环境中的关键漏洞。

Conclusion: 当前LLM在应对个性化虚假信息生成方面存在严重安全漏洞，研究结果为改进多语言和跨人口统计环境中的安全对齐和检测策略提供了基础。

Abstract: The human-like proficiency of Large Language Models (LLMs) has brought
concerns about their potential misuse for generating persuasive and
personalised disinformation at scale. While prior work has demonstrated that
LLMs can generate disinformation, specific questions around persuasiveness and
personalisation (generation of disinformation tailored to specific demographic
attributes) remain largely unstudied. This paper presents the first
large-scale, multilingual empirical study on persona-targeted disinformation
generation by LLMs. Employing a red teaming methodology, we systematically
evaluate the robustness of LLM safety mechanisms to persona-targeted prompts. A
key novel result is AI-TRAITS (AI-generaTed peRsonAlIsed disinformaTion
dataSet), a new dataset of around 1.6 million texts generated by eight
state-of-the-art LLMs. AI-TRAITS is seeded by prompts that combine 324
disinformation narratives and 150 distinct persona profiles, covering four
major languages (English, Russian, Portuguese, Hindi) and key demographic
dimensions (country, generation, political orientation). The resulting
personalised narratives are then assessed quantitatively and compared along the
dimensions of models, languages, jailbreaking rate, and personalisation
attributes. Our findings demonstrate that the use of even simple
personalisation strategies in the prompts significantly increases the
likelihood of jailbreaks for all studied LLMs. Furthermore, personalised
prompts result in altered linguistic and rhetorical patterns and amplify the
persuasiveness of the LLM-generated false narratives. These insights expose
critical vulnerabilities in current state-of-the-art LLMs and offer a
foundation for improving safety alignment and detection strategies in
multilingual and cross-demographic contexts.

</details>


### [21] [OPLoRA: Orthogonal Projection LoRA Prevents Catastrophic Forgetting during Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2510.13003)
*Yifeng Xiong,Xiaohui Xie*

Main category: cs.CL

TL;DR: OPLoRA是一种基于正交投影的LoRA改进方法，通过双面正交投影约束LoRA更新，防止对预训练知识中重要奇异方向的干扰，从而显著减少灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 传统的LoRA在微调大型语言模型时存在灾难性遗忘问题，学习到的更新会干扰编码重要预训练知识的支配奇异方向。

Method: 通过SVD分解冻结权重，使用投影矩阵P_L = I - U_k U_k^⊤和P_R = I - V_k V_k^⊤将LoRA更新约束在top-k奇异子空间的正交补空间中。

Result: 在常识推理、数学和代码生成任务上的广泛实验表明，OPLoRA显著减少了遗忘，同时在LLaMA-2 7B和Qwen2.5 7B上保持了竞争力的任务特定性能。

Conclusion: 正交投影是参数高效微调中知识保留的有效机制，OPLoRA为知识保留提供了数学保证。

Abstract: Low-Rank Adaptation (LoRA) enables efficient fine-tuning of large language
models but suffers from catastrophic forgetting when learned updates interfere
with the dominant singular directions that encode essential pre-trained
knowledge. We propose Orthogonal Projection LoRA (OPLoRA), a theoretically
grounded approach that prevents this interference through double-sided
orthogonal projections. By decomposing frozen weights via SVD, OPLoRA
constrains LoRA updates to lie entirely within the orthogonal complement of the
top-$k$ singular subspace using projections $P_L = I - U_k U_k^\top$ and $P_R =
I - V_k V_k^\top$. We prove that this construction exactly preserves the
top-$k$ singular triples, providing mathematical guarantees for knowledge
retention. To quantify subspace interference, we introduce $\rho_k$, a metric
measuring update alignment with dominant directions. Extensive experiments
across commonsense reasoning, mathematics, and code generation demonstrate that
OPLoRA significantly reduces forgetting while maintaining competitive
task-specific performance on LLaMA-2 7B and Qwen2.5 7B, establishing orthogonal
projection as an effective mechanism for knowledge preservation in
parameter-efficient fine-tuning.

</details>


### [22] [CurLL: A Developmental Framework to Evaluate Continual Learning in Language Models](https://arxiv.org/abs/2510.13008)
*Pavan Kalyan,Shubhra Mishra,Satya Lokam,Navin Goyal*

Main category: cs.CL

TL;DR: CurlL是一个基于人类5-10岁发展轨迹的持续学习数据集和基准，包含五个发展阶段，通过技能图谱将广泛技能分解为更小的能力、具体目标和可测量指标，并捕捉能力间的依赖关系。


<details>
  <summary>Details</summary>
Motivation: 当前持续学习评估缺乏基于人类发展轨迹的系统性和细粒度评估，需要能够精确分析模型逐步获取新技能能力的数据集。

Method: 生成了234亿token的合成数据集，控制技能进展、词汇复杂性和格式多样性，包含段落、理解型QA、技能测试QA和指令-响应对。使用1.35亿参数的transformer模型在独立、联合和顺序（持续）设置下进行训练。

Result: 展示了技能保持和迁移效率之间的权衡，支持对遗忘、前向迁移和后向迁移的精确分析。

Conclusion: 通过模拟人类学习模式并提供对技能依赖关系的细粒度控制，这项工作推进了语言模型的持续学习评估。

Abstract: We introduce a comprehensive continual learning dataset and benchmark (CurlL)
grounded in human developmental trajectories from ages 5-10, enabling
systematic and fine-grained assessment of models' ability to progressively
acquire new skills. CurlL spans five developmental stages (0-4) covering ages
5-10, supported by a skill graph that breaks down broad skills into smaller
abilities, concrete goals, and measurable indicators, while also capturing
which abilities build on others. We generate a 23.4B-token synthetic dataset
with controlled skill progression, vocabulary complexity, and format diversity,
comprising paragraphs, comprehension-based QA (CQA), skill-testing QA (CSQA),
and instruction-response (IR) pairs. Stage-wise token counts range from 2.12B
to 6.78B tokens, supporting precise analysis of forgetting, forward transfer,
and backward transfer. Using a 135M-parameter transformer trained under
independent, joint, and sequential (continual) setups, we show trade-offs in
skill retention and transfer efficiency. By mirroring human learning patterns
and providing fine-grained control over skill dependencies, this work advances
continual learning evaluations for language models.

</details>


### [23] [On the Role of Preference Variance in Preference Optimization](https://arxiv.org/abs/2510.13022)
*Jiacheng Guo,Zihao Li,Jiahao Qiu,Yue Wu,Mengdi Wang*

Main category: cs.CL

TL;DR: 本文研究了偏好方差（PVar）对直接偏好优化（DPO）训练效果的影响，发现高PVar的提示能产生更大的梯度更新，从而更有效地训练语言模型。


<details>
  <summary>Details</summary>
Motivation: 收集人类偏好数据成本高昂且效率低下，需要减少所需标注的方法。偏好方差可能影响DPO训练的效果。

Method: 通过理论分析建立DPO梯度范数的上界，证明其受PVar控制；使用奖励模型生成偏好数据，在AlpacaEval 2.0和Arena-Hard基准上评估；比较不同PVar水平提示的训练效果。

Result: 实验表明高PVar提示优于随机选择或低PVar提示；使用小奖励模型（1B、3B）进行选择时方法依然稳健；在UltraFeedback数据集上，仅使用前10%高PVar提示训练比使用完整数据集效果更好。

Conclusion: 偏好方差是识别信息丰富示例以进行高效LLM对齐的重要指标，高PVar提示能显著提升DPO训练效率。

Abstract: Direct Preference Optimization (DPO) has emerged as an important approach for
learning from human preferences in aligning large language models (LLMs).
However, collecting human preference data is costly and inefficient, motivating
methods to reduce the required annotations. In this work, we investigate the
impact of \emph{preference variance} (PVar), which measures the variance in
model preferences when comparing pairs of responses, on the effectiveness of
DPO training. We provide a theoretical insight by establishing an upper bound
on the DPO gradient norm for any given prompt, showing it is controlled by the
PVar of that prompt. This implies that prompts with low PVar can only produce
small gradient updates, making them less valuable for learning. We validate
this finding by fine-tuning LLMs with preferences generated by a reward model,
evaluating on two benchmarks (AlpacaEval 2.0 and Arena-Hard). Experimental
results demonstrate that prompts with higher PVar outperform randomly selected
prompts or those with lower PVar. We also show that our PVar-based selection
method is robust, when using smaller reward models (1B, 3B) for selection.
Notably, in a separate experiment using the original human annotations from the
UltraFeedback dataset, we found that training on only the top 10\% of prompts
with the highest PVar yields better evaluation performance than training on the
full dataset, highlighting the importance of preference variance in identifying
informative examples for efficient LLM alignment.

</details>


### [24] [GatePro: Parameter-Free Expert Selection Optimization for Mixture-of-Experts Models](https://arxiv.org/abs/2510.13079)
*Chen Zheng,Yuhang Cai,Deyi Liu,Jin Ma,Yiyuan Ma,Yuan Yang,Jing Liu,Yutao Zeng,Xun Zhou,Siyuan Qiao*

Main category: cs.CL

TL;DR: GatePro是一种无需参数的方法，通过在相似专家之间引入局部竞争机制来提升MoE架构中的专家选择多样性，避免功能冗余计算。


<details>
  <summary>Details</summary>
Motivation: 现有的MoE架构面临功能相似专家同时被选择的问题，导致冗余计算和有效模型容量受限，而现有的平衡损失方法无法解决根本的专家多样性问题。

Method: GatePro识别最相似的专家对，并引入局部竞争机制，防止冗余专家同时激活，同时保持自然的专家专业化。

Result: 综合评估表明GatePro在不同模型规模和基准测试中都有效，能够实现增强的专家多样性，使专家发展出更独特和互补的能力。

Conclusion: GatePro提供了一种实用的解决方案，可在任何训练阶段热插拔部署，无需额外可学习参数，有效提升MoE架构的效率。

Abstract: Modern large language models leverage Mixture-of-Experts (MoE) architectures
for efficient scaling, but face a critical challenge: functionally similar
experts are often selected simultaneously, creating redundant computation and
limiting effective model capacity. Existing auxiliary balance loss methods
improve token distribution but fail to address the underlying expert diversity
problem. We introduce GatePro, a novel parameter-free method that directly
promotes expert selection diversity. GatePro identifies the most similar expert
pairs and introduces localized competition mechanisms, preventing redundant
expert co-activation while maintaining natural expert specialization. Our
comprehensive evaluation demonstrates GatePro's effectiveness across model
scales and benchmarks. Analysis demonstrates GatePro's ability to achieve
enhanced expert diversity, where experts develop more distinct and
complementary capabilities, avoiding functional redundancy. This approach can
be deployed hot-swappable during any training phase without additional
learnable parameters, offering a practical solution for improving MoE
effectiveness.

</details>


### [25] [ESI: Epistemic Uncertainty Quantification via Semantic-preserving Intervention for Large Language Models](https://arxiv.org/abs/2510.13103)
*Mingda Li,Xinyu Li,Weinan Zhang,Longxuan Ma*

Main category: cs.CL

TL;DR: 提出了一种基于因果视角的灰盒不确定性量化方法，通过语义保持干预前后模型输出的变化来估计LLM的认知不确定性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的不确定性量化具有重要意义但具有挑战性，现有方法存在局限性。

Method: 从因果视角建立LLM不确定性与语义保持干预下不变性的联系，提出灰盒不确定性量化方法，测量语义保持干预前后模型输出的变化。

Result: 在多种LLM和问答数据集上的广泛实验表明，该方法在有效性和计算效率方面都表现出色。

Conclusion: 该方法为LLM不确定性量化提供了有效且高效的新途径，通过因果视角建立了理论依据。

Abstract: Uncertainty Quantification (UQ) is a promising approach to improve model
reliability, yet quantifying the uncertainty of Large Language Models (LLMs) is
non-trivial. In this work, we establish a connection between the uncertainty of
LLMs and their invariance under semantic-preserving intervention from a causal
perspective. Building on this foundation, we propose a novel grey-box
uncertainty quantification method that measures the variation in model outputs
before and after the semantic-preserving intervention. Through theoretical
justification, we show that our method provides an effective estimate of
epistemic uncertainty. Our extensive experiments, conducted across various LLMs
and a variety of question-answering (QA) datasets, demonstrate that our method
excels not only in terms of effectiveness but also in computational efficiency.

</details>


### [26] [Multi-Label Clinical Text Eligibility Classification and Summarization System](https://arxiv.org/abs/2510.13115)
*Surya Tejaswi Yerramsetty,Almas Fathimah*

Main category: cs.CL

TL;DR: 提出一个结合NLP和LLM的自动化系统，用于多标签临床文本资格分类和摘要生成，通过特征提取和多标签分类方法提高临床试验资格评估效率。


<details>
  <summary>Details</summary>
Motivation: 临床试验在医学进步中至关重要，但需要包含具有适当和多样化医学背景的参与者。传统方法效率低下，需要自动化系统来改进临床试验资格评估流程。

Method: 使用NLP和LLM技术，结合词嵌入(Word2Vec)、命名实体识别、计数向量化和TF-IDF等特征提取方法，开发加权TF-IDF词嵌入。应用随机森林和SVM进行多标签分类，评估TextRank、Luhn和GPT-3等摘要技术。

Result: 通过ROUGE分数评估显示所提方法有效，系统在自动化临床试验资格评估方面显示出潜力，能够提高研究效率。

Conclusion: 该系统展示了使用数据驱动方法自动化临床试验资格评估的可行性，为改进医疗研究效率提供了有前景的解决方案。

Abstract: Clinical trials are central to medical progress because they help improve
understanding of human health and the healthcare system. They play a key role
in discovering new ways to detect, prevent, or treat diseases, and it is
essential that clinical trials include participants with appropriate and
diverse medical backgrounds. In this paper, we propose a system that leverages
Natural Language Processing (NLP) and Large Language Models (LLMs) to automate
multi-label clinical text eligibility classification and summarization. The
system combines feature extraction methods such as word embeddings (Word2Vec)
and named entity recognition to identify relevant medical concepts, along with
traditional vectorization techniques such as count vectorization and TF-IDF
(Term Frequency-Inverse Document Frequency). We further explore weighted TF-IDF
word embeddings that integrate both count-based and embedding-based strengths
to capture term importance effectively. Multi-label classification using Random
Forest and SVM models is applied to categorize documents based on eligibility
criteria. Summarization techniques including TextRank, Luhn, and GPT-3 are
evaluated to concisely summarize eligibility requirements. Evaluation with
ROUGE scores demonstrates the effectiveness of the proposed methods. This
system shows potential for automating clinical trial eligibility assessment
using data-driven approaches, thereby improving research efficiency.

</details>


### [27] [Stable LLM Ensemble: Interaction between Example Representativeness and Diversity](https://arxiv.org/abs/2510.13143)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: 该研究提出了一种结合代表性示例选择和增加采样温度的方法，显著提升单样本LLM集成模型的性能，在宏F1和RMSE指标上均优于随机选择和5样本提示方法。


<details>
  <summary>Details</summary>
Motivation: 当前单样本LLM预测的准确性和鲁棒性对示例选择和集成成员多样性高度敏感，需要系统研究示例代表性和输出多样性对集成性能的影响。

Method: 比较两种单样本策略：基于质心的代表性示例（提出方法）和随机采样示例（基线），同时变化采样温度参数。

Result: 提出的方法在较高温度设置下显著优于随机选择：宏F1提高7.6%，RMSE降低10.5%；同时超过5样本提示：宏F1提高21.1%，RMSE降低24.0%。

Conclusion: 结合代表性示例选择与增加温度能为集成提供适当水平的多样性，强调了示例选择和受控多样性在设计有效单样本LLM集成中的实际重要性。

Abstract: Large language models (LLMs) have achieved remarkable results in wide range
of domains. However, the accuracy and robustness of one-shot LLM predictions
remain highly sensitive to the examples and the diversity among ensemble
members. This study systematically investigates the effects of example
representativeness (one-shot strategy) and output diversity (sampling
temperature) on LLM ensemble performance. Two one-shot strategies are compared:
centroid-based representative examples (proposed) and randomly sampled examples
(baseline) and sampling temperature also is varied. The proposed approach with
higher temperature setting significantly outperforms random selection by +7.6%
(macro-F1) and -10.5% (RMSE). Furthermore, the proposed model exceeds 5-shot
prompting by +21.1% (macro-F1) and -24.0% (RMSE). Our findings demonstrate that
combining representative example selection with increased temperature provides
the appropriate level of diversity to the ensemble. This work highlights the
practical importance of both example selection and controlled diversity in
designing effective one-shot LLM ensembles.

</details>


### [28] [I Am Aligned, But With Whom? MENA Values Benchmark for Evaluating Cultural Alignment and Multilingual Bias in LLMs](https://arxiv.org/abs/2510.13154)
*Pardis Sadat Zahraei,Ehsaneddin Asgari*

Main category: cs.CL

TL;DR: MENAValues是一个评估大语言模型在中东和北非地区文化对齐和偏见的新基准，揭示了跨语言价值偏移、推理诱导退化等关键现象。


<details>
  <summary>Details</summary>
Motivation: 中东和北非地区在当前AI评估工作中代表性不足，需要专门工具来评估LLMs与该地区文化和价值观的对齐程度。

Method: 基于大规模人类调查构建结构化数据集，在三种视角框架（中性、个性化、第三人称）和两种语言模式（英语和本地语言）下评估多种模型。

Result: 发现跨语言价值偏移、推理诱导退化和Logit泄漏三个关键现象，模型在本地语言中会将多样国家视为单一实体。

Conclusion: MENAValues提供了一个可扩展的诊断文化错位框架，为开发更具文化包容性的AI提供实证见解和方法工具。

Abstract: We introduce MENAValues, a novel benchmark designed to evaluate the cultural
alignment and multilingual biases of large language models (LLMs) with respect
to the beliefs and values of the Middle East and North Africa (MENA) region, an
underrepresented area in current AI evaluation efforts. Drawing from
large-scale, authoritative human surveys, we curate a structured dataset that
captures the sociocultural landscape of MENA with population-level response
distributions from 16 countries. To probe LLM behavior, we evaluate diverse
models across multiple conditions formed by crossing three perspective framings
(neutral, personalized, and third-person/cultural observer) with two language
modes (English and localized native languages: Arabic, Persian, Turkish). Our
analysis reveals three critical phenomena: "Cross-Lingual Value Shifts" where
identical questions yield drastically different responses based on language,
"Reasoning-Induced Degradation" where prompting models to explain their
reasoning worsens cultural alignment, and "Logit Leakage" where models refuse
sensitive questions while internal probabilities reveal strong hidden
preferences. We further demonstrate that models collapse into simplistic
linguistic categories when operating in native languages, treating diverse
nations as monolithic entities. MENAValues offers a scalable framework for
diagnosing cultural misalignment, providing both empirical insights and
methodological tools for developing more culturally inclusive AI.

</details>


### [29] [Mirror Speculative Decoding: Breaking the Serial Barrier in LLM Inference](https://arxiv.org/abs/2510.13161)
*Nikhil Bhendawade,Kumari Nishu,Arnav Kundu,Chris Bartels,Minsik Cho,Irina Belousova*

Main category: cs.CL

TL;DR: Mirror-SD是一种推理算法，通过并行异构执行和多令牌推测流式处理，打破延迟-接受率的权衡，在保持高接受率的同时显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: 传统推测解码方法在增加草案大小时会提高接受率但引入额外延迟，现有方法（Medusa、Hydra、EAGLE）要么降低接受率，要么引入限制扩展的开销。

Method: 使用分支完整展开从早期退出信号并行启动，在异构加速器（GPU和NPU）上显式映射计算以利用跨设备并行性；草案推测目标模型的前向延续，目标模型同时推测草案的修正路径；添加推测流式处理使草案每步生成多个令牌。

Result: 在SpecBench上，使用14B到66B参数的服务器规模模型，Mirror-SD实现了2.8倍到5.8倍的端到端加速，相比最强基线EAGLE3平均相对提升30%。

Conclusion: Mirror-SD通过并行异构执行和多令牌推测流式处理的双重策略，将推测解码推向高接受率和低开销的理想状态。

Abstract: Speculative decoding accelerates LLM inference by using a draft model to look
ahead, but gains are capped by the cost of autoregressive draft generation:
increasing draft size elevates acceptance rates but introduces additional
latency overhead exacerbating the speed-accuracy tradeoff. Prior methods
(Medusa, Hydra, EAGLE) partially reduce draft cost but either degrade
acceptance or introduce overheads that limit scaling. We present Mirror
Speculative Decoding (Mirror-SD), an inference algorithm that breaks the
latency-acceptance tradeoff. Mirror-SD launches branch-complete rollouts from
early-exit signals in parallel with the target model's suffix and explicitly
maps computation across heterogeneous accelerators (GPU and NPU) to exploit
cross-device parallelism. The draft speculates forward continuations for the
target to verify, while the target simultaneously speculates correction paths
for the draft, converting speculation into two complementary execution
pipelines. To further cut draft latency without weakening acceptance semantics,
we add speculative streaming so the draft emits multiple tokens per step. This
dual strategy of parallel heterogeneous execution plus multi-token speculative
streaming pushes speculative decoding toward its ideal regime of high
acceptance with low overhead. On SpecBench with server-scale models from 14B to
66B parameters, Mirror-SD delivers consistent end-to-end gains, achieving
2.8x-5.8x wall-time speedups across diverse tasks and a 30% average relative
improvement over the strongest baseline, EAGLE3.

</details>


### [30] [A Matter of Representation: Towards Graph-Based Abstract Code Generation](https://arxiv.org/abs/2510.13163)
*Nyx Iskandar,Hisham Bedri,Andy Tsen*

Main category: cs.CL

TL;DR: LLMs can generate graph-based abstract code using JSON representations, achieving high accuracy without complex pipelines, with representation choice significantly impacting performance.


<details>
  <summary>Details</summary>
Motivation: Most LLMs focus on sequential code generation, but there's little work on graph-based abstract code generation where logic is encapsulated in nodes and execution flow by edges, which is important for visual programming languages and when source code is inaccessible.

Method: Proposed and evaluated JSON representations for graphs, tested on ScratchTest benchmark based on custom Python re-implementation of Scratch to assess LLM performance in code graph space.

Result: LLMs can perform graph-based abstract code generation in single pass without specialized pipelines using correct graph representations, with different representations inducing significantly different accuracies.

Conclusion: This work establishes initial steps towards representation learning for graph-based abstract code generation, highlighting the critical role of graph representations in this task.

Abstract: Most large language models (LLMs) today excel at generating raw, sequential
code with minimal abstractions and custom structures. However, there has been
little work on graph-based abstract code generation, where significant logic is
encapsulated in predefined nodes and execution flow is determined by edges.
This is relevant for visual programming languages, and in cases where raw
source code is inaccessible to users and LLM training sets. In this work, we
propose and evaluate JSON representations for graphs to enable high accuracy
graph-based abstract code generation. We evaluate these representations on
ScratchTest, a mini-benchmark based on our custom Python re-implementation of
Scratch, which tests the LLM in code graph space. Our findings demonstrate that
LLMs can indeed perform the aforementioned generation task in a single pass
without relying on specialized or complex pipelines, given the correct graph
representations. We also show that different representations induce
significantly different accuracies, highlighting the instrumental role of
representations in this generation task. All in all, this work establishes the
first steps towards representation learning for graph-based abstract code
generation.

</details>


### [31] [CoT-Evo: Evolutionary Distillation of Chain-of-Thought for Scientific Reasoning](https://arxiv.org/abs/2510.13166)
*Kehua Feng,Keyan Ding,Zhihui Zhu,Lei Liang,Qiang Zhang,Huajun Chen*

Main category: cs.CL

TL;DR: CoT-Evo：一种进化式思维链蒸馏框架，通过多LLM思考者构建多样化推理轨迹，结合知识检索和进化算法迭代优化，生成高质量科学推理数据集，使小型模型在科学推理基准上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统思维链蒸馏在科学领域效果不佳，因为即使是先进大语言模型也经常产生错误或肤浅的推理，直接蒸馏会产生低质量训练数据，限制学生模型性能。

Method: 构建多样化推理轨迹池，自动检索领域知识进行丰富，通过新颖性驱动选择、反思性重组和变异迭代优化推理轨迹，使用评估答案正确性、连贯性和知识利用效率的适应度函数指导优化过程。

Result: 生成高质量科学推理思维链数据集，使用该数据集微调的紧凑模型在科学推理基准上达到最先进性能。

Conclusion: CoT-Evo建立了从多样化和易出错的大语言模型中合成高保真科学推理数据的可扩展方法。

Abstract: While chain-of-thought (CoT) distillation from advanced large language models
(LLMs) has proven effective in general reasoning tasks, it struggles in
scientific domains where even advanced models often produce incorrect or
superficial reasoning due to high complexity and specialized knowledge
requirements. Directly distilling from such flawed outputs results in
low-quality training data and limits the performance of smaller student models.
To overcome this, we propose CoT-Evo, an evolutionary CoT distillation
framework. It begins by constructing a diverse pool of reasoning trajectories
from multiple LLM thinkers, enriches them with automatically retrieved domain
knowledge, and iteratively refines the trajectories using novelty-driven
selection, reflective recombination and mutation. The refinement is guided by a
fitness function that evaluates answer correctness, coherence, and effective
knowledge utilization. This results in a high-quality CoT dataset tailored for
scientific reasoning. We employ this evolved dataset to fine-tune a compact
model, which achieves state-of-the-art performance on scientific reasoning
benchmarks. Our work establishes a scalable approach to synthesizing
high-fidelity scientific reasoning data from diverse and fallible LLMs.

</details>


### [32] [Putting on the Thinking Hats: A Survey on Chain of Thought Fine-tuning from the Perspective of Human Reasoning Mechanism](https://arxiv.org/abs/2510.13170)
*Xiaoshu Chen,Sihang Zhou,Ke Liang,Duanyang Yuan,Haoyuan Chen,Xiaoyu Sun,Linyuan Meng,Xinwang Liu*

Main category: cs.CL

TL;DR: 本文首次从人类推理理论的角度对CoT微调进行了系统综述，基于六顶思考帽框架对CoT微调方法进行分类分析，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有的CoT微调综述主要关注技术层面，而忽略了从人类推理机制角度的系统分析。鉴于CoT微调的最终目标是让LLM像人类一样推理，从人类认知角度研究这一技术至关重要。

Method: 基于著名的六顶思考帽框架，该框架使用六种隐喻性帽子系统化地表征常见的人类思维模式，作者通过这一视角对CoT微调方法进行分类和检验。

Result: 本文编制了现有数据集和模型性能的全面概述，并维护了一个实时GitHub仓库来持续跟踪该领域的最新进展。

Conclusion: 这项调查有望成为有价值的资源，激发这一快速演进领域的创新并促进进展。

Abstract: Chain of thought (CoT) fine-tuning aims to endow large language models (LLMs)
with reasoning capabilities by training them on curated reasoning traces. It
leverages both supervised and reinforced fine-tuning to cultivate human-like
reasoning skills in LLMs, including detailed planning, divergent thinking,
intuitive judgment, timely reflection, internal thinking, and fact perception,
etc. As CoT fine-tuning has advanced, LLMs have demonstrated substantial
improvements in tasks such as mathematical reasoning and code generation.
However, existing surveys about CoT fine-tuning primarily focus on technical
aspects and overlook a systematic analysis from the perspective of human
reasoning mechanisms. Given that the ultimate goal of CoT fine-tuning is to
enable LLMs to reason like humans, it is crucial to investigate this technique
through the lens of human cognition. To fill this gap, we present the first
comprehensive survey of CoT fine-tuning grounded in human reasoning theory.
Specifically, inspired by the well-known Six Thinking Hats framework, which
systematically characterizes common human thinking modes using six metaphorical
hats, we classify and examine CoT fine-tuning methods through this lens.
Furthermore, building upon this theory, we outline potential directions for
future research in CoT fine-tuning. In addition, we compile a comprehensive
overview of existing datasets and model performances, and a real-time GitHub
repository \footnote{https://github.com/AI-Chen/Awesome-CoT-Finetuning} that
continuously tracks recent advances in this area is maintained. We hope this
survey will serve as a valuable resource to inspire innovation and foster
progress in this rapidly evolving field.

</details>


### [33] [DSCD: Large Language Model Detoxification with Self-Constrained Decoding](https://arxiv.org/abs/2510.13183)
*Ming Dong,Jinkui Zhang,Bolong Zheng,Xinhui Tu,Po Hu,Tingting He*

Main category: cs.CL

TL;DR: 提出了一种无需参数微调的自约束解码方法DSCD，通过增强安全层、削弱幻觉和毒性层的内部token分布来实现LLM去毒，具有轻量、兼容性强和即插即用特性。


<details>
  <summary>Details</summary>
Motivation: 现有基于外部约束的解码去毒方法需要额外资源开销且会损失生成流畅性，需要一种更高效的去毒方案。

Method: DSCD方法在输出生成过程中增强安全层的内部下一个token分布，同时削弱幻觉和毒性层的分布，从而有效降低毒性并提高输出安全性。

Result: 在代表性开源LLM和公共数据集上的广泛实验验证了DSCD的有效性，在去毒和生成流畅性方面均达到SOTA性能，且效率优于现有方法。

Conclusion: DSCD作为一种实用且可扩展的解决方案，在更安全的LLM部署方面具有巨大潜力。

Abstract: Detoxification in large language models (LLMs) remains a significant research
challenge. Existing decoding detoxification methods are all based on external
constraints, which require additional resource overhead and lose generation
fluency. This work proposes Detoxification with Self-Constrained Decoding
(DSCD), a novel method for LLM detoxification without parameter fine-tuning.
DSCD strengthens the inner next-token distribution of the safety layer while
weakening that of hallucination and toxic layers during output generation. This
effectively diminishes toxicity and enhances output safety. DSCD offers
lightweight, high compatibility, and plug-and-play capabilities, readily
integrating with existing detoxification methods for further performance
improvement. Extensive experiments on representative open-source LLMs and
public datasets validate DSCD's effectiveness, demonstrating state-of-the-art
(SOTA) performance in both detoxification and generation fluency, with superior
efficiency compared to existing methods. These results highlight DSCD's
potential as a practical and scalable solution for safer LLM deployments.

</details>


### [34] [SHIELD: Classifier-Guided Prompting for Robust and Safer LVLMs](https://arxiv.org/abs/2510.13190)
*Juan Ren,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: SHIELD是一个轻量级、模型无关的预处理框架，通过细粒度安全分类和类别特定指导来防御多模态推理中的对抗性攻击，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型虽然具有强大的多模态推理能力，但也扩大了攻击面，特别是通过将有害目标隐藏在良性提示中的对抗性输入。

Method: SHIELD结合细粒度安全分类与类别特定指导，通过明确的行动（阻止、重构、转发）来组成定制的安全提示，实现细微的拒绝或安全重定向。

Result: 在五个基准测试和五个代表性LVLM上，SHIELD持续降低了越狱和不遵循率，同时保持了实用性，具有即插即用、开销可忽略的特点。

Conclusion: SHIELD作为一个实用的安全补丁，适用于弱对齐和强对齐的LVLM，易于扩展到新的攻击类型。

Abstract: Large Vision-Language Models (LVLMs) unlock powerful multimodal reasoning but
also expand the attack surface, particularly through adversarial inputs that
conceal harmful goals in benign prompts. We propose SHIELD, a lightweight,
model-agnostic preprocessing framework that couples fine-grained safety
classification with category-specific guidance and explicit actions (Block,
Reframe, Forward). Unlike binary moderators, SHIELD composes tailored safety
prompts that enforce nuanced refusals or safe redirection without retraining.
Across five benchmarks and five representative LVLMs, SHIELD consistently
lowers jailbreak and non-following rates while preserving utility. Our method
is plug-and-play, incurs negligible overhead, and is easily extendable to new
attack types -- serving as a practical safety patch for both weakly and
strongly aligned LVLMs.

</details>


### [35] [Grounding Long-Context Reasoning with Contextual Normalization for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.13191)
*Jiamin Chen,Yuchen Li,Xinyu Ma,Xinran Chen,Xiaokun Zhang,Shuaiqiang Wang,Chen Ma,Dawei Yin*

Main category: cs.CL

TL;DR: 本文研究了检索增强生成中上下文格式对模型性能的影响，发现即使是表面上的格式选择（如分隔符、结构标记）也会显著影响准确性和稳定性，并提出了上下文归一化策略来标准化上下文表示。


<details>
  <summary>Details</summary>
Motivation: 虽然先前研究主要关注检索质量和提示策略，但检索文档如何被格式化（即上下文格式）的影响尚未得到充分探索。作者发现看似表面的格式选择可能导致准确性和稳定性的显著变化。

Method: 设计了控制实验来变化上下文密度、分隔符样式和位置放置，揭示性能差异的潜在因素。基于这些见解，提出了上下文归一化策略，在生成前自适应地标准化上下文表示。

Result: 在受控和真实世界RAG基准测试上的广泛实验表明，所提出的策略持续提高了对顺序变化的鲁棒性，并增强了长上下文利用能力。

Conclusion: 可靠的RAG不仅取决于检索正确的内容，还取决于这些内容的呈现方式。研究为更好的长上下文推理提供了新的实证证据和实用技术。

Abstract: Retrieval-Augmented Generation (RAG) has become an essential approach for
extending the reasoning and knowledge capacity of large language models (LLMs).
While prior research has primarily focused on retrieval quality and prompting
strategies, the influence of how the retrieved documents are framed, i.e.,
context format, remains underexplored. We show that seemingly superficial
choices, such as delimiters or structural markers in key-value extraction, can
induce substantial shifts in accuracy and stability, even when semantic content
is identical. To systematically investigate this effect, we design controlled
experiments that vary context density, delimiter styles, and positional
placement, revealing the underlying factors that govern performance
differences. Building on these insights, we introduce Contextual Normalization,
a lightweight strategy that adaptively standardizes context representations
before generation. Extensive experiments on both controlled and real-world RAG
benchmarks across diverse settings demonstrate that the proposed strategy
consistently improves robustness to order variation and strengthens
long-context utilization. These findings underscore that reliable RAG depends
not only on retrieving the right content, but also on how that content is
presented, offering both new empirical evidence and a practical technique for
better long-context reasoning.

</details>


### [36] [StressTransfer: Stress-Aware Speech-to-Speech Translation with Emphasis Preservation](https://arxiv.org/abs/2510.13194)
*Xi Chen,Yuchen Song,Satoshi Nakamura*

Main category: cs.CL

TL;DR: 提出了一种基于LLM的跨语言重音转换方法，通过将源语言重音转换为目标语言标签来指导可控TTS模型，在保持翻译质量的同时显著提升了重音保留效果。


<details>
  <summary>Details</summary>
Motivation: 现有语音到语音翻译系统在保留词级重音等韵律特征方面存在不足，而重音对于传达说话者意图和情感至关重要。

Method: 利用LLM进行跨语言重音转换，将源语言重音映射为目标语言标签，指导可控TTS模型生成；开发自动对齐训练数据生成流程，并使用"LLM-as-Judge"进行评估。

Result: 实验表明该方法在保留重音方面显著优于基线方法，同时保持了相当的翻译质量、说话者意图和自然度。

Conclusion: 该研究强调了韵律在翻译中的重要性，为S2ST中保留副语言线索提供了一种有效且数据高效的解决方案。

Abstract: We propose a stress-aware speech-to-speech translation (S2ST) system that
preserves word-level emphasis by leveraging LLMs for cross-lingual emphasis
conversion. Our method translates source-language stress into target-language
tags that guide a controllable TTS model. To overcome data scarcity, we
developed a pipeline to automatically generate aligned training data and
introduce the "LLM-as-Judge" for evaluation. Experiments show our approach
substantially outperforms baselines in preserving emphasis while maintaining
comparable translation quality, speaker intent, and naturalness. Our work
highlights the importance of prosody in translation and provides an effective,
data-efficient solution for preserving paralinguistic cues in S2ST.

</details>


### [37] [Text Anomaly Detection with Simplified Isolation Kernel](https://arxiv.org/abs/2510.13197)
*Yang Cao,Sikun Yang,Yujiu Yang,Lianyong Qi,Ming Liu*

Main category: cs.CL

TL;DR: 提出了简化隔离核(SIK)方法，将高维密集嵌入映射到低维稀疏表示，在保持异常检测性能的同时显著降低计算和内存成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用大语言模型嵌入进行文本异常检测时面临高维密集嵌入带来的内存需求大和计算时间长的挑战。

Method: 引入简化隔离核(SIK)，通过创新的边界聚焦特征映射将高维密集嵌入转换为低维稀疏表示，具有线性时间复杂度。

Result: 在7个数据集上的实验表明，SIK在检测性能上优于11种最先进的异常检测算法，同时保持计算效率和低内存成本。

Conclusion: SIK方法有效解决了高维嵌入带来的计算和内存挑战，在文本异常检测中实现了更好的性能与效率平衡。

Abstract: Two-step approaches combining pre-trained large language model embeddings and
anomaly detectors demonstrate strong performance in text anomaly detection by
leveraging rich semantic representations. However, high-dimensional dense
embeddings extracted by large language models pose challenges due to
substantial memory requirements and high computation time. To address this
challenge, we introduce the Simplified Isolation Kernel (SIK), which maps
high-dimensional dense embeddings to lower-dimensional sparse representations
while preserving crucial anomaly characteristics. SIK has linear time
complexity and significantly reduces space complexity through its innovative
boundary-focused feature mapping. Experiments across 7 datasets demonstrate
that SIK achieves better detection performance than 11 state-of-the-art (SOTA)
anomaly detection algorithms while maintaining computational efficiency and low
memory cost. All code and demonstrations are available at
https://github.com/charles-cao/SIK.

</details>


### [38] [LLM-Guided Synthetic Augmentation (LGSA) for Mitigating Bias in AI Systems](https://arxiv.org/abs/2510.13202)
*Sai Suhruth Reddy Karri,Yashwanth Sai Nallapuneni,Laxmi Narasimha Reddy Mallireddy,Gopichand G*

Main category: cs.CL

TL;DR: LGSA方法使用大语言模型生成反事实样本来缓解AI偏见，在保持准确性的同时显著减少性别偏见差距。


<details>
  <summary>Details</summary>
Motivation: AI系统中的偏见引发伦理和实践问题，传统公平性方法依赖受保护属性标签、涉及准确性与公平性权衡，且难以跨数据集泛化。

Method: 使用大语言模型生成反事实示例，通过结构化提示创建性别转换的改写句，并进行语义相似性检查、属性验证、毒性筛选和人工抽查等质量控制。

Result: 基线模型准确率96.7%，性别偏见差距7.2%；简单交换方法将差距降至0.7%但准确率降至95.6%；LGSA实现99.1%准确率，偏见差距1.9%，改善了女性标签示例的性能。

Conclusion: LGSA是有效的偏见缓解策略，能在保持高任务准确性和标签保真度的同时增强子群体平衡。

Abstract: Bias in AI systems, especially those relying on natural language data, raises
ethical and practical concerns. Underrepresentation of certain groups often
leads to uneven performance across demographics. Traditional fairness methods,
such as pre-processing, in-processing, and post-processing, depend on
protected-attribute labels, involve accuracy-fairness trade-offs, and may not
generalize across datasets. To address these challenges, we propose LLM-Guided
Synthetic Augmentation (LGSA), which uses large language models to generate
counterfactual examples for underrepresented groups while preserving label
integrity. We evaluated LGSA on a controlled dataset of short English sentences
with gendered pronouns, professions, and binary classification labels.
Structured prompts were used to produce gender-swapped paraphrases, followed by
quality control including semantic similarity checks, attribute verification,
toxicity screening, and human spot checks. The augmented dataset expanded
training coverage and was used to train a classifier under consistent
conditions. Results show that LGSA reduces performance disparities without
compromising accuracy. The baseline model achieved 96.7 percent accuracy with a
7.2 percent gender bias gap. Simple swap augmentation reduced the gap to 0.7
percent but lowered accuracy to 95.6 percent. LGSA achieved 99.1 percent
accuracy with a 1.9 percent bias gap, improving performance on female-labeled
examples. These findings demonstrate that LGSA is an effective strategy for
bias mitigation, enhancing subgroup balance while maintaining high task
accuracy and label fidelity.

</details>


### [39] [A fully automated and scalable Parallel Data Augmentation for Low Resource Languages using Image and Text Analytics](https://arxiv.org/abs/2510.13211)
*Prawaal Sharma,Navneet Goyal,Poonam Goyal,Vishnupriyan R*

Main category: cs.CL

TL;DR: 提出了一种从报纸文章中自动提取双语平行语料库的新方法，通过机器翻译任务验证了有效性，相比基线提升了近3个BLEU分数。


<details>
  <summary>Details</summary>
Motivation: 全球语言多样性导致高质量数字语言资源分布不均，限制了大多数人口获得技术效益。低资源语言缺乏数据资源使得NLP任务难以进行。

Method: 使用图像和文本分析从报纸文章中提取双语平行语料库的可扩展全自动方法。

Result: 为两种不同语言组合构建了平行数据语料库，通过机器翻译任务验证，相比当前基线提升了近3个BLEU分数。

Conclusion: 该方法能够有效解决低资源语言的数据稀缺问题，为NLP任务提供有价值的双语资源。

Abstract: Linguistic diversity across the world creates a disparity with the
availability of good quality digital language resources thereby restricting the
technological benefits to majority of human population. The lack or absence of
data resources makes it difficult to perform NLP tasks for low-resource
languages. This paper presents a novel scalable and fully automated methodology
to extract bilingual parallel corpora from newspaper articles using image and
text analytics. We validate our approach by building parallel data corpus for
two different language combinations and demonstrate the value of this dataset
through a downstream task of machine translation and improve over the current
baseline by close to 3 BLEU points.

</details>


### [40] [Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain](https://arxiv.org/abs/2510.13255)
*Jingmin An,Yilong Song,Ruolin Yang,Nai Ding,Lingxi Lu,Yuxuan Wang,Wei Wang,Chu Zhuang,Qian Wang,Fang Fang*

Main category: cs.CL

TL;DR: HFTP工具通过频域分析发现LLMs和人类大脑在句法处理上的相似性与差异，显示LLMs与大脑左半球表征更相似，但模型升级呈现不同趋势。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否使用类似人脑的机制来处理句法结构，以及识别负责句法处理的特定计算模块。

Method: 引入分层频率标记探针（HFTP），利用频域分析识别LLMs中编码句法结构的神经元组件和大脑皮层区域。

Result: GPT-2、Gemma等模型在类似层级处理句法，而人脑依赖不同皮层区域；LLMs表征与大脑左半球更相似；模型升级趋势不同：Gemma 2比Gemma更接近大脑，Llama 3.1比Llama 2更偏离大脑。

Conclusion: HFTP为计算语言学和认知神经科学搭建了桥梁，对LLMs行为改进的可解释性提供了新见解，质疑这些进步是由类人还是非类人机制驱动。

Abstract: Large Language Models (LLMs) demonstrate human-level or even superior
language abilities, effectively modeling syntactic structures, yet the specific
computational modules responsible remain unclear. A key question is whether LLM
behavioral capabilities stem from mechanisms akin to those in the human brain.
To address these questions, we introduce the Hierarchical Frequency Tagging
Probe (HFTP), a tool that utilizes frequency-domain analysis to identify
neuron-wise components of LLMs (e.g., individual Multilayer Perceptron (MLP)
neurons) and cortical regions (via intracranial recordings) encoding syntactic
structures. Our results show that models such as GPT-2, Gemma, Gemma 2, Llama
2, Llama 3.1, and GLM-4 process syntax in analogous layers, while the human
brain relies on distinct cortical regions for different syntactic levels.
Representational similarity analysis reveals a stronger alignment between LLM
representations and the left hemisphere of the brain (dominant in language
processing). Notably, upgraded models exhibit divergent trends: Gemma 2 shows
greater brain similarity than Gemma, while Llama 3.1 shows less alignment with
the brain compared to Llama 2. These findings offer new insights into the
interpretability of LLM behavioral improvements, raising questions about
whether these advancements are driven by human-like or non-human-like
mechanisms, and establish HFTP as a valuable tool bridging computational
linguistics and cognitive neuroscience. This project is available at
https://github.com/LilTiger/HFTP.

</details>


### [41] [Do You Get the Hint? Benchmarking LLMs on the Board Game Concept](https://arxiv.org/abs/2510.13271)
*Ine Gevers,Walter Daelemans*

Main category: cs.CL

TL;DR: 论文介绍了Concept游戏作为评估LLM溯因推理能力的基准，发现LLM在此类需要抽象推理的任务上表现不佳，成功率低于40%，远低于人类的90%以上成功率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM虽然在许多基准测试中表现出色，但在需要抽象推理的任务上仍存在根本性弱点，特别是涉及网格、符号或视觉模式等不同于自然语言训练数据的表示形式。

Method: 引入Concept单词猜谜棋盘游戏作为基准，该游戏使用更接近LLM预训练数据的自然语言表示，评估LLM在多种语言（英语、荷兰语、法语、西班牙语）中的溯因推理能力。

Result: LLM在Concept游戏中表现不佳，成功率不超过40%，远低于人类的90%以上成功率。LLM在解释其他玩家的战略意图和根据序列信息更新修正初始假设方面存在困难，且在低资源语言中表现更差。

Conclusion: Concept游戏是一个有效的基准，揭示了LLM在抽象推理任务上的局限性，特别是在理解战略意图和动态信息更新方面的挑战，且多语言评估显示性能在低资源语言中进一步下降。

Abstract: Large language models (LLMs) have achieved striking successes on many
benchmarks, yet recent studies continue to expose fundamental weaknesses. In
particular, tasks that require abstract reasoning remain challenging, often
because they use representations such as grids, symbols, or visual patterns
that differ from the natural language data LLMs are trained on. In this paper,
we introduce Concept, a simple word-guessing board game, as a benchmark for
probing abductive reasoning in a representation that is much closer to LLM
pre-training data: natural language. Our results show that this game, easily
solved by humans (with a success rate of over 90\%), is still very challenging
for state-of-the-art LLMs (no model exceeds 40\% success rate). Specifically,
we observe that LLMs struggle with interpreting other players' strategic
intents, and with correcting initial hypotheses given sequential information
updates. In addition, we extend the evaluation across multiple languages, and
find that the LLM performance drops further in lower-resource languages (Dutch,
French, and Spanish) compared to English.

</details>


### [42] [Beyond Correctness: Rewarding Faithful Reasoning in Retrieval-Augmented Generation](https://arxiv.org/abs/2510.13272)
*Zhichao Xu,Zongyu Wu,Yun Zhou,Aosong Feng,Kang Zhou,Sangmin Woo,Kiran Ramnath,Yijun Tian,Xuan Qi,Weikang Qiu,Lin Lee Cheong,Haibo Ding*

Main category: cs.CL

TL;DR: VERITAS框架通过将细粒度的忠实性奖励整合到强化学习中，显著提升了基于RL的搜索代理的推理忠实性，同时在多个QA基准测试中保持可比较的任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的搜索代理方法虽然提升了问答基准测试的性能，但往往忽视中间推理步骤的质量，导致思维链不忠实的问题。

Method: 提出了VERITAS框架，在强化学习过程中整合了三种细粒度的忠实性奖励：信息-思维忠实性、思维-答案忠实性和思维-搜索忠实性。

Result: 实验表明，使用VERITAS训练的模型不仅显著提高了推理忠实性，还在七个QA基准测试中实现了可比较的任务性能。

Conclusion: VERITAS框架有效地解决了RL-based搜索代理的推理忠实性问题，为构建更可靠的检索增强生成系统提供了新思路。

Abstract: Inspired by the success of reinforcement learning (RL) in Large Language
Model (LLM) training for domains like math and code, recent works have begun
exploring how to train LLMs to use search engines more effectively as tools for
retrieval-augmented generation. Although these methods achieve performance
improvement across QA benchmarks, many prioritize final answer correctness
while overlooking the quality of intermediate reasoning steps, which may lead
to chain-of-thought unfaithfulness. In this paper, we first introduce a
comprehensive evaluation framework for evaluating RL-based search agents,
covering three distinct faithfulness metrics: information-think faithfulness,
think-answer faithfulness, and think-search faithfulness. Our evaluations
reveal that a prototypical RL-based search agent, Search-R1, has significant
room for improvement in this regard. To foster faithful reasoning, we introduce
VERITAS (Verifying Entailed Reasoning through Intermediate Traceability in
Agentic Search), a novel framework that integrates fine-grained faithfulness
rewards into the reinforcement learning process. Our experiments show that
models trained with VERITAS not only significantly improve reasoning
faithfulness, but also achieve comparable task performance across seven QA
benchmarks.

</details>


### [43] [In-Distribution Steering: Balancing Control and Coherence in Language Model Generation](https://arxiv.org/abs/2510.13285)
*Arthur Vogels,Benjamin Wong,Yann Choho,Annabelle Blangero,Milan Bhan*

Main category: cs.CL

TL;DR: 提出了一种新的激活引导方法IDS，根据输入数据在表示空间中的分布动态调整引导强度，解决了传统固定强度方法导致的控制不足或文本质量下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有激活引导方法使用固定引导强度，导致要么控制不足，要么干预过度而降低文本合理性和连贯性。

Method: IDS方法基于输入数据在表示空间中的分布自适应调整引导强度，根据输入与分布的距离动态调节干预程度。

Result: 实验表明IDS在分类任务上达到高准确率，同时生成连贯文本且避免崩溃，特别适合实际应用。

Conclusion: IDS通过自适应引导强度实现了更好的控制效果和文本质量，为实际应用提供了有效的激活引导解决方案。

Abstract: Activation steering methods control large language model (LLM) behavior by
modifying internal activations at inference time. However, most existing
activation steering methods rely on a fixed steering strength, leading to
either insufficient control or unadapted intervention that degrades text
plausibility and coherence. We introduce In-Distribution Steering (IDS), a
novel method that adapts steering strength based on the input data distribution
in representation space. IDS dynamically adjusts interventions according to how
far a given input lies within the distribution, enabling adaptive intervention
and generation stability during text generation. Experiments demonstrate that
IDS achieves strong accuracy on classification tasks while producing coherent
text without collapse, making IDS particularly well suited for real-world
applications.

</details>


### [44] [Higher Satisfaction, Lower Cost: A Technical Report on How LLMs Revolutionize Meituan's Intelligent Interaction Systems](https://arxiv.org/abs/2510.13291)
*Xuxin Cheng,Ke Zeng,Zhiquan Cao,Linyi Dai,Wenxuan Gao,Fei Han,Ai Jian,Feng Hong,Wenxing Hu,Zihe Huang,Dejian Kong,Jia Leng,Zhuoyuan Liao,Pei Liu,Jiaye Lin,Xing Ma,Jingqing Ruan,Jiaxing Song,Xiaoyu Tan,Ruixuan Xiao,Wenhui Yu,Wenyu Zhan,Haoxing Zhang,Chao Zhou,Hao Zhou,Shaodong Zheng,Ruinian Chen,Siyuan Chen,Ziyang Chen,Yiwen Dong,Yaoyou Fan,Yangyi Fang,Yang Gan,Shiguang Guo,Qi He,Chaowen Hu,Binghui Li,Dailin Li,Xiangyu Li,Yan Li,Chengjian Liu,Xiangfeng Liu,Jiahui Lv,Qiao Ma,Jiang Pan,Cong Qin,Chenxing Sun,Wen Sun,Zhonghui Wang,Abudukelimu Wuerkaixi,Xin Yang,Fangyi Yuan,Yawen Zhu,Tianyi Zhai,Jie Zhang,Runlai Zhang,Yao Xu,Yiran Zhao,Yifan Wang,Xunliang Cai,Yangen Hu,Cao Liu,Lu Pan,Xiaoli Wang,Bo Xiao,Wenyuan Yao,Qianlin Zhou,Benchang Zhu*

Main category: cs.CL

TL;DR: WOWService是一个基于大语言模型和多智能体架构的智能交互系统，针对工业应用中的冷启动训练、多轮对话、业务规则演化、多智能体协作和评估等挑战，通过在美团App的部署显著提升了用户满意度指标。


<details>
  <summary>Details</summary>
Motivation: 随着服务需求规模和复杂度的增长，智能交互系统面临多个挑战：冷启动训练数据构建困难、多轮对话性能不佳、业务规则频繁演化影响系统可操作性、单一LLM在复杂场景中不足、以及缺乏统一评估标准阻碍持续优化。

Method: WOWService集成了LLMs和多智能体架构，专注于数据构建、通用能力增强、业务场景适配、多智能体协调和自动化评估等核心模块，实现自主任务管理和协作问题解决。

Result: WOWService已在美团App部署，关键指标显著改善：用户满意度指标1（USM 1）降低27.53%，用户满意度指标2（USM 2）提升25.51%，有效捕捉用户需求并推进个性化服务。

Conclusion: WOWService通过整合LLMs和多智能体架构，成功解决了智能交互系统在工业应用中的关键挑战，证明了其在提升用户体验和系统性能方面的有效性。

Abstract: Enhancing customer experience is essential for business success, particularly
as service demands grow in scale and complexity. Generative artificial
intelligence and Large Language Models (LLMs) have empowered intelligent
interaction systems to deliver efficient, personalized, and 24/7 support. In
practice, intelligent interaction systems encounter several challenges: (1)
Constructing high-quality data for cold-start training is difficult, hindering
self-evolution and raising labor costs. (2) Multi-turn dialogue performance
remains suboptimal due to inadequate intent understanding, rule compliance, and
solution extraction. (3) Frequent evolution of business rules affects system
operability and transferability, constraining low-cost expansion and
adaptability. (4) Reliance on a single LLM is insufficient in complex
scenarios, where the absence of multi-agent frameworks and effective
collaboration undermines process completeness and service quality. (5) The
open-domain nature of multi-turn dialogues, lacking unified golden answers,
hampers quantitative evaluation and continuous optimization. To address these
challenges, we introduce WOWService, an intelligent interaction system tailored
for industrial applications. With the integration of LLMs and multi-agent
architectures, WOWService enables autonomous task management and collaborative
problem-solving. Specifically, WOWService focuses on core modules including
data construction, general capability enhancement, business scenario
adaptation, multi-agent coordination, and automated evaluation. Currently,
WOWService is deployed on the Meituan App, achieving significant gains in key
metrics, e.g., User Satisfaction Metric 1 (USM 1) -27.53% and User Satisfaction
Metric 2 (USM 2) +25.51%, demonstrating its effectiveness in capturing user
needs and advancing personalized service.

</details>


### [45] [Mismatch Aware Guidance for Robust Emotion Control in Auto-Regressive TTS Models](https://arxiv.org/abs/2510.13293)
*Yizhou Peng,Yukun Ma,Chong Zhang,Yi-Wen Chao,Chongjia Ni,Bin Ma*

Main category: cs.CL

TL;DR: 本文提出了一种自适应分类器自由引导（CFG）方案，用于解决自回归TTS模型中风格提示与文本语义内容不匹配的问题，在保持音频质量和可理解性的同时提升情感表达能力。


<details>
  <summary>Details</summary>
Motivation: 当TTS系统中期望的情感（风格提示）与文本语义内容冲突时，会导致语音听起来不自然，这破坏了实现细粒度情感控制的目标。虽然CFG是增强提示对齐的关键技术，但在自回归TTS模型中的应用仍未被充分探索，可能导致音频质量下降。

Method: 提出自适应CFG方案，根据使用大型语言模型或自然语言推理模型检测到的不同不匹配程度进行调整。该方法基于对CFG在最先进自回归TTS模型中情感表达能力影响的综合分析。

Result: 研究结果表明，提出的自适应CFG方案在保持音频质量和可理解性的同时，提高了自回归TTS模型的情感表达能力。

Conclusion: 通过自适应CFG方案有效解决了自回归TTS模型中的风格-内容不匹配问题，实现了更好的情感控制效果。

Abstract: While Text-to-Speech (TTS) systems can achieve fine-grained control over
emotional expression via natural language prompts, a significant challenge
emerges when the desired emotion (style prompt) conflicts with the semantic
content of the text. This mismatch often results in unnatural-sounding speech,
undermining the goal of achieving fine-grained emotional control.
Classifier-Free Guidance (CFG) is a key technique for enhancing prompt
alignment; however, its application to auto-regressive (AR) TTS models remains
underexplored, which can lead to degraded audio quality. This paper directly
addresses the challenge of style-content mismatch in AR TTS models by proposing
an adaptive CFG scheme that adjusts to different levels of the detected
mismatch, as measured using large language models or natural language inference
models. This solution is based on a comprehensive analysis of CFG's impact on
emotional expressiveness in state-of-the-art AR TTS models. Our results
demonstrate that the proposed adaptive CFG scheme improves the emotional
expressiveness of the AR TTS model while maintaining audio quality and
intelligibility.

</details>


### [46] [LLM one-shot style transfer for Authorship Attribution and Verification](https://arxiv.org/abs/2510.13302)
*Pablo Miralles-González,Javier Huertas-Tato,Alejandro Martín,David Camacho*

Main category: cs.CL

TL;DR: 提出了一种基于LLM预训练和上下文学习能力的无监督方法，使用LLM的对数概率来测量文本间的风格可转移性，在控制主题相关性时优于对比训练基线。


<details>
  <summary>Details</summary>
Motivation: 计算风格学通过文本中的量化模式分析写作风格，但监督和对比方法依赖具有虚假相关性的数据，经常混淆风格与主题。尽管CLM预训练在AI生成文本检测中自然使用，但在一般作者身份问题中很少被利用。

Method: 基于LLM的广泛预训练和上下文学习能力，使用LLM的对数概率来测量从一个文本到另一个文本的风格可转移性。

Result: 该方法显著优于同等规模的LLM提示方法，在控制主题相关性时比对比训练基线获得更高准确率。性能与基础模型大小相当一致地扩展，在作者身份验证情况下，通过增加测试时计算实现计算成本与准确性之间的灵活权衡。

Conclusion: LLM预训练为计算风格学提供了强大的无监督方法，能够有效区分风格与主题，并在多个任务中优于现有方法。

Abstract: Computational stylometry analyzes writing style through quantitative patterns
in text, supporting applications from forensic tasks such as identity linking
and plagiarism detection to literary attribution in the humanities. Supervised
and contrastive approaches rely on data with spurious correlations and often
confuse style with topic. Despite their natural use in AI-generated text
detection, the CLM pre-training of modern LLMs has been scarcely leveraged for
general authorship problems. We propose a novel unsupervised approach based on
this extensive pre-training and the in-context learning capabilities of LLMs,
employing the log-probabilities of an LLM to measure style transferability from
one text to another. Our method significantly outperforms LLM prompting
approaches of comparable scale and achieves higher accuracy than contrastively
trained baselines when controlling for topical correlations. Moreover,
performance scales fairly consistently with the size of the base model and, in
the case of authorship verification, with an additional mechanism that
increases test-time computation; enabling flexible trade-offs between
computational cost and accuracy.

</details>


### [47] [Embedding-Based Context-Aware Reranker](https://arxiv.org/abs/2510.13329)
*Ye Yuan,Mohammad Amin Shabani,Siqi Liu*

Main category: cs.CL

TL;DR: 提出EBCAR，一种轻量级的重排序框架，通过嵌入和混合注意力机制增强跨段落理解，在需要跨段落推理的信息检索中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: RAG系统将长文档分割成短段落进行检索，但当正确答案需要跨段落推理（如解决共指、消歧实体、聚合分散证据）时面临挑战，现有重排序方法忽视这些问题。

Method: EBCAR直接在检索段落的嵌入上操作，通过段落结构信息和混合注意力机制增强跨段落理解，捕捉文档间高层交互和文档内低层关系。

Result: 在ConTEB基准测试中，EBCAR在需要跨段落推理的信息检索任务上优于现有最先进的重排序方法，在准确性和效率方面均有优势。

Conclusion: EBCAR通过轻量级设计有效解决了RAG系统中跨段落推理的挑战，在保持高效率的同时提升了检索准确性。

Abstract: Retrieval-Augmented Generation (RAG) systems rely on retrieving relevant
evidence from a corpus to support downstream generation. The common practice of
splitting a long document into multiple shorter passages enables finer-grained
and targeted information retrieval. However, it also introduces challenges when
a correct retrieval would require inference across passages, such as resolving
coreference, disambiguating entities, and aggregating evidence scattered across
multiple sources. Many state-of-the-art (SOTA) reranking methods, despite
utilizing powerful large pretrained language models with potentially high
inference costs, still neglect the aforementioned challenges. Therefore, we
propose Embedding-Based Context-Aware Reranker (EBCAR), a lightweight reranking
framework operating directly on embeddings of retrieved passages with enhanced
cross-passage understandings through the structural information of the passages
and a hybrid attention mechanism, which captures both high-level interactions
across documents and low-level relationships within each document. We evaluate
EBCAR against SOTA rerankers on the ConTEB benchmark, demonstrating its
effectiveness for information retrieval requiring cross-passage inference and
its advantages in both accuracy and efficiency.

</details>


### [48] [Taming the Fragility of KV Cache Eviction in LLM Inference](https://arxiv.org/abs/2510.13334)
*Yuan Feng,Haoyu Guo,JunLin Lv,S. Kevin Zhou,Xike Xie*

Main category: cs.CL

TL;DR: 提出了DefensiveKV和Layer-DefensiveKV两种新的KV缓存驱逐方法，通过防御性聚合策略控制最坏情况风险，在18个数据集上显著减少生成质量损失。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存驱逐方法基于稳定性假设，认为固定的缓存条目在生成过程中始终重要，但这种假设本质上是脆弱的，使均值聚合在极端情况下容易失效。

Method: 采用两步骤线性时间防御性聚合策略，控制最坏情况风险。DefensiveKV及其扩展Layer-DefensiveKV结合了分层预算分配。

Result: 在7个任务领域（18个数据集）上，当缓存大小为20%时，相比最强基线，两种方法分别将生成质量损失减少了2.3倍和4.3倍。

Conclusion: 该方法为通过最坏情况风险管理优化缓存驱逐开辟了有前景的方向，并建立了新的性能基准。

Abstract: Large language models have revolutionized natural language processing, yet
their deployment remains hampered by the substantial memory and runtime
overhead of the transformer's Key-Value cache. To mitigate this, recent methods
employ a scoring-aggregation framework to evict unimportant cache entries,
based on the stability assumption-that a fixed subset of entries remains
consistently important during generation. However, prior work has largely
focused on refining importance indicators for scoring, while defaulting to mean
aggregation due to a faithful trust in the stability assumption. In this work,
we argue that this underlying assumption is inherently fragile, making mean
aggregation highly vulnerable in extreme cases. To counter this, we propose a
simple yet elegant defensive aggregation strategy: a two-step, linear-time
approach that controls worst-case risk, thereby defending against extreme cases
with negligible computational overhead. Embodying this strategy, we propose a
novel cache eviction method, DefensiveKV and its extension, Layer-DefensiveKV,
which incorporates layer-wise budget allocation. Across seven task domains (18
datasets), our methods reduce generation quality loss by 2.3x and 4.3x
respectively, versus the strongest baseline under a 20% cache size. These
results set new performance benchmarks and pioneer a promising direction for
optimizing cache eviction against underlying fragility through worst-case risk
management. Our code is available at https://github.com/FFY0/DefensiveKV.

</details>


### [49] [Are Proverbs the New Pythian Oracles? Exploring Sentiment in Greek Sayings](https://arxiv.org/abs/2510.13341)
*Katerina Korre,John Pavlopoulos*

Main category: cs.CL

TL;DR: 利用NLP技术分析希腊谚语的情感分布，通过LLM进行情感分类，并绘制希腊各地区情感分布地图，发现负面情感在大多数地区更为普遍。


<details>
  <summary>Details</summary>
Motivation: 谚语是跨越文化和语言边界的有趣语言现象，但许多文化因口头传统而将其智慧保留在社区内部，全球谚语景观尚未充分探索。

Method: 利用NLP技术，基于标注的希腊谚语数据集，扩展到包括当地方言，使用LLM进行谚语情感分类，并结合地理位置、方言和主题进行组合分析。

Result: LLM能够提供相当准确的谚语情感分析，尤其是在作为非常规情感极性任务处理时；在希腊大部分地区，负面情感更为普遍。

Conclusion: LLM可以有效分析谚语情感，希腊谚语整体呈现负面情感倾向，这为跨文化语言现象研究提供了新方法。

Abstract: Proverbs are among the most fascinating linguistic phenomena that transcend
cultural and linguistic boundaries. Yet, much of the global landscape of
proverbs remains underexplored, as many cultures preserve their traditional
wisdom within their own communities due to the oral tradition of the
phenomenon. Taking advantage of the current advances in Natural Language
Processing (NLP), we focus on Greek proverbs, analyzing their sentiment.
Departing from an annotated dataset of Greek proverbs, we expand it to include
local dialects, effectively mapping the annotated sentiment. We present (1) a
way to exploit LLMs in order to perform sentiment classification of proverbs,
(2) a map of Greece that provides an overview of the distribution of sentiment,
(3) a combinatory analysis in terms of the geographic position, dialect, and
topic of proverbs. Our findings show that LLMs can provide us with an accurate
enough picture of the sentiment of proverbs, especially when approached as a
non-conventional sentiment polarity task. Moreover, in most areas of Greece
negative sentiment is more prevalent.

</details>


### [50] [Protect: Towards Robust Guardrailing Stack for Trustworthy Enterprise LLM Systems](https://arxiv.org/abs/2510.13351)
*Karthik Avinash,Nikhil Pareek,Rishav Hada*

Main category: cs.CL

TL;DR: Protect是一个原生多模态护栏模型，专为企业级部署设计，能够无缝处理文本、图像和音频输入，在毒性、性别歧视、数据隐私和提示注入四个安全维度上实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在企业关键领域的部署增加，现有护栏系统在实时监督、多模态数据处理和可解释性方面存在不足，无法满足受监管环境的需求。

Method: 使用低秩适应(LoRA)在广泛的多模态数据集上训练特定类别的适配器，采用教师辅助标注流程生成高质量、上下文感知的标签。

Result: 实验结果显示在所有安全维度上都达到了最先进性能，超越了WildGuard、LlamaGuard-4和GPT-4.1等现有模型。

Conclusion: Protect为可信赖、可审计且生产就绪的安全系统奠定了坚实基础，能够跨文本、图像和音频模态运行。

Abstract: The increasing deployment of Large Language Models (LLMs) across enterprise
and mission-critical domains has underscored the urgent need for robust
guardrailing systems that ensure safety, reliability, and compliance. Existing
solutions often struggle with real-time oversight, multi-modal data handling,
and explainability -- limitations that hinder their adoption in regulated
environments. Existing guardrails largely operate in isolation, focused on text
alone making them inadequate for multi-modal, production-scale environments. We
introduce Protect, natively multi-modal guardrailing model designed to operate
seamlessly across text, image, and audio inputs, designed for enterprise-grade
deployment. Protect integrates fine-tuned, category-specific adapters trained
via Low-Rank Adaptation (LoRA) on an extensive, multi-modal dataset covering
four safety dimensions: toxicity, sexism, data privacy, and prompt injection.
Our teacher-assisted annotation pipeline leverages reasoning and explanation
traces to generate high-fidelity, context-aware labels across modalities.
Experimental results demonstrate state-of-the-art performance across all safety
dimensions, surpassing existing open and proprietary models such as WildGuard,
LlamaGuard-4, and GPT-4.1. Protect establishes a strong foundation for
trustworthy, auditable, and production-ready safety systems capable of
operating across text, image, and audio modalities.

</details>


### [51] [Personal Attribute Leakage in Federated Speech Models](https://arxiv.org/abs/2510.13357)
*Hamdan Al-Ali,Ali Reza Ghavamipour,Tommaso Caselli,Fatih Turkmen,Zeerak Talat,Hanan Aldarmaki*

Main category: cs.CL

TL;DR: 本文分析了联邦学习环境下ASR模型对属性推断攻击的脆弱性，测试了三种ASR模型在被动威胁模型下的非参数白盒攻击方法，发现预训练数据中代表性不足的属性更容易受到攻击。


<details>
  <summary>Details</summary>
Motivation: 联邦学习作为隐私保护的机器学习训练方法，其安全性需要进一步验证。本文旨在分析ASR模型在联邦学习环境下对属性推断攻击的脆弱性。

Method: 采用非参数白盒攻击方法，在被动威胁模型下测试Wav2Vec2、HuBERT和Whisper三种ASR模型，攻击仅基于权重差异而不访问原始语音数据。

Result: 攻击能够成功推断敏感的人口统计和临床属性（性别、年龄、口音、情绪、构音障碍），特别是预训练数据中代表性不足或缺失的属性更容易受到攻击，口音信息可以从所有模型中可靠推断。

Conclusion: 研究揭示了联邦ASR模型中先前未记录的脆弱性，为改进安全性提供了见解。

Abstract: Federated learning is a common method for privacy-preserving training of
machine learning models. In this paper, we analyze the vulnerability of ASR
models to attribute inference attacks in the federated setting. We test a
non-parametric white-box attack method under a passive threat model on three
ASR models: Wav2Vec2, HuBERT, and Whisper. The attack operates solely on weight
differentials without access to raw speech from target speakers. We demonstrate
attack feasibility on sensitive demographic and clinical attributes: gender,
age, accent, emotion, and dysarthria. Our findings indicate that attributes
that are underrepresented or absent in the pre-training data are more
vulnerable to such inference attacks. In particular, information about accents
can be reliably inferred from all models. Our findings expose previously
undocumented vulnerabilities in federated ASR models and offer insights towards
improved security.

</details>


### [52] [D-SMART: Enhancing LLM Dialogue Consistency via Dynamic Structured Memory And Reasoning Tree](https://arxiv.org/abs/2510.13363)
*Xiang Lei,Qin Li,Min Zhang,Min Zhang*

Main category: cs.CL

TL;DR: D-SMART是一个模型无关的框架，通过动态结构化记忆和推理树来维护多轮对话的一致性，显著提升对话质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多轮对话中经常出现事实不一致和逻辑衰退问题，现有方法如RAG和工作记忆虽然改善了信息召回，但仍依赖静态知识源和单一推理路径，无法在动态变化的对话上下文中保持一致性。

Method: 提出D-SMART框架，包含两个协同组件：(1) 动态结构化记忆(DSM)，增量构建和维护符合OWL标准的知识图谱；(2) 推理树(RT)，在知识图谱上执行显式、可追溯的多步推理搜索。

Result: 在MT-Bench-101基准测试中，D-SMART显著优于现有方法，将对话一致性得分提高了48%以上，开源模型的质量得分提升了10.1%。

Conclusion: D-SMART通过动态结构化表示和显式推理机制，有效解决了多轮对话中的一致性问题，为LLM的对话能力提供了重要改进。

Abstract: Large Language Models (LLMs) often exhibit factual inconsistencies and
logical decay in extended, multi-turn dialogues, a challenge stemming from
their reliance on static, pre-trained knowledge and an inability to reason
adaptively over the dialogue history. Prevailing mitigation strategies, such as
Retrieval-Augmented Generation (RAG) and agentic working memories, improve
information recall but still engage with fundamentally static knowledge sources
and follow pre-defined single reasoning path. This hinders their ability to
preserve factual and logical consistency of their responses in multi-turn
dialogues while the context evolves over time. To address this issue, we
propose D-SMART, a model-agnostic framework designed to maintain multi-turn
dialogue consistency by enabling LLMs to build and reason over a dynamic,
structured representation of the conversational context. This is achieved via
two synergistic components: (1) a Dynamic Structured Memory (DSM), which
incrementally constructs and maintains an authoritative, OWL-compliant
knowledge graph of the conversation; and (2) a Reasoning Tree (RT), which
executes inferences as an explicit and traceable multi-step search over the
graph. As the popular-used quality score (judged by GPT-4) can overlook logical
flaws, we introduce new NLI-based metrics to better measure multi-turn dialogue
consistency. Comprehensive experiments on the MT-Bench-101 benchmark show that
D-SMART significantly outperforms state-of-the-art baselines, elevating the
dialogue consistency score by over 48\% for both proprietary and open-source
models, and notably improves the quality score of the latter by up to 10.1\%.

</details>


### [53] [Document Intelligence in the Era of Large Language Models: A Survey](https://arxiv.org/abs/2510.13366)
*Weishi Wang,Hengchang Hu,Zhijie Zhang,Zhaochen Li,Hongxin Shao,Daniel Dahlmeier*

Main category: cs.CL

TL;DR: 本调查论文全面回顾了文档AI（DAI）的发展历程，重点分析了大型语言模型（LLMs）在该领域的当前研究进展和未来前景，涵盖多模态、多语言和检索增强等关键方向。


<details>
  <summary>Details</summary>
Motivation: 文档AI作为一个重要应用领域，正被大型语言模型显著改变。早期方法依赖编码器-解码器架构，而仅解码器的LLMs为DAI带来了革命性进展，在理解和生成方面取得显著进步。

Method: 本调查采用系统性文献综述方法，全面梳理文档AI的演进历程，分析LLMs在DAI中的研究尝试和发展前景，探索多模态、多语言和检索增强等关键方向。

Result: 论文提供了文档AI领域最新技术的结构化分析，识别了关键进展和挑战，特别是在多模态理解、跨语言处理和检索增强生成等方面。

Conclusion: 文档AI领域正经历由大型语言模型驱动的深刻变革，未来研究方向包括基于代理的方法和文档特定基础模型，对学术研究和实际应用都具有重要意义。

Abstract: Document AI (DAI) has emerged as a vital application area, and is
significantly transformed by the advent of large language models (LLMs). While
earlier approaches relied on encoder-decoder architectures, decoder-only LLMs
have revolutionized DAI, bringing remarkable advancements in understanding and
generation. This survey provides a comprehensive overview of DAI's evolution,
highlighting current research attempts and future prospects of LLMs in this
field. We explore key advancements and challenges in multimodal, multilingual,
and retrieval-augmented DAI, while also suggesting future research directions,
including agent-based approaches and document-specific foundation models. This
paper aims to provide a structured analysis of the state-of-the-art in DAI and
its implications for both academic and practical applications.

</details>


### [54] [Make an Offer They Can't Refuse: Grounding Bayesian Persuasion in Real-World Dialogues without Pre-Commitment](https://arxiv.org/abs/2510.13387)
*Buwei He,Yang Liu,Zhaowei Zhang,Zixia Jia,Huijia Wu,Zhaofeng He,Zilong Zheng,Yipeng Kang*

Main category: cs.CL

TL;DR: 该论文提出了一种基于贝叶斯说服框架的自然语言说服方法，通过承诺-沟通机制增强LLMs的战略说服能力，在单轮对话中显著提高了说服成功率。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在说服能力方面存在局限，忽视了信息不对称的战略运用，且依赖强假设的预承诺机制。

Method: 提出了两种贝叶斯说服变体：半正式自然语言(SFNL)和全自然语言(FNL)，通过承诺-沟通机制让说服者明确描述信息模式，引导被说服者进行贝叶斯信念更新。

Result: 实验表明：(1)基于BP策略的LLMs说服成功率显著高于非BP基线；(2)SFNL具有更好的可信度和逻辑一致性，FNL在自然对话中表现出更强的情绪共鸣和鲁棒性；(3)通过监督微调，小模型可以达到与大模型相当的BP性能。

Conclusion: 贝叶斯说服框架能有效提升LLMs的战略说服能力，SFNL和FNL在不同场景下各有优势，且模型规模不是性能的决定因素。

Abstract: Persuasion, a fundamental social capability for humans, remains a challenge
for AI systems such as large language models (LLMs). Current studies often
overlook the strategic use of information asymmetry in message design or rely
on strong assumptions regarding pre-commitment. In this work, we explore the
application of Bayesian Persuasion (BP) in natural language within single-turn
dialogue settings, to enhance the strategic persuasion capabilities of LLMs.
Our framework incorporates a commitment-communication mechanism, where the
persuader explicitly outlines an information schema by narrating their
potential types (e.g., honest or dishonest), thereby guiding the persuadee in
performing the intended Bayesian belief update. We evaluate two variants of our
approach: Semi-Formal-Natural-Language (SFNL) BP and Fully-Natural-Language
(FNL) BP, benchmarking them against both naive and strong non-BP (NBP)
baselines within a comprehensive evaluation framework. This framework covers a
diverse set of persuadees -- including LLM instances with varying prompts and
fine-tuning and human participants -- across tasks ranging from specially
designed persuasion scenarios to general everyday situations. Experimental
results on LLM-based agents reveal three main findings: (1) LLMs guided by BP
strategies consistently achieve higher persuasion success rates than NBP
baselines; (2) SFNL exhibits greater credibility and logical coherence, while
FNL shows stronger emotional resonance and robustness in naturalistic
conversations; (3) with supervised fine-tuning, smaller models can attain BP
performance comparable to that of larger models.

</details>


### [55] [Doing Things with Words: Rethinking Theory of Mind Simulation in Large Language Models](https://arxiv.org/abs/2510.13395)
*Agnese Lombardi,Alessandro Lenci*

Main category: cs.CL

TL;DR: GPT-4在模拟心理理论能力方面存在严重局限，其看似的心智推理能力可能源于浅层统计关联而非真正推理，无法基于信念归因选择行动，且在复杂社交互动中难以产生连贯因果效应。


<details>
  <summary>Details</summary>
Motivation: 探索生成代理模型Concordia能否有效模拟真实世界环境中的心理理论，评估GPT-4是否能从社会情境进行真正推理而非依赖语言记忆。

Method: 使用生成代理模型Concordia框架，评估GPT-4在模拟环境中的心理理论表现，特别是基于信念归因的行动选择和社交互动处理能力。

Result: GPT-4经常无法基于信念归因选择行动，难以从代理行动生成连贯因果效应，表明其心智推理能力有限且可能源于浅层统计关联。

Conclusion: 当前LLMs中所谓的心智理论能力可能被夸大，需要更严格的基于行动的评价框架来准确评估其真实推理能力。

Abstract: Language is fundamental to human cooperation, facilitating not only the
exchange of information but also the coordination of actions through shared
interpretations of situational contexts. This study explores whether the
Generative Agent-Based Model (GABM) Concordia can effectively model Theory of
Mind (ToM) within simulated real-world environments. Specifically, we assess
whether this framework successfully simulates ToM abilities and whether GPT-4
can perform tasks by making genuine inferences from social context, rather than
relying on linguistic memorization. Our findings reveal a critical limitation:
GPT-4 frequently fails to select actions based on belief attribution,
suggesting that apparent ToM-like abilities observed in previous studies may
stem from shallow statistical associations rather than true reasoning.
Additionally, the model struggles to generate coherent causal effects from
agent actions, exposing difficulties in processing complex social interactions.
These results challenge current statements about emergent ToM-like capabilities
in LLMs and highlight the need for more rigorous, action-based evaluation
frameworks.

</details>


### [56] [Investigating Lexical Change through Cross-Linguistic Colexification Patterns](https://arxiv.org/abs/2510.13407)
*Kim Gfeller,Sabine Stoll,Chundra Cathcart,Paul Widmer*

Main category: cs.CL

TL;DR: 本研究通过系统发育比较模型分析三个语系（南岛语系、印欧语系和乌拉尔语系）的词典数据，发现概念关联性越强越容易共词化且变化更慢，而使用频率高和易借用的概念对变化更快且较少共词化。


<details>
  <summary>Details</summary>
Motivation: 语言意义如何演变及其决定因素尚不完全清楚，共词化现象为研究跨语言意义变化动态提供了重要窗口。

Method: 应用系统发育比较模型分析三个语系的词典数据，评估关联性、借用性和使用频率三个预测因子的影响。

Result: 关联性强的概念对在语系树中更广泛共词化且变化更慢；高频和易借用概念对变化更快且较少共词化；不同语系间存在显著差异。

Conclusion: 概念关联性促进共词化稳定性，而使用频率和借用性加速变化，语言区域和文化因素在意义演变中起重要作用。

Abstract: One of the most intriguing features of language is its constant change, with
ongoing shifts in how meaning is expressed. Despite decades of research, the
factors that determine how and why meanings evolve remain only partly
understood. Colexification -- the phenomenon of expressing multiple distinct
concepts using the same word form -- serves as a valuable window onto the
dynamics of meaning change across languages. Here, we apply phylogenetic
comparative models to dictionary data from three language families,
Austronesian, Indo-European, and Uralic, in order to shed light on the
evolutionary dynamics underlying the colexification of concept pairs. We assess
the effects of three predictors: associativity, borrowability, and usage
frequency. Our results show that more closely related concept pairs are
colexified across a larger portion of the family tree and exhibit slower rates
of change. In contrast, concept pairs that are more frequent and more prone to
borrowing tend to change more rapidly and are less often colexified. We also
find considerable differences between the language families under study,
suggesting that areal and cultural factors may play a role.

</details>


### [57] [Evaluating Arabic Large Language Models: A Survey of Benchmarks, Methods, and Gaps](https://arxiv.org/abs/2510.13430)
*Ahmed Alzubaidi,Shaikha Alsuwaidi,Basma El Amel Boussaha,Leen AlQadi,Omar Alkaabi,Mohammed Alyafeai,Hamza Alobeidli,Hakim Hacid*

Main category: cs.CL

TL;DR: 这是首个系统性的阿拉伯语大语言模型基准评测综述，分析了40多个评测基准，涵盖NLP任务、知识领域、文化理解和专业能力，提出了四分类法并识别了关键差距。


<details>
  <summary>Details</summary>
Motivation: 缺乏对阿拉伯语LLM基准评测的系统性综述，需要为阿拉伯语NLP研究人员提供全面的参考指南，促进该领域的发展。

Method: 提出四分类法（知识、NLP任务、文化与方言、目标特定评测），分析三种主要方法：原生收集、翻译和合成生成，讨论其在真实性、规模和成本方面的权衡。

Result: 发现基准多样性显著进步，但识别出关键差距：时间评估有限、多轮对话评估不足、翻译数据集的文化错位。

Conclusion: 本研究为阿拉伯语NLP研究人员提供了全面的参考，提供了基准方法学、可复现性标准和评估指标的见解，并为未来发展提供了建议。

Abstract: This survey provides the first systematic review of Arabic LLM benchmarks,
analyzing 40+ evaluation benchmarks across NLP tasks, knowledge domains,
cultural understanding, and specialized capabilities. We propose a taxonomy
organizing benchmarks into four categories: Knowledge, NLP Tasks, Culture and
Dialects, and Target-Specific evaluations. Our analysis reveals significant
progress in benchmark diversity while identifying critical gaps: limited
temporal evaluation, insufficient multi-turn dialogue assessment, and cultural
misalignment in translated datasets. We examine three primary approaches:
native collection, translation, and synthetic generation discussing their
trade-offs regarding authenticity, scale, and cost. This work serves as a
comprehensive reference for Arabic NLP researchers, providing insights into
benchmark methodologies, reproducibility standards, and evaluation metrics
while offering recommendations for future development.

</details>


### [58] [Beyond Single-Reward: Multi-Pair, Multi-Perspective Preference Optimization for Machine Translation](https://arxiv.org/abs/2510.13434)
*Hao Wang,Linlong Xu,Heng Liu,Yangyang Liu,Xiaohu Zhao,Bo Zeng,Liangying Shao,Longyue Wang,Weihua Luo,Kaifu Zhang*

Main category: cs.CL

TL;DR: M²PO是一个用于机器翻译的多对多视角偏好优化框架，通过整合幻觉惩罚和动态质量评分来增强奖励信号，并利用多对构造策略从所有翻译候选中系统创建偏好对，显著提升翻译质量。


<details>
  <summary>Details</summary>
Motivation: 当前基于DPO的机器翻译方法存在两个根本性挑战：(1) 质量评估模型提供的奖励信号存在缺陷，忽略了翻译幻觉等关键错误；(2) 数据利用效率低下，仅选择单一胜负对而丢弃了有价值的学习信号。

Method: 提出M²PO框架，包含：1）多视角奖励引擎，结合幻觉惩罚和动态质量评分；2）多对构造策略，从所有翻译候选中系统创建全面的偏好对集合。

Result: 在WMT21-22基准测试中，M²PO显著优于现有的偏好优化方法，并与领先的专有大型语言模型表现出高度竞争力。

Conclusion: M²PO通过多视角奖励和多对构造的协同方法，能够从更丰富的质量权衡谱中学习，产生更鲁棒和忠实的翻译。

Abstract: Direct Preference Optimization (DPO) is a powerful paradigm for aligning
Large Language Models (LLMs) to human preferences in Machine Translation (MT),
but current methods are hindered by two fundamental challenges: (1) flawed
reward signals from Quality Estimation (QE) models that overlook critical
errors like translation hallucination, and (2) inefficient data utilization
that discards valuable learning signals by selecting only a single win-loss
pair. To address these limitations, we introduce M^2PO: Multi-Pair,
Multi-Perspective Preference Optimization. Our framework integrates a
multi-perspective reward engine that creates a more robust signal by combining
two key viewpoints: a new hallucination penalty for factuality, and an
innovative dynamic quality score that adaptively fuses external evaluations
with the model's own evolving judgment. This is synergistically paired with a
multi-pair construction strategy that systematically creates a comprehensive
set of preference pairs from the entire pool of translation candidates. This
synergistic approach ensures the model learns from a richer spectrum of quality
trade-offs, leading to more robust and faithful translations. On challenging
WMT21-22 benchmarks, M^2PO substantially outperforms existing preference
optimization methods and demonstrates highly competitive performance against
leading proprietary LLMs.

</details>


### [59] [LiteraryQA: Towards Effective Evaluation of Long-document Narrative QA](https://arxiv.org/abs/2510.13494)
*Tommaso Bonomo,Luca Gioffré,Roberto Navigli*

Main category: cs.CL

TL;DR: LiteraryQA是一个高质量的NarrativeQA子集，专注于文学作品，通过人工和LLM验证的流程清理了低质量QA样本和文档噪音，并发现基于n-gram的自动评估指标与人类判断相关性低，而LLM评估方法能更好地与人类排名一致。


<details>
  <summary>Details</summary>
Motivation: 当前在叙事文本上的问答系统面临挑战，需要理解长而复杂的文档，但最广泛使用的基准NarrativeQA存在文档噪音和有缺陷的QA对，影响了其可靠性。

Method: 使用人工和LLM验证的流程，识别和修正低质量QA样本，同时从源文档中移除无关文本，然后进行自动指标的元评估，并在一组长上下文LLM上进行基准测试。

Result: 分析显示所有基于n-gram的指标在系统级别上与人类判断的相关性较低，而LLM-as-a-Judge评估方法（即使是小型开源模型）能与人类识别的排名强一致。

Conclusion: LiteraryQA提供了一个更可靠的叙事文本问答基准，LLM评估方法比传统n-gram指标更适合评估此类任务，为未来研究提供了更好的评估框架。

Abstract: Question Answering (QA) on narrative text poses a unique challenge to current
systems, requiring a deep understanding of long, complex documents. However,
the reliability of NarrativeQA, the most widely used benchmark in this domain,
is hindered by noisy documents and flawed QA pairs. In this work, we introduce
LiteraryQA, a high-quality subset of NarrativeQA focused on literary works.
Using a human- and LLM-validated pipeline, we identify and correct low-quality
QA samples while removing extraneous text from source documents. We then carry
out a meta-evaluation of automatic metrics to clarify how systems should be
evaluated on LiteraryQA. This analysis reveals that all n-gram-based metrics
have a low system-level correlation to human judgment, while LLM-as-a-Judge
evaluations, even with small open-weight models, can strongly agree with the
ranking identified by humans. Finally, we benchmark a set of long-context LLMs
on LiteraryQA. We release our code and data at
https://github.com/SapienzaNLP/LiteraryQA.

</details>


### [60] [ConsintBench: Evaluating Language Models on Real-World Consumer Intent Understanding](https://arxiv.org/abs/2510.13499)
*Xiaozhe Li,TianYi Lyu,Siyi Yang,Yuxi Gong,Yizhao Yang,Jinxuan Huang,Ligao Zhang,Zhuoyi Huang,Qingwen Liu*

Main category: cs.CL

TL;DR: 本文介绍了首个专门用于意图理解（特别是消费领域）的动态实时评估基准\bench，该基准支持实时更新并通过自动化流程防止数据污染。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对真实世界人类意图理解的大规模评估基准，主要由于收集真实公共讨论数据和构建稳健评估流程的挑战。真实世界公共讨论具有交织冲突观点、不同关切、目标、情感倾向以及隐含假设等特点。

Method: 构建\bench基准，这是同类中最大最多样化的基准，支持实时更新，通过自动化策划流程防止数据污染。

Result: 成功创建了首个动态、实时的意图理解评估基准\bench，填补了该领域评估工具的空白。

Conclusion: \bench基准为评估LLMs在真实世界人类意图理解方面的能力提供了重要工具，特别是在消费领域，解决了现有评估基准缺失的问题。

Abstract: Understanding human intent is a complex, high-level task for large language
models (LLMs), requiring analytical reasoning, contextual interpretation,
dynamic information aggregation, and decision-making under uncertainty.
Real-world public discussions, such as consumer product discussions, are rarely
linear or involve a single user. Instead, they are characterized by interwoven
and often conflicting perspectives, divergent concerns, goals, emotional
tendencies, as well as implicit assumptions and background knowledge about
usage scenarios. To accurately understand such explicit public intent, an LLM
must go beyond parsing individual sentences; it must integrate multi-source
signals, reason over inconsistencies, and adapt to evolving discourse, similar
to how experts in fields like politics, economics, or finance approach complex,
uncertain environments. Despite the importance of this capability, no
large-scale benchmark currently exists for evaluating LLMs on real-world human
intent understanding, primarily due to the challenges of collecting real-world
public discussion data and constructing a robust evaluation pipeline. To bridge
this gap, we introduce \bench, the first dynamic, live evaluation benchmark
specifically designed for intent understanding, particularly in the consumer
domain. \bench is the largest and most diverse benchmark of its kind,
supporting real-time updates while preventing data contamination through an
automated curation pipeline.

</details>


### [61] [MedREK: Retrieval-Based Editing for Medical LLMs with Key-Aware Prompts](https://arxiv.org/abs/2510.13500)
*Shujun Xia,Haokun Lin,Yichen Wu,Yinan Zhou,Zixuan Li,Zhongwei Wan,Xingrun Xing,Yefeng Zheng,Xiang Li,Caifeng Shan,Zhenan Sun,Quanzheng Li*

Main category: cs.CL

TL;DR: 提出了MedREK框架，通过检索式编辑解决医疗LLMs知识过时问题，支持批量编辑，在MedVersa基准上表现优异


<details>
  <summary>Details</summary>
Motivation: 医疗LLMs面临知识快速更新和训练数据错误的问题，导致生成过时或不准确信息，限制了在临床实践中的应用。参数编辑方法会损害局部性，而现有检索编辑方法存在表示重叠和仅支持单样本编辑的局限

Method: 构建MedVersa基准评估单样本和批量编辑，提出MedREK框架：集成共享查询-键模块进行精确匹配，结合基于注意力的提示编码器提供信息指导

Result: 在各种医疗基准测试中，MedREK在不同核心指标上均取得优异性能，首次为医疗LLMs提供了经过验证的批量编辑解决方案

Conclusion: MedREK通过检索式编辑有效解决了医疗LLMs的知识更新问题，支持批量编辑，为临床实践提供了更可靠的解决方案

Abstract: LLMs hold great promise for healthcare applications, but the rapid evolution
of medical knowledge and errors in training data often cause them to generate
outdated or inaccurate information, limiting their applicability in high-stakes
clinical practice. Model editing has emerged as a potential remedy without full
retraining. While parameter-based editing often compromises locality and is
thus ill-suited for the medical domain, retrieval-based editing offers a more
viable alternative. However, it still faces two critical challenges: (1)
representation overlap within the medical knowledge space often causes
inaccurate retrieval and reduces editing accuracy; (2) existing methods are
restricted to single-sample edits, while batch-editing remains largely
unexplored despite its importance for real-world medical applications. To
address these challenges, we first construct MedVersa, \hk{an enhanced
benchmark with broader coverage of medical subjects, designed to evaluate both
single and batch edits under strict locality constraints}. We then propose
MedREK, a retrieval-based editing framework that integrates a shared query-key
module for precise matching with an attention-based prompt encoder for
informative guidance. Experimental results on various medical benchmarks
demonstrate that our MedREK achieves superior performance across different core
metrics and provides the first validated solution for batch-editing in medical
LLMs. Our code and dataset are available at
https://github.com/mylittleriver/MedREK.

</details>


### [62] [Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm Enables Fine-Grained Policy Optimization](https://arxiv.org/abs/2510.13554)
*Yang Li,Zhichen Dong,Yuhan Sun,Weixun Wang,Shaopan Xiong,Yijia Luo,Jiashun Liu,Han Lu,Jiamang Wang,Wenbo Su,Bo Zheng,Junchi Yan*

Main category: cs.CL

TL;DR: 本文通过分析注意力机制揭示了LLM的推理模式，提出了两种量化指标来识别关键推理步骤，并基于此开发了三种新的强化学习策略，在多个推理任务中实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的推理模式不透明，传统的强化学习通常对整个生成过程应用统一的信用分配，无法区分关键步骤和常规步骤。

Method: 1) 区分局部和全局注意力头，发现局部头产生短语块的对角锯齿模式，全局头揭示对未来标记有广泛影响的标记；2) 提出两个量化指标：窗口平均注意力距离和未来注意力影响；3) 基于发现的预规划-锚定机制，开发三种新的RL策略进行针对性信用分配。

Result: 揭示了LLM推理中反复出现的预规划-锚定机制，提出的三种RL策略在各种推理任务中表现出一致的性能提升。

Conclusion: 通过将优化与模型内在推理节奏对齐，将不透明的优化转变为可操作的结构感知过程，为LLM推理的透明和有效优化提供了潜在步骤。

Abstract: The reasoning pattern of Large language models (LLMs) remains opaque, and
Reinforcement learning (RL) typically applies uniform credit across an entire
generation, blurring the distinction between pivotal and routine steps. This
work positions attention as a privileged substrate that renders the internal
logic of LLMs legible, not merely as a byproduct of computation, but as a
mechanistic blueprint of reasoning itself. We first distinguish attention heads
between locally and globally focused information processing and reveal that
locally focused heads produce a sawtooth pattern near the diagonal indicating
phrasal chunks, while globally focused heads expose tokens that exert broad
downstream influence over future tokens. We formalize these with two metrics:
1) Windowed Average Attention Distance, which measures the extent of backward
attention within a clipped window; 2) Future Attention Influence, which
quantifies a token's global importance as the average attention it receives
from subsequent tokens. Taken together, these signals reveal a recurring
preplan-and-anchor mechanism, where the model first performs a long-range
contextual reference to generate an introductory token, which is immediately
followed by or coincides with a semantic anchor token that organizes subsequent
reasoning. Leveraging these insights, we introduce three novel RL strategies
that dynamically perform targeted credit assignment to critical nodes (preplan
tokens, anchor tokens, and their temporal coupling) and show consistent
performance gains across various reasoning tasks. By aligning optimization with
the model's intrinsic reasoning rhythm, we aim to transform opaque optimization
into an actionable structure-aware process, hoping to offer a potential step
toward more transparent and effective optimization of LLM reasoning.

</details>


### [63] [Sparse Subnetwork Enhancement for Underrepresented Languages in Large Language Models](https://arxiv.org/abs/2510.13580)
*Daniil Gurgurov,Josef van Genabith,Simon Ostermann*

Main category: cs.CL

TL;DR: 提出了一种通过识别语言特定子网络进行针对性微调的方法，在仅更新最多1%参数的情况下，有效提升LLM在低资源语言上的单语能力，同时保持通用性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在不同语言间表现不均，高资源语言和低资源语言之间存在显著性能差距，需要一种高效的方法来增强LLM在低资源语言上的能力。

Method: 使用语言激活概率熵识别语言特定神经元，仅微调与这些神经元相关的权重（专用子网络）在目标语言数据上。

Result: 在12种中低资源语言上的实验表明，该方法在仅更新最多1%参数的情况下，持续优于全微调、仅FFN微调、LoRA适配和随机子集微调基线。

Conclusion: 该方法为将最先进模型适配到低资源语言提供了一条成本效益高的途径，同时观察到改进的训练动态、跨语言表示对齐和系统性的权重更新变化。

Abstract: Large language models exhibit uneven performance across languages, with
substantial gaps between high- and low-resource languages. We present a
framework for enhancing monolingual capabilities of LLMs in underrepresented
languages while preserving their general-purpose performance through targeted
fine-tuning of language-specific subnetworks. Our approach identifies
language-specific neurons using Language Activation Probability Entropy and
fine-tunes only the weights associated with these neurons, a dedicated
subnetwork, on target-language data. Experiments on Llama-3.1-8B and
Mistral-Nemo-12B across 12 mid- and low-resource languages demonstrate that our
method consistently outperforms full fine-tuning, FFN-only fine-tuning, LoRA
adaptation, and random subset fine-tuning baselines while efficiently updating
only up to 1% of model parameters. Beyond performance improvements, we observe
enhanced favorable training dynamics, cross-lingual representational alignment,
and systematic weight update changes. To facilitate future research, we release
language-specific neuron identifications for over 100 languages as well as our
adaptation pipeline, offering a cost-effective pathway for adapting
state-of-the-art models to underrepresented languages.

</details>


### [64] [Deflanderization for Game Dialogue: Balancing Character Authenticity with Task Execution in LLM-based NPCs](https://arxiv.org/abs/2510.13586)
*Pasin Buakhaw,Kun Kerdthaisong,Phuree Phenhiran,Pitikorn Khlaisamniang,Supasate Vorathammathorn,Piyalitt Ittichaiwong,Nutchanon Yongsatianchot*

Main category: cs.CL

TL;DR: 该论文介绍了在CPDC 2025 Round 2竞赛中，使用轻量级提示技术和微调大模型的方法来创建动态NPC，在任务导向对话和上下文感知对话任务中取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型创建游戏中的动态非玩家角色，既能执行功能性任务，又能生成符合角色设定的对话，为游戏环境带来新的可能性。

Method: 采用两种互补策略：(1) API赛道使用轻量级提示技术，包括Deflanderization提示方法来抑制过度角色扮演并提高任务保真度；(2) GPU赛道使用微调的大模型，基于Qwen3-14B进行监督微调和低秩适配。

Result: 在竞赛中取得了优异成绩：任务1排名第2，任务3（API赛道）排名第2，任务3（GPU赛道）排名第4。

Conclusion: 结合轻量级提示技术和微调大模型的混合方法在创建动态NPC方面表现出色，证明了该方法在任务导向对话和上下文感知对话任务中的有效性。

Abstract: The emergence of large language models (LLMs) has opened new opportunities
for cre- ating dynamic non-player characters (NPCs) in gaming environments,
enabling both func- tional task execution and persona-consistent dialogue
generation. In this paper, we (Tu_Character_lab) report our participation in
the Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025 Round 2, which
eval- uates agents across three tracks: task-oriented dialogue, context-aware
dialogue, and their integration. Our approach combines two complementary
strategies: (i) lightweight prompting techniques in the API track, including a
Deflanderization prompting method to suppress excessive role-play and improve
task fidelity, and (ii) fine-tuned large models in the GPU track, leveraging
Qwen3-14B with supervisedfinetuning (SFT) and Low-Rank Adaptation(LoRA). Our
best submissions ranked 2nd on Task 1, 2nd on Task 3 (API track), and 4th on
Task 3 (GPU track).

</details>


### [65] [FreshTab: Sourcing Fresh Data for Table-to-Text Generation Evaluation](https://arxiv.org/abs/2510.13598)
*Kristýna Onderková,Ondřej Plátek,Zdeněk Kasner,Ondřej Dušek*

Main category: cs.CL

TL;DR: FreshTab是一个动态生成的表格到文本基准测试，从维基百科实时收集数据以解决LLM训练数据污染问题，支持多语言和领域敏感评估。


<details>
  <summary>Details</summary>
Motivation: 现有表格到文本基准测试存在LLM训练数据污染和领域不平衡问题，且非英语数据集有限，需要更可靠的评估方法。

Method: 从维基百科动态收集最新表格数据，支持按需生成英语、德语、俄语和法语等多语言数据集，实现领域平衡的基准测试。

Result: 自动指标显示LLM从新表格生成的见解明显较差，但LLM和人工评估未体现此差异；领域效应在所有评估中都很明显，表明领域平衡的基准更具挑战性。

Conclusion: FreshTab通过实时数据收集有效解决了数据污染问题，领域平衡的基准测试能更好地评估模型性能，多语言支持扩展了评估范围。

Abstract: Table-to-text generation (insight generation from tables) is a challenging
task that requires precision in analyzing the data. In addition, the evaluation
of existing benchmarks is affected by contamination of Large Language Model
(LLM) training data as well as domain imbalance. We introduce FreshTab, an
on-the-fly table-to-text benchmark generation from Wikipedia, to combat the LLM
data contamination problem and enable domain-sensitive evaluation. While
non-English table-to-text datasets are limited, FreshTab collects datasets in
different languages on demand (we experiment with German, Russian and French in
addition to English). We find that insights generated by LLMs from recent
tables collected by our method appear clearly worse by automatic metrics, but
this does not translate into LLM and human evaluations. Domain effects are
visible in all evaluations, showing that a~domain-balanced benchmark is more
challenging.

</details>


### [66] [NOSA: Native and Offloadable Sparse Attention](https://arxiv.org/abs/2510.13602)
*Yuxiang Huang,Chaojun Xiao,Xu Han,Zhiyuan Liu*

Main category: cs.CL

TL;DR: NOSA是一个可训练的稀疏注意力框架，通过引入显式局部性约束来减少KV缓存传输，在保持训练时注意力计算不变的同时，显著提升解码吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏注意力方法虽然减少了内存访问，但KV缓存大小未减少，限制了GPU批量大小和解码吞吐量，特别是在大规模批量推理中。

Method: NOSA通过将token选择分解为查询感知和查询无关组件，引入显式局部性约束，减少KV传输，同时保持与训练时相同的注意力计算。

Result: 预训练的1B参数模型在保持近乎无损性能的同时，相比基准方法(InfLLM-V2)实现了最高2.3倍的解码吞吐量提升。

Conclusion: NOSA框架有效解决了KV缓存卸载问题，在保持模型性能的同时显著提升了解码效率，为长上下文处理提供了更高效的解决方案。

Abstract: Trainable sparse attention has emerged as a promising solution to address the
decoding efficiency bottleneck of LLMs in long-context processing,
significantly saving memory accesses while minimally impacting task
performance. However, existing sparse attention methods leave a crucial
limitation unresolved: the size of the key-value (KV) cache remains unreduced,
which constrains on-GPU batch sizes and throttles decoding throughput,
especially in large-scale batched inference. In this paper, we show that
trainable sparse attention naturally exhibits strong locality in token
selection across adjacent decoding steps, thereby enabling KV cache offloading
without altering the underlying attention computation. However, the inherent
locality remains insufficient to achieve efficient offloading, as the transfer
of selected KV pairs between the CPU and GPU continues to dominate the overall
decoding cost. Building on this insight, we present NOSA, a trainable sparse
attention framework designed to natively support KV cache offloading. NOSA
introduces explicit locality constraints by decomposing token selection into
query-aware and query-agnostic components, thereby reducing KV transfers while
preserving the same attention computation as used during training. We pretrain
a 1B-parameter model with NOSA and conduct extensive benchmarks, showing that
it preserves near-lossless performance while achieving up to a 2.3x improvement
in decoding throughput compared with the vanilla trainable sparse attention
baseline (InfLLM-V2).

</details>


### [67] [MemoTime: Memory-Augmented Temporal Knowledge Graph Enhanced Large Language Model Reasoning](https://arxiv.org/abs/2510.13614)
*Xingyu Tan,Xiaoyang Wang,Xiwei Xu,Xin Yuan,Liming Zhu,Wenjie Zhang*

Main category: cs.CL

TL;DR: MemoTime是一个基于记忆增强的时间知识图谱框架，通过结构化基础、递归推理和持续经验学习来增强LLM的时间推理能力，在多个时间QA基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在时间理解方面存在困难，特别是在涉及多个实体、复合操作符和演化事件序列的问题时。现有的基于时间知识图谱的LLM推理方法面临四个主要挑战：多跳推理中的时间忠实性、多实体时间同步、适应不同时间操作符的检索，以及重用先前推理经验以提高稳定性和效率。

Method: 提出MemoTime框架，将复杂时间问题分解为时间层次树，实现操作符感知推理，强制单调时间戳并在统一时间边界下共同约束多个实体。包含动态证据检索层自适应选择操作符特定的检索策略，以及自演化经验记忆存储已验证的推理轨迹、工具包决策和子问题嵌入以实现跨类型重用。

Result: 在多个时间QA基准测试中，MemoTime实现了整体最先进的结果，比强基线高出24.0%。此外，MemoTime使较小模型（如Qwen3-4B）能够达到与GPT-4-Turbo相当的推理性能。

Conclusion: MemoTime通过结构化基础、递归推理和持续经验学习有效解决了LLM在时间推理中的关键挑战，显著提升了时间理解能力，并使较小模型能够达到大型模型的推理性能。

Abstract: Large Language Models (LLMs) have achieved impressive reasoning abilities,
but struggle with temporal understanding, especially when questions involve
multiple entities, compound operators, and evolving event sequences. Temporal
Knowledge Graphs (TKGs), which capture vast amounts of temporal facts in a
structured format, offer a reliable source for temporal reasoning. However,
existing TKG-based LLM reasoning methods still struggle with four major
challenges: maintaining temporal faithfulness in multi-hop reasoning, achieving
multi-entity temporal synchronization, adapting retrieval to diverse temporal
operators, and reusing prior reasoning experience for stability and efficiency.
To address these issues, we propose MemoTime, a memory-augmented temporal
knowledge graph framework that enhances LLM reasoning through structured
grounding, recursive reasoning, and continual experience learning. MemoTime
decomposes complex temporal questions into a hierarchical Tree of Time,
enabling operator-aware reasoning that enforces monotonic timestamps and
co-constrains multiple entities under unified temporal bounds. A dynamic
evidence retrieval layer adaptively selects operator-specific retrieval
strategies, while a self-evolving experience memory stores verified reasoning
traces, toolkit decisions, and sub-question embeddings for cross-type reuse.
Comprehensive experiments on multiple temporal QA benchmarks show that MemoTime
achieves overall state-of-the-art results, outperforming the strong baseline by
up to 24.0%. Furthermore, MemoTime enables smaller models (e.g., Qwen3-4B) to
achieve reasoning performance comparable to that of GPT-4-Turbo.

</details>


### [68] [Unlocking Public Catalogues: Instruction-Tuning LLMs for ICD Coding of German Tumor Diagnoses](https://arxiv.org/abs/2510.13624)
*Stefan Lenz,Lakisha Ortiz Rosario,Georg Vollmar,Arsenij Ustjanzew,Fatma Alickovic,Thomas Kindler,Torsten Panholzer*

Main category: cs.CL

TL;DR: 指令微调显著提升了开源LLM在德语肿瘤诊断ICD编码任务中的准确性，ICD-10-GM准确率从1.4-24%提升至41-58%，模型大小与性能正相关但差距缩小，推理模式性能较低且速度慢100倍。


<details>
  <summary>Details</summary>
Motivation: 德国肿瘤诊断的ICD-10-GM和ICD-O-3准确编码对结构化癌症文档至关重要，但小型开源LLM在德语语境下编码准确性不足，需要研究指令微调是否能提升其性能。

Method: 基于ICD-10-GM、ICD-O-3和OPS目录创建了超过50万个问答对作为训练数据，对Qwen、Llama和Mistral家族的8个开源模型（7-70B参数）进行指令微调，使用本地肿瘤文档系统的编码诊断作为测试数据。

Result: ICD-10-GM准确率从1.4-24%提升至41-58%，部分准确率从31-74%提升至73-83%；ICD-O-3地形编码准确率从较低水平提升至22-40%（精确）和56-67%（部分）；畸形代码输出降至0%，肿瘤诊断识别率达99%。

Conclusion: 利用公共目录构建指令数据集可以有效提升LLM在医疗文档任务中的性能，证明了开源LLM通过指令微调在德语医疗编码任务中的潜力。

Abstract: Accurate coding of tumor diagnoses with ICD-10-GM and ICD-O-3 is essential
for structured cancer documentation in Germany. Smaller open-weight LLMs are
appealing for privacy-preserving automation but often struggle with coding
accuracy in German-language contexts. This study investigates whether
instruction-based fine-tuning on public datasets improves the coding accuracy
of open-weight LLMs for German tumor diagnosis texts. The evaluation uses coded
diagnoses from the local tumor documentation system as test data. In a
systematic data quality assessment, the upper limit for ICD-10 coding
performance was estimated at 60-79% for exact and 81-94% for partial
(three-character codes only) derivation. As training data, over 500,000
question-answer pairs were created based on the ICD-10-GM, ICD-O-3, and OPS
catalogues. Eight open-weight models from the Qwen, Llama, and Mistral families
(7-70 B parameters) were fine-tuned. ICD-10-GM accuracy rose from 1.4-24% to
41-58%, and partial accuracy from 31-74% to 73-83%. The accuracy of ICD-O-3
topography coding also improved but started and remained considerably lower
with an exact accuracy of 22-40% and a partial accuracy of 56-67% after
fine-tuning. Malformed code outputs dropped to 0% for all models.
Tumor-diagnosis recognition reached 99%. Accuracy correlated positively with
model size, but gaps between small and large models narrowed after fine-tuning.
The reasoning mode in Qwen3 generally yielded a lower performance than
fine-tuning and was over 100 times slower. Our findings highlight the potential
of leveraging public catalogues to build instruction datasets that improve LLMs
in medical documentation tasks. The complete training dataset and the
best-performing checkpoints of the fine-tuned models are available from
https://huggingface.co/datasets/stefan-m-lenz/ICDOPS-QA-2024.

</details>


### [69] [Closing the Gap Between Text and Speech Understanding in LLMs](https://arxiv.org/abs/2510.13632)
*Santiago Cuervo,Skyler Seto,Maureen de Seyssel,Richard He Bai,Zijin Gu,Tatiana Likhomanenko,Navdeep Jaitly,Zakaria Aldeneh*

Main category: cs.CL

TL;DR: SALAD方法通过跨模态蒸馏和针对性合成数据，在减少语音数据使用的情况下有效缩小文本-语音理解差距，在知识、语言理解和推理任务上达到与强基线模型竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 语音适配的LLMs在语言理解任务上持续表现不如基于文本的对应模型，存在文本-语音理解差距。现有方法要么依赖昂贵的语音合成，要么依赖大规模专有数据集，需要更数据高效的方法来缩小这一差距。

Method: SALAD方法结合跨模态蒸馏与针对性合成数据，通过主动选择和学习来改善对齐同时减轻遗忘。该方法识别文本能力遗忘和跨模态不对齐是导致差距的两个主要因素。

Result: 在3B和7B LLMs上应用SALAD，在广泛领域的知识、语言理解和推理基准测试中，使用比公共语料库少一个数量级的语音数据，达到了与强开源权重模型竞争的性能。

Conclusion: SALAD提供了一种数据高效的方法来缩小文本-语音理解差距，通过同时解决文本能力遗忘和跨模态不对齐问题，在减少数据依赖的同时保持竞争性能。

Abstract: Large Language Models (LLMs) can be adapted to extend their text capabilities
to speech inputs. However, these speech-adapted LLMs consistently underperform
their text-based counterparts--and even cascaded pipelines--on language
understanding tasks. We term this shortfall the text-speech understanding gap:
the performance drop observed when a speech-adapted LLM processes spoken inputs
relative to when the original text-based LLM processes the equivalent text.
Recent approaches to narrowing this gap either rely on large-scale speech
synthesis of text corpora, which is costly and heavily dependent on synthetic
data, or on large-scale proprietary speech datasets, which are not
reproducible. As a result, there remains a need for more data-efficient
alternatives for closing the text-speech understanding gap. In this work, we
analyze the gap as driven by two factors: (i) forgetting of text capabilities
during adaptation, and (ii) cross-modal misalignment between speech and text.
Based on this analysis, we introduce SALAD--Sample-efficient Alignment with
Learning through Active selection and cross-modal Distillation--which combines
cross-modal distillation with targeted synthetic data to improve alignment
while mitigating forgetting. Applied to 3B and 7B LLMs, SALAD achieves
competitive performance with a strong open-weight model across broad-domain
benchmarks in knowledge, language understanding, and reasoning, while training
on over an order of magnitude less speech data from public corpora.

</details>


### [70] [How Sampling Affects the Detectability of Machine-written texts: A Comprehensive Study](https://arxiv.org/abs/2510.13681)
*Matthieu Dubois,François Yvon,Pablo Piantanida*

Main category: cs.CL

TL;DR: 本文研究发现，即使是微小的解码参数调整（如温度、top-p等）也会严重损害LLM文本检测器的准确性，AUROC从接近完美降至1%，揭示了当前检测方法的盲点。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成文本日益普遍且难以与人类写作区分，自动文本检测研究受到关注。许多检测器声称接近完美的准确率，但这些评估通常基于固定的生成设置，未考虑解码策略变化对检测鲁棒性的影响。

Method: 系统性地研究了基于采样的解码对可检测性的影响，重点关注模型（子）词级分布的细微变化如何影响检测性能。构建了包含37种解码配置的大规模数据集。

Result: 发现即使对解码参数进行微小调整（如温度、top-p或核采样），也会严重损害检测器准确性，在某些设置下AUROC从接近完美水平降至1%。

Conclusion: 当前检测方法存在关键盲点，需要更全面的评估协议。为促进未来研究，发布了大规模数据集、代码和评估框架。

Abstract: As texts generated by Large Language Models (LLMs) are ever more common and
often indistinguishable from human-written content, research on automatic text
detection has attracted growing attention. Many recent detectors report
near-perfect accuracy, often boasting AUROC scores above 99\%. However, these
claims typically assume fixed generation settings, leaving open the question of
how robust such systems are to changes in decoding strategies. In this work, we
systematically examine how sampling-based decoding impacts detectability, with
a focus on how subtle variations in a model's (sub)word-level distribution
affect detection performance. We find that even minor adjustments to decoding
parameters - such as temperature, top-p, or nucleus sampling - can severely
impair detector accuracy, with AUROC dropping from near-perfect levels to 1\%
in some settings. Our findings expose critical blind spots in current detection
methods and emphasize the need for more comprehensive evaluation protocols. To
facilitate future research, we release a large-scale dataset encompassing 37
decoding configurations, along with our code and evaluation framework
https://github.com/BaggerOfWords/Sampling-and-Detection

</details>


### [71] [NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching](https://arxiv.org/abs/2510.13721)
*Run Luo,Xiaobo Xia,Lu Wang,Longze Chen,Renke Shan,Jing Luo,Min Yang,Tat-Seng Chua*

Main category: cs.CL

TL;DR: NExT-OMNI是一个开源的全模态基础模型，通过离散流范式实现统一建模，支持任意模态间的理解和生成，在多轮多模态交互和跨模态检索方面优于现有统一模型。


<details>
  <summary>Details</summary>
Motivation: 现有大多数多模态模型受限于自回归架构，无法平衡理解和生成能力，且混合和解耦策略的设计冗余，限制了在更广泛场景（如跨模态检索）中的应用。

Method: 利用度量诱导概率路径和动力学最优速度的离散流范式，通过简洁的统一表示而非任务解耦设计，实现任意模态间的理解和生成。

Result: 在大规模交错文本、图像、视频和音频数据上训练，NExT-OMNI在多模态生成和理解基准测试中表现优异，在多轮多模态交互和跨模态检索方面优于先前的统一模型。

Conclusion: NExT-OMNI作为下一代多模态基础模型，通过离散流范式实现了统一建模，在理解和生成能力之间取得了平衡，具有架构优势，并开源了训练细节、数据协议、代码和模型检查点以促进进一步研究。

Abstract: Next-generation multimodal foundation models capable of any-to-any
cross-modal generation and multi-turn interaction will serve as core components
of artificial general intelligence systems, playing a pivotal role in
human-machine interaction. However, most existing multimodal models remain
constrained by autoregressive architectures, whose inherent limitations prevent
a balanced integration of understanding and generation capabilities. Although
hybrid and decoupling strategies have been explored to address these tasks
within unified frameworks separately, their redundant, non-integrated designs
limit their applicability to broader scenarios, such as cross-modal
retrieval.In this work, we introduce NExT-OMNI, an open-source omnimodal
foundation model that achieves unified modeling through discrete flow
paradigms. By leveraging metric-induced probability paths and kinetic optimal
velocities, NExT-OMNI natively supports any-to-any understanding and generation
with enhanced response efficiency, while enabling broader application scenarios
through concise unified representations rather than task-decoupled designs.
Trained on large-scale interleaved text, image, video, and audio data,
NExT-OMNI delivers competitive performance on multimodal generation and
understanding benchmarks, while outperforming prior unified models in
multi-turn multimodal interaction and cross-modal retrieval, highlighting its
architectural advantages as a next-generation multimodal foundation model. To
advance further research, we release training details, data protocols, and
open-source both the code and model checkpoints.

</details>


### [72] [GAPS: A Clinically Grounded, Automated Benchmark for Evaluating AI Clinicians](https://arxiv.org/abs/2510.13734)
*Xiuyuan Chen,Tao Sun,Dexin Su,Ailing Yu,Junwei Liu,Zhe Chen,Gangzeng Jin,Xin Wang,Jingnan Liu,Hansong Xiao,Hualei Zhou,Dongjie Tao,Chunxiao Guo,Minghui Yang,Yuan Xia,Jing Zhao,Qianrui Fan,Yanyun Wang,Shuai Zhen,Kezhong Chen,Jun Wang,Zewen Sun,Heng Zhao,Tian Guan,Shaodong Wang,Geyun Chang,Jiaming Deng,Hongchengcheng Chen,Kexin Feng,Ruzhen Li,Jiayi Geng,Changtai Zhao,Jun Wang,Guihu Lin,Peihao Li,Liqi Liu,Peng Wei,Jian Wang,Jinjie Gu,Ping Wang,Fan Yang*

Main category: cs.CL

TL;DR: 提出了GAPS框架，一个用于评估AI临床医生系统的多维评估范式，包括认知深度(G)、答案完整性(A)、鲁棒性(P)和安全性(S)。开发了全自动的指南锚定流水线来构建GAPS对齐的基准测试，解决了之前工作的可扩展性和主观性限制。


<details>
  <summary>Details</summary>
Motivation: 当前基于多项选择题或手动评分的AI临床医生系统基准测试无法捕捉真实世界临床实践所需的深度、鲁棒性和安全性。

Method: 开发了全自动的指南锚定流水线，包括构建证据邻域、创建双图和树表示、自动生成跨G级别的问题。使用DeepResearch代理合成评分标准，模拟GRADE一致、PICO驱动的证据审查。通过LLM评委集合进行评分。

Result: 验证确认自动生成的问题质量高且与临床医生判断一致。评估最先进模型发现关键失败模式：性能随推理深度增加而急剧下降，模型在答案完整性方面表现不佳，对对抗性扰动和某些安全问题高度脆弱。

Conclusion: 这种自动化的、临床基础的方法为严格评估AI临床医生系统提供了可重复和可扩展的方法，指导其向更安全、更可靠的临床实践发展。

Abstract: Current benchmarks for AI clinician systems, often based on multiple-choice
exams or manual rubrics, fail to capture the depth, robustness, and safety
required for real-world clinical practice. To address this, we introduce the
GAPS framework, a multidimensional paradigm for evaluating \textbf{G}rounding
(cognitive depth), \textbf{A}dequacy (answer completeness),
\textbf{P}erturbation (robustness), and \textbf{S}afety. Critically, we
developed a fully automated, guideline-anchored pipeline to construct a
GAPS-aligned benchmark end-to-end, overcoming the scalability and subjectivity
limitations of prior work. Our pipeline assembles an evidence neighborhood,
creates dual graph and tree representations, and automatically generates
questions across G-levels. Rubrics are synthesized by a DeepResearch agent that
mimics GRADE-consistent, PICO-driven evidence review in a ReAct loop. Scoring
is performed by an ensemble of large language model (LLM) judges. Validation
confirmed our automated questions are high-quality and align with clinician
judgment. Evaluating state-of-the-art models on the benchmark revealed key
failure modes: performance degrades sharply with increased reasoning depth
(G-axis), models struggle with answer completeness (A-axis), and they are
highly vulnerable to adversarial perturbations (P-axis) as well as certain
safety issues (S-axis). This automated, clinically-grounded approach provides a
reproducible and scalable method for rigorously evaluating AI clinician systems
and guiding their development toward safer, more reliable clinical practice.

</details>


### [73] [Assessing Web Search Credibility and Response Groundedness in Chat Assistants](https://arxiv.org/abs/2510.13749)
*Ivan Vykopal,Matúš Pikuliak,Simon Ostermann,Marián Šimko*

Main category: cs.CL

TL;DR: 本文评估了四种聊天助手（GPT-4o、GPT-5、Perplexity、Qwen Chat）在网络搜索中的可信度和引用可靠性，发现Perplexity在源可信度方面表现最佳，而GPT-4o在敏感话题上更倾向于引用非可信来源。


<details>
  <summary>Details</summary>
Motivation: 随着聊天助手集成网络搜索功能，虽然能提供更可靠的答案，但也存在放大低可信度来源错误信息的风险，需要系统评估其搜索行为。

Method: 使用100个涉及五个易出现错误信息话题的声明，评估四种聊天助手的网络搜索行为，重点关注源可信度和回答与引用来源的关联性。

Result: 不同助手之间存在差异，Perplexity获得最高的源可信度，而GPT-4o在敏感话题上对非可信来源的引用率较高。

Conclusion: 这项工作首次系统比较了常用聊天助手的事实核查行为，为评估高风险信息环境中的AI系统提供了基础。

Abstract: Chat assistants increasingly integrate web search functionality, enabling
them to retrieve and cite external sources. While this promises more reliable
answers, it also raises the risk of amplifying misinformation from
low-credibility sources. In this paper, we introduce a novel methodology for
evaluating assistants' web search behavior, focusing on source credibility and
the groundedness of responses with respect to cited sources. Using 100 claims
across five misinformation-prone topics, we assess GPT-4o, GPT-5, Perplexity,
and Qwen Chat. Our findings reveal differences between the assistants, with
Perplexity achieving the highest source credibility, whereas GPT-4o exhibits
elevated citation of non-credibility sources on sensitive topics. This work
provides the first systematic comparison of commonly used chat assistants for
fact-checking behavior, offering a foundation for evaluating AI systems in
high-stakes information environments.

</details>


### [74] [Confidence-Based Response Abstinence: Improving LLM Trustworthiness via Activation-Based Uncertainty Estimation](https://arxiv.org/abs/2510.13750)
*Zhiqi Huang,Vivek Datla,Chenyang Zhu,Alfy Samuel,Daben Liu,Anoop Kumar,Ritesh Soni*

Main category: cs.CL

TL;DR: 提出了一种基于前馈网络激活的置信度估计方法，用于检索增强生成系统，在金融客服场景中优于基线方法，并能在保持准确性的同时降低延迟。


<details>
  <summary>Details</summary>
Motivation: 在金融和医疗等高风险领域，置信度估计至关重要，因为错误答案的成本远高于不回答问题。现有方法存在信息损失问题，需要更准确的置信度评估。

Method: 利用原始前馈网络激活作为自回归信号，避免投影和softmax归一化造成的信息损失；将置信度预测建模为序列分类任务，使用Huber损失进行正则化训练以提高对噪声监督的鲁棒性。

Result: 在真实金融行业客服场景中，该方法优于强基线方法，在严格延迟约束下保持高准确性。在Llama 3.1 8B模型上，仅使用第16层激活即可保持准确性同时减少响应延迟。

Conclusion: 基于激活的置信度建模为可信赖的RAG部署提供了一条可扩展、架构感知的路径。

Abstract: We propose a method for confidence estimation in retrieval-augmented
generation (RAG) systems that aligns closely with the correctness of large
language model (LLM) outputs. Confidence estimation is especially critical in
high-stakes domains such as finance and healthcare, where the cost of an
incorrect answer outweighs that of not answering the question. Our approach
extends prior uncertainty quantification methods by leveraging raw feed-forward
network (FFN) activations as auto-regressive signals, avoiding the information
loss inherent in token logits and probabilities after projection and softmax
normalization. We model confidence prediction as a sequence classification
task, and regularize training with a Huber loss term to improve robustness
against noisy supervision. Applied in a real-world financial industry
customer-support setting with complex knowledge bases, our method outperforms
strong baselines and maintains high accuracy under strict latency constraints.
Experiments on Llama 3.1 8B model show that using activations from only the
16th layer preserves accuracy while reducing response latency. Our results
demonstrate that activation-based confidence modeling offers a scalable,
architecture-aware path toward trustworthy RAG deployment.

</details>


### [75] [The Mechanistic Emergence of Symbol Grounding in Language Models](https://arxiv.org/abs/2510.13796)
*Shuyu Wu,Ziqiao Ma,Xiaoxi Luo,Yidong Huang,Josue Torres-Fonseca,Freda Shi,Joyce Chai*

Main category: cs.CL

TL;DR: 本文通过机制性和因果分析框架，发现符号接地现象主要出现在语言模型的中间层计算中，通过注意力头聚合环境信息来支持语言形式预测，这一现象在Transformer和状态空间模型中普遍存在。


<details>
  <summary>Details</summary>
Motivation: 尽管已有初步证据表明大规模训练的语言模型可能自发出现符号接地现象，但这种现象的具体发生位置和驱动机制仍不明确，需要系统性的探索。

Method: 引入受控评估框架，通过机制性和因果分析系统追踪符号接地在内部计算中的产生过程。

Result: 发现接地现象集中在中间层计算，通过注意力头聚合环境信息实现；该现象在多模态对话和不同架构（Transformer、状态空间模型）中可复现，但在单向LSTM中不存在。

Conclusion: 研究提供了行为和机制证据，表明符号接地可以在语言模型中自发出现，这对预测和控制生成可靠性具有实际意义。

Abstract: Symbol grounding (Harnad, 1990) describes how symbols such as words acquire
their meanings by connecting to real-world sensorimotor experiences. Recent
work has shown preliminary evidence that grounding may emerge in
(vision-)language models trained at scale without using explicit grounding
objectives. Yet, the specific loci of this emergence and the mechanisms that
drive it remain largely unexplored. To address this problem, we introduce a
controlled evaluation framework that systematically traces how symbol grounding
arises within the internal computations through mechanistic and causal
analysis. Our findings show that grounding concentrates in middle-layer
computations and is implemented through the aggregate mechanism, where
attention heads aggregate the environmental ground to support the prediction of
linguistic forms. This phenomenon replicates in multimodal dialogue and across
architectures (Transformers and state-space models), but not in unidirectional
LSTMs. Our results provide behavioral and mechanistic evidence that symbol
grounding can emerge in language models, with practical implications for
predicting and potentially controlling the reliability of generation.

</details>


### [76] [Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression Beacons](https://arxiv.org/abs/2510.13797)
*Giovanni Monea,Yair Feldman,Shankar Padmanabhan,Kianté Brantley,Yoav Artzi*

Main category: cs.CL

TL;DR: 提出一种通过周期性压缩生成KV缓存的方法，使用学习到的专用令牌来压缩并淘汰缓存条目，从而解决长上下文推理中Transformer键值缓存线性增长带来的内存和计算成本问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长上下文推理中的可扩展性受到Transformer键值缓存线性增长的严重限制，这会带来显著的内存和计算成本。随着模型生成推理令牌，过去生成的令牌信息价值会逐渐减少，这为压缩创造了机会。

Method: 提出周期性压缩生成KV缓存的方法，使用学习到的专用令牌来压缩缓存条目。通过改进的联合蒸馏和强化学习框架训练模型执行这种压缩，利用强化学习输出进行蒸馏，最小化传统强化学习过程的开销。

Result: 经验证明，该方法在内存-准确率帕累托边界上优于没有缓存压缩的模型和无训练的压缩技术。

Conclusion: 该方法有效解决了长上下文推理中KV缓存线性增长的问题，通过压缩缓存实现了更好的内存效率和准确率平衡。

Abstract: The scalability of large language models for long-context reasoning is
severely constrained by the linear growth of their Transformer key-value cache,
which incurs significant memory and computational costs. We posit that as a
model generates reasoning tokens, the informational value of past generated
tokens diminishes, creating an opportunity for compression. In this work, we
propose to periodically compress the generation KV cache with a learned,
special-purpose token and evict compressed entries. We train the model to
perform this compression via a modified joint distillation and reinforcement
learning (RL) framework. Our training method minimizes overhead over the
conventional RL process, as it leverages RL outputs for distillation.
Empirically, our method achieves a superior memory-accuracy Pareto frontier
compared to both the model without cache compression and training-free
compression techniques.

</details>


### [77] [BRIEF-Pro: Universal Context Compression with Short-to-Long Synthesis for Fast and Accurate Multi-Hop Reasoning](https://arxiv.org/abs/2510.13799)
*Jia-Chen Gu,Junyi Zhang,Di Wu,Yuankai Li,Kai-Wei Chang,Nanyun Peng*

Main category: cs.CL

TL;DR: BRIEF-Pro是一个轻量级压缩器，可将检索到的文档压缩为简洁摘要，用于增强检索增强生成(RAG)系统，特别是在处理复杂多跳问题时提高性能并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 随着RAG处理复杂任务，扩展的上下文虽然提供更丰富信息，但导致更高延迟和模型认知负担增加，尤其是对于复杂的多跳问题。

Method: BRIEF-Pro使用相对较短的上下文（少于1k词）作为种子数据训练，能够对超过10k词的扩展上下文进行抽象压缩，并允许用户控制摘要长度。

Result: 在四个开放域多跳问答数据集上的实验表明，BRIEF-Pro生成更简洁相关的摘要，提升小型、大型和专有语言模型的性能。使用70B阅读器模型时，32倍压缩比LongLLMLingua的9倍压缩性能提升4.67%，计算开销仅需23%。

Conclusion: BRIEF-Pro是一种通用轻量级压缩器，能有效缓解RAG系统中长上下文带来的延迟和认知负担问题，显著提升多跳问答性能。

Abstract: As retrieval-augmented generation (RAG) tackles complex tasks, increasingly
expanded contexts offer richer information, but at the cost of higher latency
and increased cognitive load on the model. To mitigate this bottleneck,
especially for intricate multi-hop questions, we introduce BRIEF-Pro. It is a
universal, lightweight compressor that distills relevant evidence for a given
query from retrieved documents into a concise summary for seamless integration
into in-context RAG. Using seed data consisting of relatively short contexts
(fewer than 1k words), BRIEF-Pro is trained to perform abstractive compression
of extended contexts exceeding 10k words across a wide range of scenarios.
Furthermore, BRIEF-Pro offers flexible user control over summary length by
allowing users to specify the desired number of sentences. Experiments on four
open-domain multi-hop question-answering datasets show that BRIEF-Pro generates
more concise and relevant summaries, enhancing performance across small, large,
and proprietary language models. With the 70B reader model, 32x compression by
BRIEF-Pro improves QA performance by 4.67% on average over LongLLMLingua's 9x,
while requiring only 23% of its computational overhead.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [78] [Energy-Guided Diffusion Sampling for Long-Term User Behavior Prediction in Reinforcement Learning-based Recommendation](https://arxiv.org/abs/2510.12815)
*Xiaocong Chen,Siyu Wang,Lina Yao*

Main category: cs.IR

TL;DR: 提出DAC4Rec框架，将扩散过程与强化学习结合，通过扩散模型的去噪能力增强离线RL算法的鲁棒性，并采用Q值引导的策略优化来处理次优轨迹，在六个真实世界离线数据集和在线模拟环境中验证了其优化长期用户偏好的能力。


<details>
  <summary>Details</summary>
Motivation: 基于强化学习的推荐系统在离线设置中面临数据效率低和依赖预收集轨迹的问题，现有离线强化学习方法难以处理噪声数据且无法有效捕捉长期用户偏好，导致推荐策略次优。

Method: 提出DAC4Rec框架，集成扩散过程与强化学习，利用扩散模型的去噪能力增强鲁棒性，采用Q值引导的策略优化处理次优轨迹，并引入基于能量的采样策略减少推荐生成过程中的随机性。

Result: 在六个真实世界离线数据集和在线模拟环境中的广泛实验验证了DAC4Rec优化长期用户偏好的有效性，且该扩散策略可无缝集成到其他常用RL算法中。

Conclusion: DAC4Rec通过扩散增强的actor-critic方法有效解决了离线RL4RS中的挑战，展示了在优化长期用户偏好方面的优越性能，并具有广泛的适用性。

Abstract: Reinforcement learning-based recommender systems (RL4RS) have gained
attention for their ability to adapt to dynamic user preferences. However,
these systems face challenges, particularly in offline settings, where data
inefficiency and reliance on pre-collected trajectories limit their broader
applicability. While offline reinforcement learning methods leverage extensive
datasets to address these issues, they often struggle with noisy data and fail
to capture long-term user preferences, resulting in suboptimal recommendation
policies. To overcome these limitations, we propose Diffusion-enhanced
Actor-Critic for Offline RL4RS (DAC4Rec), a novel framework that integrates
diffusion processes with reinforcement learning to model complex user
preferences more effectively. DAC4Rec leverages the denoising capabilities of
diffusion models to enhance the robustness of offline RL algorithms and
incorporates a Q-value-guided policy optimization strategy to better handle
suboptimal trajectories. Additionally, we introduce an energy-based sampling
strategy to reduce randomness during recommendation generation, ensuring more
targeted and reliable outcomes. We validate the effectiveness of DAC4Rec
through extensive experiments on six real-world offline datasets and in an
online simulation environment, demonstrating its ability to optimize long-term
user preferences. Furthermore, we show that the proposed diffusion policy can
be seamlessly integrated into other commonly used RL algorithms in RL4RS,
highlighting its versatility and wide applicability.

</details>


### [79] [Maximum In-Support Return Modeling for Dynamic Recommendation with Language Model Prior](https://arxiv.org/abs/2510.12816)
*Xiaocong Chen,Siyu Wang,Lina Yao*

Main category: cs.IR

TL;DR: MDT4Rec是一个基于决策变换器的离线强化学习推荐系统框架，通过将轨迹拼接从训练阶段转移到动作推理阶段，并使用预训练LLM初始化、MLP替换线性嵌入层和LoRA微调来解决次优历史学习和复杂用户-物品交互表示问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的强化学习推荐系统面临用户反馈数据次优或稀疏的挑战，需要解决从次优历史中学习和表示复杂用户-物品交互的问题。

Method: 1) 将轨迹拼接从训练阶段转移到动作推理阶段，允许系统在必要时缩短历史上下文以忽略负面经验；2) 使用预训练LLM初始化决策变换器进行知识迁移；3) 用多层感知机替换线性嵌入层以获得更灵活的表示；4) 采用低秩适应高效微调少量参数。

Result: 在五个公共数据集和在线模拟环境中的评估表明，MDT4Rec优于现有方法。

Conclusion: MDT4Rec通过创新的轨迹拼接策略和高效的模型架构设计，有效解决了强化学习推荐系统在现实场景中面临的关键挑战，取得了优于现有方法的性能。

Abstract: Reinforcement Learning-based recommender systems (RLRS) offer an effective
way to handle sequential recommendation tasks but often face difficulties in
real-world settings, where user feedback data can be sub-optimal or sparse. In
this paper, we introduce MDT4Rec, an offline RLRS framework that builds on the
Decision Transformer (DT) to address two major challenges: learning from
sub-optimal histories and representing complex user-item interactions. First,
MDT4Rec shifts the trajectory stitching procedure from the training phase to
action inference, allowing the system to shorten its historical context when
necessary and thereby ignore negative or unsuccessful past experiences. Second,
MDT4Rec initializes DT with a pre-trained large language model (LLM) for
knowledge transfer, replaces linear embedding layers with Multi-Layer
Perceptrons (MLPs) for more flexible representations, and employs Low-Rank
Adaptation (LoRA) to efficiently fine-tune only a small subset of parameters.
We evaluate MDT4Rec on five public datasets and in an online simulation
environment, demonstrating that it outperforms existing methods.

</details>


### [80] [Post-hoc Popularity Bias Correction in GNN-based Collaborative Filtering](https://arxiv.org/abs/2510.12959)
*Md Aminul Islam,Elena Zheleva,Ren Wang*

Main category: cs.IR

TL;DR: 提出一种后处理流行度去偏方法(PPD)，通过估计交互级别的流行度并从节点表示中移除流行度分量，在不需重新训练的情况下纠正GNN推荐系统中的流行度偏差。


<details>
  <summary>Details</summary>
Motivation: 用户历史交互数据存在长尾分布，少数热门物品占据多数交互。CF模型直接在这种不平衡数据上训练容易学习流行度偏差，降低个性化推荐质量。GNN的消息传递机制会进一步传播和放大流行度偏差。现有方法通常通过修改训练目标来解决，但未能直接对抗GNN邻域聚合过程中传播的偏差。

Method: 提出PPD方法：1）估计交互级别的流行度；2）通过流行度方向向量从预训练节点表示中移除流行度分量；3）无需重新训练，直接对预训练嵌入进行后处理。

Result: 实验结果表明，该方法在GNN-based CF的流行度偏差校正方面优于现有最先进方法。

Conclusion: PPD方法能有效减少流行度偏差，同时保留用户偏好，为GNN推荐系统提供了一种高效的后处理去偏解决方案。

Abstract: User historical interaction data is the primary signal for learning user
preferences in collaborative filtering (CF). However, the training data often
exhibits a long-tailed distribution, where only a few items have the majority
of interactions. CF models trained directly on such imbalanced data are prone
to learning popularity bias, which reduces personalization and leads to
suboptimal recommendation quality. Graph Neural Networks (GNNs), while
effective for CF due to their message passing mechanism, can further propagate
and amplify popularity bias through their aggregation process. Existing
approaches typically address popularity bias by modifying training objectives
but fail to directly counteract the bias propagated during GNN's neighborhood
aggregation. Applying weights to interactions during aggregation can help
alleviate this problem, yet it risks distorting model learning due to unstable
node representations in the early stages of training. In this paper, we propose
a Post-hoc Popularity Debiasing (PPD) method that corrects for popularity bias
in GNN-based CF and operates directly on pre-trained embeddings without
requiring retraining. By estimating interaction-level popularity and removing
popularity components from node representations via a popularity direction
vector, PPD reduces bias while preserving user preferences. Experimental
results show that our method outperforms state-of-the-art approaches for
popularity bias correction in GNN-based CF.

</details>


### [81] [Retrieval-in-the-Chain: Bootstrapping Large Language Models for Generative Retrieval](https://arxiv.org/abs/2510.13095)
*Yingchen zhang,Ruqing zhang,Jiafeng Guo,Wenjun Peng,Sen Li,Fuyu Lv*

Main category: cs.IR

TL;DR: 提出R4R框架，通过结构化推理增强生成式检索，将自由形式的思维链转化为紧凑结构，并在检索过程中迭代优化推理，无需额外模型或训练。


<details>
  <summary>Details</summary>
Motivation: 现有生成式检索方法主要关注LLMs的生成能力，而忽视了其推理能力可能带来的益处。初步研究发现自由形式思维链推理虽然有效但冗长且与文档标识空间对齐不佳。

Method: R4R框架将自由形式思维链转换为结构化推理格式，在推理过程中交替进行约束解码生成候选文档标识和基于检索结果更新推理。使用单一LLM同时作为推理生成器和检索器。

Result: 在Natural Questions、MS MARCO和真实世界物品搜索基准上的广泛实验验证了R4R的有效性。

Conclusion: R4R通过结构化推理机制显著提升了生成式检索性能，证明了显式推理对生成式检索的益处，且无需额外训练或模型。

Abstract: Generative retrieval (GR) is an emerging paradigm that leverages large
language models (LLMs) to autoregressively generate document identifiers
(docids) relevant to a given query. Prior works have focused on leveraging the
generative capabilities of LLMs to improve GR, while overlooking that their
reasoning capabilities could likewise help. This raises a key question: Can
explicit reasoning benefit GR? To investigate, we first conduct a preliminary
study where an LLM is prompted to generate free-form chain-of-thought (CoT)
reasoning before performing constrained docid decoding. Although this method
outperforms standard GR, the generated reasoning tends to be verbose and poorly
aligned with the docid space. These limitations motivate the development of a
reasoning mechanism better tailored to GR.
  Therefore, we propose Reason-for-Retrieval (R4R), a reasoning-augmented
framework for GR that converts free-form CoT reasoning into a compact,
structured format, and iteratively refines the reasoning during the retrieval
process. R4R augments an existing GR method by leveraging a reasoning-capable
LLM that has been instruction-tuned for GR. At inference time, R4R first uses
the LLM to generate an initial structured reasoning; then the same LLM
alternates between (i) constrained decoding with the chosen GR method to
produce candidate docids and (ii) updating the reasoning based on retrieval
results to improve the next round. R4R does not require additional models or
training, and instead a single LLM serves as both the reasoning generator and
the retriever. Extensive experiments on Natural Questions, MS MARCO, and a
real-world item-search benchmark validate the effectiveness of R4R.

</details>


### [82] [ReMindRAG: Low-Cost LLM-Guided Knowledge Graph Traversal for Efficient RAG](https://arxiv.org/abs/2510.13193)
*Yikuan Hu,Jifeng Zhu,Lanrui Tang,Chen Huang*

Main category: cs.IR

TL;DR: 提出REMINDRAG方法，通过LLM引导的图遍历结合记忆重放机制，在知识图谱增强检索生成系统中同时提升效果和成本效率。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱增强检索生成系统难以在系统效果和成本效率之间实现有效协同，导致性能不理想或计算成本过高。

Method: 采用LLM引导的图遍历，包括节点探索、节点利用和记忆重放，将遍历经验存储在KG边嵌入中，以无训练方式实现知识记忆。

Result: 理论和实验验证了REMINDRAG的有效性，在多个基准数据集和LLM骨干网络上均优于现有基线方法。

Conclusion: REMINDRAG通过创新的记忆重放机制，成功解决了KG-RAG系统中效果与效率的平衡问题，为知识图谱增强检索生成提供了有效解决方案。

Abstract: Knowledge graphs (KGs), with their structured representation capabilities,
offer promising avenue for enhancing Retrieval Augmented Generation (RAG)
systems, leading to the development of KG-RAG systems. Nevertheless, existing
methods often struggle to achieve effective synergy between system
effectiveness and cost efficiency, leading to neither unsatisfying performance
nor excessive LLM prompt tokens and inference time. To this end, this paper
proposes REMINDRAG, which employs an LLM-guided graph traversal featuring node
exploration, node exploitation, and, most notably, memory replay, to improve
both system effectiveness and cost efficiency. Specifically, REMINDRAG
memorizes traversal experience within KG edge embeddings, mirroring the way
LLMs "memorize" world knowledge within their parameters, but in a train-free
manner. We theoretically and experimentally confirm the effectiveness of
REMINDRAG, demonstrating its superiority over existing baselines across various
benchmark datasets and LLM backbones. Our code is available at
https://github.com/kilgrims/ReMindRAG.

</details>


### [83] [LLM-guided Hierarchical Retrieval](https://arxiv.org/abs/2510.13217)
*Nilesh Gupta,Wei-Cheng Chang,Ngot Bui,Cho-Jui Hsieh,Inderjit S. Dhillon*

Main category: cs.IR

TL;DR: LATTICE是一个分层检索框架，通过语义树结构组织语料库，使LLM能够以对数搜索复杂度推理和导航大型文档集合，在零样本设置下达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现代IR系统需要处理复杂的多面查询，但现有方法存在局限性：检索-重排范式继承嵌入检索的缺点、参数化生成方法难以更新、长上下文方法计算不可行。

Method: 两阶段方法：离线阶段通过自底向上聚合或自顶向下分裂策略构建语义层次结构；在线阶段使用搜索LLM导航树结构，通过校准潜在相关性评分解决LLM判断噪声问题。

Result: 在BRIGHT基准测试中，零样本性能达到最先进水平，Recall@100提升9%，nDCG@10提升5%；与微调SOTA方法DIVER-v2在静态语料评估子集上表现相当。

Conclusion: LATTICE框架成功解决了复杂查询推理的挑战，通过层次化检索实现了高效的大规模语料导航，在零样本设置下表现出色。

Abstract: Modern IR systems are increasingly tasked with answering complex,
multi-faceted queries that require deep reasoning rather than simple keyword or
semantic matching. While LLM-based IR has shown great promise, the prevailing
retrieve-then-rerank paradigm inherits the limitations of embedding-based
retrieval; parametric generative approaches are difficult to update with new
information; and long-context methods that place the entire corpus in context
are computationally infeasible for large document collections. To address these
challenges, we introduce LATTICE, a hierarchical retrieval framework that
enables an LLM to reason over and navigate large corpora with logarithmic
search complexity by imposing a semantic tree structure on the corpus. Our
approach consists of two stages: (1) an offline phase that organizes the corpus
into a semantic hierarchy via either a bottom-up agglomerative strategy or a
top-down divisive strategy using multi-level summaries and (2) an online
traversal phase where a search LLM navigates this tree. A central challenge in
such LLM-guided search is that the model's relevance judgments are noisy,
context-dependent, and unaware of the hierarchy, making cross-branch and
cross-level comparisons difficult. To overcome this, we propose a traversal
algorithm that estimates calibrated latent relevance scores from local LLM
outputs and aggregates them into a global path relevance metric. Our
training-free framework achieves state-of-the-art zero-shot performance on the
reasoning-intensive BRIGHT benchmark, demonstrating up to 9% improvement in
Recall@100 and 5% in nDCG@10 over the next best zero-shot baseline.
Furthermore, compared to the fine-tuned SOTA method DIVER-v2, LATTICE attains
comparable results on BRIGHT subsets that use a static corpus for evaluation.

</details>


### [84] [Beyond Static LLM Policies: Imitation-Enhanced Reinforcement Learning for Recommendation](https://arxiv.org/abs/2510.13229)
*Yi Zhang,Lili Xie,Ruihong Qiu,Jiajun Liu,Sen Wang*

Main category: cs.IR

TL;DR: 提出一种基于离线强化学习的新框架，利用LLM生成的轨迹进行模仿学习，避免直接部署LLM带来的延迟和幻觉问题，在基准数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LLMs在推荐系统中具有巨大潜力，但直接部署面临延迟、幻觉和偏见等挑战，需要找到更高效的集成方式。

Method: 使用逆强化学习从LLM演示中提取奖励模型，通过离线强化学习框架训练推荐策略，无需微调LLM。

Result: 在两个基准数据集上的实验验证了方法的有效性，性能优于最先进的基于强化学习和上下文学习的方法。

Conclusion: 该框架成功地将LLM的语义理解能力转移到高效的推荐策略中，解决了直接使用LLM的局限性。

Abstract: Recommender systems (RecSys) have become critical tools for enhancing user
engagement by delivering personalized content across diverse digital platforms.
Recent advancements in large language models (LLMs) demonstrate significant
potential for improving RecSys, primarily due to their exceptional
generalization capabilities and sophisticated contextual understanding, which
facilitate the generation of flexible and interpretable recommendations.
However, the direct deployment of LLMs as primary recommendation policies
presents notable challenges, including persistent latency issues stemming from
frequent API calls and inherent model limitations such as hallucinations and
biases. To address these issues, this paper proposes a novel offline
reinforcement learning (RL) framework that leverages imitation learning from
LLM-generated trajectories. Specifically, inverse reinforcement learning is
employed to extract robust reward models from LLM demonstrations. This approach
negates the need for LLM fine-tuning, thereby substantially reducing
computational overhead. Simultaneously, the RL policy is guided by the
cumulative rewards derived from these demonstrations, effectively transferring
the semantic insights captured by the LLM. Comprehensive experiments conducted
on two benchmark datasets validate the effectiveness of the proposed method,
demonstrating superior performance when compared against state-of-the-art
RL-based and in-context learning baselines. The code can be found at
https://github.com/ArronDZhang/IL-Rec.

</details>


### [85] [Improving Visual Recommendation on E-commerce Platforms Using Vision-Language Models](https://arxiv.org/abs/2510.13359)
*Yuki Yada,Sho Akiyama,Ryo Watanabe,Yuta Ueno,Yusuke Shido,Andre Rusli*

Main category: cs.IR

TL;DR: 本研究将视觉语言模型（SigLIP）应用于Mercari电商平台的商品推荐，通过微调模型并使用商品图像-标题对生成嵌入，在离线和在线测试中均取得显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 在拥有数千万月活跃用户的大规模电商平台上，推荐视觉相似商品对于帮助用户高效发现符合偏好的商品至关重要。

Method: 使用Mercari三个月内收集的100万商品图像-标题对微调基于sigmoid对比损失的视觉语言模型SigLIP，开发用于推荐系统的图像编码器生成商品嵌入。

Result: 离线分析中nDCG@5提升9.1%；在线A/B测试中点击率提升50%，转化率提升14%。

Conclusion: 基于视觉语言模型的编码器在电商商品推荐中具有显著效果，为开发基于视觉相似度的推荐系统提供了实用见解。

Abstract: On large-scale e-commerce platforms with tens of millions of active monthly
users, recommending visually similar products is essential for enabling users
to efficiently discover items that align with their preferences. This study
presents the application of a vision-language model (VLM) -- which has
demonstrated strong performance in image recognition and image-text retrieval
tasks -- to product recommendations on Mercari, a major consumer-to-consumer
marketplace used by more than 20 million monthly users in Japan. Specifically,
we fine-tuned SigLIP, a VLM employing a sigmoid-based contrastive loss, using
one million product image-title pairs from Mercari collected over a three-month
period, and developed an image encoder for generating item embeddings used in
the recommendation system. Our evaluation comprised an offline analysis of
historical interaction logs and an online A/B test in a production environment.
In offline analysis, the model achieved a 9.1% improvement in nDCG@5 compared
with the baseline. In the online A/B test, the click-through rate improved by
50% whereas the conversion rate improved by 14% compared with the existing
model. These results demonstrate the effectiveness of VLM-based encoders for
e-commerce product recommendations and provide practical insights into the
development of visual similarity-based recommendation systems.

</details>


### [86] [MADREC: A Multi-Aspect Driven LLM Agent for Explainable and Adaptive Recommendation](https://arxiv.org/abs/2510.13371)
*Jiin Park,Misuk Kim*

Main category: cs.IR

TL;DR: MADRec是一个基于大语言模型的多方面驱动推荐代理，通过无监督提取评论中的多方面信息构建用户和物品画像，支持直接推荐、序列推荐和解释生成，在精度和可解释性方面优于传统和LLM基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有将大语言模型集成到推荐系统中的方法大多局限于简单的文本生成或基于静态提示的推理，无法捕捉用户偏好和真实世界交互的复杂性。

Method: 通过基于方面类别的总结生成结构化画像，应用重排序构建高密度输入，当输出中缺少真实物品时使用自反馈机制动态调整推理标准。

Result: 在多个领域的实验中，MADRec在精度和可解释性方面均优于传统和LLM基线方法，人工评估进一步证实了生成解释的说服力。

Conclusion: MADRec证明了自主LLM推荐代理在捕捉复杂用户偏好和生成有说服力解释方面的有效性，为LLM在推荐系统中的应用提供了新思路。

Abstract: Recent attempts to integrate large language models (LLMs) into recommender
systems have gained momentum, but most remain limited to simple text generation
or static prompt-based inference, failing to capture the complexity of user
preferences and real-world interactions. This study proposes the Multi-Aspect
Driven LLM Agent MADRec, an autonomous LLM-based recommender that constructs
user and item profiles by unsupervised extraction of multi-aspect information
from reviews and performs direct recommendation, sequential recommendation, and
explanation generation. MADRec generates structured profiles via
aspect-category-based summarization and applies Re-Ranking to construct
high-density inputs. When the ground-truth item is missing from the output, the
Self-Feedback mechanism dynamically adjusts the inference criteria. Experiments
across multiple domains show that MADRec outperforms traditional and LLM-based
baselines in both precision and explainability, with human evaluation further
confirming the persuasiveness of the generated explanations.

</details>


### [87] [RAG Meets Temporal Graphs: Time-Sensitive Modeling and Retrieval for Evolving Knowledge](https://arxiv.org/abs/2510.13590)
*Jiale Han,Austin Cheung,Yubai Wei,Zheng Yu,Xusheng Wang,Bing Zhu,Yi Yang*

Main category: cs.IR

TL;DR: 提出Temporal GraphRAG (TG-RAG)方法，通过构建双层时间图（时间知识图和时间层次图）来解决RAG系统的时间感知问题，支持增量更新并显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统忽视了知识的时间敏感性，难以区分不同时间点的相同事实，且评估主要基于静态语料库，缺乏对更新成本和检索稳定性的考量。

Method: 将外部语料建模为双层时间图：包含时间戳关系的时间知识图和时间层次图。为每个时间节点生成多粒度时间摘要，支持增量更新，在推理时动态检索查询时间语义范围内的子图。

Result: TG-RAG在时间敏感问答任务上显著优于现有基线方法，证明了该方法在处理时间知识和增量更新方面的有效性。

Conclusion: TG-RAG通过显式建模时间信息，有效解决了RAG系统的时间感知问题，为处理动态演化的知识提供了可行方案。

Abstract: Knowledge is inherently time-sensitive and continuously evolves over time.
Although current Retrieval-Augmented Generation (RAG) systems enrich LLMs with
external knowledge, they largely ignore this temporal nature. This raises two
challenges for RAG. First, current RAG methods lack effective time-aware
representations. Same facts of different time are difficult to distinguish with
vector embeddings or conventional knowledge graphs. Second, most RAG
evaluations assume a static corpus, leaving a blind spot regarding update costs
and retrieval stability as knowledge evolves. To make RAG time-aware, we
propose Temporal GraphRAG (TG-RAG), which models external corpora as a bi-level
temporal graph consisting of a temporal knowledge graph with timestamped
relations and a hierarchical time graph. Multi-granularity temporal summaries
are generated for each time node to capture both key events and broader trends
at that time. The design supports incremental updates by extracting new
temporal facts from the incoming corpus and merging them into the existing
graph. The temporal graph explicitly represents identical facts at different
times as distinct edges to avoid ambiguity, and the time hierarchy graph allows
only generating reports for new leaf time nodes and their ancestors, ensuring
effective and efficient updates. During inference, TG-RAG dynamically retrieves
a subgraph within the temporal and semantic scope of the query, enabling
precise evidence gathering. Moreover, we introduce ECT-QA, a time-sensitive
question-answering dataset featuring both specific and abstract queries, along
with a comprehensive evaluation protocol designed to assess incremental update
capabilities of RAG systems. Extensive experiments show that TG-RAG
significantly outperforms existing baselines, demonstrating the effectiveness
of our method in handling temporal knowledge and incremental updates.

</details>


### [88] [HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based Sequential Recommendation](https://arxiv.org/abs/2510.13738)
*Jingyi Zhou,Cheng Chen,Kai Zuo,Manjie Xu,Zhendong Fu,Yibo Chen,Xu Tang,Yao Hu*

Main category: cs.IR

TL;DR: HyMiRec是一个混合多兴趣序列推荐框架，通过轻量级推荐器提取长序列的粗粒度兴趣嵌入，结合LLM捕获细粒度兴趣嵌入，使用残差码本压缩用户历史嵌入，并通过解耦多兴趣学习模块建模用户多样化偏好。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推荐方法存在两个关键限制：1）由于推理延迟和特征获取带宽限制，只能截断用户行为序列，丢失长期偏好信号；2）大多依赖单嵌入的下一项预测，忽略了用户兴趣的多面性，限制了推荐多样性。

Method: 提出HyMiRec框架：1）轻量级推荐器提取长序列的粗粒度兴趣嵌入；2）LLM推荐器捕获细粒度兴趣嵌入；3）基于余弦相似度的残差码本实现高效压缩和重用用户历史嵌入；4）解耦多兴趣学习模块使用多个兴趣查询自适应学习分离的多个兴趣信号。

Result: 在基准数据集和工业数据集上的广泛实验表明，HyMiRec优于现有最先进方法。在线A/B测试证实了其在真实推荐系统中的持续改进效果。

Conclusion: HyMiRec通过混合架构有效解决了LLM推荐中的长期兴趣建模和多样性问题，在理论和实践中都表现出优越性能。

Abstract: Large language models (LLMs) have recently demonstrated strong potential for
sequential recommendation. However, current LLM-based approaches face critical
limitations in modeling users' long-term and diverse interests. First, due to
inference latency and feature fetching bandwidth constraints, existing methods
typically truncate user behavior sequences to include only the most recent
interactions, resulting in the loss of valuable long-range preference signals.
Second, most current methods rely on next-item prediction with a single
predicted embedding, overlooking the multifaceted nature of user interests and
limiting recommendation diversity. To address these challenges, we propose
HyMiRec, a hybrid multi-interest sequential recommendation framework, which
leverages a lightweight recommender to extracts coarse interest embeddings from
long user sequences and an LLM-based recommender to captures refined interest
embeddings. To alleviate the overhead of fetching features, we introduce a
residual codebook based on cosine similarity, enabling efficient compression
and reuse of user history embeddings. To model the diverse preferences of
users, we design a disentangled multi-interest learning module, which leverages
multiple interest queries to learn disentangles multiple interest signals
adaptively, allowing the model to capture different facets of user intent.
Extensive experiments are conducted on both benchmark datasets and a collected
industrial dataset, demonstrating our effectiveness over existing
state-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec
brings consistent improvements in real-world recommendation systems.

</details>
