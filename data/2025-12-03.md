<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 43]
- [cs.IR](#cs.IR) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Deep Research: A Systematic Survey](https://arxiv.org/abs/2512.02038)
*Zhengliang Shi,Yiqun Chen,Haitao Li,Weiwei Sun,Shiyu Ni,Yougang Lyu,Run-Ze Fan,Bowen Jin,Yixuan Weng,Minjun Zhu,Qiujie Xie,Xinyu Guo,Qu Yang,Jiayi Wu,Jujia Zhao,Xiaqiang Tang,Xinbei Ma,Cunxiang Wang,Jiaxin Mao,Qingyao Ai,Jen-Tse Huang,Wenxuan Wang,Yue Zhang,Yiming Yang,Zhaopeng Tu,Zhaochun Ren*

Main category: cs.CL

TL;DR: 这是一篇关于深度研究系统的综述论文，系统梳理了将大语言模型与外部工具结合以完成复杂开放任务的研究范式。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽然强大，但在需要批判性思维、多源信息和可验证输出的开放任务上仍有局限，需要超越单次提示或标准检索增强生成的方法。深度研究旨在结合LLM的推理能力与外部工具，使其成为能够完成复杂开放任务的研究代理。

Method: 论文采用系统性综述方法，提出了深度研究的四阶段路线图，介绍了四个关键组件（查询规划、信息获取、记忆管理、答案生成）及其子分类，总结了优化技术（提示工程、监督微调、代理强化学习），并整合了评估标准和开放挑战。

Result: 论文提供了深度研究系统的全面系统概述，包括清晰的路线图、基础组件、实际实现技术、重要挑战和未来方向，旨在指导和促进该领域的未来发展。

Conclusion: 深度研究是一个快速发展的领域，将LLM与外部工具结合能够显著提升解决复杂开放任务的能力。该综述为研究者提供了系统框架，并承诺将持续更新以反映最新进展。

Abstract: Large language models (LLMs) have rapidly evolved from text generators into powerful problem solvers. Yet, many open tasks demand critical thinking, multi-source, and verifiable outputs, which are beyond single-shot prompting or standard retrieval-augmented generation. Recently, numerous studies have explored Deep Research (DR), which aims to combine the reasoning capabilities of LLMs with external tools, such as search engines, thereby empowering LLMs to act as research agents capable of completing complex, open-ended tasks. This survey presents a comprehensive and systematic overview of deep research systems, including a clear roadmap, foundational components, practical implementation techniques, important challenges, and future directions. Specifically, our main contributions are as follows: (i) we formalize a three-stage roadmap and distinguish deep research from related paradigms; (ii) we introduce four key components: query planning, information acquisition, memory management, and answer generation, each paired with fine-grained sub-taxonomies; (iii) we summarize optimization techniques, including prompting, supervised fine-tuning, and agentic reinforcement learning; and (iv) we consolidate evaluation criteria and open challenges, aiming to guide and facilitate future development. As the field of deep research continues to evolve rapidly, we are committed to continuously updating this survey to reflect the latest progress in this area.

</details>


### [2] [Human-Level and Beyond: Benchmarking Large Language Models Against Clinical Pharmacists in Prescription Review](https://arxiv.org/abs/2512.02024)
*Yan Yang,Mouxiao Bian,Peiling Li,Bingjian Wen,Ruiyao Chen,Kangkun Mao,Xiaojun Ye,Tianbin Li,Pengcheng Chen,Bing Han,Jie Xu,Kaifeng Qiu,Junyan Wu*

Main category: cs.CL

TL;DR: RxBench是一个用于处方审核的全面基准测试，包含多种题型，评估了18个先进LLM，发现领先模型在某些任务上可匹敌或超越执业药师，并通过微调使中等模型达到顶尖水平。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在临床决策支持中的快速应用，特别是在处方审核领域，需要系统化、细粒度的评估框架来评估模型能力，揭示其优势和局限。

Method: 开发了RxBench基准测试，涵盖常见处方审核类别，整合了14种常见处方错误类型，包含1150道单选题、230道多选题和879道简答题，所有题目均由经验丰富的临床药师审核。评估了18个最先进的LLM，并与执业药师进行对比。

Result: Gemini-2.5-pro-preview-05-06、Grok-4-0709和DeepSeek-R1-0528表现最佳，形成第一梯队。领先的LLM在某些任务上可匹敌或超越人类药师。通过对中等模型进行针对性微调，可获得与顶尖通用LLM相当的简答题性能。

Conclusion: RxBench建立了一个标准化、错误类型导向的评估框架，不仅揭示了前沿LLM在处方审核中的能力和局限，还为构建更可靠、专业化的临床工具提供了基础资源。

Abstract: The rapid advancement of large language models (LLMs) has accelerated their integration into clinical decision support, particularly in prescription review. To enable systematic and fine-grained evaluation, we developed RxBench, a comprehensive benchmark that covers common prescription review categories and consolidates 14 frequent types of prescription errors drawn from authoritative pharmacy references. RxBench consists of 1,150 single-choice, 230 multiple-choice, and 879 short-answer items, all reviewed by experienced clinical pharmacists. We benchmarked 18 state-of-the-art LLMs and identified clear stratification of performance across tasks. Notably, Gemini-2.5-pro-preview-05-06, Grok-4-0709, and DeepSeek-R1-0528 consistently formed the first tier, outperforming other models in both accuracy and robustness. Comparisons with licensed pharmacists indicated that leading LLMs can match or exceed human performance in certain tasks. Furthermore, building on insights from our benchmark evaluation, we performed targeted fine-tuning on a mid-tier model, resulting in a specialized model that rivals leading general-purpose LLMs in performance on short-answer question tasks. The main contribution of RxBench lies in establishing a standardized, error-type-oriented framework that not only reveals the capabilities and limitations of frontier LLMs in prescription review but also provides a foundational resource for building more reliable and specialized clinical tools.

</details>


### [3] [ADORE: Autonomous Domain-Oriented Relevance Engine for E-commerce](https://arxiv.org/abs/2512.02555)
*Zheng Fang,Donghao Xie,Ming Pang,Chunyuan Yuan,Xue Jiang,Changping Peng,Zhangang Lin,Zheng Luo*

Main category: cs.CL

TL;DR: ADORE是一个自持续框架，通过结合规则感知相关性判别、错误类型感知数据合成和关键属性增强知识蒸馏，解决电商搜索中语义鸿沟和硬样本稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 电商搜索中的相关性建模面临两个主要挑战：基于术语匹配的方法（如BM25）存在语义鸿沟，而神经模型则依赖于领域特定硬样本的稀缺性。

Method: ADORE框架包含三个创新模块：1) 规则感知相关性判别模块，使用思维链LLM生成意图对齐的训练数据，并通过Kahneman-Tversky优化与用户行为对齐；2) 错误类型感知数据合成模块，自动生成对抗样本来增强鲁棒性；3) 关键属性增强知识蒸馏模块，将领域特定属性层次注入可部署的学生模型中。

Result: 大规模实验和在线A/B测试验证了ADORE的有效性。该框架为工业应用中的资源高效、认知对齐的相关性建模建立了新范式。

Conclusion: ADORE通过自动化标注、对抗生成和蒸馏过程，克服了数据稀缺问题，同时增强了推理能力，为电商搜索相关性建模提供了创新解决方案。

Abstract: Relevance modeling in e-commerce search remains challenged by semantic gaps in term-matching methods (e.g., BM25) and neural models' reliance on the scarcity of domain-specific hard samples. We propose ADORE, a self-sustaining framework that synergizes three innovations: (1) A Rule-aware Relevance Discrimination module, where a Chain-of-Thought LLM generates intent-aligned training data, refined via Kahneman-Tversky Optimization (KTO) to align with user behavior; (2) An Error-type-aware Data Synthesis module that auto-generates adversarial examples to harden robustness; and (3) A Key-attribute-enhanced Knowledge Distillation module that injects domain-specific attribute hierarchies into a deployable student model. ADORE automates annotation, adversarial generation, and distillation, overcoming data scarcity while enhancing reasoning. Large-scale experiments and online A/B testing verify the effectiveness of ADORE. The framework establishes a new paradigm for resource-efficient, cognitively aligned relevance modeling in industrial applications.

</details>


### [4] [Towards Unification of Hallucination Detection and Fact Verification for Large Language Models](https://arxiv.org/abs/2512.02772)
*Weihang Su,Jianming Long,Changyue Wang,Shiyu Lin,Jingyan Xu,Ziyi Ye,Qingyao Ai,Yiqun Liu*

Main category: cs.CL

TL;DR: UniFact是一个统一评估框架，首次实现了事实核查与幻觉检测的直接比较，发现两者互补且混合方法性能最优。


<details>
  <summary>Details</summary>
Motivation: 大语言模型经常产生看似流畅但事实错误的幻觉内容，这影响了实际应用中的可信度。当前存在模型中心的幻觉检测和文本中心的事实核查两种研究范式，但它们各自独立发展，形成了研究分裂，阻碍了整体进展。

Method: 提出UniFact统一评估框架，通过动态生成模型输出和相应的事实性标签，实现事实核查和幻觉检测在实例层面的直接比较。在多个LLM家族和检测方法上进行了大规模实验。

Result: 三个关键发现：1) 没有哪种范式普遍更优；2) 幻觉检测和事实核查捕捉了事实错误的不同方面，具有互补性；3) 整合两种方法的混合方法始终达到最先进的性能。还首次深入分析了两种范式为何分化的原因。

Conclusion: 需要一个新的、整合的研究议程来统一大语言模型中的幻觉检测和事实核查。实验结果表明两者互补，混合方法效果最好，呼吁结束研究分裂，促进统一发展。

Abstract: Large Language Models (LLMs) frequently exhibit hallucinations, generating content that appears fluent and coherent but is factually incorrect. Such errors undermine trust and hinder their adoption in real-world applications. To address this challenge, two distinct research paradigms have emerged: model-centric Hallucination Detection (HD) and text-centric Fact Verification (FV). Despite sharing the same goal, these paradigms have evolved in isolation, using distinct assumptions, datasets, and evaluation protocols. This separation has created a research schism that hinders their collective progress. In this work, we take a decisive step toward bridging this divide. We introduce UniFact, a unified evaluation framework that enables direct, instance-level comparison between FV and HD by dynamically generating model outputs and corresponding factuality labels. Through large-scale experiments across multiple LLM families and detection methods, we reveal three key findings: (1) No paradigm is universally superior; (2) HD and FV capture complementary facets of factual errors; and (3) hybrid approaches that integrate both methods consistently achieve state-of-the-art performance. Beyond benchmarking, we provide the first in-depth analysis of why FV and HD diverged, as well as empirical evidence supporting the need for their unification. The comprehensive experimental results call for a new, integrated research agenda toward unifying Hallucination Detection and Fact Verification in LLMs.
  We have open-sourced all the code, data, and baseline implementation at: https://github.com/oneal2000/UniFact/

</details>


### [5] [Mirror, Mirror on the Wall -- Which is the Best Model of Them All?](https://arxiv.org/abs/2512.02043)
*Dina Sayed,Heiko Schuldt*

Main category: cs.CL

TL;DR: 本文分析了LLM模型选择的定量维度，以医疗领域为案例研究，提出了一种系统化的模型选择方法论(MSM)。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型快速涌现，为特定用例选择最合适模型的过程变得日益复杂，需要同时考虑定性维度（模型适用性）和定量维度（性能表现）。

Method: 通过探索当前排行榜和基准测试来分析定量维度，以医疗领域为案例研究，展示定量评估的演变、现状和实际意义，并提出模型选择方法论(MSM)。

Result: 分析了LLM排行榜和基准测试的现状，展示了医疗领域模型评估的演变和实际意义，提出了一个系统化的模型选择框架。

Conclusion: 模型选择需要综合考虑定性和定量维度，提出的MSM方法论为导航、优先级排序和选择最适合特定用例的模型提供了系统化指导。

Abstract: Large Language Models (LLMs) have become one of the most transformative tools across many applications, as they have significantly boosted productivity and achieved impressive results in various domains such as finance, healthcare, education, telecommunications, and law, among others. Typically, state-of-the-art (SOTA) foundation models are developed by large corporations based on large data collections and substantial computational and financial resources required to pretrain such models from scratch. These foundation models then serve as the basis for further development and domain adaptation for specific use cases or tasks. However, given the dynamic and fast-paced nature of launching new foundation models, the process of selecting the most suitable model for a particular use case, application, or domain becomes increasingly complex. We argue that there are two main dimensions that need to be taken into consideration when selecting a model for further training: a qualitative dimension (which model is best suited for a task based on information, for instance, taken from model cards) and a quantitative dimension (which is the best performing model). The quantitative performance of models is assessed through leaderboards, which rank models based on standardized benchmarks and provide a consistent framework for comparing different LLMs. In this work, we address the analysis of the quantitative dimension by exploring the current leaderboards and benchmarks. To illustrate this analysis, we focus on the medical domain as a case study, demonstrating the evolution, current landscape, and practical significance of this quantitative evaluation dimension. Finally, we propose a Model Selection Methodology (MSM), a systematic approach designed to guide the navigation, prioritization, and selection of the model that best aligns with a given use case.

</details>


### [6] [Beyond Confidence: Adaptive and Coherent Decoding for Diffusion Language Models](https://arxiv.org/abs/2512.02044)
*Kecheng Chen,Ziru Liu,Xijia Tao,Hui Liu,Xinyu Fu,Suiyun Zhang,Dandan Tu,Lingpeng Kong,Rui Liu,Haoliang Li*

Main category: cs.CL

TL;DR: CCD是一种新的扩散语言模型推理框架，通过轨迹修正机制和自适应采样策略，在提升生成质量的同时加速推理过程。


<details>
  <summary>Details</summary>
Motivation: 现有扩散语言模型推理方法通常依赖局部即时指标（如置信度或熵），缺乏更可靠的全局视角，导致采样轨迹不一致和生成质量欠佳。

Method: 提出连贯上下文解码（CCD）框架，包含两个核心创新：1）轨迹修正机制，利用历史上下文增强序列连贯性，早期拒绝次优路径；2）自适应采样策略，根据一致性度量动态调整每个步骤的解码预算。

Result: 在Dream和LLaDA等多个基准测试中，方法在推理速度和性能上同时提升，实现最高3.48倍加速和3.91%性能改进。

Conclusion: CCD通过理论驱动的轨迹修正和自适应采样，有效解决了扩散语言模型推理中的连贯性和效率问题，为高质量高效生成提供了新思路。

Abstract: Diffusion Language Models (DLMs) have recently achieved significant success due to their any-order generation capabilities. However, existing inference methods typically rely on local, immediate-step metrics such as confidence or entropy which inherently lack a more reliable perspective. This limitation frequently leads to inconsistent sampling trajectories and suboptimal generation quality. To address this, we propose Coherent Contextual Decoding (CCD), a novel inference framework built upon two core innovations. First, CCD employs a trajectory rectification mechanism that leverages historical context to enhance sequence coherence, enabling the early rejection of suboptimal paths. We demonstrate that this mechanism is theoretically equivalent to modeling the consistency of historical steps via the conditional mutual information between context and token predictions. Building on this theoretical insight, we further address the inefficiency of conventional uniform decoding budgets. Instead of rigid allocations based on diffusion steps, we introduce an adaptive sampling strategy that dynamically adjusts the unmasking budget for each step according to our consistency metric. Consequently, our method significantly improves the quality of generation trajectories while accelerating the sampling process. Empirically, our method achieves a simultaneous enhancement in both inference speed and performance across diverse benchmarks on Dream and LLaDA, delivering up to 3.48x speedup alongside 3.91% performance improvement.

</details>


### [7] [Reversing Large Language Models for Efficient Training and Fine-Tuning](https://arxiv.org/abs/2512.02056)
*Eshed Gal,Moshe Eliasof,Javier Turek,Uri Ascher,Eran Treister,Eldad Haber*

Main category: cs.CL

TL;DR: 提出基于对称和辛微分方程的可逆LLM架构，通过时间可逆动态在反向传播中检索隐藏状态，无需存储激活值，大幅降低内存消耗，并可将现有非可逆LLM转换为可逆架构。


<details>
  <summary>Details</summary>
Motivation: LLM训练成本高昂且耗时，通常需要对预训练模型进行微调。标准架构需要存储所有中间激活值，导致内存消耗巨大，限制了批处理大小和吞吐量。

Method: 1. 引入基于对称和辛微分方程的可逆架构，利用时间可逆动态在反向传播中检索隐藏状态，无需存储激活值；2. 提出将现有非可逆LLM转换为可逆架构的高效微调方法。

Result: 在多个数据集和基准测试中，多个LLM上显示出相当或改进的性能，同时大幅降低内存消耗，允许在相同内存下处理更大批处理大小，提高吞吐量。

Conclusion: 提出的可逆架构为降低LLM训练和微调的内存和计算成本提供了可扩展且高效的路径，使现有预训练模型能够被有效利用。

Abstract: Large Language Models (LLMs) are known for their expensive and time-consuming training. Thus, oftentimes, LLMs are fine-tuned to address a specific task, given the pretrained weights of a pre-trained LLM considered a foundation model. In this work, we introduce memory-efficient, reversible architectures for LLMs, inspired by symmetric and symplectic differential equations, and investigate their theoretical properties. Different from standard, baseline architectures that store all intermediate activations, the proposed models use time-reversible dynamics to retrieve hidden states during backpropagation, relieving the need to store activations. This property allows for a drastic reduction in memory consumption, allowing for the processing of larger batch sizes for the same available memory, thereby offering improved throughput. In addition, we propose an efficient method for converting existing, non-reversible LLMs into reversible architectures through fine-tuning, rendering our approach practical for exploiting existing pre-trained models. Our results show comparable or improved performance on several datasets and benchmarks, on several LLMs, building a scalable and efficient path towards reducing the memory and computational costs associated with both training from scratch and fine-tuning of LLMs.

</details>


### [8] [Dialect Identification Using Resource-Efficient Fine-Tuning Approaches](https://arxiv.org/abs/2512.02074)
*Zirui Lin,Haris Gulzar,Monnika Roslianna Busto,Akiko Masaki,Takeharu Eda,Kazuhiro Nakadai*

Main category: cs.CL

TL;DR: 该研究探索将内存高效微调（MEFT）方法应用于预训练语音模型，显著减少GPU内存使用并加速训练，同时保持方言识别准确性。


<details>
  <summary>Details</summary>
Motivation: 方言识别（DI）有助于改善下游语音任务，但微调语音模型计算成本和内存需求高。现有的参数高效微调（PEFT）方法参数效率高但内存效率和训练速度提升有限。

Method: 探索将最初为语言处理提出的内存高效微调（MEFT）方法应用于通用预训练语音模型，全面分析不同MEFT方法的GPU内存使用和微调速度。以Whisper模型为案例，在KeSpeech数据集的六个汉语子方言识别任务上进行微调。

Result: 将GPU内存使用减少高达73.25%，训练速度加速2.1倍，同时保持与传统微调和PEFT方法相当的准确性。

Conclusion: MEFT方法在语音模型的方言识别任务中能显著提高内存效率和训练速度，为资源受限环境下的语音模型微调提供了有效解决方案。

Abstract: Dialect Identification (DI) is a task to recognize different dialects within the same language from a speech signal. DI can help to improve the downstream speech related tasks even when speakers have a strong dialect. However, fine-tuning a speech model for tasks like DI is expensive in terms of computation cost and memory requirement. Recent studies have explored fine-tuning pre-trained speech models for tasks like DI using Parameter-Efficient Fine-Tuning (PEFT) methods, which offer parameter efficiency but limited improvement in memory efficiency and training speed. To address these challenges, we explore Memory-Efficient Fine-Tuning (MEFT) methods, originally proposed for language processing, and apply them to the general-purpose pre-trained speech model. We then comprehensively analyze the GPU memory usage and fine-tuning speed based on various MEFT methods. As a case study, we fine-tune the Whisper model to identify six Mandarin subdialects from the KeSpeech dataset, reducing GPU memory usage by up to 73.25% and accelerating training speed by a factor of 2.1, while maintaining accuracy comparable to vanilla fine-tuning and PEFT methods.

</details>


### [9] [Feature Selection Empowered BERT for Detection of Hate Speech with Vocabulary Augmentation](https://arxiv.org/abs/2512.02141)
*Pritish N. Desai,Tanay Kewalramani,Srimanta Mandal*

Main category: cs.CL

TL;DR: 提出一种数据高效的BERT微调策略，通过TF-IDF样本选择减少75%训练数据，同时添加领域特定词汇增强检测能力，在保持性能的同时提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的辱骂性言论持续演变，新俚语和混淆术语不断出现以规避检测系统，现有方法面临词汇覆盖不足和计算成本高的问题。

Method: 1) 使用TF-IDF样本选择机制保留最具信息量的75%训练样本；2) 扩展BERT分词器，添加辱骂性语境中常见的领域特定俚语和词汇变体。

Result: 在广泛使用的仇恨言论数据集上，该方法在减少75%训练数据的情况下仍保持竞争力性能，显著提高了计算效率。

Conclusion: 该方法为可扩展和自适应的辱骂内容审核提供了有效解决方案，通过数据高效微调和词汇增强应对不断演变的网络滥用语言。

Abstract: Abusive speech on social media poses a persistent and evolving challenge, driven by the continuous emergence of novel slang and obfuscated terms designed to circumvent detection systems. In this work, we present a data efficient strategy for fine tuning BERT on hate speech classification by significantly reducing training set size without compromising performance. Our approach employs a TF IDF-based sample selection mechanism to retain only the most informative 75 percent of examples, thereby minimizing training overhead. To address the limitations of BERT's native vocabulary in capturing evolving hate speech terminology, we augment the tokenizer with domain-specific slang and lexical variants commonly found in abusive contexts. Experimental results on a widely used hate speech dataset demonstrate that our method achieves competitive performance while improving computational efficiency, highlighting its potential for scalable and adaptive abusive content moderation.

</details>


### [10] [Think Before You Prune: Self-Reflective Structured Pruning for Reasoning Language Models](https://arxiv.org/abs/2512.02185)
*Ziyan Wang,Enmao Diao,Qi Le,Pu Wang,Guanchu Wang,Minwoo Lee,Shu-ping Yeh,Li Yang*

Main category: cs.CL

TL;DR: 提出RESP框架，通过自生成校准、仅解码梯度重要性估计和渐进式再生，显著提升推理LLMs的结构化剪枝效果，在20-40%稀疏度下保持接近密集模型的性能。


<details>
  <summary>Details</summary>
Motivation: 推理LLMs（如o1、DeepSeek-R1、Qwen3）虽然推理能力强，但模型大、推理输出长，部署成本高且不适合资源受限环境。现有剪枝方法对推理LLMs效果差，即使中等稀疏度（如20%）也会严重损害准确性和推理连贯性。

Method: RESP框架：1）自生成校准：使用模型自身生成推理轨迹作为校准数据；2）仅解码梯度重要性估计：基于解码过程梯度评估参数重要性；3）渐进式再生：随着稀疏度增加保持校准保真度。

Result: 在Qwen3-8B上，RESP显著优于现有结构化剪枝方法：在20-30%稀疏度下保持接近密集模型准确率；40%稀疏度时，GSM8K达到81.3%（比最强基线高66.87%），MathQA达到59.6%（高47%）。

Conclusion: RESP通过将剪枝决策与模型推理动态对齐，有效解决了推理LLMs剪枝的挑战，为资源受限环境部署高效推理模型提供了可行方案。

Abstract: Reasoning LLMs (RLMs) such as OpenAI o1, DeepSeek-R1, and Qwen3 deliver strong multi-step reasoning through chain-of-thought generation, but their large model sizes and lengthy decode-time outputs make them costly to deploy and unsuitable for resource-constrained settings. To reduce computing and memory cost, pruning offers a promising solution by removing unimportant parameters. However, despite their success on standard LLMs, existing pruning methods severely damage RLMs, as even moderate sparsity (e.g., 20%) can collapse accuracy and completely disrupt the model's reasoning coherence. We begin by analyzing why existing pruning pipelines fail on reasoning LLMs and find that their brittleness largely stems from a mismatch between the calibration data, the pruning objective, and the model's decode-time reasoning behavior. Our study further shows that the most reliable calibration signal comes not from human-written labels but from the model's own self-generated reasoning traces, which more accurately reflect its inference distribution. Guided by these insights, we introduce RESP, a self-reflective structured pruning framework that aligns pruning decisions with the model's reasoning dynamics through self-generated calibration, decode-only gradient-based importance estimation, and progressive regeneration that maintains calibration fidelity as sparsity increases. Experiments on Qwen3-8B demonstrate that RESP markedly outperforms existing structured pruning methods on both GSM8K and MathQA, preserving near-dense accuracy at 20-30% sparsity and substantially mitigating performance collapse at higher sparsity levels. At 40% sparsity, RESP attains 81.3% accuracy on GSM8K and 59.6% on MathQA, surpassing the strongest baselines by 66.87% and 47%, respectively.

</details>


### [11] [A Knowledge-Based Language Model: Deducing Grammatical Knowledge in a Multi-Agent Language Acquisition Simulation](https://arxiv.org/abs/2512.02195)
*David Ph. Shakouri,Crit Cremers,Niels O. Schiller*

Main category: cs.CL

TL;DR: MODOMA是一个用于无监督语言习得实验的计算多智能体实验室环境，通过成人和儿童智能体之间的交互实现语言习得，能够生成和解析目标语言的新话语。


<details>
  <summary>Details</summary>
Motivation: 为计算语言习得实验提供新的可能性，创建一个完全参数化、可控制所有实验方面、并能明确表示和查询习得语法知识的系统。

Method: 使用多智能体框架，包含成人和儿童两个语言模型智能体，采用统计和基于规则的程序，通过交互实现无监督语言习得。

Result: 儿童智能体能够基于成人智能体生成的不同数量的训练和测试数据，习得并表示功能和内容范畴；机器生成数据中发现了与人类生成数据相似的已知模式。

Conclusion: 实验成功实现了儿童智能体对离散语法范畴的习得，验证了MODOMA方法在建模语言习得方面的有效性。

Abstract: This paper presents an initial study performed by the MODOMA system. The MODOMA is a computational multi-agent laboratory environment for unsupervised language acquisition experiments such that acquisition is based on the interaction between two language models, an adult and a child agent. Although this framework employs statistical as well as rule-based procedures, the result of language acquisition is a knowledge-based language model, which can be used to generate and parse new utterances of the target language. This system is fully parametrized and researchers can control all aspects of the experiments while the results of language acquisition, that is, the acquired grammatical knowledge, are explicitly represented and can be consulted. Thus, this system introduces novel possibilities for conducting computational language acquisition experiments. The experiments presented by this paper demonstrate that functional and content categories can be acquired and represented by the daughter agent based on training and test data containing different amounts of exemplars generated by the adult agent. Interestingly, similar patterns, which are well-established for human-generated data, are also found for these machine-generated data. As the procedures resulted in the successful acquisition of discrete grammatical categories by the child agent, these experiments substantiate the validity of the MODOMA approach to modelling language acquisition.

</details>


### [12] [Swivuriso: The South African Next Voices Multilingual Speech Dataset](https://arxiv.org/abs/2512.02201)
*Vukosi Marivatee,Kayode Olaleye,Sitwala Mundia,Andinda Bakainga,Unarine Netshifhefhe,Mahmooda Milanzie,Tsholofelo Hope Mogale,Thapelo Sindane,Zainab Abdulrasaq,Kesego Mokgosi,Chijioke Okorie,Nia Zion Van Wyk,Graham Morrissey,Dale Dunbar,Francois Smit,Tsosheletso Chidi,Rooweither Mabuya,Andiswa Bukula,Respect Mlambo,Tebogo Macucwa,Idris Abdulmumin,and Seani Rananga*

Main category: cs.CL

TL;DR: Swivuriso是一个3000小时的多语言语音数据集，涵盖7种南非语言，用于ASR技术开发和基准测试


<details>
  <summary>Details</summary>
Motivation: 解决现有ASR数据集在多种南非语言中的显著空白，支持非洲语言的语音识别技术发展

Method: 设计了数据集创建原则、伦理考虑和数据收集程序，涵盖农业、医疗和通用领域主题

Result: 提供了使用该数据训练/微调ASR模型的基线结果，并与相关语言的其他ASR数据集进行了比较

Conclusion: Swivuriso数据集填补了南非语言ASR研究的空白，为多语言语音识别技术发展提供了重要资源

Abstract: This paper introduces Swivuriso, a 3000-hour multilingual speech dataset developed as part of the African Next Voices project, to support the development and benchmarking of automatic speech recognition (ASR) technologies in seven South African languages. Covering agriculture, healthcare, and general domain topics, Swivuriso addresses significant gaps in existing ASR datasets. We describe the design principles, ethical considerations, and data collection procedures that guided the dataset creation. We present baseline results of training/finetuning ASR models with this data and compare to other ASR datasets for the langauges concerned.

</details>


### [13] [Lightweight Latent Reasoning for Narrative Tasks](https://arxiv.org/abs/2512.02240)
*Alexander Gurung,Nikolay Malkin,Mirella Lapata*

Main category: cs.CL

TL;DR: LiteReason是一种轻量级潜在推理方法，通过连续潜在令牌帮助模型"跳过"推理步骤，在保持性能的同时大幅减少计算成本


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通过生成长推理链处理复杂任务，但优化这些推理链需要高计算成本，特别是在涉及大量令牌处理的叙事相关任务中

Method: 提出LiteReason方法，包含轻量级推理投影器模块，生成连续潜在令牌帮助跳过推理步骤；在强化学习中，策略模型决定何时激活投影器，在潜在推理和离散推理之间切换

Result: 在情节漏洞检测和书籍章节生成任务中，LiteReason优于潜在推理基线，接近非潜在强化学习训练性能，同时减少最终推理长度77-92%

Conclusion: LiteReason引导强化学习训练达到性能-计算权衡曲线中更高效的部分，在保持性能的同时显著降低计算成本

Abstract: Large language models (LLMs) tackle complex tasks by generating long chains of thought or "reasoning traces" that act as latent variables in the generation of an output given a query. A model's ability to generate such traces can be optimized with reinforcement learning (RL) to improve their utility in predicting an answer. This optimization comes at a high computational cost, especially for narrative-related tasks that involve retrieving and processing many tokens. To this end, we propose LiteReason, a latent reasoning method that can be interleaved with standard token sampling and easily combined with RL techniques. LiteReason employs a lightweight Reasoning Projector module, trained to produce continuous latent tokens that help the model 'skip' reasoning steps. During RL, the policy model decides when to activate the projector, switching between latent and discrete reasoning as needed. Experimental results on plot hole detection and book chapter generation show that our method outperforms latent reasoning baselines and comes close to matching non-latent RL training, while reducing final reasoning length by 77-92%. Overall, LiteReason guides RL training to a more efficient part of the performance-computation tradeoff curve.

</details>


### [14] [DETAIL Matters: Measuring the Impact of Prompt Specificity on Reasoning in Large Language Models](https://arxiv.org/abs/2512.02246)
*Olivia Kim*

Main category: cs.CL

TL;DR: DETAIL框架评估提示词特异性对LLM推理性能的影响，发现更具体的提示能提高准确性，尤其对小模型和程序性任务有效。


<details>
  <summary>Details</summary>
Motivation: 提示设计对大型语言模型的推理性能至关重要，但提示特异性（详细程度）的影响尚未得到充分研究。需要系统评估不同特异性水平的提示如何影响模型表现。

Method: 提出DETAIL框架：1) 使用GPT-4生成多级特异性提示；2) 通过困惑度量化提示特异性；3) 使用基于GPT的语义等价性评估正确性；4) 在30个新颖推理任务上对GPT-4和O3-mini进行实验。

Result: 实验表明：1) 特异性提高准确性，尤其对小模型和程序性任务效果更明显；2) 揭示了需要自适应提示策略；3) 提供了支持进一步研究的工具和数据。

Conclusion: 提示特异性显著影响LLM推理性能，需要根据模型大小和任务类型采用自适应提示策略。DETAIL框架为提示工程研究提供了有价值的工具和基准。

Abstract: Prompt design plays a critical role in the reasoning performance of large language models (LLMs), yet the impact of prompt specificity - how detailed or vague a prompt is - remains understudied. This paper introduces DETAIL, a framework for evaluating LLM performance across varying levels of prompt specificity. We generate multi-level prompts using GPT-4, quantify specificity via perplexity, and assess correctness using GPT-based semantic equivalence. Experiments on 30 novel reasoning tasks across GPT-4 and O3-mini reveal that specificity improves accuracy, especially for smaller models and procedural tasks. Our results highlight the need for adaptive prompting strategies and provide tools and data to support further research.

</details>


### [15] [CAIRNS: Balancing Readability and Scientific Accuracy in Climate Adaptation Question Answering](https://arxiv.org/abs/2512.02251)
*Liangji Kong,Aditya Joshi,Sarvnaz Karimi*

Main category: cs.CL

TL;DR: CAIRNS是一个气候适应问答框架，通过结构化ScholarGuide提示提高可读性和引用可靠性，利用一致性加权混合评估器进行稳健评估，无需微调或强化学习即可从复杂网络证据源获取可信答案。


<details>
  <summary>Details</summary>
Motivation: 气候适应策略对维持粮食生产至关重要，但这些信息分散在非结构化数据（如科学文献）和结构化数据（如政府API）中，专家难以从复杂网络证据源获取可信的初步答案。

Method: 提出CAIRNS框架：1）使用结构化ScholarGuide提示增强可读性和引用可靠性；2）采用一致性加权混合评估器，利用模型间一致性和专家判断进行稳健评估；3）无需微调或强化学习。

Result: 在专家策划的问答数据集上，CAIRNS在大多数指标上优于基线方法；彻底的消融研究在所有指标上确认了结果；LLM评估与人类判断的相关性分析验证了评估方法的有效性。

Conclusion: CAIRNS框架能够为农业专家提供可读、可验证且基于领域知识的问答系统，有效整合复杂的气候适应证据源，为气候适应决策提供支持。

Abstract: Climate adaptation strategies are proposed in response to climate change. They are practised in agriculture to sustain food production. These strategies can be found in unstructured data (for example, scientific literature from the Elsevier website) or structured (heterogeneous climate data via government APIs). We present Climate Adaptation question-answering with Improved Readability and Noted Sources (CAIRNS), a framework that enables experts -- farmer advisors -- to obtain credible preliminary answers from complex evidence sources from the web. It enhances readability and citation reliability through a structured ScholarGuide prompt and achieves robust evaluation via a consistency-weighted hybrid evaluator that leverages inter-model agreement with experts. Together, these components enable readable, verifiable, and domain-grounded question-answering without fine-tuning or reinforcement learning. Using a previously reported dataset of expert-curated question-answers, we show that CAIRNS outperforms the baselines on most of the metrics. Our thorough ablation study confirms the results on all metrics. To validate our LLM-based evaluation, we also report an analysis of correlations against human judgment.

</details>


### [16] [HealthContradict: Evaluating Biomedical Knowledge Conflicts in Language Models](https://arxiv.org/abs/2512.02299)
*Boya Zhang,Alban Bornet,Rui Yang,Nan Liu,Douglas Teodoro*

Main category: cs.CL

TL;DR: 研究人员创建了HealthContradict数据集来评估语言模型在矛盾生物医学上下文中的推理能力，发现微调的生物医学模型既能利用正确上下文又能抵抗错误上下文。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解语言模型如何使用上下文信息回答健康问题，以及当上下文存在矛盾时模型的表现如何。现有医学问答评估基准无法充分区分模型在矛盾上下文中的推理能力。

Method: 创建了HealthContradict数据集，包含920个专家验证的实例，每个实例包括健康问题、科学证据支持的事实答案以及两个呈现矛盾立场的文档。采用多种提示设置（正确、错误或矛盾上下文）来测量对模型输出的影响。

Result: HealthContradict相比现有医学问答评估基准能更好地区分语言模型的上下文推理能力。实验表明，微调的生物医学语言模型的优势不仅在于预训练获得的参数知识，还在于它们能够利用正确上下文同时抵抗错误上下文。

Conclusion: 该研究通过HealthContradict数据集揭示了语言模型在矛盾生物医学上下文中的推理能力，强调了微调生物医学模型在利用正确信息和抵抗错误信息方面的双重优势，为评估医疗AI系统的可靠性提供了重要工具。

Abstract: How do language models use contextual information to answer health questions? How are their responses impacted by conflicting contexts? We assess the ability of language models to reason over long, conflicting biomedical contexts using HealthContradict, an expert-verified dataset comprising 920 unique instances, each consisting of a health-related question, a factual answer supported by scientific evidence, and two documents presenting contradictory stances. We consider several prompt settings, including correct, incorrect or contradictory context, and measure their impact on model outputs. Compared to existing medical question-answering evaluation benchmarks, HealthContradict provides greater distinctions of language models' contextual reasoning capabilities. Our experiments show that the strength of fine-tuned biomedical language models lies not only in their parametric knowledge from pretraining, but also in their ability to exploit correct context while resisting incorrect context.

</details>


### [17] [When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers](https://arxiv.org/abs/2512.02304)
*Jack Lu,Ryan Teehan,Jinran Jin,Mengye Ren*

Main category: cs.CL

TL;DR: 该论文系统研究了37个不同模型在9个基准测试上的验证器性能，发现跨模型族验证效果最佳，后训练会降低自我改进但增强跨族改进，数学和逻辑任务具有最高的可验证性。


<details>
  <summary>Details</summary>
Motivation: 先前关于求解器-验证器交互的研究有限，主要关注自我验证，很少研究验证器如何判断来自同一模型族或不同模型族的输出。现代大语言模型经过大量后训练，但其对验证的影响尚不清楚。

Method: 研究涵盖37个模型，跨越多个模型族、不同规模以及基础模型与后训练变体，在9个基准测试上进行评估，涵盖逻辑推理、结构化谜题、符号计算、数学、常识、事实回忆和领域知识。比较自我验证、同族验证和跨族验证，并引入并经验验证验证器增益指标。

Result: 跨模型族验证特别有效；后训练会降低自我改进但增强跨族改进；数学和逻辑任务表现出最高的固有可验证性。验证器增益和误报率随模型规模和后训练而变化。

Conclusion: 该研究为大语言模型中验证器的作用提供了系统性见解，表明跨模型族验证是提高性能的有效策略，后训练改变了验证动态，不同任务类型的可验证性存在显著差异。

Abstract: Large language models (LLMs) can act as both problem solvers and solution verifiers, with verifiers improving solver performance by selecting high-quality answers from a pool of candidates. However, prior studies of solver-verifier interactions have been limited, focusing mainly on self-verification and rarely examining how verifiers judge outputs from models in their own or in another model family. Modern LLMs also undergo extensive post-training, but its effect on verification remains unclear. We present a systematic study across 37 models spanning multiple families, sizes, and base vs. post-trained variants, evaluated on 9 benchmarks covering logical reasoning, structured puzzles, symbolic computation, mathematics, commonsense, factual recall, and domain knowledge. We compare self-verification with verification within the same family and across different families. To support this, we introduce and empirically validate verifier gain, a metric that predicts the performance improvements from test-time verifier-based rejection sampling. We analyze how metrics like verifier gain and false positive rate scale with model size and post-training, and characterize differences in dataset verifiability. Our findings show that cross-family verification is especially effective; post-training reduces self-improvement but strengthens cross-family improvement; and mathematical and logical tasks exhibit the highest inherent verifiability.

</details>


### [18] [Memory-Augmented Knowledge Fusion with Safety-Aware Decoding for Domain-Adaptive Question Answering](https://arxiv.org/abs/2512.02363)
*Lei Fu,Xiang Chen,Kaige Gao Xinyue Huang,Kejian Tong*

Main category: cs.CL

TL;DR: KARMA框架通过双编码器融合结构化与非结构化知识、门控记忆单元动态调节知识集成、安全感知可控解码器，提升服务领域问答系统的准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 服务领域的特定领域问答系统面临整合异构知识源同时确保准确性和安全性的挑战。现有大语言模型在医疗政策、政府福利等敏感领域常存在事实一致性和上下文对齐问题。

Method: 提出KARMA框架，包含：1) 双编码器架构融合结构化与非结构化知识源；2) 门控记忆单元动态调节外部知识集成；3) 安全感知可控解码器，使用安全分类和引导生成技术减少不安全输出。

Result: 在专有问答数据集上的大量实验表明，KARMA在答案质量和安全性方面均优于强基线模型。

Conclusion: 该研究为服务场景中构建可信赖且自适应的问答系统提供了全面解决方案。

Abstract: Domain-specific question answering (QA) systems for services face unique challenges in integrating heterogeneous knowledge sources while ensuring both accuracy and safety. Existing large language models often struggle with factual consistency and context alignment in sensitive domains such as healthcare policies and government welfare. In this work, we introduce Knowledge-Aware Reasoning and Memory-Augmented Adaptation (KARMA), a novel framework designed to enhance QA performance in care scenarios. KARMA incorporates a dual-encoder architecture to fuse structured and unstructured knowledge sources, a gated memory unit to dynamically regulate external knowledge integration, and a safety-aware controllable decoder that mitigates unsafe outputs using safety classification and guided generation techniques. Extensive experiments on a proprietary QA dataset demonstrate that KARMA outperforms strong baselines in both answer quality and safety. This study offers a comprehensive solution for building trustworthy and adaptive QA systems in service contexts.

</details>


### [19] [TaleFrame: An Interactive Story Generation System with Fine-Grained Control and Large Language Models](https://arxiv.org/abs/2512.02402)
*Yunchao Wang,Guodao Sun,Zihang Fu,Zhehao Liu,Kaixing Du,Haidong Gao,Ronghua Liang*

Main category: cs.CL

TL;DR: TaleFrame是一个结合LLM与HCI的创意故事生成系统，通过结构化信息（实体、事件、关系、大纲）实现细粒度控制，提供直观界面让用户通过拖拽等交互方式创作故事，并支持多维度评估和迭代优化。


<details>
  <summary>Details</summary>
Motivation: 当前故事生成系统难以准确将用户意图转化为满意的故事输出，主要原因是缺乏细粒度控制和输入规范不清晰，限制了系统的实用性。

Method: 1. 将故事结构分解为四个基本单元：实体、事件、关系和故事大纲；2. 利用Tinystories数据集构建包含9,851个JSON格式条目的偏好数据集；3. 微调本地Llama模型实现JSON2Story转换；4. 提供直观界面支持用户通过拖拽、附加、连接等交互方式创建和编辑故事单元。

Result: 1. 开发了TaleFrame系统，能够将结构化数据转化为连贯的故事；2. 系统支持七维度评估（如创意性、结构完整性）并提供改进建议；3. 用户可以通过迭代调整获得满意结果；4. 定量评估和用户研究证明了系统的实用性。

Conclusion: TaleFrame通过结构化故事生成方法和直观的交互界面，有效解决了现有故事生成系统控制粒度不足的问题，实现了更精确的用户意图到故事输出的转换，提升了创意故事生成系统的实用性和用户体验。

Abstract: With the advancement of natural language generation (NLG) technologies, creative story generation systems have gained increasing attention. However, current systems often fail to accurately translate user intent into satisfactory story outputs due to a lack of fine-grained control and unclear input specifications, limiting their applicability. To address this, we propose TaleFrame, a system that combines large language models (LLMs) with human-computer interaction (HCI) to generate stories through structured information, enabling precise control over the generation process. The innovation of TaleFrame lies in decomposing the story structure into four basic units: entities, events, relationships, and story outline. We leverage the Tinystories dataset, parsing and constructing a preference dataset consisting of 9,851 JSON-formatted entries, which is then used to fine-tune a local Llama model. By employing this JSON2Story approach, structured data is transformed into coherent stories. TaleFrame also offers an intuitive interface that supports users in creating and editing entities and events and generates stories through the structured framework. Users can control these units through simple interactions (e.g., drag-and-drop, attach, and connect), thus influencing the details and progression of the story. The generated stories can be evaluated across seven dimensions (e.g., creativity, structural integrity), with the system providing suggestions for refinement based on these evaluations. Users can iteratively adjust the story until a satisfactory result is achieved. Finally, we conduct quantitative evaluation and user studies that demonstrate the usefulness of TaleFrame. Dataset available at https://huggingface.co/datasets/guodaosun/tale-frame.

</details>


### [20] [A Concise Review of Hallucinations in LLMs and their Mitigation](https://arxiv.org/abs/2512.02527)
*Parth Pulkundwar,Vivek Dhanawade,Rohit Yadav,Minal Sonkar,Medha Asurlekar,Sarita Rathod*

Main category: cs.CL

TL;DR: 本文对语言模型中的幻觉问题进行了全面综述，包括幻觉类型、成因及缓解方法，为理解和解决该问题提供了简明资源。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型面临幻觉问题的挑战，这严重威胁自然语言处理领域的发展。需要系统理解当前各种幻觉类型、产生原因以及减少幻觉的方法。

Method: 本文采用综述研究方法，系统梳理和总结语言模型幻觉的相关文献，提供简洁明了的分类和分析框架。

Result: 论文提供了幻觉问题的全面概述，包括不同类型的幻觉分类、产生机制分析，以及现有的缓解策略和技术方法。

Conclusion: 幻觉是语言模型面临的重要挑战，需要系统研究和应对。本文为理解和缓解幻觉问题提供了有价值的参考资源。

Abstract: Traditional language models face a challenge from hallucinations. Their very presence casts a large, dangerous shadow over the promising realm of natural language processing. It becomes crucial to understand the various kinds of hallucinations that occur nowadays, their origins, and ways of reducing them. This document provides a concise and straightforward summary of that. It serves as a one-stop resource for a general understanding of hallucinations and how to mitigate them.

</details>


### [21] [What Signals Really Matter for Misinformation Tasks? Evaluating Fake-News Detection and Virality Prediction under Real-World Constraints](https://arxiv.org/abs/2512.02552)
*Francesco Paolo Savatteri,Chahan Vidal-Gorène,Florian Cafiero*

Main category: cs.CL

TL;DR: 该研究评估了在线虚假信息检测和传播预测任务，发现文本内容是虚假新闻检测的强判别器，而传播预测更具挑战性且对标签构建高度敏感。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于评估在线虚假信息检测和传播预测在需要快速反应的运营环境中的实际表现，比较不同特征和模型的实用性。

Method: 使用EVONS和FakeNewsNet数据集，比较文本嵌入（RoBERTa、Mistral）与轻量级数值特征（时间、关注者数、验证状态、点赞数）以及序列模型（GRU、门控架构、Transformer编码器）。

Result: 文本内容单独就是虚假新闻检测的强判别器，而纯数值管道在语言模型不可用或计算受限时仍然可行。传播预测比虚假新闻检测困难得多，且对标签构建高度敏感。维度缩减分析表明非线性结构对传播预测比虚假新闻检测更具信息性。

Conclusion: 文本嵌入在虚假新闻检测中表现优异，而传播预测需要更谨慎的标签设计和特征处理。研究强调了评估设计的重要性，并指出了当前API限制下的实际约束。

Abstract: We present an evaluation-driven study of two practical tasks regarding online misinformation: (i) fake-news detection and (ii) virality prediction in the context of operational settings, with the necessity for rapid reaction. Using the EVONS and FakeNewsNet datasets, we compare textual embeddings (RoBERTa; with a control using Mistral) against lightweight numeric features (timing, follower counts, verification, likes) and sequence models (GRU, gating architectures, Transformer encoders). We show that textual content alone is a strong discriminator for fake-news detection, while numeric-only pipelines remain viable when language models are unavailable or compute is constrained. Virality prediction is markedly harder than fake-news detection and is highly sensitive to label construction; in our setup, a median-based ''viral'' split (<50 likes) is pragmatic but underestimates real-world virality, and time-censoring for engagement features is desirable yet difficult under current API limits. Dimensionality-reduction analyses suggest non-linear structure is more informative for virality than for fake-news detection (t-SNE > PCA on numeric features). Swapping RoBERTa for Mistral embeddings yields only modest deltas, leaving conclusions unchanged. We discuss implications for evaluation design and report reproducibility constraints that realistically affect the field. We release splits and code where possible and provide guidance for metric selection.

</details>


### [22] [DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models](https://arxiv.org/abs/2512.02556)
*DeepSeek-AI,Aixin Liu,Aoxue Mei,Bangcai Lin,Bing Xue,Bingxuan Wang,Bingzheng Xu,Bochao Wu,Bowei Zhang,Chaofan Lin,Chen Dong,Chengda Lu,Chenggang Zhao,Chengqi Deng,Chenhao Xu,Chong Ruan,Damai Dai,Daya Guo,Dejian Yang,Deli Chen,Erhang Li,Fangqi Zhou,Fangyun Lin,Fucong Dai,Guangbo Hao,Guanting Chen,Guowei Li,H. Zhang,Hanwei Xu,Hao Li,Haofen Liang,Haoran Wei,Haowei Zhang,Haowen Luo,Haozhe Ji,Honghui Ding,Hongxuan Tang,Huanqi Cao,Huazuo Gao,Hui Qu,Hui Zeng,Jialiang Huang,Jiashi Li,Jiaxin Xu,Jiewen Hu,Jingchang Chen,Jingting Xiang,Jingyang Yuan,Jingyuan Cheng,Jinhua Zhu,Jun Ran,Junguang Jiang,Junjie Qiu,Junlong Li,Junxiao Song,Kai Dong,Kaige Gao,Kang Guan,Kexin Huang,Kexing Zhou,Kezhao Huang,Kuai Yu,Lean Wang,Lecong Zhang,Lei Wang,Liang Zhao,Liangsheng Yin,Lihua Guo,Lingxiao Luo,Linwang Ma,Litong Wang,Liyue Zhang,M. S. Di,M. Y Xu,Mingchuan Zhang,Minghua Zhang,Minghui Tang,Mingxu Zhou,Panpan Huang,Peixin Cong,Peiyi Wang,Qiancheng Wang,Qihao Zhu,Qingyang Li,Qinyu Chen,Qiushi Du,Ruiling Xu,Ruiqi Ge,Ruisong Zhang,Ruizhe Pan,Runji Wang,Runqiu Yin,Runxin Xu,Ruomeng Shen,Ruoyu Zhang,S. H. Liu,Shanghao Lu,Shangyan Zhou,Shanhuang Chen,Shaofei Cai,Shaoyuan Chen,Shengding Hu,Shengyu Liu,Shiqiang Hu,Shirong Ma,Shiyu Wang,Shuiping Yu,Shunfeng Zhou,Shuting Pan,Songyang Zhou,Tao Ni,Tao Yun,Tian Pei,Tian Ye,Tianyuan Yue,Wangding Zeng,Wen Liu,Wenfeng Liang,Wenjie Pang,Wenjing Luo,Wenjun Gao,Wentao Zhang,Xi Gao,Xiangwen Wang,Xiao Bi,Xiaodong Liu,Xiaohan Wang,Xiaokang Chen,Xiaokang Zhang,Xiaotao Nie,Xin Cheng,Xin Liu,Xin Xie,Xingchao Liu,Xingkai Yu,Xingyou Li,Xinyu Yang,Xinyuan Li,Xu Chen,Xuecheng Su,Xuehai Pan,Xuheng Lin,Xuwei Fu,Y. Q. Wang,Yang Zhang,Yanhong Xu,Yanru Ma,Yao Li,Yao Li,Yao Zhao,Yaofeng Sun,Yaohui Wang,Yi Qian,Yi Yu,Yichao Zhang,Yifan Ding,Yifan Shi,Yiliang Xiong,Ying He,Ying Zhou,Yinmin Zhong,Yishi Piao,Yisong Wang,Yixiao Chen,Yixuan Tan,Yixuan Wei,Yiyang Ma,Yiyuan Liu,Yonglun Yang,Yongqiang Guo,Yongtong Wu,Yu Wu,Yuan Cheng,Yuan Ou,Yuanfan Xu,Yuduan Wang,Yue Gong,Yuhan Wu,Yuheng Zou,Yukun Li,Yunfan Xiong,Yuxiang Luo,Yuxiang You,Yuxuan Liu,Yuyang Zhou,Z. F. Wu,Z. Z. Ren,Zehua Zhao,Zehui Ren,Zhangli Sha,Zhe Fu,Zhean Xu,Zhenda Xie,Zhengyan Zhang,Zhewen Hao,Zhibin Gou,Zhicheng Ma,Zhigang Yan,Zhihong Shao,Zhixian Huang,Zhiyu Wu,Zhuoshu Li,Zhuping Zhang,Zian Xu,Zihao Wang,Zihui Gu,Zijia Zhu,Zilin Li,Zipeng Zhang,Ziwei Xie,Ziyi Gao,Zizheng Pan,Zongqing Yao,Bei Feng,Hui Li,J. L. Cai,Jiaqi Ni,Lei Xu,Meng Li,Ning Tian,R. J. Chen,R. L. Jin,S. S. Li,Shuang Zhou,Tianyu Sun,X. Q. Li,Xiangyue Jin,Xiaojin Shen,Xiaosha Chen,Xinnan Song,Xinyi Zhou,Y. X. Zhu,Yanping Huang,Yaohui Li,Yi Zheng,Yuchen Zhu,Yunxian Ma,Zhen Huang,Zhipeng Xu,Zhongyu Zhang,Dongjie Ji,Jian Liang,Jianzhong Guo,Jin Chen,Leyi Xia,Miaojun Wang,Mingming Li,Peng Zhang,Ruyi Chen,Shangmian Sun,Shaoqing Wu,Shengfeng Ye,T. Wang,W. L. Xiao,Wei An,Xianzu Wang,Xiaowen Sun,Xiaoxiang Wang,Ying Tang,Yukun Zha,Zekai Zhang,Zhe Ju,Zhen Zhang,Zihua Qu*

Main category: cs.CL

TL;DR: DeepSeek-V3.2是一个高效推理模型，通过稀疏注意力机制、强化学习框架和大规模代理任务合成管道，在计算效率和推理性能上取得突破，其特殊变体在数学和信息学竞赛中达到金牌水平。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在计算效率与推理性能之间的权衡问题，开发一个既能保持高效计算又具备强大推理和代理能力的模型。

Method: 采用三种关键技术：1) DeepSeek稀疏注意力(DSA)机制降低计算复杂度；2) 可扩展的强化学习框架进行后训练；3) 大规模代理任务合成管道生成训练数据。

Result: DeepSeek-V3.2在计算效率与推理性能上取得平衡，其特殊变体DeepSeek-V3.2-Speciale超越GPT-5，推理能力与Gemini-3.0-Pro相当，在2025年国际数学奥林匹克和信息学奥林匹克竞赛中获得金牌表现。

Conclusion: DeepSeek-V3.2成功实现了计算效率与推理性能的和谐统一，通过创新的注意力机制、强化学习框架和数据合成方法，为高效推理模型的发展提供了新方向。

Abstract: We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.

</details>


### [23] [From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks](https://arxiv.org/abs/2512.02580)
*Changpeng Yang,Jinyang Wu,Yuchen Liu,Shuai Zhang,Yang Li,Qiliang Liang,Hongzhen Wang,Shuai Nie,Jiaming Xu,Runyu Shi,Ying Huang,Guoquan Zhang*

Main category: cs.CL

TL;DR: CAPO提出了一种基于优势信号的课程学习机制，通过先使用正优势样本进行模仿学习建立基础，再引入负信号培养判别能力，从而提升大语言模型在复杂推理任务中的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在训练大语言模型时，从早期阶段就不加区分地混合正负优势信号，可能导致模糊的指导和有限的性能提升。

Method: 提出CAPO（Curriculum Advantage Policy Optimization），一种基于优势信号的自适应课程机制。该方法首先使用仅包含正优势的样本进行模仿学习以建立坚实基础，随后逐步引入负信号来培养判别能力。

Result: 该方法与多种优化方法（GRPO、PPO、RLOO、Reinforce++）兼容，在数学推理任务中实现了稳定且显著的改进，并能有效泛化到多模态图形用户界面（GUI）推理场景。

Conclusion: CAPO作为一个通用且鲁棒的优化框架，通过课程化的优势信号处理策略，有效提升了强化学习在大语言模型训练中的效果和泛化能力。

Abstract: Reinforcement learning has emerged as a paradigm for post-training large language models, boosting their reasoning capabilities. Such approaches compute an advantage value for each sample, reflecting better or worse performance than expected, thereby yielding both positive and negative signals for training. However, the indiscriminate mixing of the two signals in existing methods, especially from the early stages, may lead to ambiguous guidance and limited gains. To address this issue, we propose **CAPO** (**C**urriculum **A**dvantage **P**olicy **O**ptimization), an adaptive curriculum mechanism based on advantage signals. The proposed mechanism bootstraps imitation learning with positive-only advantage samples to establish robust foundations, and subsequently introduces negative signals to cultivate discriminative capabilities, thereby improving generalization across complex scenarios. Compatible with diverse optimization methods including GRPO, PPO, RLOO, and Reinforce++, our method consistently achieves stable and significant improvements in mathematical reasoning tasks, and further generalizes effectively to multimodal Graphical User Interface (GUI) reasoning scenarios, establishing itself as a versatile and robust optimization framework.

</details>


### [24] [Spoken Conversational Agents with Large Language Models](https://arxiv.org/abs/2512.02593)
*Chao-Han Huck Yang,Andreas Stolcke,Larry Heck*

Main category: cs.CL

TL;DR: 本教程系统梳理了从级联ASR/NLU系统到端到端语音大语言模型的发展路径，涵盖多模态对齐、联合训练等关键技术，并提供实用实现方案和系统级路线图。


<details>
  <summary>Details</summary>
Motivation: 语音对话代理正在向语音原生大语言模型演进，需要系统梳理从传统级联架构到端到端系统的技术发展路径，为研究者和实践者提供清晰的路线图。

Method: 通过框架化文本LLM到音频的适配、跨模态对齐、联合语音-文本训练等方法；回顾数据集、评估指标和鲁棒性；比较级联与端到端、后ASR校正、流式处理等设计选择。

Result: 建立了工业助手与当前开放域和任务导向代理的联系，突出了可复现基线，并提供了隐私、安全和评估等开放问题的分析框架。

Conclusion: 本教程为参与者提供了实用的实现方案和清晰的系统级路线图，帮助理解语音对话代理从传统架构向语音原生LLM演进的技术路径和未来挑战。

Abstract: Spoken conversational agents are converging toward voice-native LLMs. This tutorial distills the path from cascaded ASR/NLU to end-to-end, retrieval-and vision-grounded systems. We frame adaptation of text LLMs to audio, cross-modal alignment, and joint speech-text training; review datasets, metrics, and robustness across accents and compare design choices (cascaded vs. E2E, post-ASR correction, streaming). We link industrial assistants to current open-domain and task-oriented agents, highlight reproducible baselines, and outline open problems in privacy, safety, and evaluation. Attendees leave with practical recipes and a clear systems-level roadmap.

</details>


### [25] [Input Order Shapes LLM Semantic Alignment in Multi-Document Summarization](https://arxiv.org/abs/2512.02665)
*Jing Ma*

Main category: cs.CL

TL;DR: 研究发现LLM在生成多文档摘要时存在首因效应，更倾向于与第一个看到的文档保持语义对齐，这对依赖LLM生成概述的应用和智能体系统构成风险。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM被广泛用于多文档摘要（如Google AI Overviews），但尚不清楚它们是否平等对待所有输入文档，这在实际应用中可能带来偏差风险。

Method: 构建40个关于堕胎议题的支持-中立-反对文章三元组，将每个三元组以6种不同顺序排列，使用Gemini 2.5 Flash生成中立概述，通过ROUGE-L、BERTScore和SummaC评估摘要与源文章的关系。

Result: 单因素方差分析显示BERTScore在所有立场上都存在显著的首因效应，摘要与第一个看到的文章语义更一致。成对比较进一步表明位置1与位置2、3有显著差异，而位置2和3之间无差异，证实了对第一个文档的选择性偏好。

Conclusion: LLM在多文档摘要中存在首因效应，这给依赖LLM生成概述的应用和智能体AI系统带来风险，因为涉及LLM的步骤可能不成比例地影响下游行动。

Abstract: Large language models (LLMs) are now used in settings such as Google's AI Overviews, where it summarizes multiple long documents. However, it remains unclear whether they weight all inputs equally. Focusing on abortion-related news, we construct 40 pro-neutral-con article triplets, permute each triplet into six input orders, and prompt Gemini 2.5 Flash to generate a neutral overview. We evaluate each summary against its source articles using ROUGE-L (lexical overlap), BERTScore (semantic similarity), and SummaC (factual consistency). One-way ANOVA reveals a significant primacy effect for BERTScore across all stances, indicating that summaries are more semantically aligned with the first-seen article. Pairwise comparisons further show that Position 1 differs significantly from Positions 2 and 3, while the latter two do not differ from each other, confirming a selective preference for the first document. The findings present risks for applications that rely on LLM-generated overviews and for agentic AI systems, where the steps involving LLMs can disproportionately influence downstream actions.

</details>


### [26] [An Empirical Survey of Model Merging Algorithms for Social Bias Mitigation](https://arxiv.org/abs/2512.02689)
*Daiki Shirafuji,Tatsuhiko Saito,Yasutomo Kimura*

Main category: cs.CL

TL;DR: 本文实证调查了七种模型合并算法在减轻大语言模型社会偏见方面的效果，发现偏见减少与下游任务性能之间存在权衡，其中SLERP在中等插值权重下表现最为平衡。


<details>
  <summary>Details</summary>
Motivation: 大语言模型(LLMs)会继承甚至放大预训练语料中的社会偏见，威胁公平性和社会信任。虽然已有研究探索通过编辑LLM参数来减轻偏见，但缺乏对不同模型合并算法的实证比较。

Method: 实证调查了七种模型合并算法：Linear、Karcher Mean、SLERP、NuSLERP、TIES、DELLA和Nearswap，应用于GPT、LLaMA和Qwen家族的13个开源权重模型。使用三个偏见数据集(BBQ、BOLD和HONEST)进行综合评估，并在SuperGLUE基准的下游任务中测量这些技术对LLM性能的影响。

Result: 发现偏见减少与下游性能之间存在权衡：实现更大偏见减轻的方法会降低准确性，特别是在需要阅读理解、常识和因果推理的任务上。在合并算法中，Linear、SLERP和Nearswap在保持整体性能的同时持续减少偏见，其中中等插值权重的SLERP成为最平衡的选择。

Conclusion: 模型合并算法在偏见减轻方面具有潜力，但过度的去偏见化或不适当的合并方法可能导致重要语言能力的退化。SLERP在中等插值权重下提供了偏见减少和性能保持的最佳平衡。

Abstract: Large language models (LLMs) are known to inherit and even amplify societal biases present in their pre-training corpora, threatening fairness and social trust. To address this issue, recent work has explored ``editing'' LLM parameters to mitigate social bias with model merging approaches; however, there is no empirical comparison. In this work, we empirically survey seven algorithms: Linear, Karcher Mean, SLERP, NuSLERP, TIES, DELLA, and Nearswap, applying 13 open weight models in the GPT, LLaMA, and Qwen families. We perform a comprehensive evaluation using three bias datasets (BBQ, BOLD, and HONEST) and measure the impact of these techniques on LLM performance in downstream tasks of the SuperGLUE benchmark. We find a trade-off between bias reduction and downstream performance: methods achieving greater bias mitigation degrade accuracy, particularly on tasks requiring reading comprehension and commonsense and causal reasoning. Among the merging algorithms, Linear, SLERP, and Nearswap consistently reduce bias while maintaining overall performance, with SLERP at moderate interpolation weights emerging as the most balanced choice. These results highlight the potential of model merging algorithms for bias mitigation, while indicating that excessive debiasing or inappropriate merging methods may lead to the degradation of important linguistic abilities.

</details>


### [27] [CREST: Universal Safety Guardrails Through Cluster-Guided Cross-Lingual Transfer](https://arxiv.org/abs/2512.02711)
*Lavish Bansal,Naman Mishra*

Main category: cs.CL

TL;DR: CREST是一个参数高效的跨语言安全分类模型，仅用0.5B参数支持100种语言，通过在13种高资源语言上训练实现跨语言泛化，在安全基准测试中优于同类规模模型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全护栏主要针对高资源语言，导致使用低资源语言的大量人口代表性不足，需要开发能够服务全球人口的通用、语言无关的安全系统。

Method: 采用参数高效的多语言安全分类模型，通过基于聚类的跨语言迁移方法，仅使用13种高资源语言训练数据，实现从少数语言到100种语言的泛化能力。

Result: 在六个安全基准测试中，CREST超越了同类规模的最先进安全护栏，并与参数数量显著更大（2.5B参数及以上）的模型取得了竞争性结果。

Conclusion: 研究揭示了语言特定安全护栏的局限性，强调了开发能够有效扩展到服务全球人口的通用、语言无关安全系统的重要性。

Abstract: Ensuring content safety in large language models (LLMs) is essential for their deployment in real-world applications. However, existing safety guardrails are predominantly tailored for high-resource languages, leaving a significant portion of the world's population underrepresented who communicate in low-resource languages. To address this, we introduce CREST (CRoss-lingual Efficient Safety Transfer), a parameter-efficient multilingual safety classification model that supports 100 languages with only 0.5B parameters. By training on a strategically chosen subset of only 13 high-resource languages, our model utilizes cluster-based cross-lingual transfer from a few to 100 languages, enabling effective generalization to both unseen high-resource and low-resource languages. This approach addresses the challenge of limited training data in low-resource settings. We conduct comprehensive evaluations across six safety benchmarks to demonstrate that CREST outperforms existing state-of-the-art guardrails of comparable scale and achieves competitive results against models with significantly larger parameter counts (2.5B parameters and above). Our findings highlight the limitations of language-specific guardrails and underscore the importance of developing universal, language-agnostic safety systems that can scale effectively to serve global populations.

</details>


### [28] [Emergent Bayesian Behaviour and Optimal Cue Combination in LLMs](https://arxiv.org/abs/2512.02719)
*Julian Ma,Jun Wang,Zafeirios Fountas*

Main category: cs.CL

TL;DR: LLMs在贝叶斯一致性方面表现出类似人类的行为，但准确率并不保证鲁棒性，揭示了能力与策略之间的关键分离。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否像人类一样在没有明确训练或指令的情况下表现出最优的多模态整合行为，探索其隐式计算策略。

Method: 采用心理物理学范式，创建BayesBench基准测试（四个幅度估计任务），评估9个LLM，通过控制噪声、上下文和指令提示来测量多模态线索整合的性能和行为。

Result: 虽然能力强的模型经常以贝叶斯一致的方式适应，但准确率并不保证鲁棒性。GPT-5 Mini在文本任务上准确率完美，但无法有效整合视觉线索。

Conclusion: 揭示了能力与策略之间的关键分离，表明以准确率为中心的基准测试可能过度关注性能而忽略了脆弱的确定性处理。这些发现揭示了处理不确定性的原则性行为。

Abstract: Large language models (LLMs) excel at explicit reasoning, but their implicit computational strategies remain underexplored. Decades of psychophysics research show that humans intuitively process and integrate noisy signals using near-optimal Bayesian strategies in perceptual tasks. We ask whether LLMs exhibit similar behaviour and perform optimal multimodal integration without explicit training or instruction. Adopting the psychophysics paradigm, we infer computational principles of LLMs from systematic behavioural studies. We introduce a behavioural benchmark - BayesBench: four magnitude estimation tasks (length, location, distance, and duration) over text and image, inspired by classic psychophysics, and evaluate a diverse set of nine LLMs alongside human judgments for calibration. Through controlled ablations of noise, context, and instruction prompts, we measure performance, behaviour and efficiency in multimodal cue-combination. Beyond accuracy and efficiency metrics, we introduce a Bayesian Consistency Score that detects Bayes-consistent behavioural shifts even when accuracy saturates. Our results show that while capable models often adapt in Bayes-consistent ways, accuracy does not guarantee robustness. Notably, GPT-5 Mini achieves perfect text accuracy but fails to integrate visual cues efficiently. This reveals a critical dissociation between capability and strategy, suggesting accuracy-centric benchmarks may over-index on performance while missing brittle uncertainty handling. These findings reveal emergent principled handling of uncertainty and highlight the correlation between accuracy and Bayesian tendencies. We release our psychophysics benchmark and consistency metric (https://bayes-bench.github.io) as evaluation tools and to inform future multimodal architecture designs.

</details>


### [29] [SurveyEval: Towards Comprehensive Evaluation of LLM-Generated Academic Surveys](https://arxiv.org/abs/2512.02763)
*Jiahao Zhao,Shuaixing Zhang,Nan Xu,Lei Wang*

Main category: cs.CL

TL;DR: SurveyEval：一个评估自动生成调研报告的综合性基准，从整体质量、大纲连贯性和参考文献准确性三个维度进行评估，旨在解决LLM调研系统评估难题。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的自动调研系统通过检索、组织和内容合成的端到端生成流程改变用户获取信息的方式，如何评估这种复杂系统成为一个重要挑战。现有研究主要关注开发新的生成流程，但缺乏有效的评估方法。

Method: 提出SurveyEval基准，从三个维度评估自动生成的调研报告：1)整体质量，2)大纲连贯性，3)参考文献准确性。在7个学科领域扩展评估，并增强LLM-as-a-Judge框架，加入人类参考以加强评估与人类判断的一致性。

Result: 评估结果显示，通用长文本或论文写作系统倾向于生成质量较低的调研报告，而专门的调研生成系统能够提供显著更高质量的结果。SurveyEval可作为可扩展的测试平台。

Conclusion: SurveyEval为理解和改进跨不同学科和评估标准的自动调研系统提供了一个全面的评估框架，有助于推动该领域的发展。

Abstract: LLM-based automatic survey systems are transforming how users acquire information from the web by integrating retrieval, organization, and content synthesis into end-to-end generation pipelines. While recent works focus on developing new generation pipelines, how to evaluate such complex systems remains a significant challenge. To this end, we introduce SurveyEval, a comprehensive benchmark that evaluates automatically generated surveys across three dimensions: overall quality, outline coherence, and reference accuracy. We extend the evaluation across 7 subjects and augment the LLM-as-a-Judge framework with human references to strengthen evaluation-human alignment. Evaluation results show that while general long-text or paper-writing systems tend to produce lower-quality surveys, specialized survey-generation systems are able to deliver substantially higher-quality results. We envision SurveyEval as a scalable testbed to understand and improve automatic survey systems across diverse subjects and evaluation criteria.

</details>


### [30] [PEFT-Factory: Unified Parameter-Efficient Fine-Tuning of Autoregressive Large Language Models](https://arxiv.org/abs/2512.02764)
*Robert Belanec,Ivan Srba,Maria Bielikova*

Main category: cs.CL

TL;DR: PEFT-Factory是一个统一的参数高效微调框架，提供19种PEFT方法、27个数据集和专用评估指标，改善PEFT方法的可复现性和基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前许多新引入的PEFT方法难以复现、部署或相互比较，需要统一的框架来解决这些问题。

Method: 基于流行的LLaMA-Factory构建下游框架，采用模块化设计支持扩展，原生提供19种PEFT方法、27个分类和文本生成数据集（涵盖12个任务），以及标准和PEFT专用评估指标。

Result: PEFT-Factory提供了一个即用、可控且稳定的环境，显著改善了PEFT方法的可复现性和基准测试能力。

Conclusion: PEFT-Factory是一个有效的统一框架，解决了PEFT方法在实际应用中的复现和比较难题，为研究社区提供了实用的工具。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods address the increasing size of Large Language Models (LLMs). Currently, many newly introduced PEFT methods are challenging to replicate, deploy, or compare with one another. To address this, we introduce PEFT-Factory, a unified framework for efficient fine-tuning LLMs using both off-the-shelf and custom PEFT methods. While its modular design supports extensibility, it natively provides a representative set of 19 PEFT methods, 27 classification and text generation datasets addressing 12 tasks, and both standard and PEFT-specific evaluation metrics. As a result, PEFT-Factory provides a ready-to-use, controlled, and stable environment, improving replicability and benchmarking of PEFT methods. PEFT-Factory is a downstream framework that originates from the popular LLaMA-Factory, and is publicly available at https://github.com/kinit-sk/PEFT-Factory

</details>


### [31] [Making Dialogue Grounding Data Rich: A Three-Tier Data Synthesis Framework for Generalized Referring Expression Comprehension](https://arxiv.org/abs/2512.02791)
*Juexi Shao,Siyou Li,Yujian Gan,Chris Madge,Vanja Karan,Massimo Poesio*

Main category: cs.CL

TL;DR: 提出一种三层次数据合成方法，用于对话式广义指代表达理解任务，通过平衡真实性和可控性生成可扩展的监督数据，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有对话式广义指代表达理解系统在训练和评估域之间存在分布偏移问题，且标注的对话接地数据稀缺，限制了模型性能。

Method: 采用三层次数据合成方法，平衡真实性和可控性，为对话条件接地生成可扩展的监督数据。

Result: 在合成数据上微调的模型在标准评估指标上相比先前方法取得了持续且显著的改进。

Conclusion: 提出的数据合成方法有效解决了对话式广义指代表达理解中的数据稀缺和分布偏移问题，显著提升了模型性能。

Abstract: Dialogue-Based Generalized Referring Expressions Comprehension (GREC) requires models to ground the expression and unlimited targets in complex visual scenes while resolving coreference across a long dialogue context. However, existing systems struggle under distribution shift between training and evaluation domains, a gap exacerbated by the scarcity of annotated dialogue grounding data. We address this challenge with a three-tier data-synthesis method that balances realism and controllability to produce scalable supervision for dialogue-conditioned grounding. Fine-tuning on the synthesized data yields consistent, substantial improvements over prior approaches across standard evaluation metrics.

</details>


### [32] [TriLex: A Framework for Multilingual Sentiment Analysis in Low-Resource South African Languages](https://arxiv.org/abs/2512.02799)
*Mike Nkongolo,Hilton Vorster,Josh Warren,Trevor Naick,Deandre Vanmali,Masana Mashapha,Luke Brand,Alyssa Fernandes,Janco Calitz,Sibusiso Makhoba*

Main category: cs.CL

TL;DR: TriLex框架通过三阶段检索增强方法扩展低资源非洲语言情感词典，显著提升情感分析性能


<details>
  <summary>Details</summary>
Motivation: 低资源非洲语言在情感分析中代表性不足，限制了多语言NLP系统的词汇覆盖和性能

Method: 提出TriLex三阶段检索增强框架：1) 基于语料库的提取，2) 跨语言映射，3) RAG驱动的词汇精炼；使用扩展词典评估AfroXLMR和AfriBERTa模型

Result: AfroXLMR表现最佳，在isiXhosa和isiZulu上F1分数超过80%，跨语言稳定性强；AfriBERTa虽未预训练目标语言，仍达到约64% F1分数；两种模型均优于传统机器学习基线，集成分析进一步提升精度和鲁棒性

Conclusion: TriLex是用于低资源南非语言多语言情感词典扩展和情感建模的可扩展有效框架

Abstract: Low-resource African languages remain underrepresented in sentiment analysis, limiting both lexical coverage and the performance of multilingual Natural Language Processing (NLP) systems. This study proposes TriLex, a three-stage retrieval augmented framework that unifies corpus-based extraction, cross lingual mapping, and retrieval augmented generation (RAG) driven lexical refinement to systematically expand sentiment lexicons for low-resource languages. Using the enriched lexicon, the performance of two prominent African pretrained language models (AfroXLMR and AfriBERTa) is evaluated across multiple case studies. Results demonstrate that AfroXLMR delivers superior performance, achieving F1-scores above 80% for isiXhosa and isiZulu and exhibiting strong cross-lingual stability. Although AfriBERTa lacks pre-training on these target languages, it still achieves reliable F1-scores around 64%, validating its utility in computationally constrained settings. Both models outperform traditional machine learning baselines, and ensemble analyses further enhance precision and robustness. The findings establish TriLex as a scalable and effective framework for multilingual sentiment lexicon expansion and sentiment modeling in low-resource South African languages.

</details>


### [33] [SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment](https://arxiv.org/abs/2512.02807)
*Yixuan Tang,Yi Yang*

Main category: cs.CL

TL;DR: 提出stable rank作为从模型内部表示中提取的无标注质量信号，并基于此开发SR-GRPO强化学习方法，无需外部监督即可提升LLM对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐方法依赖外部监督存在严重局限：人工标注稀缺且主观，奖励模型易受奖励攻击，自评估方法存在提示敏感性和偏见。需要寻找更稳定、可扩展的内在质量信号。

Method: 提出stable rank概念，通过计算隐藏状态总方差与主导方向方差的比值来衡量表示的有效维度。基于此开发Stable Rank Group Relative Policy Optimization (SR-GRPO)，将stable rank作为强化学习的奖励信号。

Result: stable rank在RewardBench上达到84.04%准确率，通过Best-of-N采样比贪婪解码平均提升11.3个百分点任务准确率。SR-GRPO将Qwen2.5-1.5B-Instruct在STEM任务上提升10%，数学推理提升19%，优于学习奖励模型和自评估基线。

Conclusion: 质量信号可以从模型内部几何结构中提取，这为无需外部监督的可扩展对齐提供了新路径，stable rank作为内在质量信号具有实际应用价值。

Abstract: Aligning Large Language Models (LLMs) with human preferences typically relies on external supervision, which faces critical limitations: human annotations are scarce and subjective, reward models are vulnerable to reward hacking, and self-evaluation methods suffer from prompt sensitivity and biases. In this work, we propose stable rank, an intrinsic, annotation-free quality signal derived from model representations. Stable rank measures the effective dimensionality of hidden states by computing the ratio of total variance to dominant-direction variance, capturing quality through how information distributes across representation dimensions. Empirically, stable rank achieves 84.04% accuracy on RewardBench and improves task accuracy by an average of 11.3 percentage points over greedy decoding via Best-of-N sampling. Leveraging this insight, we introduce Stable Rank Group Relative Policy Optimization (SR-GRPO), which uses stable rank as a reward signal for reinforcement learning. Without external supervision, SR-GRPO improves Qwen2.5-1.5B-Instruct by 10% on STEM and 19% on mathematical reasoning, outperforming both learned reward models and self-evaluation baselines. Our findings demonstrate that quality signals can be extracted from internal model geometry, offering a path toward scalable alignment without external supervision.

</details>


### [34] [A benchmark dataset for evaluating Syndrome Differentiation and Treatment in large language models](https://arxiv.org/abs/2512.02816)
*Kunning Li,Jianbin Guo,Zhaoyang Shang,Yiqing Liu,Hongmin Du,Lingling Liu,Yuping Zhao,Lifeng Dong*

Main category: cs.CL

TL;DR: 提出了一个名为TCM-BEST4SDT的综合性中医临床案例基准测试，用于评估大语言模型在中医"辨证论治"领域的应用能力，包含四个任务和三种评估机制。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在中医领域的应用日益增多，但现有评估方法主要局限于知识问答或辨证准确性，缺乏对治疗决策能力的评估。中医"辨证论治"具有个体化、整体性和多样性的特点，需要更全面的评估框架。

Method: 1. 由中医专家主导构建临床案例基准测试；2. 采用专门设计的奖励模型量化处方-证候一致性；3. 数据标注遵循严格流程；4. 包含四个任务：中医基础知识、医学伦理、LLM内容安全和辨证论治；5. 整合三种评估机制：选择题评估、评判模型评估和奖励模型评估。

Result: 在15个主流大语言模型（包括通用和中医领域模型）上验证了TCM-BEST4SDT的有效性。该基准测试现已公开可用，以促进智能中医研究的发展。

Conclusion: TCM-BEST4SDT是一个全面、基于临床案例的中医大语言模型评估基准，能够有效评估模型在辨证论治各方面的能力，填补了现有评估方法的空白，为智能中医研究提供了重要工具。

Abstract: The emergence of Large Language Models (LLMs) within the Traditional Chinese Medicine (TCM) domain presents an urgent need to assess their clinical application capabilities. However, such evaluations are challenged by the individualized, holistic, and diverse nature of TCM's "Syndrome Differentiation and Treatment" (SDT). Existing benchmarks are confined to knowledge-based question-answering or the accuracy of syndrome differentiation, often neglecting assessment of treatment decision-making. Here, we propose a comprehensive, clinical case-based benchmark spearheaded by TCM experts, and a specialized reward model employed to quantify prescription-syndrome congruence. Data annotation follows a rigorous pipeline. This benchmark, designated TCM-BEST4SDT, encompasses four tasks, including TCM Basic Knowledge, Medical Ethics, LLM Content Safety, and SDT. The evaluation framework integrates three mechanisms, namely selected-response evaluation, judge model evaluation, and reward model evaluation. The effectiveness of TCM-BEST4SDT was corroborated through experiments on 15 mainstream LLMs, spanning both general and TCM domains. To foster the development of intelligent TCM research, TCM-BEST4SDT is now publicly available.

</details>


### [35] [BOOM: Beyond Only One Modality KIT's Multimodal Multilingual Lecture Companion](https://arxiv.org/abs/2512.02817)
*Sai Koneru,Fabian Retkowski,Christian Huber,Lukas Hilgert,Seymanur Akti,Enes Yavuz Ugan,Alexander Waibel,Jan Niehues*

Main category: cs.CL

TL;DR: BOOM是一个多模态多语言讲座伴侣系统，能够同时翻译讲座音频和幻灯片，生成同步的翻译文本、本地化幻灯片和合成语音，为在线学习提供完整的本地化体验。


<details>
  <summary>Details</summary>
Motivation: 教育全球化和在线学习的快速增长使得教育内容本地化成为关键挑战。讲座材料本质上是多模态的，结合了语音音频和视觉幻灯片，需要能够处理多种输入模态的系统。为了提供可访问和完整的学习体验，翻译必须保留所有模态：用于阅读的文本、用于视觉理解的幻灯片以及用于听觉学习的语音。

Method: 提出BOOM系统，这是一个端到端的多模态多语言讲座伴侣，联合翻译讲座音频和幻灯片，生成跨三种模态的同步输出：翻译文本、保留视觉元素的本地化幻灯片以及合成语音。系统采用端到端方法，使学生能够以母语访问讲座，同时力求完全保留原始内容。

Result: 实验表明，具有幻灯片感知的转录在下游任务（如摘要和问答）中也带来了级联效益。作者发布了幻灯片翻译代码并集成到Lecture Translator中。

Conclusion: BOOM系统通过多模态联合翻译方法解决了教育内容本地化的关键挑战，为学生提供了完整的母语学习体验，同时保留了原始讲座的所有模态内容，对在线教育的可访问性有重要意义。

Abstract: The globalization of education and rapid growth of online learning have made localizing educational content a critical challenge. Lecture materials are inherently multimodal, combining spoken audio with visual slides, which requires systems capable of processing multiple input modalities. To provide an accessible and complete learning experience, translations must preserve all modalities: text for reading, slides for visual understanding, and speech for auditory learning. We present \textbf{BOOM}, a multimodal multilingual lecture companion that jointly translates lecture audio and slides to produce synchronized outputs across three modalities: translated text, localized slides with preserved visual elements, and synthesized speech. This end-to-end approach enables students to access lectures in their native language while aiming to preserve the original content in its entirety. Our experiments demonstrate that slide-aware transcripts also yield cascading benefits for downstream tasks such as summarization and question answering. We release our Slide Translation code at https://github.com/saikoneru/image-translator and integrate it in Lecture Translator at https://gitlab.kit.edu/kit/isl-ai4lt/lt-middleware/ltpipeline}\footnote{All released code and models are licensed under the MIT License.

</details>


### [36] [promptolution: A Unified, Modular Framework for Prompt Optimization](https://arxiv.org/abs/2512.02840)
*Tom Zehle,Timo Heiß,Moritz Schlager,Matthias Aßenmacher,Matthias Feurer*

Main category: cs.CL

TL;DR: promptolution：一个统一、模块化的开源框架，集成了多种离散提示优化器，为实践者和研究者提供完整的提示优化组件


<details>
  <summary>Details</summary>
Motivation: 虽然许多研究论文证明了提示优化的有效性，但实际应用受到阻碍，因为现有实现通常依赖于未维护和孤立的研究代码库

Method: 开发promptolution框架，这是一个统一且模块化的开源系统，集成了多种当代离散提示优化器，同时保持对底层LLM实现的不可知性

Result: 创建了一个可扩展的框架，为实践者和研究者提供了在单个系统中进行提示优化所需的所有组件

Conclusion: promptolution框架解决了提示优化研究和实践之间的鸿沟，通过提供统一、可扩展的开源系统来促进实际应用

Abstract: Prompt optimization has become crucial for enhancing the performance of large language models (LLMs) across a broad range of tasks. Although many research papers show its effectiveness, practical adoption is hindered as existing implementations are often tied to unmaintained and isolated research codebases. To address this, we introduce promptolution, a unified and modular open-source framework that provides all components required for prompt optimization within a single extensible system for both practitioners and researchers. It integrates multiple contemporary discrete prompt optimizers while remaining agnostic to the underlying LLM implementation.

</details>


### [37] [Cross-Lingual Prompt Steerability: Towards Accurate and Robust LLM Behavior across Languages](https://arxiv.org/abs/2512.02841)
*Lechen Zhang,Yusheng Zhou,Tolga Ergen,Lajanugen Logeswaran,Moontae Lee,David Jurgens*

Main category: cs.CL

TL;DR: 本文研究了系统提示词在跨语言大语言模型推理中的作用，提出了一个四维评估框架，并通过大规模实验发现某些提示词组件与稳健的多语言行为相关，开发了自动优化框架，能提升各项指标5-10%。


<details>
  <summary>Details</summary>
Motivation: 系统提示词为大型语言模型在推理时提供轻量级但强大的条件设置机制。虽然先前研究主要集中在英语环境，但实际部署需要单一提示词能在多种语言中可靠工作。本文旨在研究不同系统提示词如何引导模型实现准确和稳健的跨语言行为。

Method: 提出了一个统一的四维评估框架来评估多语言环境中的系统提示词。在五种语言、三个大型语言模型和三个基准测试上进行了大规模实验。开发了一个针对多语言设置的提示词优化框架，并分析了超过1000万个推理单元。

Result: 研究发现某些提示词组件（如思维链、情感和场景）与稳健的多语言行为相关。优化框架能自动发现提升所有指标5-10%的提示词。分析显示性能更好的系统提示词能诱导更结构化、一致性的推理模式，同时减少不必要的语言切换。

Conclusion: 系统提示词优化是实现准确和稳健多语言大型语言模型行为的可扩展路径。性能更好的提示词能产生更一致的结构化推理，减少语言切换，从而提升跨语言性能。

Abstract: System prompts provide a lightweight yet powerful mechanism for conditioning large language models (LLMs) at inference time. While prior work has focused on English-only settings, real-world deployments benefit from having a single prompt to operate reliably across languages. This paper presents a comprehensive study of how different system prompts steer models toward accurate and robust cross-lingual behavior. We propose a unified four-dimensional evaluation framework to assess system prompts in multilingual environments. Through large-scale experiments on five languages, three LLMs, and three benchmarks, we uncover that certain prompt components, such as CoT, emotion, and scenario, correlate with robust multilingual behavior. We develop a prompt optimization framework for multilingual settings and show it can automatically discover prompts that improve all metrics by 5-10%. Finally, we analyze over 10 million reasoning units and find that more performant system prompts induce more structured and consistent reasoning patterns, while reducing unnecessary language-switching. Together, we highlight system prompt optimization as a scalable path to accurate and robust multilingual LLM behavior.

</details>


### [38] [Bangla Hate Speech Classification with Fine-tuned Transformer Models](https://arxiv.org/abs/2512.02845)
*Yalda Keivan Jafari,Krishno Dey*

Main category: cs.CL

TL;DR: 该论文研究了BLP 2025共享任务中的孟加拉语仇恨言论检测，比较了传统机器学习方法和基于Transformer的模型，发现语言特定的预训练模型BanglaBERT表现最佳。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语作为拥有超过2.3亿使用者的语言，在计算资源方面代表性不足，特别是在社交媒体自动内容审核方面存在迫切需求。低资源语言的仇恨言论识别面临数据集不足、拼写异质性和语言多样性等挑战。

Method: 研究复制了官方基线方法（多数类、随机、支持向量机），并增加了逻辑回归、随机森林和决策树作为基线。同时使用了基于Transformer的模型，包括DistilBERT、BanglaBERT、m-BERT和XLM-RoBERTa进行仇恨言论分类。

Result: 所有基于Transformer的模型（除DistilBERT外）在子任务中都优于基线方法。在Transformer模型中，BanglaBERT在两个子任务中都取得了最佳性能。尽管规模较小，BanglaBERT的表现优于m-BERT和XLM-RoBERTa，表明语言特定的预训练非常重要。

Conclusion: 研究结果强调了预训练语言模型在低资源孟加拉语中的潜力和必要性，语言特定的预训练对于提升性能至关重要，为孟加拉语自然语言处理任务提供了有价值的见解。

Abstract: Hate speech recognition in low-resource lan- guages remains a difficult problem due to in- sufficient datasets, orthographic heterogeneity, and linguistic variety. Bangla is spoken by more than 230 million people of Bangladesh and India (West Bengal). Despite the grow- ing need for automated moderation on social media platforms, Bangla is significantly under- represented in computational resources. In this work, we study Subtask 1A and Subtask 1B of the BLP 2025 Shared Task on hate speech detection. We reproduce the official base- lines (e.g., Majority, Random, Support Vec- tor Machine) and also produce and consider Logistic Regression, Random Forest, and De- cision Tree as baseline methods. We also uti- lized transformer-based models such as Dis- tilBERT, BanglaBERT, m-BERT, and XLM- RoBERTa for hate speech classification. All the transformer-based models outperformed base- line methods for the subtasks, except for Distil- BERT. Among the transformer-based models, BanglaBERT produces the best performance for both subtasks. Despite being smaller in size, BanglaBERT outperforms both m-BERT and XLM-RoBERTa, which suggests language- specific pre-training is very important. Our results highlight the potential and need for pre- trained language models for the low-resource Bangla language.

</details>


### [39] [Think in Parallel, Answer as One: Logit Averaging for Open-Ended Reasoning](https://arxiv.org/abs/2512.02874)
*Haonan Wang,Chao Du,Kenji Kawaguchi,Tianyu Pang*

Main category: cs.CL

TL;DR: ThinkMerge是一种无需训练、即插即用的解码策略，通过并行推理轨迹的logit平均来提升开放端推理任务性能，在代码生成和深度研究任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 多数投票在封闭式问答中有效，但不适用于代码生成和深度研究等开放式推理任务，因为在这些任务中"多数"完整解决方案的定义不明确。

Method: 提出ThinkMerge解码策略：运行K个并行推理轨迹，在同步点平均它们的下一个token的logits，生成单一连贯输出。与vLLM/SGLang无缝集成，兼容Top-p/Top-k等标准解码技术。

Result: 在AIME和GPQA上匹配或超越多数投票；在LiveCodeBench（困难）上，DeepCoder-14B-Preview的pass@1提升+8.28%，Qwen3-8B提升+7.58%；在GAIA、BrowseComp-en/zh和XbenchDeepSearch上提升基于网络的深度研究代理性能。

Conclusion: ThinkMerge证明了并行测试时扩展可以在不依赖完整输出投票的情况下有益于开放式推理，为开放端任务提供了一种有效的解码策略。

Abstract: Majority voting has proven effective for close-ended question answering by aggregating parallel reasoning traces. However, it is not directly applicable to open-ended reasoning, such as code generation and web-based deep research, where a "majority" over complete solutions is ill-defined. We introduce ThinkMerge, a training-free, plug-and-play decoding strategy that runs K parallel reasoning traces and averages their next-token logits at synchronization points to produce a single coherent output. ThinkMerge integrates seamlessly with vLLM/SGLang and remains compatible with standard decoding techniques such as Top-p/Top-k. Empirically, it matches or surpasses majority voting on AIME and GPQA, while delivering consistent gains on open-ended coding tasks: on LiveCodeBench (hard), pass@1 improves by +8.28% for DeepCoder-14B-Preview and +7.58% for Qwen3-8B. Beyond code, we further show that ThinkMerge improves web-based deep-research agents (e.g., WebSailor-7B/32B) across GAIA, BrowseComp-en/zh, and XbenchDeepSearch. These results demonstrate that parallel test-time scaling can benefit open-ended reasoning without relying on voting over complete outputs.

</details>


### [40] [Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules](https://arxiv.org/abs/2512.02892)
*Amr Mohamed,Yang Zhang,Michalis Vazirgiannis,Guokan Shang*

Main category: cs.CL

TL;DR: SchED是一种无需训练、模型无关的早期退出算法，通过聚合全跨度logit边界并在满足平滑、进度相关的置信度阈值时停止解码，显著加速扩散大语言模型的推理速度。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型(dLLMs)相比自回归模型具有潜力，但其迭代采样过程缓慢，严重阻碍了实际应用。需要一种能够在不牺牲性能的前提下加速推理的方法。

Method: 提出SchED算法：1) 聚合全跨度logit边界；2) 设定平滑、进度相关的置信度阈值；3) 当置信度达到阈值时提前停止解码。该方法是无需训练的、模型无关的。

Result: 在Dream和LLaDA两个dLLM家族上评估：指令调优模型获得3.8-4.0倍加速，性能保留99.8-100%；基础模型获得一致加速增益，性能保留99.1-100%，激进设置下可达2.34倍加速。在长文本生成任务上明显优于现有早期退出方法。

Conclusion: SchED通过将真实的置信度稳定转化为计算节省，使dLLM解码显著更高效，为扩散大语言模型的实用化提供了有效的加速解决方案。

Abstract: Diffusion large language models (dLLMs) offer a promising alternative to autoregressive models, but their practical utility is severely hampered by slow, iterative sampling. We present SchED, a training-free, model-agnostic early-exit algorithm that aggregates full-span logit margins and halts decoding once a smooth, progress-dependent confidence threshold is met. We evaluated SchED on two dLLM families (Dream and LLaDA), in base and instruction-tuned variants across ten benchmarks spanning downstream tasks including multiple-choice question answering (MCQ), math, long-form QA/summarization, and translation. SchED delivers large, stable accelerations: on instruction-tuned models, it achieves $3.8$-$4.0\times$ speedups while retaining $99.8$-$100\%$ of the baseline score on average. On base models, SchED yields consistent speedup gains with $99.1$-$100\%$ performance retention, with up to $2.34\times$ under more aggressive settings. Using a conservative speed metric that heavily penalizes quality loss (QPS, $γ{=}4$), we show that SchED is robust and clearly outperforms prior confidence-based early-exit methods, which break down on long-form generation. An entropy analysis of the model's token predictions reveals that instruction tuning speeds up the decay of predictive entropy. By turning genuine confidence stabilization into computational savings, SchED makes dLLM decoding substantially more efficient.

</details>


### [41] [AutoNeural: Co-Designing Vision-Language Models for NPU Inference](https://arxiv.org/abs/2512.02924)
*Wei Chen,Liangmin Wu,Yunhai Hu,Zhiyuan Li,Zhiyuan Cheng,Yicheng Qian,Lingyue Zhu,Zhipeng Hu,Luoyi Liang,Qiang Tang,Zhen Liu,Han Yang*

Main category: cs.CL

TL;DR: AutoNeural：专为NPU设计的视觉语言模型架构，通过MobileNetV5风格视觉编码器和SSM-Transformer混合语言模型，实现整数推理和线性时间复杂度，显著提升边缘AI效率。


<details>
  <summary>Details</summary>
Motivation: 当前为GPU设计的视觉语言模型在NPU上表现不佳，主要由于ViT的量化脆弱性和自回归注意力机制的I/O限制，无法充分利用NPU的高算术吞吐量。

Method: 1) 用MobileNetV5风格的深度可分离卷积替换标准ViT编码器，确保激活分布有界以支持稳定INT4/8/16量化；2) 语言模型结合状态空间模型原理和Transformer层，使用门控卷积实现线性时间复杂度，消除KV缓存的内存I/O开销。

Result: 视觉编码器量化误差降低7倍，端到端延迟降低14倍，解码速度提升3倍，上下文窗口长度提升4倍。在Qualcomm SA8295P SoC上的真实汽车案例验证了驾驶舱应用的实时性能。

Conclusion: 针对NPU约束重新设计模型拓扑是实现鲁棒多模态边缘智能的前提条件，AutoNeural展示了硬件感知模型设计的有效性。

Abstract: While Neural Processing Units (NPUs) offer high theoretical efficiency for edge AI, state-of-the-art Vision--Language Models (VLMs) tailored for GPUs often falter on these substrates. We attribute this hardware-model mismatch to two primary factors: the quantization brittleness of Vision Transformers (ViTs) and the I/O-bound nature of autoregressive attention mechanisms, which fail to utilize the high arithmetic throughput of NPUs. To bridge this gap, we propose AutoNeural, an NPU-native VLM architecture co-designed for integer-only inference. We replace the standard ViT encoder with a MobileNetV5-style backbone utilizing depthwise separable convolutions, which ensures bounded activation distributions for stable INT4/8/16 quantization. Complementing this, our language backbone integrates State-Space Model (SSM) principles with Transformer layers, employing efficient gated convolutions to achieve linear-time complexity. This hybrid design eliminates the heavy memory I/O overhead of Key-Value caching during generation. Our approach delivers substantial efficiency gains, reducing quantization error of vision encoder by up to 7x and end-to-end latency by 14x compared to conventional baselines. The AutoNeural also delivers 3x decoding speed and 4x longer context window than the baseline. We validate these improvements via a real-world automotive case study on the Qualcomm SA8295P SoC, demonstrating real-time performance for cockpit applications. Our results highlight that rethinking model topology specifically for NPU constraints is a prerequisite for robust multi-modal edge intelligence.

</details>


### [42] [Fine-Tuned Large Language Models for Logical Translation: Reducing Hallucinations with Lang2Logic](https://arxiv.org/abs/2512.02987)
*Muyu Pan,Dheeraj Kodakandla,Mahfuza Farooque*

Main category: cs.CL

TL;DR: 提出一个结合传统NLP技术与微调语言模型的框架，将英文句子转换为逻辑表达式再转为CNF形式，以减少LLM在逻辑翻译中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: LLM在自然语言到形式逻辑的自动翻译中存在幻觉问题，而逻辑翻译任务需要高精度，这阻碍了自动化推理在软件系统调试、循环不变量查找和规范遵循中的应用。

Method: 采用结合传统NLP技术（自定义语法、符号计算库）和微调语言模型的新框架，将英文句子先转换为逻辑表达式，再翻译为合取范式（CNF）用于可满足性求解。

Result: 早期实验表明，在不同语法设置下训练的微调模型能够有意识地纠正原始模型的同类幻觉，从而提供可靠的CNF生成。

Conclusion: 该框架通过结合传统NLP技术和微调LLM，有效减少了逻辑翻译中的幻觉问题，为自动化推理提供了更可靠的CNF生成能力。

Abstract: Recent advances in natural language processing (NLP), particularly large language models (LLMs), have motivated the automatic translation of natural language statements into formal logic without human intervention. This enables automated reasoning and facilitates debugging, finding loop invariants, and adhering to specifications in software systems. However, hallucinations-incorrect outputs generated by LLMs are challenging, particularly for logical translation tasks requiring precision. This work introduces a novel framework that inputs English sentences, converts them into logical expressions, and then translates them into Conjunctive Normal Form (CNF) for satisfiability solving. It employs classical NLP techniques with self-defined grammar, symbolic computation libraries, and a fine-tuned language model to reduce hallucinations. In the early experiments, we observed that the fine-tuned model, trained on different grammar settings, could intentionally correct the same types of hallucinations made by the original model. Thus, it provides reliable CNF generation.

</details>


### [43] [The Moral Consistency Pipeline: Continuous Ethical Evaluation for Large Language Models](https://arxiv.org/abs/2512.03026)
*Saeid Jamshidi,Kawser Wazed Nafi,Arghavan Moradi Dakhel,Negar Shahabi,Foutse Khomh*

Main category: cs.CL

TL;DR: MoCoP是一个无需数据集、闭环的道德一致性评估框架，用于持续评估和解释LLMs的道德稳定性，通过自主生成、评估和优化伦理场景来揭示模型道德行为的长期模式。


<details>
  <summary>Details</summary>
Motivation: 现有对齐框架依赖静态数据集和事后评估，难以捕捉伦理推理在不同上下文或时间尺度上的演变，需要更动态、连续的方法来评估LLMs的道德一致性。

Method: 提出Moral Consistency Pipeline (MoCoP)，包含三个支持层：(i)词汇完整性分析，(ii)语义风险估计，(iii)基于推理的判断建模，采用自维持架构自主生成、评估和优化伦理场景，无需外部监督。

Result: 在GPT-4-Turbo和DeepSeek上的实验显示，MoCoP能有效捕捉长期道德行为，发现道德维度与毒性维度呈强负相关(rET=-0.81, p<0.001)，与响应延迟近乎零相关(rEL≈0)，表明道德一致性和语言安全是模型行为的稳定可解释特征。

Conclusion: MoCoP通过将伦理评估重构为动态、模型无关的道德内省形式，为可扩展的持续审计提供了可复现基础，推动了自主AI系统中计算道德学的研究。

Abstract: The rapid advancement and adaptability of Large Language Models (LLMs) highlight the need for moral consistency, the capacity to maintain ethically coherent reasoning across varied contexts. Existing alignment frameworks, structured approaches designed to align model behavior with human ethical and social norms, often rely on static datasets and post-hoc evaluations, offering limited insight into how ethical reasoning may evolve across different contexts or temporal scales. This study presents the Moral Consistency Pipeline (MoCoP), a dataset-free, closed-loop framework for continuously evaluating and interpreting the moral stability of LLMs. MoCoP combines three supporting layers: (i) lexical integrity analysis, (ii) semantic risk estimation, and (iii) reasoning-based judgment modeling within a self-sustaining architecture that autonomously generates, evaluates, and refines ethical scenarios without external supervision. Our empirical results on GPT-4-Turbo and DeepSeek suggest that MoCoP effectively captures longitudinal ethical behavior, revealing a strong inverse relationship between ethical and toxicity dimensions (correlation rET = -0.81, p value less than 0.001) and a near-zero association with response latency (correlation rEL approximately equal to 0). These findings demonstrate that moral coherence and linguistic safety tend to emerge as stable and interpretable characteristics of model behavior rather than short-term fluctuations. Furthermore, by reframing ethical evaluation as a dynamic, model-agnostic form of moral introspection, MoCoP offers a reproducible foundation for scalable, continuous auditing and advances the study of computational morality in autonomous AI systems.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [44] [Q-BERT4Rec: Quantized Semantic-ID Representation Learning for Multimodal Recommendation](https://arxiv.org/abs/2512.02474)
*Haofeng Huang,Ling Gai*

Main category: cs.IR

TL;DR: Q-Bert4Rec：一种多模态序列推荐框架，通过语义量化和跨模态融合提升推荐性能


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的序列推荐方法（如BERT4Rec）依赖离散的物品ID，缺乏语义信息且忽略丰富的多模态内容（文本、图像），导致泛化能力弱和可解释性有限。

Method: 提出三阶段框架：1) 跨模态语义注入，通过动态Transformer融合文本、视觉和结构特征丰富ID嵌入；2) 语义量化，通过残差向量量化将融合表示离散化为有意义的token；3) 多掩码预训练和微调，使用跨度、尾部、多区域等多种掩码策略提升序列理解能力。

Result: 在公开的Amazon基准测试中，Q-Bert4Rec显著优于多种现有强基线方法，验证了语义量化对多模态序列推荐的有效性。

Conclusion: Q-Bert4Rec通过统一语义表示和量化建模，有效解决了传统方法缺乏语义信息和多模态内容的问题，为多模态序列推荐提供了新的解决方案。

Abstract: Sequential recommendation plays a critical role in modern online platforms such as e-commerce, advertising, and content streaming, where accurately predicting users' next interactions is essential for personalization. Recent Transformer-based methods like BERT4Rec have shown strong modeling capability, yet they still rely on discrete item IDs that lack semantic meaning and ignore rich multimodal information (e.g., text and image). This leads to weak generalization and limited interpretability. To address these challenges, we propose Q-Bert4Rec, a multimodal sequential recommendation framework that unifies semantic representation and quantized modeling. Specifically, Q-Bert4Rec consists of three stages: (1) cross-modal semantic injection, which enriches randomly initialized ID embeddings through a dynamic transformer that fuses textual, visual, and structural features; (2) semantic quantization, which discretizes fused representations into meaningful tokens via residual vector quantization; and (3) multi-mask pretraining and fine-tuning, which leverage diverse masking strategies -- span, tail, and multi-region -- to improve sequential understanding. We validate our model on public Amazon benchmarks and demonstrate that Q-Bert4Rec significantly outperforms many strong existing methods, confirming the effectiveness of semantic tokenization for multimodal sequential recommendation. Our source code will be publicly available on GitHub after publishing.

</details>


### [45] [AskNearby: An LLM-Based Application for Neighborhood Information Retrieval and Personalized Cognitive-Map Recommendations](https://arxiv.org/abs/2512.02502)
*Luyao Niu,Zhicheng Deng,Boyang Li,Nuoxian Huang,Ruiqi Liu,Wenjia Zhang*

Main category: cs.IR

TL;DR: AskNearby是一个AI驱动的社区应用，通过三层RAG管道和认知地图模型，解决15分钟生活圈内的本地生活信息可及性问题，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于位置的系统主要关注城市级任务，忽视了影响本地化决策的空间、时间和认知因素，无法满足15分钟城市愿景中对本地生活信息高效可靠访问的需求。

Method: 提出AskNearby应用，整合：(1)三层检索增强生成(RAG)管道，结合基于图、语义向量和地理检索；(2)认知地图模型，编码用户的社区熟悉度和偏好。

Result: 在真实社区数据集上的实验表明，AskNearby在检索准确性和推荐质量上显著优于基于LLM和地图的基线，在时空基础和认知感知排名方面表现稳健。实际部署进一步验证了其有效性。

Conclusion: 通过解决本地生活信息可及性挑战，AskNearby赋能居民更有效地发现本地资源、规划日常活动和参与社区生活，推动15分钟城市愿景的实现。

Abstract: The "15-minute city" envisions neighborhoods where residents can meet daily needs via a short walk or bike ride. Realizing this vision requires not only physical proximity but also efficient and reliable access to information about nearby places, services, and events. Existing location-based systems, however, focus mainly on city-level tasks and neglect the spatial, temporal, and cognitive factors that shape localized decision-making. We conceptualize this gap as the Local Life Information Accessibility (LLIA) problem and introduce AskNearby, an AI-driven community application that unifies retrieval and recommendation within the 15-minute life circle. AskNearby integrates (i) a three-layer Retrieval-Augmented Generation (RAG) pipeline that synergizes graph-based, semantic-vector, and geographic retrieval with (ii) a cognitive-map model that encodes each user's neighborhood familiarity and preferences. Experiments on real-world community datasets demonstrate that AskNearby significantly outperforms LLM-based and map-based baselines in retrieval accuracy and recommendation quality, achieving robust performance in spatiotemporal grounding and cognitive-aware ranking. Real-world deployments further validate its effectiveness. By addressing the LLIA challenge, AskNearby empowers residents to more effectively discover local resources, plan daily activities, and engage in community life.

</details>
