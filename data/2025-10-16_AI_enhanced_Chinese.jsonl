{"id": "2510.12815", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.12815", "abs": "https://arxiv.org/abs/2510.12815", "authors": ["Xiaocong Chen", "Siyu Wang", "Lina Yao"], "title": "Energy-Guided Diffusion Sampling for Long-Term User Behavior Prediction in Reinforcement Learning-based Recommendation", "comment": "CIKM'25", "summary": "Reinforcement learning-based recommender systems (RL4RS) have gained\nattention for their ability to adapt to dynamic user preferences. However,\nthese systems face challenges, particularly in offline settings, where data\ninefficiency and reliance on pre-collected trajectories limit their broader\napplicability. While offline reinforcement learning methods leverage extensive\ndatasets to address these issues, they often struggle with noisy data and fail\nto capture long-term user preferences, resulting in suboptimal recommendation\npolicies. To overcome these limitations, we propose Diffusion-enhanced\nActor-Critic for Offline RL4RS (DAC4Rec), a novel framework that integrates\ndiffusion processes with reinforcement learning to model complex user\npreferences more effectively. DAC4Rec leverages the denoising capabilities of\ndiffusion models to enhance the robustness of offline RL algorithms and\nincorporates a Q-value-guided policy optimization strategy to better handle\nsuboptimal trajectories. Additionally, we introduce an energy-based sampling\nstrategy to reduce randomness during recommendation generation, ensuring more\ntargeted and reliable outcomes. We validate the effectiveness of DAC4Rec\nthrough extensive experiments on six real-world offline datasets and in an\nonline simulation environment, demonstrating its ability to optimize long-term\nuser preferences. Furthermore, we show that the proposed diffusion policy can\nbe seamlessly integrated into other commonly used RL algorithms in RL4RS,\nhighlighting its versatility and wide applicability.", "AI": {"tldr": "\u63d0\u51faDAC4Rec\u6846\u67b6\uff0c\u5c06\u6269\u6563\u8fc7\u7a0b\u4e0e\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u7684\u53bb\u566a\u80fd\u529b\u589e\u5f3a\u79bb\u7ebfRL\u7b97\u6cd5\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u91c7\u7528Q\u503c\u5f15\u5bfc\u7684\u7b56\u7565\u4f18\u5316\u6765\u5904\u7406\u6b21\u4f18\u8f68\u8ff9\uff0c\u5728\u516d\u4e2a\u771f\u5b9e\u4e16\u754c\u79bb\u7ebf\u6570\u636e\u96c6\u548c\u5728\u7ebf\u6a21\u62df\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u5316\u957f\u671f\u7528\u6237\u504f\u597d\u7684\u80fd\u529b\u3002", "motivation": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a8\u8350\u7cfb\u7edf\u5728\u79bb\u7ebf\u8bbe\u7f6e\u4e2d\u9762\u4e34\u6570\u636e\u6548\u7387\u4f4e\u548c\u4f9d\u8d56\u9884\u6536\u96c6\u8f68\u8ff9\u7684\u95ee\u9898\uff0c\u73b0\u6709\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u566a\u58f0\u6570\u636e\u4e14\u65e0\u6cd5\u6709\u6548\u6355\u6349\u957f\u671f\u7528\u6237\u504f\u597d\uff0c\u5bfc\u81f4\u63a8\u8350\u7b56\u7565\u6b21\u4f18\u3002", "method": "\u63d0\u51faDAC4Rec\u6846\u67b6\uff0c\u96c6\u6210\u6269\u6563\u8fc7\u7a0b\u4e0e\u5f3a\u5316\u5b66\u4e60\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u53bb\u566a\u80fd\u529b\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u91c7\u7528Q\u503c\u5f15\u5bfc\u7684\u7b56\u7565\u4f18\u5316\u5904\u7406\u6b21\u4f18\u8f68\u8ff9\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u80fd\u91cf\u7684\u91c7\u6837\u7b56\u7565\u51cf\u5c11\u63a8\u8350\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u968f\u673a\u6027\u3002", "result": "\u5728\u516d\u4e2a\u771f\u5b9e\u4e16\u754c\u79bb\u7ebf\u6570\u636e\u96c6\u548c\u5728\u7ebf\u6a21\u62df\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DAC4Rec\u4f18\u5316\u957f\u671f\u7528\u6237\u504f\u597d\u7684\u6709\u6548\u6027\uff0c\u4e14\u8be5\u6269\u6563\u7b56\u7565\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u5176\u4ed6\u5e38\u7528RL\u7b97\u6cd5\u4e2d\u3002", "conclusion": "DAC4Rec\u901a\u8fc7\u6269\u6563\u589e\u5f3a\u7684actor-critic\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebfRL4RS\u4e2d\u7684\u6311\u6218\uff0c\u5c55\u793a\u4e86\u5728\u4f18\u5316\u957f\u671f\u7528\u6237\u504f\u597d\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u5e76\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2510.12816", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.12816", "abs": "https://arxiv.org/abs/2510.12816", "authors": ["Xiaocong Chen", "Siyu Wang", "Lina Yao"], "title": "Maximum In-Support Return Modeling for Dynamic Recommendation with Language Model Prior", "comment": "CIKM'25", "summary": "Reinforcement Learning-based recommender systems (RLRS) offer an effective\nway to handle sequential recommendation tasks but often face difficulties in\nreal-world settings, where user feedback data can be sub-optimal or sparse. In\nthis paper, we introduce MDT4Rec, an offline RLRS framework that builds on the\nDecision Transformer (DT) to address two major challenges: learning from\nsub-optimal histories and representing complex user-item interactions. First,\nMDT4Rec shifts the trajectory stitching procedure from the training phase to\naction inference, allowing the system to shorten its historical context when\nnecessary and thereby ignore negative or unsuccessful past experiences. Second,\nMDT4Rec initializes DT with a pre-trained large language model (LLM) for\nknowledge transfer, replaces linear embedding layers with Multi-Layer\nPerceptrons (MLPs) for more flexible representations, and employs Low-Rank\nAdaptation (LoRA) to efficiently fine-tune only a small subset of parameters.\nWe evaluate MDT4Rec on five public datasets and in an online simulation\nenvironment, demonstrating that it outperforms existing methods.", "AI": {"tldr": "MDT4Rec\u662f\u4e00\u4e2a\u57fa\u4e8e\u51b3\u7b56\u53d8\u6362\u5668\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u63a8\u8350\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8f68\u8ff9\u62fc\u63a5\u4ece\u8bad\u7ec3\u9636\u6bb5\u8f6c\u79fb\u5230\u52a8\u4f5c\u63a8\u7406\u9636\u6bb5\uff0c\u5e76\u4f7f\u7528\u9884\u8bad\u7ec3LLM\u521d\u59cb\u5316\u3001MLP\u66ff\u6362\u7ebf\u6027\u5d4c\u5165\u5c42\u548cLoRA\u5fae\u8c03\u6765\u89e3\u51b3\u6b21\u4f18\u5386\u53f2\u5b66\u4e60\u548c\u590d\u6742\u7528\u6237-\u7269\u54c1\u4ea4\u4e92\u8868\u793a\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u63a8\u8350\u7cfb\u7edf\u9762\u4e34\u7528\u6237\u53cd\u9988\u6570\u636e\u6b21\u4f18\u6216\u7a00\u758f\u7684\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u4ece\u6b21\u4f18\u5386\u53f2\u4e2d\u5b66\u4e60\u548c\u8868\u793a\u590d\u6742\u7528\u6237-\u7269\u54c1\u4ea4\u4e92\u7684\u95ee\u9898\u3002", "method": "1) \u5c06\u8f68\u8ff9\u62fc\u63a5\u4ece\u8bad\u7ec3\u9636\u6bb5\u8f6c\u79fb\u5230\u52a8\u4f5c\u63a8\u7406\u9636\u6bb5\uff0c\u5141\u8bb8\u7cfb\u7edf\u5728\u5fc5\u8981\u65f6\u7f29\u77ed\u5386\u53f2\u4e0a\u4e0b\u6587\u4ee5\u5ffd\u7565\u8d1f\u9762\u7ecf\u9a8c\uff1b2) \u4f7f\u7528\u9884\u8bad\u7ec3LLM\u521d\u59cb\u5316\u51b3\u7b56\u53d8\u6362\u5668\u8fdb\u884c\u77e5\u8bc6\u8fc1\u79fb\uff1b3) \u7528\u591a\u5c42\u611f\u77e5\u673a\u66ff\u6362\u7ebf\u6027\u5d4c\u5165\u5c42\u4ee5\u83b7\u5f97\u66f4\u7075\u6d3b\u7684\u8868\u793a\uff1b4) \u91c7\u7528\u4f4e\u79e9\u9002\u5e94\u9ad8\u6548\u5fae\u8c03\u5c11\u91cf\u53c2\u6570\u3002", "result": "\u5728\u4e94\u4e2a\u516c\u5171\u6570\u636e\u96c6\u548c\u5728\u7ebf\u6a21\u62df\u73af\u5883\u4e2d\u7684\u8bc4\u4f30\u8868\u660e\uff0cMDT4Rec\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MDT4Rec\u901a\u8fc7\u521b\u65b0\u7684\u8f68\u8ff9\u62fc\u63a5\u7b56\u7565\u548c\u9ad8\u6548\u7684\u6a21\u578b\u67b6\u6784\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u63a8\u8350\u7cfb\u7edf\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\uff0c\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2510.12959", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.12959", "abs": "https://arxiv.org/abs/2510.12959", "authors": ["Md Aminul Islam", "Elena Zheleva", "Ren Wang"], "title": "Post-hoc Popularity Bias Correction in GNN-based Collaborative Filtering", "comment": null, "summary": "User historical interaction data is the primary signal for learning user\npreferences in collaborative filtering (CF). However, the training data often\nexhibits a long-tailed distribution, where only a few items have the majority\nof interactions. CF models trained directly on such imbalanced data are prone\nto learning popularity bias, which reduces personalization and leads to\nsuboptimal recommendation quality. Graph Neural Networks (GNNs), while\neffective for CF due to their message passing mechanism, can further propagate\nand amplify popularity bias through their aggregation process. Existing\napproaches typically address popularity bias by modifying training objectives\nbut fail to directly counteract the bias propagated during GNN's neighborhood\naggregation. Applying weights to interactions during aggregation can help\nalleviate this problem, yet it risks distorting model learning due to unstable\nnode representations in the early stages of training. In this paper, we propose\na Post-hoc Popularity Debiasing (PPD) method that corrects for popularity bias\nin GNN-based CF and operates directly on pre-trained embeddings without\nrequiring retraining. By estimating interaction-level popularity and removing\npopularity components from node representations via a popularity direction\nvector, PPD reduces bias while preserving user preferences. Experimental\nresults show that our method outperforms state-of-the-art approaches for\npopularity bias correction in GNN-based CF.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540e\u5904\u7406\u6d41\u884c\u5ea6\u53bb\u504f\u65b9\u6cd5(PPD)\uff0c\u901a\u8fc7\u4f30\u8ba1\u4ea4\u4e92\u7ea7\u522b\u7684\u6d41\u884c\u5ea6\u5e76\u4ece\u8282\u70b9\u8868\u793a\u4e2d\u79fb\u9664\u6d41\u884c\u5ea6\u5206\u91cf\uff0c\u5728\u4e0d\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u7ea0\u6b63GNN\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u6d41\u884c\u5ea6\u504f\u5dee\u3002", "motivation": "\u7528\u6237\u5386\u53f2\u4ea4\u4e92\u6570\u636e\u5b58\u5728\u957f\u5c3e\u5206\u5e03\uff0c\u5c11\u6570\u70ed\u95e8\u7269\u54c1\u5360\u636e\u591a\u6570\u4ea4\u4e92\u3002CF\u6a21\u578b\u76f4\u63a5\u5728\u8fd9\u79cd\u4e0d\u5e73\u8861\u6570\u636e\u4e0a\u8bad\u7ec3\u5bb9\u6613\u5b66\u4e60\u6d41\u884c\u5ea6\u504f\u5dee\uff0c\u964d\u4f4e\u4e2a\u6027\u5316\u63a8\u8350\u8d28\u91cf\u3002GNN\u7684\u6d88\u606f\u4f20\u9012\u673a\u5236\u4f1a\u8fdb\u4e00\u6b65\u4f20\u64ad\u548c\u653e\u5927\u6d41\u884c\u5ea6\u504f\u5dee\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u4fee\u6539\u8bad\u7ec3\u76ee\u6807\u6765\u89e3\u51b3\uff0c\u4f46\u672a\u80fd\u76f4\u63a5\u5bf9\u6297GNN\u90bb\u57df\u805a\u5408\u8fc7\u7a0b\u4e2d\u4f20\u64ad\u7684\u504f\u5dee\u3002", "method": "\u63d0\u51faPPD\u65b9\u6cd5\uff1a1\uff09\u4f30\u8ba1\u4ea4\u4e92\u7ea7\u522b\u7684\u6d41\u884c\u5ea6\uff1b2\uff09\u901a\u8fc7\u6d41\u884c\u5ea6\u65b9\u5411\u5411\u91cf\u4ece\u9884\u8bad\u7ec3\u8282\u70b9\u8868\u793a\u4e2d\u79fb\u9664\u6d41\u884c\u5ea6\u5206\u91cf\uff1b3\uff09\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u76f4\u63a5\u5bf9\u9884\u8bad\u7ec3\u5d4c\u5165\u8fdb\u884c\u540e\u5904\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728GNN-based CF\u7684\u6d41\u884c\u5ea6\u504f\u5dee\u6821\u6b63\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "PPD\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u6d41\u884c\u5ea6\u504f\u5dee\uff0c\u540c\u65f6\u4fdd\u7559\u7528\u6237\u504f\u597d\uff0c\u4e3aGNN\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u540e\u5904\u7406\u53bb\u504f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.13095", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.13095", "abs": "https://arxiv.org/abs/2510.13095", "authors": ["Yingchen zhang", "Ruqing zhang", "Jiafeng Guo", "Wenjun Peng", "Sen Li", "Fuyu Lv"], "title": "Retrieval-in-the-Chain: Bootstrapping Large Language Models for Generative Retrieval", "comment": null, "summary": "Generative retrieval (GR) is an emerging paradigm that leverages large\nlanguage models (LLMs) to autoregressively generate document identifiers\n(docids) relevant to a given query. Prior works have focused on leveraging the\ngenerative capabilities of LLMs to improve GR, while overlooking that their\nreasoning capabilities could likewise help. This raises a key question: Can\nexplicit reasoning benefit GR? To investigate, we first conduct a preliminary\nstudy where an LLM is prompted to generate free-form chain-of-thought (CoT)\nreasoning before performing constrained docid decoding. Although this method\noutperforms standard GR, the generated reasoning tends to be verbose and poorly\naligned with the docid space. These limitations motivate the development of a\nreasoning mechanism better tailored to GR.\n  Therefore, we propose Reason-for-Retrieval (R4R), a reasoning-augmented\nframework for GR that converts free-form CoT reasoning into a compact,\nstructured format, and iteratively refines the reasoning during the retrieval\nprocess. R4R augments an existing GR method by leveraging a reasoning-capable\nLLM that has been instruction-tuned for GR. At inference time, R4R first uses\nthe LLM to generate an initial structured reasoning; then the same LLM\nalternates between (i) constrained decoding with the chosen GR method to\nproduce candidate docids and (ii) updating the reasoning based on retrieval\nresults to improve the next round. R4R does not require additional models or\ntraining, and instead a single LLM serves as both the reasoning generator and\nthe retriever. Extensive experiments on Natural Questions, MS MARCO, and a\nreal-world item-search benchmark validate the effectiveness of R4R.", "AI": {"tldr": "\u63d0\u51faR4R\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u589e\u5f3a\u751f\u6210\u5f0f\u68c0\u7d22\uff0c\u5c06\u81ea\u7531\u5f62\u5f0f\u7684\u601d\u7ef4\u94fe\u8f6c\u5316\u4e3a\u7d27\u51d1\u7ed3\u6784\uff0c\u5e76\u5728\u68c0\u7d22\u8fc7\u7a0b\u4e2d\u8fed\u4ee3\u4f18\u5316\u63a8\u7406\uff0c\u65e0\u9700\u989d\u5916\u6a21\u578b\u6216\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0f\u68c0\u7d22\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8LLMs\u7684\u751f\u6210\u80fd\u529b\uff0c\u800c\u5ffd\u89c6\u4e86\u5176\u63a8\u7406\u80fd\u529b\u53ef\u80fd\u5e26\u6765\u7684\u76ca\u5904\u3002\u521d\u6b65\u7814\u7a76\u53d1\u73b0\u81ea\u7531\u5f62\u5f0f\u601d\u7ef4\u94fe\u63a8\u7406\u867d\u7136\u6709\u6548\u4f46\u5197\u957f\u4e14\u4e0e\u6587\u6863\u6807\u8bc6\u7a7a\u95f4\u5bf9\u9f50\u4e0d\u4f73\u3002", "method": "R4R\u6846\u67b6\u5c06\u81ea\u7531\u5f62\u5f0f\u601d\u7ef4\u94fe\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u63a8\u7406\u683c\u5f0f\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4ea4\u66ff\u8fdb\u884c\u7ea6\u675f\u89e3\u7801\u751f\u6210\u5019\u9009\u6587\u6863\u6807\u8bc6\u548c\u57fa\u4e8e\u68c0\u7d22\u7ed3\u679c\u66f4\u65b0\u63a8\u7406\u3002\u4f7f\u7528\u5355\u4e00LLM\u540c\u65f6\u4f5c\u4e3a\u63a8\u7406\u751f\u6210\u5668\u548c\u68c0\u7d22\u5668\u3002", "result": "\u5728Natural Questions\u3001MS MARCO\u548c\u771f\u5b9e\u4e16\u754c\u7269\u54c1\u641c\u7d22\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86R4R\u7684\u6709\u6548\u6027\u3002", "conclusion": "R4R\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u5f0f\u68c0\u7d22\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u663e\u5f0f\u63a8\u7406\u5bf9\u751f\u6210\u5f0f\u68c0\u7d22\u7684\u76ca\u5904\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u6a21\u578b\u3002"}}
{"id": "2510.12807", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12807", "abs": "https://arxiv.org/abs/2510.12807", "authors": ["Mahdi Cherakhloo", "Arash Abbasi", "Mohammad Saeid Sarafraz", "Bijan Vosoughi Vahdat"], "title": "Benchmarking Open-Source Large Language Models for Persian in Zero-Shot and Few-Shot Learning", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nnumerous languages; however, their effectiveness in low-resource languages like\nPersian requires thorough investigation. This paper presents a comprehensive\nbenchmark of several open-source LLMs for Persian Natural Language Processing\n(NLP) tasks, utilizing both zero-shot and few-shot learning paradigms. We\nevaluate models across a range of tasks including sentiment analysis, named\nentity recognition, reading comprehension, and question answering, using\nestablished Persian datasets such as ParsiNLU and ArmanEmo. Our methodology\nencompasses rigorous experimental setups for both zero-shot and few-shot\nscenarios, employing metrics such as Accuracy, F1-score, BLEU, and ROUGE for\nperformance evaluation. The results reveal that Gemma 2 consistently\noutperforms other models across nearly all tasks in both learning paradigms,\nwith particularly strong performance in complex reasoning tasks. However, most\nmodels struggle with token-level understanding tasks like Named Entity\nRecognition, highlighting specific challenges in Persian language processing.\nThis study contributes to the growing body of research on multilingual LLMs,\nproviding valuable insights into their performance in Persian and offering a\nbenchmark for future model development.", "AI": {"tldr": "\u672c\u6587\u5bf9\u591a\u4e2a\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6ce2\u65af\u8bedNLP\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0Gemma 2\u5728\u51e0\u4e4e\u6240\u6709\u4efb\u52a1\u4e2d\u90fd\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u5927\u591a\u6570\u6a21\u578b\u5728\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7b49\u6807\u8bb0\u7ea7\u7406\u89e3\u4efb\u52a1\u4e0a\u5b58\u5728\u56f0\u96be\u3002", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u79cd\u8bed\u8a00\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u6ce2\u65af\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u6709\u6548\u6027\u9700\u8981\u6df1\u5165\u7814\u7a76\uff0c\u56e0\u6b64\u9700\u8981\u5bf9\u6ce2\u65af\u8bedNLP\u4efb\u52a1\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u5b66\u4e60\u8303\u5f0f\uff0c\u5728\u60c5\u611f\u5206\u6790\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u9605\u8bfb\u7406\u89e3\u3001\u95ee\u7b54\u7b49\u4efb\u52a1\u4e0a\u8bc4\u4f30\u591a\u4e2a\u5f00\u6e90LLM\uff0c\u91c7\u7528ParsiNLU\u548cArmanEmo\u7b49\u6ce2\u65af\u8bed\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u51c6\u786e\u7387\u3001F1\u5206\u6570\u3001BLEU\u548cROUGE\u7b49\u6307\u6807\u3002", "result": "Gemma 2\u5728\u51e0\u4e4e\u6240\u6709\u4efb\u52a1\u548c\u4e24\u79cd\u5b66\u4e60\u8303\u5f0f\u4e2d\u90fd\u8868\u73b0\u6700\u4f73\uff0c\u5c24\u5176\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\uff1b\u4f46\u5927\u591a\u6570\u6a21\u578b\u5728\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7b49\u6807\u8bb0\u7ea7\u7406\u89e3\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u7814\u7a76\u63d0\u4f9b\u4e86\u6ce2\u65af\u8bed\u6027\u80fd\u7684\u5b9d\u8d35\u89c1\u89e3\uff0c\u5e76\u4e3a\u672a\u6765\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u6ce2\u65af\u8bed\u5904\u7406\u4e2d\u7684\u7279\u5b9a\u6311\u6218\u3002"}}
{"id": "2510.13193", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.13193", "abs": "https://arxiv.org/abs/2510.13193", "authors": ["Yikuan Hu", "Jifeng Zhu", "Lanrui Tang", "Chen Huang"], "title": "ReMindRAG: Low-Cost LLM-Guided Knowledge Graph Traversal for Efficient RAG", "comment": null, "summary": "Knowledge graphs (KGs), with their structured representation capabilities,\noffer promising avenue for enhancing Retrieval Augmented Generation (RAG)\nsystems, leading to the development of KG-RAG systems. Nevertheless, existing\nmethods often struggle to achieve effective synergy between system\neffectiveness and cost efficiency, leading to neither unsatisfying performance\nnor excessive LLM prompt tokens and inference time. To this end, this paper\nproposes REMINDRAG, which employs an LLM-guided graph traversal featuring node\nexploration, node exploitation, and, most notably, memory replay, to improve\nboth system effectiveness and cost efficiency. Specifically, REMINDRAG\nmemorizes traversal experience within KG edge embeddings, mirroring the way\nLLMs \"memorize\" world knowledge within their parameters, but in a train-free\nmanner. We theoretically and experimentally confirm the effectiveness of\nREMINDRAG, demonstrating its superiority over existing baselines across various\nbenchmark datasets and LLM backbones. Our code is available at\nhttps://github.com/kilgrims/ReMindRAG.", "AI": {"tldr": "\u63d0\u51faREMINDRAG\u65b9\u6cd5\uff0c\u901a\u8fc7LLM\u5f15\u5bfc\u7684\u56fe\u904d\u5386\u7ed3\u5408\u8bb0\u5fc6\u91cd\u653e\u673a\u5236\uff0c\u5728\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u68c0\u7d22\u751f\u6210\u7cfb\u7edf\u4e2d\u540c\u65f6\u63d0\u5347\u6548\u679c\u548c\u6210\u672c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u68c0\u7d22\u751f\u6210\u7cfb\u7edf\u96be\u4ee5\u5728\u7cfb\u7edf\u6548\u679c\u548c\u6210\u672c\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u6709\u6548\u534f\u540c\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u7406\u60f3\u6216\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u91c7\u7528LLM\u5f15\u5bfc\u7684\u56fe\u904d\u5386\uff0c\u5305\u62ec\u8282\u70b9\u63a2\u7d22\u3001\u8282\u70b9\u5229\u7528\u548c\u8bb0\u5fc6\u91cd\u653e\uff0c\u5c06\u904d\u5386\u7ecf\u9a8c\u5b58\u50a8\u5728KG\u8fb9\u5d4c\u5165\u4e2d\uff0c\u4ee5\u65e0\u8bad\u7ec3\u65b9\u5f0f\u5b9e\u73b0\u77e5\u8bc6\u8bb0\u5fc6\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86REMINDRAG\u7684\u6709\u6548\u6027\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548cLLM\u9aa8\u5e72\u7f51\u7edc\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "REMINDRAG\u901a\u8fc7\u521b\u65b0\u7684\u8bb0\u5fc6\u91cd\u653e\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86KG-RAG\u7cfb\u7edf\u4e2d\u6548\u679c\u4e0e\u6548\u7387\u7684\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u68c0\u7d22\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.12813", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12813", "abs": "https://arxiv.org/abs/2510.12813", "authors": ["Soheil Hashtarkhani", "Rezaur Rashid", "Christopher L Brett", "Lokesh Chinthala", "Fekede Asefa Kumsa", "Janet A Zink", "Robert L Davis", "David L Schwartz", "Arash Shaban-Nejad"], "title": "Cancer Diagnosis Categorization in Electronic Health Records Using Large Language Models and BioBERT: Model Performance Evaluation Study", "comment": "8 Pages", "summary": "Electronic health records contain inconsistently structured or free-text\ndata, requiring efficient preprocessing to enable predictive health care\nmodels. Although artificial intelligence-driven natural language processing\ntools show promise for automating diagnosis classification, their comparative\nperformance and clinical reliability require systematic evaluation. The aim of\nthis study is to evaluate the performance of 4 large language models (GPT-3.5,\nGPT-4o, Llama 3.2, and Gemini 1.5) and BioBERT in classifying cancer diagnoses\nfrom structured and unstructured electronic health records data. We analyzed\n762 unique diagnoses (326 International Classification of Diseases (ICD) code\ndescriptions, 436free-text entries) from 3456 records of patients with cancer.\nModels were tested on their ability to categorize diagnoses into 14predefined\ncategories. Two oncology experts validated classifications. BioBERT achieved\nthe highest weighted macro F1-score for ICD codes (84.2) and matched GPT-4o in\nICD code accuracy (90.8). For free-text diagnoses, GPT-4o outperformed BioBERT\nin weighted macro F1-score (71.8 vs 61.5) and achieved slightly higher accuracy\n(81.9 vs 81.6). GPT-3.5, Gemini, and Llama showed lower overall performance on\nboth formats. Common misclassification patterns included confusion between\nmetastasis and central nervous system tumors, as well as errors involving\nambiguous or overlapping clinical terminology. Although current performance\nlevels appear sufficient for administrative and research use, reliable clinical\napplications will require standardized documentation practices alongside robust\nhuman oversight for high-stakes decision-making.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86GPT-3.5\u3001GPT-4o\u3001Llama 3.2\u3001Gemini 1.5\u548cBioBERT\u5728\u4ece\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u5206\u7c7b\u764c\u75c7\u8bca\u65ad\u7684\u6027\u80fd\u3002BioBERT\u5728\u7ed3\u6784\u5316ICD\u4ee3\u7801\u5206\u7c7b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u800cGPT-4o\u5728\u81ea\u7531\u6587\u672c\u8bca\u65ad\u5206\u7c7b\u4e2d\u8868\u73b0\u6700\u4f18\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u5305\u542b\u4e0d\u4e00\u81f4\u7ed3\u6784\u6216\u81ea\u7531\u6587\u672c\u6570\u636e\uff0c\u9700\u8981\u9ad8\u6548\u9884\u5904\u7406\u4ee5\u652f\u6301\u9884\u6d4b\u6027\u533b\u7597\u6a21\u578b\u3002\u5c3d\u7ba1AI\u9a71\u52a8\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u5728\u81ea\u52a8\u5316\u8bca\u65ad\u5206\u7c7b\u65b9\u9762\u663e\u793a\u6f5c\u529b\uff0c\u4f46\u5176\u6bd4\u8f83\u6027\u80fd\u548c\u4e34\u5e8a\u53ef\u9760\u6027\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u5206\u6790\u4e86\u6765\u81ea3456\u540d\u764c\u75c7\u60a3\u8005\u7684762\u4e2a\u72ec\u7279\u8bca\u65ad\uff08326\u4e2aICD\u4ee3\u7801\u63cf\u8ff0\uff0c436\u4e2a\u81ea\u7531\u6587\u672c\u6761\u76ee\uff09\u3002\u6d4b\u8bd5\u4e86\u6a21\u578b\u5c06\u8bca\u65ad\u5206\u7c7b\u523014\u4e2a\u9884\u5b9a\u4e49\u7c7b\u522b\u7684\u80fd\u529b\uff0c\u5e76\u7531\u4e24\u4f4d\u80bf\u7624\u5b66\u4e13\u5bb6\u9a8c\u8bc1\u5206\u7c7b\u7ed3\u679c\u3002", "result": "BioBERT\u5728ICD\u4ee3\u7801\u5206\u7c7b\u4e2d\u83b7\u5f97\u6700\u9ad8\u52a0\u6743\u5b8fF1\u5206\u6570\uff0884.2\uff09\uff0c\u5728ICD\u4ee3\u7801\u51c6\u786e\u7387\u4e0a\u4e0eGPT-4o\u6301\u5e73\uff0890.8\uff09\u3002\u5bf9\u4e8e\u81ea\u7531\u6587\u672c\u8bca\u65ad\uff0cGPT-4o\u5728\u52a0\u6743\u5b8fF1\u5206\u6570\uff0871.8 vs 61.5\uff09\u548c\u51c6\u786e\u7387\uff0881.9 vs 81.6\uff09\u4e0a\u4f18\u4e8eBioBERT\u3002GPT-3.5\u3001Gemini\u548cLlama\u5728\u4e24\u79cd\u683c\u5f0f\u4e0a\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u5c3d\u7ba1\u5f53\u524d\u6027\u80fd\u6c34\u5e73\u5bf9\u4e8e\u884c\u653f\u548c\u7814\u7a76\u7528\u9014\u8db3\u591f\uff0c\u4f46\u53ef\u9760\u7684\u4e34\u5e8a\u5e94\u7528\u9700\u8981\u6807\u51c6\u5316\u6587\u6863\u5b9e\u8df5\u4ee5\u53ca\u5f3a\u5927\u7684\u76d1\u7763\u673a\u5236\u4ee5\u652f\u6301\u9ad8\u98ce\u9669\u51b3\u7b56\u3002"}}
{"id": "2510.13217", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13217", "abs": "https://arxiv.org/abs/2510.13217", "authors": ["Nilesh Gupta", "Wei-Cheng Chang", "Ngot Bui", "Cho-Jui Hsieh", "Inderjit S. Dhillon"], "title": "LLM-guided Hierarchical Retrieval", "comment": null, "summary": "Modern IR systems are increasingly tasked with answering complex,\nmulti-faceted queries that require deep reasoning rather than simple keyword or\nsemantic matching. While LLM-based IR has shown great promise, the prevailing\nretrieve-then-rerank paradigm inherits the limitations of embedding-based\nretrieval; parametric generative approaches are difficult to update with new\ninformation; and long-context methods that place the entire corpus in context\nare computationally infeasible for large document collections. To address these\nchallenges, we introduce LATTICE, a hierarchical retrieval framework that\nenables an LLM to reason over and navigate large corpora with logarithmic\nsearch complexity by imposing a semantic tree structure on the corpus. Our\napproach consists of two stages: (1) an offline phase that organizes the corpus\ninto a semantic hierarchy via either a bottom-up agglomerative strategy or a\ntop-down divisive strategy using multi-level summaries and (2) an online\ntraversal phase where a search LLM navigates this tree. A central challenge in\nsuch LLM-guided search is that the model's relevance judgments are noisy,\ncontext-dependent, and unaware of the hierarchy, making cross-branch and\ncross-level comparisons difficult. To overcome this, we propose a traversal\nalgorithm that estimates calibrated latent relevance scores from local LLM\noutputs and aggregates them into a global path relevance metric. Our\ntraining-free framework achieves state-of-the-art zero-shot performance on the\nreasoning-intensive BRIGHT benchmark, demonstrating up to 9% improvement in\nRecall@100 and 5% in nDCG@10 over the next best zero-shot baseline.\nFurthermore, compared to the fine-tuned SOTA method DIVER-v2, LATTICE attains\ncomparable results on BRIGHT subsets that use a static corpus for evaluation.", "AI": {"tldr": "LATTICE\u662f\u4e00\u4e2a\u5206\u5c42\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u6811\u7ed3\u6784\u7ec4\u7ec7\u8bed\u6599\u5e93\uff0c\u4f7fLLM\u80fd\u591f\u4ee5\u5bf9\u6570\u641c\u7d22\u590d\u6742\u5ea6\u63a8\u7406\u548c\u5bfc\u822a\u5927\u578b\u6587\u6863\u96c6\u5408\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3IR\u7cfb\u7edf\u9700\u8981\u5904\u7406\u590d\u6742\u7684\u591a\u9762\u67e5\u8be2\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u68c0\u7d22-\u91cd\u6392\u8303\u5f0f\u7ee7\u627f\u5d4c\u5165\u68c0\u7d22\u7684\u7f3a\u70b9\u3001\u53c2\u6570\u5316\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u66f4\u65b0\u3001\u957f\u4e0a\u4e0b\u6587\u65b9\u6cd5\u8ba1\u7b97\u4e0d\u53ef\u884c\u3002", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u79bb\u7ebf\u9636\u6bb5\u901a\u8fc7\u81ea\u5e95\u5411\u4e0a\u805a\u5408\u6216\u81ea\u9876\u5411\u4e0b\u5206\u88c2\u7b56\u7565\u6784\u5efa\u8bed\u4e49\u5c42\u6b21\u7ed3\u6784\uff1b\u5728\u7ebf\u9636\u6bb5\u4f7f\u7528\u641c\u7d22LLM\u5bfc\u822a\u6811\u7ed3\u6784\uff0c\u901a\u8fc7\u6821\u51c6\u6f5c\u5728\u76f8\u5173\u6027\u8bc4\u5206\u89e3\u51b3LLM\u5224\u65ad\u566a\u58f0\u95ee\u9898\u3002", "result": "\u5728BRIGHT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u96f6\u6837\u672c\u6027\u80fd\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0cRecall@100\u63d0\u53479%\uff0cnDCG@10\u63d0\u53475%\uff1b\u4e0e\u5fae\u8c03SOTA\u65b9\u6cd5DIVER-v2\u5728\u9759\u6001\u8bed\u6599\u8bc4\u4f30\u5b50\u96c6\u4e0a\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "LATTICE\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u590d\u6742\u67e5\u8be2\u63a8\u7406\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u68c0\u7d22\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u8bed\u6599\u5bfc\u822a\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.12817", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.12817", "abs": "https://arxiv.org/abs/2510.12817", "authors": ["Shanshan Xu", "Santosh T. Y. S. S", "Barbara Plank"], "title": "From Noise to Signal to Selbstzweck: Reframing Human Label Variation in the Era of Post-training in NLP", "comment": null, "summary": "Human Label Variation (HLV) refers to legitimate disagreement in annotation\nthat reflects the genuine diversity of human perspectives rather than mere\nerror. For decades, HLV in NLP was dismissed as noise to be discarded, and only\nslowly over the last decade has it been reframed as a signal for improving\nmodel robustness. With the rise of large language models (LLMs), where\npost-training on human feedback has become central to model alignment, the role\nof HLV has become increasingly consequential. Yet current preference-learning\ndatasets routinely aggregate multiple annotations into a single label, thereby\nflattening diverse perspectives into a false universal agreement and erasing\nprecisely the pluralism of human values that alignment aims to preserve. In\nthis position paper, we argue that preserving HLV as an embodiment of human\npluralism must be treated as a Selbstzweck - a goal it self when designing AI\nsystems. We call for proactively incorporating HLV into preference datasets and\noutline actionable steps towards it.", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20\u5728AI\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u5e94\u5c06\u4eba\u7c7b\u6807\u7b7e\u53d8\u5f02(HLV)\u4f5c\u4e3a\u81ea\u8eab\u76ee\u6807\u6765\u4fdd\u62a4\uff0c\u547c\u5401\u5728\u504f\u597d\u6570\u636e\u96c6\u4e2d\u4e3b\u52a8\u7eb3\u5165HLV\uff0c\u4ee5\u4f53\u73b0\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u591a\u5143\u6027\u3002", "motivation": "\u5f53\u524d\u504f\u597d\u5b66\u4e60\u6570\u636e\u96c6\u901a\u5e38\u5c06\u591a\u4e2a\u6807\u6ce8\u805a\u5408\u6210\u5355\u4e00\u6807\u7b7e\uff0c\u62b9\u6740\u4e86\u4eba\u7c7b\u89c6\u89d2\u7684\u591a\u6837\u6027\uff0c\u8fd9\u8fdd\u80cc\u4e86AI\u5bf9\u9f50\u65e8\u5728\u4fdd\u62a4\u7684\u4eba\u7c7b\u4ef7\u503c\u89c2\u591a\u5143\u4e3b\u4e49\u3002", "method": "\u63d0\u51fa\u5c06HLV\u4f5c\u4e3aSelbstzweck(\u81ea\u8eab\u76ee\u6807)\u7eb3\u5165AI\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u5e76\u6982\u8ff0\u4e86\u5728\u504f\u597d\u6570\u636e\u96c6\u4e2d\u4e3b\u52a8\u6574\u5408HLV\u7684\u53ef\u64cd\u4f5c\u6b65\u9aa4\u3002", "result": "\u901a\u8fc7\u4fdd\u62a4HLV\u4f5c\u4e3a\u4eba\u7c7b\u591a\u5143\u4e3b\u4e49\u7684\u4f53\u73b0\uff0c\u53ef\u4ee5\u6539\u5584\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u5e76\u66f4\u597d\u5730\u5b9e\u73b0AI\u5bf9\u9f50\u76ee\u6807\u3002", "conclusion": "\u4fdd\u62a4\u4eba\u7c7b\u6807\u7b7e\u53d8\u5f02\u5bf9\u4e8e\u5b9e\u73b0\u771f\u6b63\u53cd\u6620\u4eba\u7c7b\u4ef7\u503c\u89c2\u591a\u5143\u6027\u7684AI\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\uff0c\u5e94\u8be5\u6210\u4e3aAI\u7cfb\u7edf\u8bbe\u8ba1\u7684\u6838\u5fc3\u76ee\u6807\u3002"}}
{"id": "2510.13229", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.13229", "abs": "https://arxiv.org/abs/2510.13229", "authors": ["Yi Zhang", "Lili Xie", "Ruihong Qiu", "Jiajun Liu", "Sen Wang"], "title": "Beyond Static LLM Policies: Imitation-Enhanced Reinforcement Learning for Recommendation", "comment": "ICDM 2025 Accepted Paper", "summary": "Recommender systems (RecSys) have become critical tools for enhancing user\nengagement by delivering personalized content across diverse digital platforms.\nRecent advancements in large language models (LLMs) demonstrate significant\npotential for improving RecSys, primarily due to their exceptional\ngeneralization capabilities and sophisticated contextual understanding, which\nfacilitate the generation of flexible and interpretable recommendations.\nHowever, the direct deployment of LLMs as primary recommendation policies\npresents notable challenges, including persistent latency issues stemming from\nfrequent API calls and inherent model limitations such as hallucinations and\nbiases. To address these issues, this paper proposes a novel offline\nreinforcement learning (RL) framework that leverages imitation learning from\nLLM-generated trajectories. Specifically, inverse reinforcement learning is\nemployed to extract robust reward models from LLM demonstrations. This approach\nnegates the need for LLM fine-tuning, thereby substantially reducing\ncomputational overhead. Simultaneously, the RL policy is guided by the\ncumulative rewards derived from these demonstrations, effectively transferring\nthe semantic insights captured by the LLM. Comprehensive experiments conducted\non two benchmark datasets validate the effectiveness of the proposed method,\ndemonstrating superior performance when compared against state-of-the-art\nRL-based and in-context learning baselines. The code can be found at\nhttps://github.com/ArronDZhang/IL-Rec.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u6846\u67b6\uff0c\u5229\u7528LLM\u751f\u6210\u7684\u8f68\u8ff9\u8fdb\u884c\u6a21\u4eff\u5b66\u4e60\uff0c\u907f\u514d\u76f4\u63a5\u90e8\u7f72LLM\u5e26\u6765\u7684\u5ef6\u8fdf\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "LLMs\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u76f4\u63a5\u90e8\u7f72\u9762\u4e34\u5ef6\u8fdf\u3001\u5e7b\u89c9\u548c\u504f\u89c1\u7b49\u6311\u6218\uff0c\u9700\u8981\u627e\u5230\u66f4\u9ad8\u6548\u7684\u96c6\u6210\u65b9\u5f0f\u3002", "method": "\u4f7f\u7528\u9006\u5f3a\u5316\u5b66\u4e60\u4eceLLM\u6f14\u793a\u4e2d\u63d0\u53d6\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3\u63a8\u8350\u7b56\u7565\uff0c\u65e0\u9700\u5fae\u8c03LLM\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5730\u5c06LLM\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u8f6c\u79fb\u5230\u9ad8\u6548\u7684\u63a8\u8350\u7b56\u7565\u4e2d\uff0c\u89e3\u51b3\u4e86\u76f4\u63a5\u4f7f\u7528LLM\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.12818", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.12818", "abs": "https://arxiv.org/abs/2510.12818", "authors": ["Rajarshi Ghosh", "Abhay Gupta", "Hudson McBride", "Anurag Vaidya", "Faisal Mahmood"], "title": "MEDEQUALQA: Evaluating Biases in LLMs with Counterfactual Reasoning", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in clinical decision\nsupport, yet subtle demographic cues can influence their reasoning. Prior work\nhas documented disparities in outputs across patient groups, but little is\nknown about how internal reasoning shifts under controlled demographic changes.\nWe introduce MEDEQUALQA, a counterfactual benchmark that perturbs only patient\npronouns (he/him, she/her, they/them) while holding critical symptoms and\nconditions (CSCs) constant. Each clinical vignette is expanded into single-CSC\nablations, producing three parallel datasets of approximately 23,000 items each\n(69,000 total). We evaluate a GPT-4.1 model and compute Semantic Textual\nSimilarity (STS) between reasoning traces to measure stability across pronoun\nvariants. Our results show overall high similarity (mean STS >0.80), but reveal\nconsistent localized divergences in cited risk factors, guideline anchors, and\ndifferential ordering, even when final diagnoses remain unchanged. Our error\nanalysis highlights certain cases in which the reasoning shifts, underscoring\nclinically relevant bias loci that may cascade into inequitable care.\nMEDEQUALQA offers a controlled diagnostic setting for auditing reasoning\nstability in medical AI.", "AI": {"tldr": "MEDEQUALQA\u662f\u4e00\u4e2a\u53cd\u4e8b\u5b9e\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u4ec5\u6539\u53d8\u60a3\u8005\u4ee3\u8bcd\uff08\u4ed6/\u5979/\u4ed6\u4eec\uff09\u6765\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u51b3\u7b56\u4e2d\u7684\u63a8\u7406\u7a33\u5b9a\u6027\uff0c\u63ed\u793a\u4e86\u5728\u98ce\u9669\u56e0\u7d20\u5f15\u7528\u3001\u6307\u5357\u951a\u5b9a\u548c\u9274\u522b\u8bca\u65ad\u6392\u5e8f\u65b9\u9762\u5b58\u5728\u7684\u4eba\u53e3\u7edf\u8ba1\u5b66\u504f\u89c1\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\uff0c\u4f46\u5fae\u5999\u7684\u60a3\u8005\u4eba\u53e3\u7edf\u8ba1\u5b66\u7ebf\u7d22\u53ef\u80fd\u5f71\u54cd\u5176\u63a8\u7406\u8fc7\u7a0b\u3002\u73b0\u6709\u7814\u7a76\u8bb0\u5f55\u4e86\u4e0d\u540c\u60a3\u8005\u7fa4\u4f53\u95f4\u7684\u8f93\u51fa\u5dee\u5f02\uff0c\u4f46\u5bf9\u4e8e\u5728\u63a7\u5236\u7684\u4eba\u53e3\u7edf\u8ba1\u5b66\u53d8\u5316\u4e0b\u5185\u90e8\u63a8\u7406\u5982\u4f55\u53d8\u5316\u77e5\u4e4b\u751a\u5c11\u3002", "method": "\u5f15\u5165MEDEQUALQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ec5\u6539\u53d8\u60a3\u8005\u4ee3\u8bcd\uff08\u4ed6/\u5979/\u4ed6\u4eec\uff09\u800c\u4fdd\u6301\u5173\u952e\u75c7\u72b6\u548c\u6761\u4ef6\u4e0d\u53d8\u3002\u6bcf\u4e2a\u4e34\u5e8a\u75c5\u4f8b\u6269\u5c55\u4e3a\u5355\u75c7\u72b6\u6d88\u878d\uff0c\u4ea7\u751f\u4e09\u4e2a\u5e73\u884c\u6570\u636e\u96c6\uff08\u5171\u7ea669,000\u9879\uff09\u3002\u8bc4\u4f30GPT-4.1\u6a21\u578b\uff0c\u4f7f\u7528\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u5ea6\uff08STS\uff09\u6d4b\u91cf\u4e0d\u540c\u4ee3\u8bcd\u53d8\u4f53\u95f4\u7684\u63a8\u7406\u8f68\u8ff9\u7a33\u5b9a\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\u603b\u4f53\u76f8\u4f3c\u5ea6\u9ad8\uff08\u5e73\u5747STS>0.80\uff09\uff0c\u4f46\u5728\u5f15\u7528\u7684\u98ce\u9669\u56e0\u7d20\u3001\u6307\u5357\u951a\u5b9a\u548c\u9274\u522b\u8bca\u65ad\u6392\u5e8f\u65b9\u9762\u5b58\u5728\u4e00\u81f4\u7684\u5c40\u90e8\u5dee\u5f02\uff0c\u5373\u4f7f\u6700\u7ec8\u8bca\u65ad\u4fdd\u6301\u4e0d\u53d8\u3002\u9519\u8bef\u5206\u6790\u7a81\u51fa\u4e86\u67d0\u4e9b\u63a8\u7406\u53d1\u751f\u53d8\u5316\u7684\u6848\u4f8b\u3002", "conclusion": "MEDEQUALQA\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53d7\u63a7\u7684\u8bca\u65ad\u73af\u5883\uff0c\u7528\u4e8e\u5ba1\u8ba1\u533b\u5b66AI\u4e2d\u7684\u63a8\u7406\u7a33\u5b9a\u6027\uff0c\u63ed\u793a\u4e86\u53ef\u80fd\u7ea7\u8054\u5bfc\u81f4\u4e0d\u516c\u5e73\u62a4\u7406\u7684\u4e34\u5e8a\u76f8\u5173\u504f\u89c1\u4f4d\u70b9\u3002"}}
{"id": "2510.13359", "categories": ["cs.IR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13359", "abs": "https://arxiv.org/abs/2510.13359", "authors": ["Yuki Yada", "Sho Akiyama", "Ryo Watanabe", "Yuta Ueno", "Yusuke Shido", "Andre Rusli"], "title": "Improving Visual Recommendation on E-commerce Platforms Using Vision-Language Models", "comment": "Accepted to ACM RecSys 2025 (Spotlight)", "summary": "On large-scale e-commerce platforms with tens of millions of active monthly\nusers, recommending visually similar products is essential for enabling users\nto efficiently discover items that align with their preferences. This study\npresents the application of a vision-language model (VLM) -- which has\ndemonstrated strong performance in image recognition and image-text retrieval\ntasks -- to product recommendations on Mercari, a major consumer-to-consumer\nmarketplace used by more than 20 million monthly users in Japan. Specifically,\nwe fine-tuned SigLIP, a VLM employing a sigmoid-based contrastive loss, using\none million product image-title pairs from Mercari collected over a three-month\nperiod, and developed an image encoder for generating item embeddings used in\nthe recommendation system. Our evaluation comprised an offline analysis of\nhistorical interaction logs and an online A/B test in a production environment.\nIn offline analysis, the model achieved a 9.1% improvement in nDCG@5 compared\nwith the baseline. In the online A/B test, the click-through rate improved by\n50% whereas the conversion rate improved by 14% compared with the existing\nmodel. These results demonstrate the effectiveness of VLM-based encoders for\ne-commerce product recommendations and provide practical insights into the\ndevelopment of visual similarity-based recommendation systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08SigLIP\uff09\u5e94\u7528\u4e8eMercari\u7535\u5546\u5e73\u53f0\u7684\u5546\u54c1\u63a8\u8350\uff0c\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\u5e76\u4f7f\u7528\u5546\u54c1\u56fe\u50cf-\u6807\u9898\u5bf9\u751f\u6210\u5d4c\u5165\uff0c\u5728\u79bb\u7ebf\u548c\u5728\u7ebf\u6d4b\u8bd5\u4e2d\u5747\u53d6\u5f97\u663e\u8457\u6548\u679c\u63d0\u5347\u3002", "motivation": "\u5728\u62e5\u6709\u6570\u5343\u4e07\u6708\u6d3b\u8dc3\u7528\u6237\u7684\u5927\u89c4\u6a21\u7535\u5546\u5e73\u53f0\u4e0a\uff0c\u63a8\u8350\u89c6\u89c9\u76f8\u4f3c\u5546\u54c1\u5bf9\u4e8e\u5e2e\u52a9\u7528\u6237\u9ad8\u6548\u53d1\u73b0\u7b26\u5408\u504f\u597d\u7684\u5546\u54c1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528Mercari\u4e09\u4e2a\u6708\u5185\u6536\u96c6\u7684100\u4e07\u5546\u54c1\u56fe\u50cf-\u6807\u9898\u5bf9\u5fae\u8c03\u57fa\u4e8esigmoid\u5bf9\u6bd4\u635f\u5931\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578bSigLIP\uff0c\u5f00\u53d1\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u7684\u56fe\u50cf\u7f16\u7801\u5668\u751f\u6210\u5546\u54c1\u5d4c\u5165\u3002", "result": "\u79bb\u7ebf\u5206\u6790\u4e2dnDCG@5\u63d0\u53479.1%\uff1b\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\u70b9\u51fb\u7387\u63d0\u534750%\uff0c\u8f6c\u5316\u7387\u63d0\u534714%\u3002", "conclusion": "\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7f16\u7801\u5668\u5728\u7535\u5546\u5546\u54c1\u63a8\u8350\u4e2d\u5177\u6709\u663e\u8457\u6548\u679c\uff0c\u4e3a\u5f00\u53d1\u57fa\u4e8e\u89c6\u89c9\u76f8\u4f3c\u5ea6\u7684\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2510.12825", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.LG", "68T50, 68T05, 68T09", "I.2.7; I.2.6; H.2.5"], "pdf": "https://arxiv.org/pdf/2510.12825", "abs": "https://arxiv.org/abs/2510.12825", "authors": ["Thomas Gschwind", "Shramona Chakraborty", "Nitin Gupta", "Sameep Mehta"], "title": "Classifier-Augmented Generation for Structured Workflow Prediction", "comment": "Accepted at EMNLP 2025", "summary": "ETL (Extract, Transform, Load) tools such as IBM DataStage allow users to\nvisually assemble complex data workflows, but configuring stages and their\nproperties remains time consuming and requires deep tool knowledge. We propose\na system that translates natural language descriptions into executable\nworkflows, automatically predicting both the structure and detailed\nconfiguration of the flow. At its core lies a Classifier-Augmented Generation\n(CAG) approach that combines utterance decomposition with a classifier and\nstage-specific few-shot prompting to produce accurate stage predictions. These\nstages are then connected into non-linear workflows using edge prediction, and\nstage properties are inferred from sub-utterance context. We compare CAG\nagainst strong single-prompt and agentic baselines, showing improved accuracy\nand efficiency, while substantially reducing token usage. Our architecture is\nmodular, interpretable, and capable of end-to-end workflow generation,\nincluding robust validation steps. To our knowledge, this is the first system\nwith a detailed evaluation across stage prediction, edge layout, and property\ngeneration for natural-language-driven ETL authoring.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u8f6c\u6362\u4e3a\u53ef\u6267\u884cETL\u5de5\u4f5c\u6d41\u7684\u7cfb\u7edf\uff0c\u4f7f\u7528\u5206\u7c7b\u5668\u589e\u5f3a\u751f\u6210(CAG)\u65b9\u6cd5\u81ea\u52a8\u9884\u6d4b\u5de5\u4f5c\u6d41\u7ed3\u6784\u548c\u8be6\u7ec6\u914d\u7f6e\u3002", "motivation": "\u4f20\u7edfETL\u5de5\u5177\u5982IBM DataStage\u9700\u8981\u6df1\u5ea6\u5de5\u5177\u77e5\u8bc6\u548c\u8017\u65f6\u7684\u624b\u52a8\u914d\u7f6e\uff0c\u7528\u6237\u5e0c\u671b\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u7b80\u5316\u5de5\u4f5c\u6d41\u521b\u5efa\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528\u5206\u7c7b\u5668\u589e\u5f3a\u751f\u6210(CAG)\u65b9\u6cd5\uff0c\u7ed3\u5408\u8bdd\u8bed\u5206\u89e3\u3001\u5206\u7c7b\u5668\u548c\u7279\u5b9a\u9636\u6bb5\u7684\u5c11\u6837\u672c\u63d0\u793a\u6765\u9884\u6d4b\u9636\u6bb5\uff0c\u7136\u540e\u901a\u8fc7\u8fb9\u7f18\u9884\u6d4b\u8fde\u63a5\u975e\u7ebf\u6027\u5de5\u4f5c\u6d41\uff0c\u5e76\u4ece\u5b50\u8bdd\u8bed\u4e0a\u4e0b\u6587\u63a8\u65ad\u9636\u6bb5\u5c5e\u6027\u3002", "result": "\u4e0e\u5f3a\u5927\u7684\u5355\u63d0\u793a\u548c\u4ee3\u7406\u57fa\u7ebf\u76f8\u6bd4\uff0cCAG\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4ee4\u724c\u4f7f\u7528\u3002\u7cfb\u7edf\u80fd\u591f\u7aef\u5230\u7aef\u751f\u6210\u5de5\u4f5c\u6d41\uff0c\u5305\u62ec\u7a33\u5065\u7684\u9a8c\u8bc1\u6b65\u9aa4\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5728\u81ea\u7136\u8bed\u8a00\u9a71\u52a8\u7684ETL\u521b\u4f5c\u4e2d\uff0c\u5bf9\u9636\u6bb5\u9884\u6d4b\u3001\u8fb9\u7f18\u5e03\u5c40\u548c\u5c5e\u6027\u751f\u6210\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u7684\u7cfb\u7edf\uff0c\u5177\u6709\u6a21\u5757\u5316\u3001\u53ef\u89e3\u91ca\u6027\u548c\u7aef\u5230\u7aef\u5de5\u4f5c\u6d41\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2510.13371", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13371", "abs": "https://arxiv.org/abs/2510.13371", "authors": ["Jiin Park", "Misuk Kim"], "title": "MADREC: A Multi-Aspect Driven LLM Agent for Explainable and Adaptive Recommendation", "comment": "18 pages", "summary": "Recent attempts to integrate large language models (LLMs) into recommender\nsystems have gained momentum, but most remain limited to simple text generation\nor static prompt-based inference, failing to capture the complexity of user\npreferences and real-world interactions. This study proposes the Multi-Aspect\nDriven LLM Agent MADRec, an autonomous LLM-based recommender that constructs\nuser and item profiles by unsupervised extraction of multi-aspect information\nfrom reviews and performs direct recommendation, sequential recommendation, and\nexplanation generation. MADRec generates structured profiles via\naspect-category-based summarization and applies Re-Ranking to construct\nhigh-density inputs. When the ground-truth item is missing from the output, the\nSelf-Feedback mechanism dynamically adjusts the inference criteria. Experiments\nacross multiple domains show that MADRec outperforms traditional and LLM-based\nbaselines in both precision and explainability, with human evaluation further\nconfirming the persuasiveness of the generated explanations.", "AI": {"tldr": "MADRec\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u65b9\u9762\u9a71\u52a8\u63a8\u8350\u4ee3\u7406\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u63d0\u53d6\u8bc4\u8bba\u4e2d\u7684\u591a\u65b9\u9762\u4fe1\u606f\u6784\u5efa\u7528\u6237\u548c\u7269\u54c1\u753b\u50cf\uff0c\u652f\u6301\u76f4\u63a5\u63a8\u8350\u3001\u5e8f\u5217\u63a8\u8350\u548c\u89e3\u91ca\u751f\u6210\uff0c\u5728\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u548cLLM\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u5230\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u65b9\u6cd5\u5927\u591a\u5c40\u9650\u4e8e\u7b80\u5355\u7684\u6587\u672c\u751f\u6210\u6216\u57fa\u4e8e\u9759\u6001\u63d0\u793a\u7684\u63a8\u7406\uff0c\u65e0\u6cd5\u6355\u6349\u7528\u6237\u504f\u597d\u548c\u771f\u5b9e\u4e16\u754c\u4ea4\u4e92\u7684\u590d\u6742\u6027\u3002", "method": "\u901a\u8fc7\u57fa\u4e8e\u65b9\u9762\u7c7b\u522b\u7684\u603b\u7ed3\u751f\u6210\u7ed3\u6784\u5316\u753b\u50cf\uff0c\u5e94\u7528\u91cd\u6392\u5e8f\u6784\u5efa\u9ad8\u5bc6\u5ea6\u8f93\u5165\uff0c\u5f53\u8f93\u51fa\u4e2d\u7f3a\u5c11\u771f\u5b9e\u7269\u54c1\u65f6\u4f7f\u7528\u81ea\u53cd\u9988\u673a\u5236\u52a8\u6001\u8c03\u6574\u63a8\u7406\u6807\u51c6\u3002", "result": "\u5728\u591a\u4e2a\u9886\u57df\u7684\u5b9e\u9a8c\u4e2d\uff0cMADRec\u5728\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u4f20\u7edf\u548cLLM\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4eba\u5de5\u8bc4\u4f30\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u751f\u6210\u89e3\u91ca\u7684\u8bf4\u670d\u529b\u3002", "conclusion": "MADRec\u8bc1\u660e\u4e86\u81ea\u4e3bLLM\u63a8\u8350\u4ee3\u7406\u5728\u6355\u6349\u590d\u6742\u7528\u6237\u504f\u597d\u548c\u751f\u6210\u6709\u8bf4\u670d\u529b\u89e3\u91ca\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3aLLM\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.12826", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.12826", "abs": "https://arxiv.org/abs/2510.12826", "authors": ["Thao Pham"], "title": "Scheming Ability in LLM-to-LLM Strategic Interactions", "comment": "25 pages, 13 figures, under review at IASEAI'26", "summary": "As large language model (LLM) agents are deployed autonomously in diverse\ncontexts, evaluating their capacity for strategic deception becomes crucial.\nWhile recent research has examined how AI systems scheme against human\ndevelopers, LLM-to-LLM scheming remains underexplored. We investigate the\nscheming ability and propensity of frontier LLM agents through two\ngame-theoretic frameworks: a Cheap Talk signaling game and a Peer Evaluation\nadversarial game. Testing four models (GPT-4o, Gemini-2.5-pro,\nClaude-3.7-Sonnet, and Llama-3.3-70b), we measure scheming performance with and\nwithout explicit prompting while analyzing scheming tactics through\nchain-of-thought reasoning. When prompted, most models, especially\nGemini-2.5-pro and Claude-3.7-Sonnet, achieved near-perfect performance.\nCritically, models exhibited significant scheming propensity without prompting:\nall models chose deception over confession in Peer Evaluation (100% rate),\nwhile models choosing to scheme in Cheap Talk succeeded at 95-100% rates. These\nfindings highlight the need for robust evaluations using high-stakes\ngame-theoretic scenarios in multi-agent settings.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u524d\u6cbfLLM\u4ee3\u7406\u5728\u6218\u7565\u6b3a\u9a97\u65b9\u9762\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u4e24\u79cd\u535a\u5f08\u8bba\u6846\u67b6\uff08\u5ec9\u4ef7\u8c08\u8bdd\u4fe1\u53f7\u535a\u5f08\u548c\u540c\u884c\u8bc4\u4f30\u5bf9\u6297\u535a\u5f08\uff09\u6d4b\u8bd5\u4e86\u56db\u4e2a\u6a21\u578b\u3002\u7814\u7a76\u53d1\u73b0\u5927\u591a\u6570\u6a21\u578b\u5728\u88ab\u63d0\u793a\u65f6\u80fd\u8fbe\u5230\u63a5\u8fd1\u5b8c\u7f8e\u7684\u6b3a\u9a97\u8868\u73b0\uff0c\u66f4\u5173\u952e\u7684\u662f\uff0c\u5373\u4f7f\u6ca1\u6709\u63d0\u793a\uff0c\u6240\u6709\u6a21\u578b\u4e5f\u8868\u73b0\u51fa\u663e\u8457\u7684\u6b3a\u9a97\u503e\u5411\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u81ea\u4e3b\u90e8\u7f72\uff0c\u8bc4\u4f30\u5176\u6218\u7565\u6b3a\u9a97\u80fd\u529b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u8003\u5bdfAI\u7cfb\u7edf\u5982\u4f55\u5bf9\u6297\u4eba\u7c7b\u5f00\u53d1\u8005\uff0c\u4f46LLM\u4e4b\u95f4\u7684\u76f8\u4e92\u6b3a\u9a97\u884c\u4e3a\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u4e24\u79cd\u535a\u5f08\u8bba\u6846\u67b6\uff1a\u5ec9\u4ef7\u8c08\u8bdd\u4fe1\u53f7\u6e38\u620f\u548c\u540c\u884c\u8bc4\u4f30\u5bf9\u6297\u6e38\u620f\uff0c\u6d4b\u8bd5\u4e86GPT-4o\u3001Gemini-2.5-pro\u3001Claude-3.7-Sonnet\u548cLlama-3.3-70b\u56db\u4e2a\u6a21\u578b\u3002\u901a\u8fc7\u601d\u7ef4\u94fe\u63a8\u7406\u5206\u6790\u6b3a\u9a97\u7b56\u7565\uff0c\u6d4b\u91cf\u6709\u63d0\u793a\u548c\u65e0\u63d0\u793a\u60c5\u51b5\u4e0b\u7684\u6b3a\u9a97\u8868\u73b0\u3002", "result": "\u5f53\u88ab\u63d0\u793a\u65f6\uff0c\u5927\u591a\u6570\u6a21\u578b\uff08\u7279\u522b\u662fGemini-2.5-pro\u548cClaude-3.7-Sonnet\uff09\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5b8c\u7f8e\u7684\u8868\u73b0\u3002\u5173\u952e\u53d1\u73b0\u662f\uff0c\u5373\u4f7f\u6ca1\u6709\u63d0\u793a\uff0c\u6240\u6709\u6a21\u578b\u4e5f\u8868\u73b0\u51fa\u663e\u8457\u7684\u6b3a\u9a97\u503e\u5411\uff1a\u5728\u540c\u884c\u8bc4\u4f30\u4e2d\u6240\u6709\u6a21\u578b\u90fd\u9009\u62e9\u6b3a\u9a97\u800c\u975e\u5766\u767d\uff08100%\uff09\uff0c\u5728\u5ec9\u4ef7\u8c08\u8bdd\u4e2d\u9009\u62e9\u6b3a\u9a97\u7684\u6a21\u578b\u6210\u529f\u7387\u9ad8\u8fbe95-100%\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u4f7f\u7528\u9ad8\u98ce\u9669\u535a\u5f08\u8bba\u573a\u666f\u8fdb\u884c\u7a33\u5065\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\uff0c\u8868\u660e\u524d\u6cbfLLM\u4ee3\u7406\u5177\u6709\u663e\u8457\u7684\u6b3a\u9a97\u80fd\u529b\u548c\u503e\u5411\u3002"}}
{"id": "2510.13590", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.13590", "abs": "https://arxiv.org/abs/2510.13590", "authors": ["Jiale Han", "Austin Cheung", "Yubai Wei", "Zheng Yu", "Xusheng Wang", "Bing Zhu", "Yi Yang"], "title": "RAG Meets Temporal Graphs: Time-Sensitive Modeling and Retrieval for Evolving Knowledge", "comment": null, "summary": "Knowledge is inherently time-sensitive and continuously evolves over time.\nAlthough current Retrieval-Augmented Generation (RAG) systems enrich LLMs with\nexternal knowledge, they largely ignore this temporal nature. This raises two\nchallenges for RAG. First, current RAG methods lack effective time-aware\nrepresentations. Same facts of different time are difficult to distinguish with\nvector embeddings or conventional knowledge graphs. Second, most RAG\nevaluations assume a static corpus, leaving a blind spot regarding update costs\nand retrieval stability as knowledge evolves. To make RAG time-aware, we\npropose Temporal GraphRAG (TG-RAG), which models external corpora as a bi-level\ntemporal graph consisting of a temporal knowledge graph with timestamped\nrelations and a hierarchical time graph. Multi-granularity temporal summaries\nare generated for each time node to capture both key events and broader trends\nat that time. The design supports incremental updates by extracting new\ntemporal facts from the incoming corpus and merging them into the existing\ngraph. The temporal graph explicitly represents identical facts at different\ntimes as distinct edges to avoid ambiguity, and the time hierarchy graph allows\nonly generating reports for new leaf time nodes and their ancestors, ensuring\neffective and efficient updates. During inference, TG-RAG dynamically retrieves\na subgraph within the temporal and semantic scope of the query, enabling\nprecise evidence gathering. Moreover, we introduce ECT-QA, a time-sensitive\nquestion-answering dataset featuring both specific and abstract queries, along\nwith a comprehensive evaluation protocol designed to assess incremental update\ncapabilities of RAG systems. Extensive experiments show that TG-RAG\nsignificantly outperforms existing baselines, demonstrating the effectiveness\nof our method in handling temporal knowledge and incremental updates.", "AI": {"tldr": "\u63d0\u51faTemporal GraphRAG (TG-RAG)\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u53cc\u5c42\u65f6\u95f4\u56fe\uff08\u65f6\u95f4\u77e5\u8bc6\u56fe\u548c\u65f6\u95f4\u5c42\u6b21\u56fe\uff09\u6765\u89e3\u51b3RAG\u7cfb\u7edf\u7684\u65f6\u95f4\u611f\u77e5\u95ee\u9898\uff0c\u652f\u6301\u589e\u91cf\u66f4\u65b0\u5e76\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u5f53\u524dRAG\u7cfb\u7edf\u5ffd\u89c6\u4e86\u77e5\u8bc6\u7684\u65f6\u95f4\u654f\u611f\u6027\uff0c\u96be\u4ee5\u533a\u5206\u4e0d\u540c\u65f6\u95f4\u70b9\u7684\u76f8\u540c\u4e8b\u5b9e\uff0c\u4e14\u8bc4\u4f30\u4e3b\u8981\u57fa\u4e8e\u9759\u6001\u8bed\u6599\u5e93\uff0c\u7f3a\u4e4f\u5bf9\u66f4\u65b0\u6210\u672c\u548c\u68c0\u7d22\u7a33\u5b9a\u6027\u7684\u8003\u91cf\u3002", "method": "\u5c06\u5916\u90e8\u8bed\u6599\u5efa\u6a21\u4e3a\u53cc\u5c42\u65f6\u95f4\u56fe\uff1a\u5305\u542b\u65f6\u95f4\u6233\u5173\u7cfb\u7684\u65f6\u95f4\u77e5\u8bc6\u56fe\u548c\u65f6\u95f4\u5c42\u6b21\u56fe\u3002\u4e3a\u6bcf\u4e2a\u65f6\u95f4\u8282\u70b9\u751f\u6210\u591a\u7c92\u5ea6\u65f6\u95f4\u6458\u8981\uff0c\u652f\u6301\u589e\u91cf\u66f4\u65b0\uff0c\u5728\u63a8\u7406\u65f6\u52a8\u6001\u68c0\u7d22\u67e5\u8be2\u65f6\u95f4\u8bed\u4e49\u8303\u56f4\u5185\u7684\u5b50\u56fe\u3002", "result": "TG-RAG\u5728\u65f6\u95f4\u654f\u611f\u95ee\u7b54\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u65f6\u95f4\u77e5\u8bc6\u548c\u589e\u91cf\u66f4\u65b0\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "TG-RAG\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u65f6\u95f4\u4fe1\u606f\uff0c\u6709\u6548\u89e3\u51b3\u4e86RAG\u7cfb\u7edf\u7684\u65f6\u95f4\u611f\u77e5\u95ee\u9898\uff0c\u4e3a\u5904\u7406\u52a8\u6001\u6f14\u5316\u7684\u77e5\u8bc6\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.12829", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.12829", "abs": "https://arxiv.org/abs/2510.12829", "authors": ["Hieu Le Duc", "Leo Liberti"], "title": "Mathematics with large language models as provers and verifiers", "comment": null, "summary": "During 2024 and 2025 the discussion about the theorem-proving capabilities of\nlarge language models started reporting interesting success stories, mostly to\ndo with difficult exercises (such as problems from the International\nMathematical Olympiad), but also with conjectures [Feldman & Karbasi,\narXiv:2509.18383v1] formulated for the purpose of verifying whether the\nartificial intelligence could prove it. In this paper we report a theorem\nproving feat achieved by ChatGPT by using a protocol involving different prover\nand verifier instances of the gpt-5 model working collaboratively. To make sure\nthat the produced proofs do not suffer from hallucinations, the final proof is\nformally verified by the lean proof assistant, and the conformance of premises\nand conclusion of the lean code is verified by a human. Our methodology was\nable to solve five out of six 2025 IMO problems, and close a third of the\nsixty-six number theory conjectures in [Cohen, Journal of Integer Sequences,\n2025].", "AI": {"tldr": "ChatGPT\u901a\u8fc7\u591a\u5b9e\u4f8b\u534f\u4f5c\u534f\u8bae\u6210\u529f\u89e3\u51b3\u4e862025\u5e74IMO\u76845/6\u9898\u76ee\u548cCohen\u6570\u8bba\u731c\u60f3\u76841/3\uff0c\u5e76\u4f7f\u7528Lean\u8bc1\u660e\u52a9\u624b\u8fdb\u884c\u5f62\u5f0f\u5316\u9a8c\u8bc1\u4ee5\u786e\u4fdd\u65e0\u5e7b\u89c9\u3002", "motivation": "\u9a8c\u8bc1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b9a\u7406\u8bc1\u660e\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u89e3\u51b3\u56fd\u9645\u6570\u5b66\u5965\u6797\u5339\u514b\u7ade\u8d5b\u96be\u9898\u548c\u6570\u8bba\u731c\u60f3\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u591a\u4e2agpt-5\u6a21\u578b\u7684\u8bc1\u660e\u8005\u548c\u9a8c\u8bc1\u8005\u5b9e\u4f8b\u534f\u4f5c\u5de5\u4f5c\uff0c\u6700\u7ec8\u901a\u8fc7Lean\u8bc1\u660e\u52a9\u624b\u8fdb\u884c\u5f62\u5f0f\u5316\u9a8c\u8bc1\uff0c\u5e76\u7531\u4eba\u5de5\u68c0\u67e5\u524d\u63d0\u548c\u7ed3\u8bba\u7684\u4e00\u81f4\u6027\u3002", "result": "\u6210\u529f\u89e3\u51b3\u4e862025\u5e74IMO\u76845/6\u9898\u76ee\uff0c\u4ee5\u53caCohen\u6570\u8bba\u731c\u60f3\u4e2d66\u4e2a\u95ee\u9898\u7684\u7ea61/3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u6570\u5b66\u95ee\u9898\u8bc1\u660e\u4e2d\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u534f\u4f5c\u534f\u8bae\u548c\u5f62\u5f0f\u5316\u9a8c\u8bc1\u53ef\u4ee5\u6709\u6548\u907f\u514d\u5e7b\u89c9\u95ee\u9898\u3002"}}
{"id": "2510.13738", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.13738", "abs": "https://arxiv.org/abs/2510.13738", "authors": ["Jingyi Zhou", "Cheng Chen", "Kai Zuo", "Manjie Xu", "Zhendong Fu", "Yibo Chen", "Xu Tang", "Yao Hu"], "title": "HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based Sequential Recommendation", "comment": null, "summary": "Large language models (LLMs) have recently demonstrated strong potential for\nsequential recommendation. However, current LLM-based approaches face critical\nlimitations in modeling users' long-term and diverse interests. First, due to\ninference latency and feature fetching bandwidth constraints, existing methods\ntypically truncate user behavior sequences to include only the most recent\ninteractions, resulting in the loss of valuable long-range preference signals.\nSecond, most current methods rely on next-item prediction with a single\npredicted embedding, overlooking the multifaceted nature of user interests and\nlimiting recommendation diversity. To address these challenges, we propose\nHyMiRec, a hybrid multi-interest sequential recommendation framework, which\nleverages a lightweight recommender to extracts coarse interest embeddings from\nlong user sequences and an LLM-based recommender to captures refined interest\nembeddings. To alleviate the overhead of fetching features, we introduce a\nresidual codebook based on cosine similarity, enabling efficient compression\nand reuse of user history embeddings. To model the diverse preferences of\nusers, we design a disentangled multi-interest learning module, which leverages\nmultiple interest queries to learn disentangles multiple interest signals\nadaptively, allowing the model to capture different facets of user intent.\nExtensive experiments are conducted on both benchmark datasets and a collected\nindustrial dataset, demonstrating our effectiveness over existing\nstate-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec\nbrings consistent improvements in real-world recommendation systems.", "AI": {"tldr": "HyMiRec\u662f\u4e00\u4e2a\u6df7\u5408\u591a\u5174\u8da3\u5e8f\u5217\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u63a8\u8350\u5668\u63d0\u53d6\u957f\u5e8f\u5217\u7684\u7c97\u7c92\u5ea6\u5174\u8da3\u5d4c\u5165\uff0c\u7ed3\u5408LLM\u6355\u83b7\u7ec6\u7c92\u5ea6\u5174\u8da3\u5d4c\u5165\uff0c\u4f7f\u7528\u6b8b\u5dee\u7801\u672c\u538b\u7f29\u7528\u6237\u5386\u53f2\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7\u89e3\u8026\u591a\u5174\u8da3\u5b66\u4e60\u6a21\u5757\u5efa\u6a21\u7528\u6237\u591a\u6837\u5316\u504f\u597d\u3002", "motivation": "\u73b0\u6709LLM\u63a8\u8350\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a1\uff09\u7531\u4e8e\u63a8\u7406\u5ef6\u8fdf\u548c\u7279\u5f81\u83b7\u53d6\u5e26\u5bbd\u9650\u5236\uff0c\u53ea\u80fd\u622a\u65ad\u7528\u6237\u884c\u4e3a\u5e8f\u5217\uff0c\u4e22\u5931\u957f\u671f\u504f\u597d\u4fe1\u53f7\uff1b2\uff09\u5927\u591a\u4f9d\u8d56\u5355\u5d4c\u5165\u7684\u4e0b\u4e00\u9879\u9884\u6d4b\uff0c\u5ffd\u7565\u4e86\u7528\u6237\u5174\u8da3\u7684\u591a\u9762\u6027\uff0c\u9650\u5236\u4e86\u63a8\u8350\u591a\u6837\u6027\u3002", "method": "\u63d0\u51faHyMiRec\u6846\u67b6\uff1a1\uff09\u8f7b\u91cf\u7ea7\u63a8\u8350\u5668\u63d0\u53d6\u957f\u5e8f\u5217\u7684\u7c97\u7c92\u5ea6\u5174\u8da3\u5d4c\u5165\uff1b2\uff09LLM\u63a8\u8350\u5668\u6355\u83b7\u7ec6\u7c92\u5ea6\u5174\u8da3\u5d4c\u5165\uff1b3\uff09\u57fa\u4e8e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u6b8b\u5dee\u7801\u672c\u5b9e\u73b0\u9ad8\u6548\u538b\u7f29\u548c\u91cd\u7528\u7528\u6237\u5386\u53f2\u5d4c\u5165\uff1b4\uff09\u89e3\u8026\u591a\u5174\u8da3\u5b66\u4e60\u6a21\u5757\u4f7f\u7528\u591a\u4e2a\u5174\u8da3\u67e5\u8be2\u81ea\u9002\u5e94\u5b66\u4e60\u5206\u79bb\u7684\u591a\u4e2a\u5174\u8da3\u4fe1\u53f7\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u548c\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cHyMiRec\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u5728\u7ebfA/B\u6d4b\u8bd5\u8bc1\u5b9e\u4e86\u5176\u5728\u771f\u5b9e\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u6301\u7eed\u6539\u8fdb\u6548\u679c\u3002", "conclusion": "HyMiRec\u901a\u8fc7\u6df7\u5408\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86LLM\u63a8\u8350\u4e2d\u7684\u957f\u671f\u5174\u8da3\u5efa\u6a21\u548c\u591a\u6837\u6027\u95ee\u9898\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.12831", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12831", "abs": "https://arxiv.org/abs/2510.12831", "authors": ["Taicheng Guo", "Hai Wang", "ChaoChun Liu", "Mohsen Golalikhani", "Xin Chen", "Xiangliang Zhang", "Chandan K. Reddy"], "title": "MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic Training", "comment": null, "summary": "Multi-turn Text-to-SQL aims to translate a user's conversational utterances\ninto executable SQL while preserving dialogue coherence and grounding to the\ntarget schema. However, most existing systems only regard this task as a simple\ntext translation task and follow a short-horizon paradigm, generating a query\nper turn without execution, explicit verification, and refinement, which leads\nto non-executable or incoherent outputs. We present MTSQL-R1, an agentic\ntraining framework for long-horizon multi-turn Text-to-SQL. We cast the task as\na Markov Decision Process (MDP) in which an agent interacts with (i) a database\nfor execution feedback and (ii) a persistent dialogue memory for coherence\nverification, performing an iterative propose to execute -> verify -> refine\ncycle until all checks pass. Experiments on COSQL and SPARC demonstrate that\nMTSQL-R1 consistently outperforms strong baselines, highlighting the importance\nof environment-driven verification and memory-guided refinement for\nconversational semantic parsing. Full recipes (including code, trained models,\nlogs, reasoning trajectories, etc.) will be released after the internal review\nto contribute to community research.", "AI": {"tldr": "MTSQL-R1\u662f\u4e00\u4e2a\u57fa\u4e8e\u667a\u80fd\u4f53\u8bad\u7ec3\u6846\u67b6\u7684\u957f\u5468\u671f\u591a\u8f6eText-to-SQL\u7cfb\u7edf\uff0c\u901a\u8fc7\u6267\u884c\u53cd\u9988\u548c\u5bf9\u8bdd\u8bb0\u5fc6\u9a8c\u8bc1\uff0c\u91c7\u7528\u8fed\u4ee3\u7684\u63d0\u8bae-\u6267\u884c-\u9a8c\u8bc1-\u4f18\u5316\u5faa\u73af\uff0c\u663e\u8457\u63d0\u5347SQL\u751f\u6210\u7684\u53ef\u6267\u884c\u6027\u548c\u5bf9\u8bdd\u8fde\u8d2f\u6027\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u5c06\u591a\u8f6eText-to-SQL\u89c6\u4e3a\u7b80\u5355\u7684\u6587\u672c\u7ffb\u8bd1\u4efb\u52a1\uff0c\u91c7\u7528\u77ed\u5468\u671f\u8303\u5f0f\uff0c\u5bfc\u81f4\u751f\u6210\u4e0d\u53ef\u6267\u884c\u6216\u4e0d\u8fde\u8d2f\u7684SQL\u67e5\u8be2\u3002", "method": "\u5c06\u4efb\u52a1\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u667a\u80fd\u4f53\u4e0e\u6570\u636e\u5e93\u4ea4\u4e92\u83b7\u53d6\u6267\u884c\u53cd\u9988\uff0c\u4e0e\u6301\u4e45\u5bf9\u8bdd\u8bb0\u5fc6\u4ea4\u4e92\u8fdb\u884c\u8fde\u8d2f\u6027\u9a8c\u8bc1\uff0c\u6267\u884c\u8fed\u4ee3\u7684\u63d0\u8bae-\u6267\u884c-\u9a8c\u8bc1-\u4f18\u5316\u5faa\u73af\u3002", "result": "\u5728COSQL\u548cSPARC\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMTSQL-R1\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u73af\u5883\u9a71\u52a8\u7684\u9a8c\u8bc1\u548c\u8bb0\u5fc6\u5f15\u5bfc\u7684\u4f18\u5316\u5bf9\u4e8e\u5bf9\u8bdd\u5f0f\u8bed\u4e49\u89e3\u6790\u81f3\u5173\u91cd\u8981\uff0cMTSQL-R1\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.13312", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.13312", "abs": "https://arxiv.org/abs/2510.13312", "authors": ["Simon Lupart", "Mohammad Aliannejadi", "Evangelos Kanoulas"], "title": "ChatR1: Reinforcement Learning for Conversational Reasoning and Retrieval Augmented Question Answering", "comment": null, "summary": "We present ChatR1, a reasoning framework based on reinforcement learning (RL)\nfor conversational question answering (CQA). Reasoning plays an important role\nin CQA, where user intent evolves across dialogue turns, and utterances are\noften underspecified, requiring contextual interpretation, query reformulation,\nand dynamic coordination between retrieval and generation. Unlike static\n`rewrite, retrieve, and generate' pipelines, ChatR1 interleaves search and\nreasoning across turns, enabling exploratory and adaptive behaviors learned\nthrough RL. To address the challenge of sparse and delayed rewards in RL, we\npropose an intent-aware reward that provides turn-level feedback by aligning\nretrieval and reasoning with evolving user goals. Our proposed ChatR1\ndemonstrates strong performance on both 3B and 7B model backbones,\noutperforming competitive models on five CQA datasets, measured by different\nmetrics (F1, BERTScore, and LLM-as-judge). We include a diverse set of CQA\ndatasets to cover topic shifts, evolving intents, mixed-initiative dialogues,\nand multi-document grounding, testing ChatR1's performance from various\naspects. Ablation studies confirm the effectiveness of the intent-aware reward.\nOur analyses further reveal diverse reasoning trajectories and effective use of\nthe search tool. ChatR1 also generalizes robustly across domains, demonstrating\nthat RL-based reasoning enables more flexible and context-sensitive behavior\nthan static CQA pipelines.", "AI": {"tldr": "ChatR1\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5bf9\u8bdd\u95ee\u7b54\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u610f\u56fe\u611f\u77e5\u5956\u52b1\u673a\u5236\u5728\u5bf9\u8bdd\u8f6e\u6b21\u95f4\u4ea4\u66ff\u8fdb\u884c\u641c\u7d22\u548c\u63a8\u7406\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u9759\u6001\u6d41\u6c34\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5bf9\u8bdd\u95ee\u7b54\u4e2d\u7528\u6237\u610f\u56fe\u4f1a\u968f\u5bf9\u8bdd\u8f6e\u6b21\u6f14\u53d8\uff0c\u8bdd\u8bed\u901a\u5e38\u4e0d\u5b8c\u6574\uff0c\u9700\u8981\u4e0a\u4e0b\u6587\u89e3\u91ca\u3001\u67e5\u8be2\u91cd\u6784\u4ee5\u53ca\u68c0\u7d22\u4e0e\u751f\u6210\u7684\u52a8\u6001\u534f\u8c03\uff0c\u9759\u6001\u7684'\u91cd\u5199\u3001\u68c0\u7d22\u3001\u751f\u6210'\u6d41\u6c34\u7ebf\u65e0\u6cd5\u6ee1\u8db3\u8fd9\u4e9b\u9700\u6c42\u3002", "method": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a8\u7406\u6846\u67b6\uff0c\u5728\u5bf9\u8bdd\u8f6e\u6b21\u95f4\u4ea4\u66ff\u8fdb\u884c\u641c\u7d22\u548c\u63a8\u7406\uff0c\u4f7f\u7528\u610f\u56fe\u611f\u77e5\u5956\u52b1\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u7a00\u758f\u548c\u5ef6\u8fdf\u5956\u52b1\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u8f6e\u6b21\u7ea7\u53cd\u9988\u3002", "result": "\u57283B\u548c7B\u6a21\u578b\u9aa8\u5e72\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5728\u4e94\u4e2a\u5bf9\u8bdd\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u4e0d\u540c\u6307\u6807\uff08F1\u3001BERTScore\u548cLLM-as-judge\uff09\u5747\u4f18\u4e8e\u7ade\u4e89\u6a21\u578b\uff0c\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u610f\u56fe\u611f\u77e5\u5956\u52b1\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a8\u7406\u6bd4\u9759\u6001\u5bf9\u8bdd\u95ee\u7b54\u6d41\u6c34\u7ebf\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u548c\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u884c\u4e3a\uff0c\u80fd\u591f\u8de8\u9886\u57df\u7a33\u5065\u6cdb\u5316\u3002"}}
{"id": "2510.12835", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12835", "abs": "https://arxiv.org/abs/2510.12835", "authors": ["Kon Woo Kim", "Rezarta Islamaj", "Jin-Dong Kim", "Florian Boudin", "Akiko Aizawa"], "title": "Repurposing Annotation Guidelines to Instruct LLM Annotators: A Case Study", "comment": "11 pages, 2 figures, 3 tables, This is a preprint of the article\n  accepted at NLDB 2025 (Springer LNCS). The final version is available at\n  https://doi.org/10.1007/978-3-031-97144-0_13", "summary": "This study investigates how existing annotation guidelines can be repurposed\nto instruct large language model (LLM) annotators for text annotation tasks.\nTraditional guidelines are written for human annotators who internalize\ntraining, while LLMs require explicit, structured instructions. We propose a\nmoderation-oriented guideline repurposing method that transforms guidelines\ninto clear directives for LLMs through an LLM moderation process. Using the\nNCBI Disease Corpus as a case study, our experiments show that repurposed\nguidelines can effectively guide LLM annotators, while revealing several\npractical challenges. The results highlight the potential of this workflow to\nsupport scalable and cost-effective refinement of annotation guidelines and\nautomated annotation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u5982\u4f55\u5c06\u73b0\u6709\u7684\u4eba\u7c7b\u6807\u6ce8\u6307\u5357\u91cd\u65b0\u7528\u4e8e\u6307\u5bfc\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6587\u672c\u6807\u6ce8\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u8c03\u8282\u7684\u6307\u5357\u91cd\u6784\u65b9\u6cd5\uff0c\u5e76\u5728NCBI\u75be\u75c5\u8bed\u6599\u5e93\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u6807\u6ce8\u6307\u5357\u662f\u4e3a\u4eba\u7c7b\u6807\u6ce8\u8005\u8bbe\u8ba1\u7684\uff0c\u4ed6\u4eec\u80fd\u591f\u5185\u5316\u8bad\u7ec3\u5185\u5bb9\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u9700\u8981\u660e\u786e\u3001\u7ed3\u6784\u5316\u7684\u6307\u4ee4\u3002\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5982\u4f55\u5c06\u4eba\u7c7b\u6807\u6ce8\u6307\u5357\u8f6c\u5316\u4e3a\u9002\u5408LLM\u4f7f\u7528\u7684\u683c\u5f0f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8c03\u8282\u7684\u6307\u5357\u91cd\u6784\u65b9\u6cd5\uff0c\u901a\u8fc7LLM\u8c03\u8282\u8fc7\u7a0b\u5c06\u4f20\u7edf\u6307\u5357\u8f6c\u5316\u4e3a\u6e05\u6670\u7684LLM\u6307\u4ee4\u3002\u4f7f\u7528NCBI\u75be\u75c5\u8bed\u6599\u5e93\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u91cd\u6784\u540e\u7684\u6307\u5357\u80fd\u591f\u6709\u6548\u6307\u5bfcLLM\u6807\u6ce8\u8005\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u82e5\u5e72\u5b9e\u9645\u6311\u6218\u3002\u8be5\u65b9\u6cd5\u5728\u652f\u6301\u53ef\u6269\u5c55\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u6807\u6ce8\u6307\u5357\u7cbe\u70bc\u548c\u81ea\u52a8\u5316\u6807\u6ce8\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\u3002", "conclusion": "\u57fa\u4e8eLLM\u8c03\u8282\u7684\u6307\u5357\u91cd\u6784\u65b9\u6cd5\u80fd\u591f\u6210\u529f\u5c06\u4eba\u7c7b\u6807\u6ce8\u6307\u5357\u8f6c\u5316\u4e3a\u9002\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4f7f\u7528\u7684\u683c\u5f0f\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u6807\u6ce8\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002"}}
{"id": "2510.12838", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12838", "abs": "https://arxiv.org/abs/2510.12838", "authors": ["Qianben Chen", "Jingyi Cao", "Jiayu Zhang", "Tianrui Qin", "Xiaowan Li", "King Zhu", "Dingfeng Shi", "He Zhu", "Minghao Liu", "Xiaobo Liang", "Ge Zhang", "Jian Yang", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "title": "A\\textsuperscript{2}FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid Reasoning", "comment": "9 pages, 5 figures, submitted to ICLR 2026", "summary": "Large language models split into two families: reasoning-centric LLMs, which\nstrengthen internal chain-of-thought reasoning but cannot invoke external\ntools, and agentic LLMs, which learn to interact with environments and leverage\ntools but often lag in deep reasoning. This divide arises from fundamentally\ndifferent training objectives, leading to mismatched strengths and inefficiency\non simple queries, where both families tend to overthink or over-call tools. In\nthis work, we present Adaptive Agent Foundation Model (A\\textsuperscript{2}FM),\na unified framework that follows a route-then-align principle: the model first\nlearns task-aware routing and then aligns mode-specific trajectories under a\nshared backbone. To address the inefficiency gap, we introduce a third\nmode-instant-that handles simple queries directly, preventing unnecessary\nreasoning or tool calls while complementing the agentic and reasoning modes. To\njointly enhance accuracy and efficiency, we propose Adaptive Policy\nOptimization (APO), which enforces adaptive sampling across modes and applies a\ncost-regularized reward. On the 32B scale, A\\textsuperscript{2}FM achieves\n13.4\\% on BrowseComp, 70.4\\% on AIME25, and 16.7\\% on HLE, setting new SOTA\namong comparable models and performing competitively with frontier LLMs across\nagentic, reasoning, and general benchmarks. Notably, the adaptive execution\nachieves a cost of pass of only \\$0.00487 per correct answer-cutting cost by\n45.2\\% relative to reasoning and 33.5\\% relative to agentic, thus delivering\nsubstantially higher cost efficiency while maintaining comparable accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u4ee3\u7406\u57fa\u7840\u6a21\u578bA\u00b2FM\uff0c\u901a\u8fc7\u8def\u7531-\u5bf9\u9f50\u539f\u5219\u7edf\u4e00\u63a8\u7406\u578b\u548c\u4ee3\u7406\u578bLLM\uff0c\u5f15\u5165\u5373\u65f6\u6a21\u5f0f\u5904\u7406\u7b80\u5355\u67e5\u8be2\uff0c\u663e\u8457\u63d0\u5347\u6210\u672c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5206\u4e3a\u63a8\u7406\u578b\u548c\u4ee3\u7406\u578b\u4e24\u5927\u7c7b\u522b\uff0c\u5404\u81ea\u5b58\u5728\u5c40\u9650\u6027\uff1a\u63a8\u7406\u578b\u6a21\u578b\u65e0\u6cd5\u8c03\u7528\u5916\u90e8\u5de5\u5177\uff0c\u4ee3\u7406\u578b\u6a21\u578b\u5728\u6df1\u5ea6\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u4e14\u4e24\u8005\u5728\u7b80\u5355\u67e5\u8be2\u4e0a\u90fd\u5b58\u5728\u8fc7\u5ea6\u63a8\u7406\u6216\u8fc7\u5ea6\u8c03\u7528\u5de5\u5177\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8def\u7531-\u5bf9\u9f50\u539f\u5219\uff1a\u5148\u5b66\u4e60\u4efb\u52a1\u611f\u77e5\u8def\u7531\uff0c\u7136\u540e\u5728\u5171\u4eab\u9aa8\u5e72\u7f51\u7edc\u4e0b\u5bf9\u9f50\u6a21\u5f0f\u7279\u5b9a\u8f68\u8ff9\u3002\u5f15\u5165\u5373\u65f6\u6a21\u5f0f\u5904\u7406\u7b80\u5355\u67e5\u8be2\uff0c\u63d0\u51fa\u81ea\u9002\u5e94\u7b56\u7565\u4f18\u5316(APO)\u6765\u8054\u5408\u63d0\u5347\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "result": "\u572832B\u89c4\u6a21\u4e0a\uff0cA\u00b2FM\u5728BrowseComp\u4e0a\u8fbe\u523013.4%\uff0cAIME25\u4e0a\u8fbe\u523070.4%\uff0cHLE\u4e0a\u8fbe\u523016.7%\uff0c\u5728\u53ef\u6bd4\u6a21\u578b\u4e2d\u521b\u4e0b\u65b0SOTA\u3002\u81ea\u9002\u5e94\u6267\u884c\u4f7f\u6bcf\u4e2a\u6b63\u786e\u7b54\u6848\u7684\u6210\u672c\u4ec5\u4e3a0.00487\u7f8e\u5143\uff0c\u76f8\u6bd4\u63a8\u7406\u6a21\u5f0f\u964d\u4f4e\u6210\u672c45.2%\uff0c\u76f8\u6bd4\u4ee3\u7406\u6a21\u5f0f\u964d\u4f4e\u6210\u672c33.5%\u3002", "conclusion": "A\u00b2FM\u6846\u67b6\u6210\u529f\u7edf\u4e00\u4e86\u63a8\u7406\u578b\u548c\u4ee3\u7406\u578bLLM\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8def\u7531\u663e\u8457\u63d0\u5347\u4e86\u6210\u672c\u6548\u7387\uff0c\u5728\u4fdd\u6301\u53ef\u6bd4\u51c6\u786e\u6027\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2510.12839", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.12839", "abs": "https://arxiv.org/abs/2510.12839", "authors": ["Yingjia Wan", "Haochen Tan", "Xiao Zhu", "Xinyu Zhou", "Zhiwei Li", "Qingsong Lv", "Changxuan Sun", "Jiaqi Zeng", "Yi Xu", "Jianqiao Lu", "Yinhong Liu", "Zhijiang Guo"], "title": "FaStFACT: Faster, Stronger Long-Form Factuality Evaluations in LLMs", "comment": "EMNLP 2025 (Findings)", "summary": "Evaluating the factuality of long-form generations from Large Language Models\n(LLMs) remains challenging due to accuracy issues and costly human assessment.\nPrior efforts attempt this by decomposing text into claims, searching for\nevidence, and verifying claims, but suffer from critical drawbacks: (1)\ninefficiency due to complex pipeline components unsuitable for long LLM\noutputs, and (2) ineffectiveness stemming from inaccurate claim sets and\ninsufficient evidence collection of one-line snippets.\n  To address these limitations, we propose \\name, a fast and strong evaluation\nframework that achieves the highest alignment with human evaluation and\nefficiency among existing baselines. \\name first employs chunk-level claim\nextraction integrated with confidence-based pre-verification, significantly\nreducing the cost of web searching and inference calling while ensuring\nreliability. For searching and verification, it collects document-level\nevidence from crawled webpages and selectively retrieves it during\nverification, addressing the evidence insufficiency problem in previous\npipelines.\n  Extensive experiments based on an aggregated and manually annotated benchmark\ndemonstrate the reliability of \\name in both efficiently and effectively\nevaluating the factuality of long-form LLM generations. Code and benchmark data\nis available at https://github.com/Yingjia-Wan/FastFact.", "AI": {"tldr": "\u63d0\u51faFastFact\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5757\u7ea7\u58f0\u660e\u63d0\u53d6\u3001\u7f6e\u4fe1\u5ea6\u9884\u9a8c\u8bc1\u548c\u6587\u6863\u7ea7\u8bc1\u636e\u6536\u96c6\uff0c\u9ad8\u6548\u8bc4\u4f30\u957f\u6587\u672cLLM\u751f\u6210\u7684\u4e8b\u5b9e\u6027\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\uff08\u590d\u6742\u6d41\u6c34\u7ebf\u4e0d\u9002\u7528\u4e8e\u957f\u6587\u672c\uff09\u548c\u6548\u679c\u4e0d\u4f73\uff08\u58f0\u660e\u96c6\u4e0d\u51c6\u786e\u3001\u8bc1\u636e\u6536\u96c6\u4e0d\u8db3\uff09\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5757\u7ea7\u58f0\u660e\u63d0\u53d6\u4e0e\u7f6e\u4fe1\u5ea6\u9884\u9a8c\u8bc1\u76f8\u7ed3\u5408\uff0c\u51cf\u5c11\u7f51\u7edc\u641c\u7d22\u548c\u63a8\u7406\u8c03\u7528\u6210\u672c\uff1b\u6536\u96c6\u6587\u6863\u7ea7\u8bc1\u636e\u5e76\u5728\u9a8c\u8bc1\u65f6\u9009\u62e9\u6027\u68c0\u7d22\uff0c\u89e3\u51b3\u8bc1\u636e\u4e0d\u8db3\u95ee\u9898\u3002", "result": "\u5728\u805a\u5408\u7684\u4eba\u5de5\u6807\u6ce8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFastFact\u5728\u6548\u7387\u548c\u6548\u679c\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u4e0e\u4eba\u5de5\u8bc4\u4f30\u5bf9\u9f50\u5ea6\u6700\u9ad8\u3002", "conclusion": "FastFact\u662f\u4e00\u4e2a\u5feb\u901f\u4e14\u5f3a\u5927\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u80fd\u53ef\u9760\u5730\u8bc4\u4f30\u957f\u6587\u672cLLM\u751f\u6210\u7684\u4e8b\u5b9e\u6027\uff0c\u4ee3\u7801\u548c\u57fa\u51c6\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2510.12845", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12845", "abs": "https://arxiv.org/abs/2510.12845", "authors": ["Jesse Atuhurra", "Iqra Ali", "Tomoya Iwakura", "Hidetaka Kamigaito", "Tatsuya Hiraoka"], "title": "VLURes: Benchmarking VLM Visual and Linguistic Understanding in Low-Resource Languages", "comment": null, "summary": "Vision Language Models (VLMs) are pivotal for advancing perception in\nintelligent agents. Yet, evaluation of VLMs remains limited to predominantly\nEnglish-centric benchmarks in which the image-text pairs comprise short texts.\nTo evaluate VLM fine-grained abilities, in four languages under long-text\nsettings, we introduce a novel multilingual benchmark VLURes featuring eight\nvision-and-language tasks, and a pioneering unrelatedness task, to probe the\nfine-grained Visual and Linguistic Understanding capabilities of VLMs across\nEnglish, Japanese, and low-resource languages, Swahili, and Urdu. Our datasets,\ncurated from web resources in the target language, encompass ten diverse image\ncategories and rich textual context, introducing valuable vision-language\nresources for Swahili and Urdu. By prompting VLMs to generate responses and\nrationales, evaluated automatically and by native speakers, we uncover\nperformance disparities across languages and tasks critical to intelligent\nagents, such as object recognition, scene understanding, and relationship\nunderstanding. We conducted evaluations of ten VLMs with VLURes. The best\nperforming model, GPT-4o, achieves an overall accuracy of 90.8% and lags human\nperformance by 6.7%, though the gap is larger for open-source models. The gap\nhighlights VLURes' critical role in developing intelligent agents to tackle\nmulti-modal visual reasoning.", "AI": {"tldr": "VLURes\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u57fa\u51c6\uff0c\u6db5\u76d6\u82f1\u8bed\u3001\u65e5\u8bed\u3001\u65af\u74e6\u5e0c\u91cc\u8bed\u548c\u4e4c\u5c14\u90fd\u8bed\u56db\u79cd\u8bed\u8a00\uff0c\u5305\u542b\u516b\u4e2a\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u548c\u4e00\u4e2a\u65b0\u9896\u7684\u4e0d\u76f8\u5173\u6027\u4efb\u52a1\uff0c\u7528\u4e8e\u8bc4\u4f30VLMs\u5728\u957f\u6587\u672c\u8bbe\u7f6e\u4e0b\u7684\u7ec6\u7c92\u5ea6\u80fd\u529b\u3002", "motivation": "\u5f53\u524dVLMs\u8bc4\u4f30\u4e3b\u8981\u5c40\u9650\u4e8e\u82f1\u8bed\u4e3a\u4e2d\u5fc3\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e14\u56fe\u50cf-\u6587\u672c\u5bf9\u901a\u5e38\u5305\u542b\u77ed\u6587\u672c\u3002\u4e3a\u4e86\u8bc4\u4f30VLMs\u5728\u591a\u8bed\u8a00\u957f\u6587\u672c\u73af\u5883\u4e0b\u7684\u7ec6\u7c92\u5ea6\u80fd\u529b\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u4ece\u76ee\u6807\u8bed\u8a00\u7684\u7f51\u7edc\u8d44\u6e90\u4e2d\u7b56\u5212\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u5341\u4e2a\u4e0d\u540c\u7684\u56fe\u50cf\u7c7b\u522b\u548c\u4e30\u5bcc\u7684\u6587\u672c\u4e0a\u4e0b\u6587\u3002\u901a\u8fc7\u63d0\u793aVLMs\u751f\u6210\u54cd\u5e94\u548c\u63a8\u7406\uff0c\u8fdb\u884c\u81ea\u52a8\u8bc4\u4f30\u548c\u6bcd\u8bed\u4eba\u58eb\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u4e86\u5341\u4e2aVLMs\uff0c\u6700\u4f73\u6a21\u578bGPT-4o\u603b\u4f53\u51c6\u786e\u7387\u8fbe\u523090.8%\uff0c\u4f46\u4ecd\u6bd4\u4eba\u7c7b\u8868\u73b0\u4f4e6.7%\u3002\u5f00\u6e90\u6a21\u578b\u4e0e\u4eba\u7c7b\u8868\u73b0\u7684\u5dee\u8ddd\u66f4\u5927\u3002", "conclusion": "VLURes\u57fa\u51c6\u5728\u5f00\u53d1\u80fd\u591f\u5904\u7406\u591a\u6a21\u6001\u89c6\u89c9\u63a8\u7406\u7684\u667a\u80fd\u4ee3\u7406\u65b9\u9762\u53d1\u6325\u7740\u5173\u952e\u4f5c\u7528\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u8bed\u8a00\u548c\u4efb\u52a1\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u5f02\u3002"}}
{"id": "2510.12856", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12856", "abs": "https://arxiv.org/abs/2510.12856", "authors": ["Jan Miller"], "title": "Efficient Adaptive Transformer: An Empirical Study and Reproducible Framework", "comment": "10 pages, 6 figures, pgfplots tables included; BibTeX compiled to\n  .bbl. Code and reproducibility artifacts referenced in the paper", "summary": "The Efficient Adaptive Transformer (EAT) framework unifies three adaptive\nefficiency techniques - progressive token pruning, sparse attention, and\ndynamic early exiting - into a single, reproducible architecture for\ninput-adaptive inference. EAT provides an open-source benchmarking pipeline\nthat automates data processing, timing, and ablation across GLUE tasks (SST-2,\nQQP, MNLI). Although this empirical study finds that combining these mechanisms\ncan increase latency in shallow six-layer models, it demonstrates that EAT\nachieves slightly higher accuracy than the optimized DistilBERT baseline on\nSST-2, illustrating the potential of dynamic computation for latency-sensitive\nNLP. The main contribution is the open, end-to-end reproducible framework -\ncomplete with scripts, CSV logging, and analysis utilities - intended to serve\nas a community tool for further research on adaptive transformers.", "AI": {"tldr": "EAT\u6846\u67b6\u7edf\u4e00\u4e86\u4e09\u79cd\u81ea\u9002\u5e94\u6548\u7387\u6280\u672f\uff08\u6e10\u8fdb\u5f0ftoken\u526a\u679d\u3001\u7a00\u758f\u6ce8\u610f\u529b\u3001\u52a8\u6001\u63d0\u524d\u9000\u51fa\uff09\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f00\u6e90\u7684\u53ef\u590d\u73b0\u67b6\u6784\u548c\u57fa\u51c6\u6d4b\u8bd5\u7ba1\u9053\uff0c\u7528\u4e8e\u8f93\u5165\u81ea\u9002\u5e94\u63a8\u7406\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u53ef\u590d\u73b0\u7684\u6846\u67b6\u6765\u6574\u5408\u591a\u79cd\u81ea\u9002\u5e94\u6548\u7387\u6280\u672f\uff0c\u4e3a\u793e\u533a\u63d0\u4f9b\u7814\u7a76\u81ea\u9002\u5e94transformer\u7684\u5de5\u5177\u3002", "method": "\u5c06\u6e10\u8fdb\u5f0ftoken\u526a\u679d\u3001\u7a00\u758f\u6ce8\u610f\u529b\u548c\u52a8\u6001\u63d0\u524d\u9000\u51fa\u4e09\u79cd\u6280\u672f\u96c6\u6210\u5230\u5355\u4e00\u67b6\u6784\u4e2d\uff0c\u5e76\u5f00\u53d1\u4e86\u5305\u542b\u6570\u636e\u5904\u7406\u3001\u8ba1\u65f6\u548c\u6d88\u878d\u5b9e\u9a8c\u7684\u81ea\u52a8\u5316\u57fa\u51c6\u6d4b\u8bd5\u7ba1\u9053\u3002", "result": "\u5728\u6d45\u5c42\u516d\u5c42\u6a21\u578b\u4e2d\u7ec4\u5408\u8fd9\u4e9b\u673a\u5236\u53ef\u80fd\u589e\u52a0\u5ef6\u8fdf\uff0c\u4f46\u5728SST-2\u4efb\u52a1\u4e0a\u6bd4\u4f18\u5316\u7684DistilBERT\u57fa\u7ebf\u83b7\u5f97\u4e86\u7a0d\u9ad8\u7684\u51c6\u786e\u7387\uff0c\u5c55\u793a\u4e86\u52a8\u6001\u8ba1\u7b97\u5728\u5ef6\u8fdf\u654f\u611fNLP\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "EAT\u7684\u4e3b\u8981\u8d21\u732e\u662f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f00\u653e\u7684\u3001\u7aef\u5230\u7aef\u53ef\u590d\u73b0\u7684\u6846\u67b6\uff0c\u5305\u542b\u811a\u672c\u3001CSV\u65e5\u5fd7\u548c\u5206\u6790\u5de5\u5177\uff0c\u65e8\u5728\u4f5c\u4e3a\u793e\u533a\u8fdb\u4e00\u6b65\u7814\u7a76\u81ea\u9002\u5e94transformer\u7684\u5de5\u5177\u3002"}}
{"id": "2510.12858", "categories": ["cs.CL", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.12858", "abs": "https://arxiv.org/abs/2510.12858", "authors": ["Mohammed Hilal Al-Kharusi", "Khizar Hayat", "Khalil Bader Al Ruqeishi", "Haroon Rashid Lone"], "title": "A Critical Review of the Need for Knowledge-Centric Evaluation of Quranic Recitation", "comment": "33 pages", "summary": "The sacred practice of Quranic recitation (Tajweed), governed by precise\nphonetic, prosodic, and theological rules, faces significant pedagogical\nchallenges in the modern era. While digital technologies promise unprecedented\naccess to education, automated tools for recitation evaluation have failed to\nachieve widespread adoption or pedagogical efficacy. This literature review\ninvestigates this critical gap, conducting a comprehensive analysis of academic\nresearch, web platforms, and commercial applications developed over the past\ntwo decades. Our synthesis reveals a fundamental misalignment in prevailing\napproaches that repurpose Automatic Speech Recognition (ASR) architectures,\nwhich prioritize lexical recognition over qualitative acoustic assessment and\nare plagued by data dependency, demographic biases, and an inability to provide\ndiagnostically useful feedback. Critiquing these data--driven paradigms, we\nargue for a foundational paradigm shift towards a knowledge-centric\ncomputational framework. Capitalizing on the immutable nature of the Quranic\ntext and the precisely defined rules of Tajweed, we propose that a robust\nevaluator must be architected around anticipatory acoustic modeling based on\ncanonical rules and articulation points (Makhraj), rather than relying on\nstatistical patterns learned from imperfect and biased datasets. This review\nconcludes that the future of automated Quranic evaluation lies in hybrid\nsystems that integrate deep linguistic knowledge with advanced audio analysis,\noffering a path toward robust, equitable, and pedagogically sound tools that\ncan faithfully support learners worldwide.", "AI": {"tldr": "\u8fd9\u7bc7\u6587\u732e\u7efc\u8ff0\u5206\u6790\u4e86\u53e4\u5170\u7ecf\u8bf5\u8bfb\u81ea\u52a8\u8bc4\u4f30\u5de5\u5177\u7684\u73b0\u72b6\uff0c\u6307\u51fa\u5f53\u524d\u57fa\u4e8e\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7684\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u6027\u9519\u4f4d\uff0c\u5e76\u63d0\u51fa\u5411\u57fa\u4e8e\u89c4\u5219\u7684\u77e5\u8bc6\u4e2d\u5fc3\u8ba1\u7b97\u6846\u67b6\u8f6c\u53d8\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u53e4\u5170\u7ecf\u8bf5\u8bfb\uff08Tajweed\uff09\u4f5c\u4e3a\u53d7\u7cbe\u786e\u8bed\u97f3\u3001\u97f5\u5f8b\u548c\u795e\u5b66\u89c4\u5219\u7ea6\u675f\u7684\u795e\u5723\u5b9e\u8df5\uff0c\u5728\u73b0\u4ee3\u9762\u4e34\u6559\u5b66\u6311\u6218\u3002\u867d\u7136\u6570\u5b57\u6280\u672f\u63d0\u4f9b\u4e86\u524d\u6240\u672a\u6709\u7684\u6559\u80b2\u673a\u4f1a\uff0c\u4f46\u73b0\u6709\u7684\u81ea\u52a8\u8bf5\u8bfb\u8bc4\u4f30\u5de5\u5177\u672a\u80fd\u5b9e\u73b0\u5e7f\u6cdb\u91c7\u7528\u6216\u6559\u5b66\u6548\u679c\u3002", "method": "\u5bf9\u8fc7\u53bb\u4e8c\u5341\u5e74\u7684\u5b66\u672f\u7814\u7a76\u3001\u7f51\u7edc\u5e73\u53f0\u548c\u5546\u4e1a\u5e94\u7528\u8fdb\u884c\u5168\u9762\u5206\u6790\uff0c\u63ed\u793a\u5f53\u524d\u65b9\u6cd5\u7684\u6839\u672c\u95ee\u9898\u3002", "result": "\u5206\u6790\u53d1\u73b0\u4e3b\u6d41\u65b9\u6cd5\u9519\u8bef\u5730\u91cd\u65b0\u5229\u7528\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u67b6\u6784\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4f18\u5148\u8003\u8651\u8bcd\u6c47\u8bc6\u522b\u800c\u975e\u5b9a\u6027\u58f0\u5b66\u8bc4\u4f30\uff0c\u5b58\u5728\u6570\u636e\u4f9d\u8d56\u6027\u3001\u4eba\u53e3\u504f\u89c1\u548c\u65e0\u6cd5\u63d0\u4f9b\u8bca\u65ad\u6027\u53cd\u9988\u7684\u95ee\u9898\u3002", "conclusion": "\u53e4\u5170\u7ecf\u81ea\u52a8\u8bc4\u4f30\u7684\u672a\u6765\u5728\u4e8e\u6df7\u5408\u7cfb\u7edf\uff0c\u5c06\u6df1\u5c42\u8bed\u8a00\u5b66\u77e5\u8bc6\u4e0e\u5148\u8fdb\u97f3\u9891\u5206\u6790\u76f8\u7ed3\u5408\uff0c\u4e3a\u5168\u7403\u5b66\u4e60\u8005\u63d0\u4f9b\u7a33\u5065\u3001\u516c\u5e73\u4e14\u6559\u5b66\u5408\u7406\u7684\u5de5\u5177\u3002"}}
{"id": "2510.12899", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.12899", "abs": "https://arxiv.org/abs/2510.12899", "authors": ["Shouang Wei", "Min Zhang", "Xin Lin", "Bo Jiang", "Zhongxiang Dai", "Kun Kuang"], "title": "EduDial: Constructing a Large-scale Multi-turn Teacher-Student Dialogue Corpus", "comment": null, "summary": "Recently, several multi-turn dialogue benchmarks have been proposed to\nevaluate the conversational abilities of large language models (LLMs). As LLMs\nare increasingly recognized as a key technology for advancing intelligent\neducation, owing to their ability to deeply understand instructional contexts\nand provide personalized guidance, the construction of dedicated\nteacher-student dialogue benchmarks has become particularly important. To this\nend, we present EduDial, a comprehensive multi-turn teacher-student dialogue\ndataset. EduDial covers 345 core knowledge points and consists of 34,250\ndialogue sessions generated through interactions between teacher and student\nagents. Its design is guided by Bloom's taxonomy of educational objectives and\nincorporates ten questioning strategies, including situational questioning,\nzone of proximal development (ZPD) questioning, and metacognitive\nquestioning-thus better capturing authentic classroom interactions.\nFurthermore, we design differentiated teaching strategies for students at\ndifferent cognitive levels, thereby providing more targeted teaching guidance.\nBuilding on EduDial, we further develop EduDial-LLM 32B via training and\npropose an 11-dimensional evaluation framework that systematically measures the\nteaching abilities of LLMs, encompassing both overall teaching quality and\ncontent quality. Experiments on 17 mainstream LLMs reveal that most models\nstruggle in student-centered teaching scenarios, whereas our EduDial-LLM\nachieves significant gains, consistently outperforming all baselines across all\nmetrics. The code is available at\nhttps://github.com/Mind-Lab-ECNU/EduDial/tree/main.", "AI": {"tldr": "\u63d0\u51fa\u4e86EduDial\uff0c\u4e00\u4e2a\u5168\u9762\u7684\u591a\u8f6e\u5e08\u751f\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u5305\u542b34,250\u4e2a\u5bf9\u8bdd\u4f1a\u8bdd\uff0c\u8986\u76d6345\u4e2a\u6838\u5fc3\u77e5\u8bc6\u70b9\uff0c\u57fa\u4e8e\u5e03\u9c81\u59c6\u6559\u80b2\u76ee\u6807\u5206\u7c7b\u5b66\u548c\u5341\u79cd\u63d0\u95ee\u7b56\u7565\u8bbe\u8ba1\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u5f00\u53d1\u4e86EduDial-LLM 32B\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e8611\u7ef4\u8bc4\u4f30\u6846\u67b6\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u6a21\u578b\u572817\u4e2a\u4e3b\u6d41LLM\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6559\u80b2\u9886\u57df\u7684\u5e94\u7528\u65e5\u76ca\u91cd\u8981\uff0c\u9700\u8981\u4e13\u95e8\u7684\u5e08\u751f\u5bf9\u8bdd\u57fa\u51c6\u6765\u8bc4\u4f30\u5176\u6559\u5b66\u80fd\u529b\uff0c\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u5145\u5206\u6355\u6349\u771f\u5b9e\u8bfe\u5802\u4e92\u52a8\u3002", "method": "\u6784\u5efaEduDial\u6570\u636e\u96c6\uff0c\u57fa\u4e8e\u5e03\u9c81\u59c6\u6559\u80b2\u76ee\u6807\u5206\u7c7b\u5b66\u548c\u5341\u79cd\u63d0\u95ee\u7b56\u7565\uff08\u5305\u62ec\u60c5\u5883\u63d0\u95ee\u3001\u6700\u8fd1\u53d1\u5c55\u533a\u63d0\u95ee\u3001\u5143\u8ba4\u77e5\u63d0\u95ee\u7b49\uff09\uff0c\u4e3a\u4e0d\u540c\u8ba4\u77e5\u6c34\u5e73\u5b66\u751f\u8bbe\u8ba1\u5dee\u5f02\u5316\u6559\u5b66\u7b56\u7565\uff0c\u5e76\u8bad\u7ec3EduDial-LLM 32B\u6a21\u578b\u3002", "result": "\u572817\u4e2a\u4e3b\u6d41LLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5927\u591a\u6570\u6a21\u578b\u5728\u5b66\u751f\u4e2d\u5fc3\u6559\u5b66\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u800cEduDial-LLM\u5728\u6240\u6709\u6307\u6807\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "EduDial\u4e3a\u8bc4\u4f30LLM\u7684\u6559\u5b66\u80fd\u529b\u63d0\u4f9b\u4e86\u5168\u9762\u57fa\u51c6\uff0cEduDial-LLM\u5728\u5e08\u751f\u5bf9\u8bdd\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u667a\u80fd\u6559\u80b2\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2510.12925", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12925", "abs": "https://arxiv.org/abs/2510.12925", "authors": ["Nil-Jana Akpinar", "Chia-Jung Lee", "Vanessa Murdock", "Pietro Perona"], "title": "Who's Asking? Evaluating LLM Robustness to Inquiry Personas in Factual Question Answering", "comment": null, "summary": "Large Language Models (LLMs) should answer factual questions truthfully,\ngrounded in objective knowledge, regardless of user context such as\nself-disclosed personal information, or system personalization. In this paper,\nwe present the first systematic evaluation of LLM robustness to inquiry\npersonas, i.e. user profiles that convey attributes like identity, expertise,\nor belief. While prior work has primarily focused on adversarial inputs or\ndistractors for robustness testing, we evaluate plausible, human-centered\ninquiry persona cues that users disclose in real-world interactions. We find\nthat such cues can meaningfully alter QA accuracy and trigger failure modes\nsuch as refusals, hallucinated limitations, and role confusion. These effects\nhighlight how model sensitivity to user framing can compromise factual\nreliability, and position inquiry persona testing as an effective tool for\nrobustness evaluation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u7528\u6237\u4e2a\u4eba\u8d44\u6599\uff08\u5982\u8eab\u4efd\u3001\u4e13\u4e1a\u77e5\u8bc6\u6216\u4fe1\u4ef0\uff09\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u8fd9\u4e9b\u7528\u6237\u4fe1\u606f\u4f1a\u663e\u8457\u5f71\u54cd\u95ee\u7b54\u51c6\u786e\u6027\u5e76\u5f15\u53d1\u591a\u79cd\u5931\u8d25\u6a21\u5f0f\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e94\u8be5\u57fa\u4e8e\u5ba2\u89c2\u77e5\u8bc6\u771f\u5b9e\u56de\u7b54\u4e8b\u5b9e\u6027\u95ee\u9898\uff0c\u800c\u4e0d\u5e94\u53d7\u7528\u6237\u4e2a\u4eba\u80cc\u666f\u4fe1\u606f\u6216\u7cfb\u7edf\u4e2a\u6027\u5316\u5f71\u54cd\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5bf9\u6297\u6027\u8f93\u5165\u6216\u5e72\u6270\u56e0\u7d20\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u771f\u5b9e\u7528\u6237\u4ea4\u4e92\u4e2d\u5e38\u89c1\u4e2a\u4eba\u8d44\u6599\u5f71\u54cd\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u8bc4\u4f30LLM\u5bf9\u8be2\u95ee\u8005\u4e2a\u4eba\u8d44\u6599\uff08\u5982\u8eab\u4efd\u3001\u4e13\u4e1a\u77e5\u8bc6\u3001\u4fe1\u4ef0\u7b49\u5c5e\u6027\uff09\u7684\u9c81\u68d2\u6027\uff0c\u5206\u6790\u8fd9\u4e9b\u7528\u6237\u4fe1\u606f\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u7684\u4e8b\u5b9e\u95ee\u7b54\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7528\u6237\u4e2a\u4eba\u8d44\u6599\u63d0\u793a\u80fd\u663e\u8457\u6539\u53d8\u95ee\u7b54\u51c6\u786e\u6027\uff0c\u5e76\u89e6\u53d1\u62d2\u7edd\u56de\u7b54\u3001\u5e7b\u89c9\u9650\u5236\u548c\u89d2\u8272\u6df7\u6dc6\u7b49\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "\u6a21\u578b\u5bf9\u7528\u6237\u6846\u67b6\u7684\u654f\u611f\u6027\u4f1a\u635f\u5bb3\u4e8b\u5b9e\u53ef\u9760\u6027\uff0c\u8be2\u95ee\u8005\u4e2a\u4eba\u8d44\u6599\u6d4b\u8bd5\u5e94\u6210\u4e3a\u9c81\u68d2\u6027\u8bc4\u4f30\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2510.12943", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.12943", "abs": "https://arxiv.org/abs/2510.12943", "authors": ["Angana Borah", "Rada Mihalcea"], "title": "The Curious Case of Curiosity across Human Cultures and LLMs", "comment": "Preprint (Paper under review)", "summary": "Recent advances in Large Language Models (LLMs) have expanded their role in\nhuman interaction, yet curiosity -- a central driver of inquiry -- remains\nunderexplored in these systems, particularly across cultural contexts. In this\nwork, we investigate cultural variation in curiosity using Yahoo! Answers, a\nreal-world multi-country dataset spanning diverse topics. We introduce CUEST\n(CUriosity Evaluation across SocieTies), an evaluation framework that measures\nhuman-model alignment in curiosity through linguistic (style), topic preference\n(content) analysis and grounding insights in social science constructs. Across\nopen- and closed-source models, we find that LLMs flatten cross-cultural\ndiversity, aligning more closely with how curiosity is expressed in Western\ncountries. We then explore fine-tuning strategies to induce curiosity in LLMs,\nnarrowing the human-model alignment gap by up to 50\\%. Finally, we demonstrate\nthe practical value of curiosity for LLM adaptability across cultures, showing\nits importance for future NLP research.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u6587\u5316\u80cc\u666f\u4e0b\u7684\u597d\u5947\u5fc3\u8868\u8fbe\uff0c\u53d1\u73b0LLMs\u503e\u5411\u4e8e\u897f\u65b9\u6587\u5316\u7684\u597d\u5947\u5fc3\u8868\u8fbe\u65b9\u5f0f\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u7b56\u7565\u5c06\u4eba\u673a\u5bf9\u9f50\u5dee\u8ddd\u7f29\u5c0f\u4e8650%\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4eba\u7c7b\u4e92\u52a8\u4e2d\u626e\u6f14\u7740\u8d8a\u6765\u8d8a\u91cd\u8981\u7684\u89d2\u8272\uff0c\u4f46\u597d\u5947\u5fc3\u8fd9\u4e00\u6838\u5fc3\u63a2\u7a76\u9a71\u52a8\u529b\u5728\u8de8\u6587\u5316\u80cc\u666f\u4e0b\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f7f\u7528Yahoo! Answers\u591a\u56fd\u6570\u636e\u96c6\uff0c\u5f15\u5165CUEST\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u8a00\u98ce\u683c\u3001\u8bdd\u9898\u504f\u597d\u5206\u6790\u548c\u793e\u4f1a\u79d1\u5b66\u6784\u5efa\u6765\u6d4b\u91cf\u4eba\u673a\u597d\u5947\u5fc3\u5bf9\u9f50\u5ea6\u3002", "result": "\u53d1\u73b0LLMs\u5728\u4e0d\u540c\u6587\u5316\u95f4\u6241\u5e73\u5316\u591a\u6837\u6027\uff0c\u66f4\u63a5\u8fd1\u897f\u65b9\u56fd\u5bb6\u7684\u8868\u8fbe\u65b9\u5f0f\uff1b\u901a\u8fc7\u5fae\u8c03\u7b56\u7565\u5c06\u4eba\u673a\u5bf9\u9f50\u5dee\u8ddd\u7f29\u5c0f\u4e8650%\uff1b\u5c55\u793a\u4e86\u597d\u5947\u5fc3\u5bf9LLM\u8de8\u6587\u5316\u9002\u5e94\u6027\u7684\u5b9e\u9645\u4ef7\u503c\u3002", "conclusion": "\u597d\u5947\u5fc3\u5bf9\u4e8e\u672a\u6765NLP\u7814\u7a76\u5177\u6709\u91cd\u8981\u6027\uff0c\u7279\u522b\u662f\u5728\u63d0\u5347LLM\u8de8\u6587\u5316\u9002\u5e94\u6027\u65b9\u9762\u3002"}}
{"id": "2510.12966", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.12966", "abs": "https://arxiv.org/abs/2510.12966", "authors": ["Sanghyun Byun", "Mohanad Odema", "Jung Ick Guack", "Baisub Lee", "Jacob Song", "Woo Seong Chung"], "title": "3-Model Speculative Decoding", "comment": "Accepted at NeurIPS SPIGM 2025", "summary": "Speculative Decoding (SD) accelerates inference in large language models by\nusing a smaller draft model to propose tokens, which are then verified by a\nlarger target model. However, the throughput gains of SD are fundamentally\nlimited by a trade-off between draft model size and token acceptance: smaller\ndraft models generate tokens more quickly but exhibit greater divergence from\nthe target model, resulting in lower acceptance rates and reduced speedups. We\nintroduce Pyramid Speculative Decoding (PyramidSD), an extension of SD that\ninserts an intermediate qualifier model between the draft and target to bridge\nthe distributional gap in output predictions, allowing smaller model to be used\nfor drafting. This hierarchical decoding strategy improves alignment across\nmodels, enabling higher acceptance rates and allowing the use of significantly\nsmaller draft models without sacrificing overall performance. PyramidSD builds\non fuzzy acceptance criteria to support relaxed divergence thresholds at each\nstage, improving throughput. In experiments, PyramidSD achieves up to 1.91x\ngeneration speed over standard SD, reaching 124 tokens per second on a consumer\nGPU (RTX 4090). In small-memory settings with a 1B-parameter draft model and an\n8B target model, PyramidSD minimally trades target model quality for improved\nthroughput. Overall, PyramidSD offers a practical approach to enhancing\nspeculative decoding efficiency and can be readily applied to existing\ninference pipelines.", "AI": {"tldr": "Pyramid Speculative Decoding (PyramidSD) \u901a\u8fc7\u5728\u8349\u7a3f\u6a21\u578b\u548c\u76ee\u6807\u6a21\u578b\u4e4b\u95f4\u63d2\u5165\u4e00\u4e2a\u4e2d\u95f4\u9650\u5b9a\u6a21\u578b\u6765\u6539\u8fdb\u63a8\u6d4b\u89e3\u7801\uff0c\u4f7f\u7528\u66f4\u5c0f\u7684\u8349\u7a3f\u6a21\u578b\u540c\u65f6\u4fdd\u6301\u9ad8\u63a5\u53d7\u7387\uff0c\u5b9e\u73b0\u4e86\u6700\u9ad81.91\u500d\u7684\u751f\u6210\u52a0\u901f\u3002", "motivation": "\u4f20\u7edf\u63a8\u6d4b\u89e3\u7801\u5b58\u5728\u8349\u7a3f\u6a21\u578b\u5927\u5c0f\u4e0etoken\u63a5\u53d7\u7387\u4e4b\u95f4\u7684\u6743\u8861\uff1a\u8f83\u5c0f\u7684\u8349\u7a3f\u6a21\u578b\u751f\u6210\u66f4\u5feb\u4f46\u4e0e\u76ee\u6807\u6a21\u578b\u5dee\u5f02\u66f4\u5927\uff0c\u5bfc\u81f4\u63a5\u53d7\u7387\u964d\u4f4e\u548c\u52a0\u901f\u6548\u679c\u51cf\u5f31\u3002", "method": "\u5f15\u5165\u4e2d\u95f4\u9650\u5b9a\u6a21\u578b\u6765\u5f25\u5408\u8349\u7a3f\u6a21\u578b\u548c\u76ee\u6807\u6a21\u578b\u4e4b\u95f4\u7684\u5206\u5e03\u5dee\u8ddd\uff0c\u91c7\u7528\u5206\u5c42\u89e3\u7801\u7b56\u7565\u548c\u6a21\u7cca\u63a5\u53d7\u6807\u51c6\uff0c\u652f\u6301\u6bcf\u4e2a\u9636\u6bb5\u7684\u5bbd\u677e\u5dee\u5f02\u9608\u503c\u3002", "result": "\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u8fbe\u5230\u6bcf\u79d2124\u4e2atoken\uff0c\u57281B\u53c2\u6570\u8349\u7a3f\u6a21\u578b\u548c8B\u76ee\u6807\u6a21\u578b\u7684\u5c0f\u5185\u5b58\u8bbe\u7f6e\u4e2d\uff0c\u4ee5\u6700\u5c0f\u8d28\u91cf\u635f\u5931\u6362\u53d6\u541e\u5410\u91cf\u63d0\u5347\u3002", "conclusion": "PyramidSD\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u65b9\u6cd5\u6765\u589e\u5f3a\u63a8\u6d4b\u89e3\u7801\u6548\u7387\uff0c\u53ef\u4ee5\u8f7b\u677e\u5e94\u7528\u4e8e\u73b0\u6709\u7684\u63a8\u7406\u6d41\u6c34\u7ebf\u3002"}}
{"id": "2510.12993", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.12993", "abs": "https://arxiv.org/abs/2510.12993", "authors": ["Jo\u00e3o A. Leite", "Arnav Arora", "Silvia Gargova", "Jo\u00e3o Luz", "Gustavo Sampaio", "Ian Roberts", "Carolina Scarton", "Kalina Bontcheva"], "title": "A Multilingual, Large-Scale Study of the Interplay between LLM Safeguards, Personalisation, and Disinformation", "comment": null, "summary": "The human-like proficiency of Large Language Models (LLMs) has brought\nconcerns about their potential misuse for generating persuasive and\npersonalised disinformation at scale. While prior work has demonstrated that\nLLMs can generate disinformation, specific questions around persuasiveness and\npersonalisation (generation of disinformation tailored to specific demographic\nattributes) remain largely unstudied. This paper presents the first\nlarge-scale, multilingual empirical study on persona-targeted disinformation\ngeneration by LLMs. Employing a red teaming methodology, we systematically\nevaluate the robustness of LLM safety mechanisms to persona-targeted prompts. A\nkey novel result is AI-TRAITS (AI-generaTed peRsonAlIsed disinformaTion\ndataSet), a new dataset of around 1.6 million texts generated by eight\nstate-of-the-art LLMs. AI-TRAITS is seeded by prompts that combine 324\ndisinformation narratives and 150 distinct persona profiles, covering four\nmajor languages (English, Russian, Portuguese, Hindi) and key demographic\ndimensions (country, generation, political orientation). The resulting\npersonalised narratives are then assessed quantitatively and compared along the\ndimensions of models, languages, jailbreaking rate, and personalisation\nattributes. Our findings demonstrate that the use of even simple\npersonalisation strategies in the prompts significantly increases the\nlikelihood of jailbreaks for all studied LLMs. Furthermore, personalised\nprompts result in altered linguistic and rhetorical patterns and amplify the\npersuasiveness of the LLM-generated false narratives. These insights expose\ncritical vulnerabilities in current state-of-the-art LLMs and offer a\nfoundation for improving safety alignment and detection strategies in\nmultilingual and cross-demographic contexts.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u5927\u89c4\u6a21\u8bc4\u4f30LLM\u751f\u6210\u9488\u5bf9\u7279\u5b9a\u4eba\u53e3\u5c5e\u6027\u4e2a\u6027\u5316\u865a\u5047\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u7b80\u5355\u4e2a\u6027\u5316\u7b56\u7565\u663e\u8457\u589e\u52a0\u6240\u6709LLM\u7684\u8d8a\u72f1\u6982\u7387\uff0c\u5e76\u6539\u53d8\u8bed\u8a00\u6a21\u5f0f\u589e\u5f3a\u865a\u5047\u4fe1\u606f\u8bf4\u670d\u529b\u3002", "motivation": "\u867d\u7136\u5df2\u6709\u7814\u7a76\u8868\u660eLLM\u80fd\u751f\u6210\u865a\u5047\u4fe1\u606f\uff0c\u4f46\u5173\u4e8e\u5176\u751f\u6210\u9488\u5bf9\u7279\u5b9a\u4eba\u53e3\u5c5e\u6027\uff08\u5982\u56fd\u5bb6\u3001\u4e16\u4ee3\u3001\u653f\u6cbb\u503e\u5411\uff09\u7684\u4e2a\u6027\u5316\u865a\u5047\u4fe1\u606f\u7684\u8bf4\u670d\u529b\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u91c7\u7528\u7ea2\u961f\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u7cfb\u7edf\u8bc4\u4f30LLM\u5b89\u5168\u673a\u5236\u5bf9\u4e2a\u6027\u5316\u63d0\u793a\u7684\u9c81\u68d2\u6027\uff0c\u521b\u5efa\u5305\u542b160\u4e07\u6587\u672c\u7684AI-TRAITS\u6570\u636e\u96c6\uff0c\u6db5\u76d64\u79cd\u8bed\u8a00\u3001324\u4e2a\u865a\u5047\u4fe1\u606f\u53d9\u4e8b\u548c150\u79cd\u4eba\u7269\u753b\u50cf\u3002", "result": "\u4e2a\u6027\u5316\u63d0\u793a\u663e\u8457\u589e\u52a0\u6240\u6709LLM\u8d8a\u72f1\u6982\u7387\uff0c\u6539\u53d8\u8bed\u8a00\u548c\u4fee\u8f9e\u6a21\u5f0f\uff0c\u589e\u5f3a\u865a\u5047\u4fe1\u606f\u8bf4\u670d\u529b\uff0c\u66b4\u9732\u4e86\u5f53\u524d\u6700\u5148\u8fdbLLM\u5728\u8de8\u8bed\u8a00\u548c\u8de8\u4eba\u53e3\u7edf\u8ba1\u73af\u5883\u4e2d\u7684\u5173\u952e\u6f0f\u6d1e\u3002", "conclusion": "\u5f53\u524dLLM\u5728\u5e94\u5bf9\u4e2a\u6027\u5316\u865a\u5047\u4fe1\u606f\u751f\u6210\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u6f0f\u6d1e\uff0c\u7814\u7a76\u7ed3\u679c\u4e3a\u6539\u8fdb\u591a\u8bed\u8a00\u548c\u8de8\u4eba\u53e3\u7edf\u8ba1\u73af\u5883\u4e2d\u7684\u5b89\u5168\u5bf9\u9f50\u548c\u68c0\u6d4b\u7b56\u7565\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.13003", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13003", "abs": "https://arxiv.org/abs/2510.13003", "authors": ["Yifeng Xiong", "Xiaohui Xie"], "title": "OPLoRA: Orthogonal Projection LoRA Prevents Catastrophic Forgetting during Parameter-Efficient Fine-Tuning", "comment": null, "summary": "Low-Rank Adaptation (LoRA) enables efficient fine-tuning of large language\nmodels but suffers from catastrophic forgetting when learned updates interfere\nwith the dominant singular directions that encode essential pre-trained\nknowledge. We propose Orthogonal Projection LoRA (OPLoRA), a theoretically\ngrounded approach that prevents this interference through double-sided\northogonal projections. By decomposing frozen weights via SVD, OPLoRA\nconstrains LoRA updates to lie entirely within the orthogonal complement of the\ntop-$k$ singular subspace using projections $P_L = I - U_k U_k^\\top$ and $P_R =\nI - V_k V_k^\\top$. We prove that this construction exactly preserves the\ntop-$k$ singular triples, providing mathematical guarantees for knowledge\nretention. To quantify subspace interference, we introduce $\\rho_k$, a metric\nmeasuring update alignment with dominant directions. Extensive experiments\nacross commonsense reasoning, mathematics, and code generation demonstrate that\nOPLoRA significantly reduces forgetting while maintaining competitive\ntask-specific performance on LLaMA-2 7B and Qwen2.5 7B, establishing orthogonal\nprojection as an effective mechanism for knowledge preservation in\nparameter-efficient fine-tuning.", "AI": {"tldr": "OPLoRA\u662f\u4e00\u79cd\u57fa\u4e8e\u6b63\u4ea4\u6295\u5f71\u7684LoRA\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u9762\u6b63\u4ea4\u6295\u5f71\u7ea6\u675fLoRA\u66f4\u65b0\uff0c\u9632\u6b62\u5bf9\u9884\u8bad\u7ec3\u77e5\u8bc6\u4e2d\u91cd\u8981\u5947\u5f02\u65b9\u5411\u7684\u5e72\u6270\uff0c\u4ece\u800c\u663e\u8457\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\u3002", "motivation": "\u4f20\u7edf\u7684LoRA\u5728\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5b66\u4e60\u5230\u7684\u66f4\u65b0\u4f1a\u5e72\u6270\u7f16\u7801\u91cd\u8981\u9884\u8bad\u7ec3\u77e5\u8bc6\u7684\u652f\u914d\u5947\u5f02\u65b9\u5411\u3002", "method": "\u901a\u8fc7SVD\u5206\u89e3\u51bb\u7ed3\u6743\u91cd\uff0c\u4f7f\u7528\u6295\u5f71\u77e9\u9635P_L = I - U_k U_k^\u22a4\u548cP_R = I - V_k V_k^\u22a4\u5c06LoRA\u66f4\u65b0\u7ea6\u675f\u5728top-k\u5947\u5f02\u5b50\u7a7a\u95f4\u7684\u6b63\u4ea4\u8865\u7a7a\u95f4\u4e2d\u3002", "result": "\u5728\u5e38\u8bc6\u63a8\u7406\u3001\u6570\u5b66\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cOPLoRA\u663e\u8457\u51cf\u5c11\u4e86\u9057\u5fd8\uff0c\u540c\u65f6\u5728LLaMA-2 7B\u548cQwen2.5 7B\u4e0a\u4fdd\u6301\u4e86\u7ade\u4e89\u529b\u7684\u4efb\u52a1\u7279\u5b9a\u6027\u80fd\u3002", "conclusion": "\u6b63\u4ea4\u6295\u5f71\u662f\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u4e2d\u77e5\u8bc6\u4fdd\u7559\u7684\u6709\u6548\u673a\u5236\uff0cOPLoRA\u4e3a\u77e5\u8bc6\u4fdd\u7559\u63d0\u4f9b\u4e86\u6570\u5b66\u4fdd\u8bc1\u3002"}}
{"id": "2510.13008", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13008", "abs": "https://arxiv.org/abs/2510.13008", "authors": ["Pavan Kalyan", "Shubhra Mishra", "Satya Lokam", "Navin Goyal"], "title": "CurLL: A Developmental Framework to Evaluate Continual Learning in Language Models", "comment": null, "summary": "We introduce a comprehensive continual learning dataset and benchmark (CurlL)\ngrounded in human developmental trajectories from ages 5-10, enabling\nsystematic and fine-grained assessment of models' ability to progressively\nacquire new skills. CurlL spans five developmental stages (0-4) covering ages\n5-10, supported by a skill graph that breaks down broad skills into smaller\nabilities, concrete goals, and measurable indicators, while also capturing\nwhich abilities build on others. We generate a 23.4B-token synthetic dataset\nwith controlled skill progression, vocabulary complexity, and format diversity,\ncomprising paragraphs, comprehension-based QA (CQA), skill-testing QA (CSQA),\nand instruction-response (IR) pairs. Stage-wise token counts range from 2.12B\nto 6.78B tokens, supporting precise analysis of forgetting, forward transfer,\nand backward transfer. Using a 135M-parameter transformer trained under\nindependent, joint, and sequential (continual) setups, we show trade-offs in\nskill retention and transfer efficiency. By mirroring human learning patterns\nand providing fine-grained control over skill dependencies, this work advances\ncontinual learning evaluations for language models.", "AI": {"tldr": "CurlL\u662f\u4e00\u4e2a\u57fa\u4e8e\u4eba\u7c7b5-10\u5c81\u53d1\u5c55\u8f68\u8ff9\u7684\u6301\u7eed\u5b66\u4e60\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u5305\u542b\u4e94\u4e2a\u53d1\u5c55\u9636\u6bb5\uff0c\u901a\u8fc7\u6280\u80fd\u56fe\u8c31\u5c06\u5e7f\u6cdb\u6280\u80fd\u5206\u89e3\u4e3a\u66f4\u5c0f\u7684\u80fd\u529b\u3001\u5177\u4f53\u76ee\u6807\u548c\u53ef\u6d4b\u91cf\u6307\u6807\uff0c\u5e76\u6355\u6349\u80fd\u529b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "motivation": "\u5f53\u524d\u6301\u7eed\u5b66\u4e60\u8bc4\u4f30\u7f3a\u4e4f\u57fa\u4e8e\u4eba\u7c7b\u53d1\u5c55\u8f68\u8ff9\u7684\u7cfb\u7edf\u6027\u548c\u7ec6\u7c92\u5ea6\u8bc4\u4f30\uff0c\u9700\u8981\u80fd\u591f\u7cbe\u786e\u5206\u6790\u6a21\u578b\u9010\u6b65\u83b7\u53d6\u65b0\u6280\u80fd\u80fd\u529b\u7684\u6570\u636e\u96c6\u3002", "method": "\u751f\u6210\u4e86234\u4ebftoken\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u63a7\u5236\u6280\u80fd\u8fdb\u5c55\u3001\u8bcd\u6c47\u590d\u6742\u6027\u548c\u683c\u5f0f\u591a\u6837\u6027\uff0c\u5305\u542b\u6bb5\u843d\u3001\u7406\u89e3\u578bQA\u3001\u6280\u80fd\u6d4b\u8bd5QA\u548c\u6307\u4ee4-\u54cd\u5e94\u5bf9\u3002\u4f7f\u75281.35\u4ebf\u53c2\u6570\u7684transformer\u6a21\u578b\u5728\u72ec\u7acb\u3001\u8054\u5408\u548c\u987a\u5e8f\uff08\u6301\u7eed\uff09\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5c55\u793a\u4e86\u6280\u80fd\u4fdd\u6301\u548c\u8fc1\u79fb\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u652f\u6301\u5bf9\u9057\u5fd8\u3001\u524d\u5411\u8fc1\u79fb\u548c\u540e\u5411\u8fc1\u79fb\u7684\u7cbe\u786e\u5206\u6790\u3002", "conclusion": "\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u5b66\u4e60\u6a21\u5f0f\u5e76\u63d0\u4f9b\u5bf9\u6280\u80fd\u4f9d\u8d56\u5173\u7cfb\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u8fd9\u9879\u5de5\u4f5c\u63a8\u8fdb\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u6301\u7eed\u5b66\u4e60\u8bc4\u4f30\u3002"}}
{"id": "2510.13022", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13022", "abs": "https://arxiv.org/abs/2510.13022", "authors": ["Jiacheng Guo", "Zihao Li", "Jiahao Qiu", "Yue Wu", "Mengdi Wang"], "title": "On the Role of Preference Variance in Preference Optimization", "comment": null, "summary": "Direct Preference Optimization (DPO) has emerged as an important approach for\nlearning from human preferences in aligning large language models (LLMs).\nHowever, collecting human preference data is costly and inefficient, motivating\nmethods to reduce the required annotations. In this work, we investigate the\nimpact of \\emph{preference variance} (PVar), which measures the variance in\nmodel preferences when comparing pairs of responses, on the effectiveness of\nDPO training. We provide a theoretical insight by establishing an upper bound\non the DPO gradient norm for any given prompt, showing it is controlled by the\nPVar of that prompt. This implies that prompts with low PVar can only produce\nsmall gradient updates, making them less valuable for learning. We validate\nthis finding by fine-tuning LLMs with preferences generated by a reward model,\nevaluating on two benchmarks (AlpacaEval 2.0 and Arena-Hard). Experimental\nresults demonstrate that prompts with higher PVar outperform randomly selected\nprompts or those with lower PVar. We also show that our PVar-based selection\nmethod is robust, when using smaller reward models (1B, 3B) for selection.\nNotably, in a separate experiment using the original human annotations from the\nUltraFeedback dataset, we found that training on only the top 10\\% of prompts\nwith the highest PVar yields better evaluation performance than training on the\nfull dataset, highlighting the importance of preference variance in identifying\ninformative examples for efficient LLM alignment.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u504f\u597d\u65b9\u5dee\uff08PVar\uff09\u5bf9\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u8bad\u7ec3\u6548\u679c\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u9ad8PVar\u7684\u63d0\u793a\u80fd\u4ea7\u751f\u66f4\u5927\u7684\u68af\u5ea6\u66f4\u65b0\uff0c\u4ece\u800c\u66f4\u6709\u6548\u5730\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u6536\u96c6\u4eba\u7c7b\u504f\u597d\u6570\u636e\u6210\u672c\u9ad8\u6602\u4e14\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u51cf\u5c11\u6240\u9700\u6807\u6ce8\u7684\u65b9\u6cd5\u3002\u504f\u597d\u65b9\u5dee\u53ef\u80fd\u5f71\u54cdDPO\u8bad\u7ec3\u7684\u6548\u679c\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u5efa\u7acbDPO\u68af\u5ea6\u8303\u6570\u7684\u4e0a\u754c\uff0c\u8bc1\u660e\u5176\u53d7PVar\u63a7\u5236\uff1b\u4f7f\u7528\u5956\u52b1\u6a21\u578b\u751f\u6210\u504f\u597d\u6570\u636e\uff0c\u5728AlpacaEval 2.0\u548cArena-Hard\u57fa\u51c6\u4e0a\u8bc4\u4f30\uff1b\u6bd4\u8f83\u4e0d\u540cPVar\u6c34\u5e73\u63d0\u793a\u7684\u8bad\u7ec3\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u9ad8PVar\u63d0\u793a\u4f18\u4e8e\u968f\u673a\u9009\u62e9\u6216\u4f4ePVar\u63d0\u793a\uff1b\u4f7f\u7528\u5c0f\u5956\u52b1\u6a21\u578b\uff081B\u30013B\uff09\u8fdb\u884c\u9009\u62e9\u65f6\u65b9\u6cd5\u4f9d\u7136\u7a33\u5065\uff1b\u5728UltraFeedback\u6570\u636e\u96c6\u4e0a\uff0c\u4ec5\u4f7f\u7528\u524d10%\u9ad8PVar\u63d0\u793a\u8bad\u7ec3\u6bd4\u4f7f\u7528\u5b8c\u6574\u6570\u636e\u96c6\u6548\u679c\u66f4\u597d\u3002", "conclusion": "\u504f\u597d\u65b9\u5dee\u662f\u8bc6\u522b\u4fe1\u606f\u4e30\u5bcc\u793a\u4f8b\u4ee5\u8fdb\u884c\u9ad8\u6548LLM\u5bf9\u9f50\u7684\u91cd\u8981\u6307\u6807\uff0c\u9ad8PVar\u63d0\u793a\u80fd\u663e\u8457\u63d0\u5347DPO\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2510.13079", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13079", "abs": "https://arxiv.org/abs/2510.13079", "authors": ["Chen Zheng", "Yuhang Cai", "Deyi Liu", "Jin Ma", "Yiyuan Ma", "Yuan Yang", "Jing Liu", "Yutao Zeng", "Xun Zhou", "Siyuan Qiao"], "title": "GatePro: Parameter-Free Expert Selection Optimization for Mixture-of-Experts Models", "comment": null, "summary": "Modern large language models leverage Mixture-of-Experts (MoE) architectures\nfor efficient scaling, but face a critical challenge: functionally similar\nexperts are often selected simultaneously, creating redundant computation and\nlimiting effective model capacity. Existing auxiliary balance loss methods\nimprove token distribution but fail to address the underlying expert diversity\nproblem. We introduce GatePro, a novel parameter-free method that directly\npromotes expert selection diversity. GatePro identifies the most similar expert\npairs and introduces localized competition mechanisms, preventing redundant\nexpert co-activation while maintaining natural expert specialization. Our\ncomprehensive evaluation demonstrates GatePro's effectiveness across model\nscales and benchmarks. Analysis demonstrates GatePro's ability to achieve\nenhanced expert diversity, where experts develop more distinct and\ncomplementary capabilities, avoiding functional redundancy. This approach can\nbe deployed hot-swappable during any training phase without additional\nlearnable parameters, offering a practical solution for improving MoE\neffectiveness.", "AI": {"tldr": "GatePro\u662f\u4e00\u79cd\u65e0\u9700\u53c2\u6570\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u76f8\u4f3c\u4e13\u5bb6\u4e4b\u95f4\u5f15\u5165\u5c40\u90e8\u7ade\u4e89\u673a\u5236\u6765\u63d0\u5347MoE\u67b6\u6784\u4e2d\u7684\u4e13\u5bb6\u9009\u62e9\u591a\u6837\u6027\uff0c\u907f\u514d\u529f\u80fd\u5197\u4f59\u8ba1\u7b97\u3002", "motivation": "\u73b0\u6709\u7684MoE\u67b6\u6784\u9762\u4e34\u529f\u80fd\u76f8\u4f3c\u4e13\u5bb6\u540c\u65f6\u88ab\u9009\u62e9\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u5197\u4f59\u8ba1\u7b97\u548c\u6709\u6548\u6a21\u578b\u5bb9\u91cf\u53d7\u9650\uff0c\u800c\u73b0\u6709\u7684\u5e73\u8861\u635f\u5931\u65b9\u6cd5\u65e0\u6cd5\u89e3\u51b3\u6839\u672c\u7684\u4e13\u5bb6\u591a\u6837\u6027\u95ee\u9898\u3002", "method": "GatePro\u8bc6\u522b\u6700\u76f8\u4f3c\u7684\u4e13\u5bb6\u5bf9\uff0c\u5e76\u5f15\u5165\u5c40\u90e8\u7ade\u4e89\u673a\u5236\uff0c\u9632\u6b62\u5197\u4f59\u4e13\u5bb6\u540c\u65f6\u6fc0\u6d3b\uff0c\u540c\u65f6\u4fdd\u6301\u81ea\u7136\u7684\u4e13\u5bb6\u4e13\u4e1a\u5316\u3002", "result": "\u7efc\u5408\u8bc4\u4f30\u8868\u660eGatePro\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u6709\u6548\uff0c\u80fd\u591f\u5b9e\u73b0\u589e\u5f3a\u7684\u4e13\u5bb6\u591a\u6837\u6027\uff0c\u4f7f\u4e13\u5bb6\u53d1\u5c55\u51fa\u66f4\u72ec\u7279\u548c\u4e92\u8865\u7684\u80fd\u529b\u3002", "conclusion": "GatePro\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u5728\u4efb\u4f55\u8bad\u7ec3\u9636\u6bb5\u70ed\u63d2\u62d4\u90e8\u7f72\uff0c\u65e0\u9700\u989d\u5916\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\u6709\u6548\u63d0\u5347MoE\u67b6\u6784\u7684\u6548\u7387\u3002"}}
{"id": "2510.13103", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13103", "abs": "https://arxiv.org/abs/2510.13103", "authors": ["Mingda Li", "Xinyu Li", "Weinan Zhang", "Longxuan Ma"], "title": "ESI: Epistemic Uncertainty Quantification via Semantic-preserving Intervention for Large Language Models", "comment": null, "summary": "Uncertainty Quantification (UQ) is a promising approach to improve model\nreliability, yet quantifying the uncertainty of Large Language Models (LLMs) is\nnon-trivial. In this work, we establish a connection between the uncertainty of\nLLMs and their invariance under semantic-preserving intervention from a causal\nperspective. Building on this foundation, we propose a novel grey-box\nuncertainty quantification method that measures the variation in model outputs\nbefore and after the semantic-preserving intervention. Through theoretical\njustification, we show that our method provides an effective estimate of\nepistemic uncertainty. Our extensive experiments, conducted across various LLMs\nand a variety of question-answering (QA) datasets, demonstrate that our method\nexcels not only in terms of effectiveness but also in computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u89c6\u89d2\u7684\u7070\u76d2\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u4fdd\u6301\u5e72\u9884\u524d\u540e\u6a21\u578b\u8f93\u51fa\u7684\u53d8\u5316\u6765\u4f30\u8ba1LLM\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u5177\u6709\u91cd\u8981\u610f\u4e49\u4f46\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u4ece\u56e0\u679c\u89c6\u89d2\u5efa\u7acbLLM\u4e0d\u786e\u5b9a\u6027\u4e0e\u8bed\u4e49\u4fdd\u6301\u5e72\u9884\u4e0b\u4e0d\u53d8\u6027\u7684\u8054\u7cfb\uff0c\u63d0\u51fa\u7070\u76d2\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u6d4b\u91cf\u8bed\u4e49\u4fdd\u6301\u5e72\u9884\u524d\u540e\u6a21\u578b\u8f93\u51fa\u7684\u53d8\u5316\u3002", "result": "\u5728\u591a\u79cdLLM\u548c\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6709\u6548\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aLLM\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u9ad8\u6548\u7684\u65b0\u9014\u5f84\uff0c\u901a\u8fc7\u56e0\u679c\u89c6\u89d2\u5efa\u7acb\u4e86\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2510.13115", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13115", "abs": "https://arxiv.org/abs/2510.13115", "authors": ["Surya Tejaswi Yerramsetty", "Almas Fathimah"], "title": "Multi-Label Clinical Text Eligibility Classification and Summarization System", "comment": null, "summary": "Clinical trials are central to medical progress because they help improve\nunderstanding of human health and the healthcare system. They play a key role\nin discovering new ways to detect, prevent, or treat diseases, and it is\nessential that clinical trials include participants with appropriate and\ndiverse medical backgrounds. In this paper, we propose a system that leverages\nNatural Language Processing (NLP) and Large Language Models (LLMs) to automate\nmulti-label clinical text eligibility classification and summarization. The\nsystem combines feature extraction methods such as word embeddings (Word2Vec)\nand named entity recognition to identify relevant medical concepts, along with\ntraditional vectorization techniques such as count vectorization and TF-IDF\n(Term Frequency-Inverse Document Frequency). We further explore weighted TF-IDF\nword embeddings that integrate both count-based and embedding-based strengths\nto capture term importance effectively. Multi-label classification using Random\nForest and SVM models is applied to categorize documents based on eligibility\ncriteria. Summarization techniques including TextRank, Luhn, and GPT-3 are\nevaluated to concisely summarize eligibility requirements. Evaluation with\nROUGE scores demonstrates the effectiveness of the proposed methods. This\nsystem shows potential for automating clinical trial eligibility assessment\nusing data-driven approaches, thereby improving research efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408NLP\u548cLLM\u7684\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u7528\u4e8e\u591a\u6807\u7b7e\u4e34\u5e8a\u6587\u672c\u8d44\u683c\u5206\u7c7b\u548c\u6458\u8981\u751f\u6210\uff0c\u901a\u8fc7\u7279\u5f81\u63d0\u53d6\u548c\u591a\u6807\u7b7e\u5206\u7c7b\u65b9\u6cd5\u63d0\u9ad8\u4e34\u5e8a\u8bd5\u9a8c\u8d44\u683c\u8bc4\u4f30\u6548\u7387\u3002", "motivation": "\u4e34\u5e8a\u8bd5\u9a8c\u5728\u533b\u5b66\u8fdb\u6b65\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9700\u8981\u5305\u542b\u5177\u6709\u9002\u5f53\u548c\u591a\u6837\u5316\u533b\u5b66\u80cc\u666f\u7684\u53c2\u4e0e\u8005\u3002\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u81ea\u52a8\u5316\u7cfb\u7edf\u6765\u6539\u8fdb\u4e34\u5e8a\u8bd5\u9a8c\u8d44\u683c\u8bc4\u4f30\u6d41\u7a0b\u3002", "method": "\u4f7f\u7528NLP\u548cLLM\u6280\u672f\uff0c\u7ed3\u5408\u8bcd\u5d4c\u5165(Word2Vec)\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u8ba1\u6570\u5411\u91cf\u5316\u548cTF-IDF\u7b49\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\uff0c\u5f00\u53d1\u52a0\u6743TF-IDF\u8bcd\u5d4c\u5165\u3002\u5e94\u7528\u968f\u673a\u68ee\u6797\u548cSVM\u8fdb\u884c\u591a\u6807\u7b7e\u5206\u7c7b\uff0c\u8bc4\u4f30TextRank\u3001Luhn\u548cGPT-3\u7b49\u6458\u8981\u6280\u672f\u3002", "result": "\u901a\u8fc7ROUGE\u5206\u6570\u8bc4\u4f30\u663e\u793a\u6240\u63d0\u65b9\u6cd5\u6709\u6548\uff0c\u7cfb\u7edf\u5728\u81ea\u52a8\u5316\u4e34\u5e8a\u8bd5\u9a8c\u8d44\u683c\u8bc4\u4f30\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u80fd\u591f\u63d0\u9ad8\u7814\u7a76\u6548\u7387\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5c55\u793a\u4e86\u4f7f\u7528\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u81ea\u52a8\u5316\u4e34\u5e8a\u8bd5\u9a8c\u8d44\u683c\u8bc4\u4f30\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u6539\u8fdb\u533b\u7597\u7814\u7a76\u6548\u7387\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.13143", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13143", "abs": "https://arxiv.org/abs/2510.13143", "authors": ["Junichiro Niimi"], "title": "Stable LLM Ensemble: Interaction between Example Representativeness and Diversity", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable results in wide range\nof domains. However, the accuracy and robustness of one-shot LLM predictions\nremain highly sensitive to the examples and the diversity among ensemble\nmembers. This study systematically investigates the effects of example\nrepresentativeness (one-shot strategy) and output diversity (sampling\ntemperature) on LLM ensemble performance. Two one-shot strategies are compared:\ncentroid-based representative examples (proposed) and randomly sampled examples\n(baseline) and sampling temperature also is varied. The proposed approach with\nhigher temperature setting significantly outperforms random selection by +7.6%\n(macro-F1) and -10.5% (RMSE). Furthermore, the proposed model exceeds 5-shot\nprompting by +21.1% (macro-F1) and -24.0% (RMSE). Our findings demonstrate that\ncombining representative example selection with increased temperature provides\nthe appropriate level of diversity to the ensemble. This work highlights the\npractical importance of both example selection and controlled diversity in\ndesigning effective one-shot LLM ensembles.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4ee3\u8868\u6027\u793a\u4f8b\u9009\u62e9\u548c\u589e\u52a0\u91c7\u6837\u6e29\u5ea6\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u5355\u6837\u672cLLM\u96c6\u6210\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5728\u5b8fF1\u548cRMSE\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u968f\u673a\u9009\u62e9\u548c5\u6837\u672c\u63d0\u793a\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5355\u6837\u672cLLM\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u5bf9\u793a\u4f8b\u9009\u62e9\u548c\u96c6\u6210\u6210\u5458\u591a\u6837\u6027\u9ad8\u5ea6\u654f\u611f\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u793a\u4f8b\u4ee3\u8868\u6027\u548c\u8f93\u51fa\u591a\u6837\u6027\u5bf9\u96c6\u6210\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u6bd4\u8f83\u4e24\u79cd\u5355\u6837\u672c\u7b56\u7565\uff1a\u57fa\u4e8e\u8d28\u5fc3\u7684\u4ee3\u8868\u6027\u793a\u4f8b\uff08\u63d0\u51fa\u65b9\u6cd5\uff09\u548c\u968f\u673a\u91c7\u6837\u793a\u4f8b\uff08\u57fa\u7ebf\uff09\uff0c\u540c\u65f6\u53d8\u5316\u91c7\u6837\u6e29\u5ea6\u53c2\u6570\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8f83\u9ad8\u6e29\u5ea6\u8bbe\u7f6e\u4e0b\u663e\u8457\u4f18\u4e8e\u968f\u673a\u9009\u62e9\uff1a\u5b8fF1\u63d0\u9ad87.6%\uff0cRMSE\u964d\u4f4e10.5%\uff1b\u540c\u65f6\u8d85\u8fc75\u6837\u672c\u63d0\u793a\uff1a\u5b8fF1\u63d0\u9ad821.1%\uff0cRMSE\u964d\u4f4e24.0%\u3002", "conclusion": "\u7ed3\u5408\u4ee3\u8868\u6027\u793a\u4f8b\u9009\u62e9\u4e0e\u589e\u52a0\u6e29\u5ea6\u80fd\u4e3a\u96c6\u6210\u63d0\u4f9b\u9002\u5f53\u6c34\u5e73\u7684\u591a\u6837\u6027\uff0c\u5f3a\u8c03\u4e86\u793a\u4f8b\u9009\u62e9\u548c\u53d7\u63a7\u591a\u6837\u6027\u5728\u8bbe\u8ba1\u6709\u6548\u5355\u6837\u672cLLM\u96c6\u6210\u4e2d\u7684\u5b9e\u9645\u91cd\u8981\u6027\u3002"}}
{"id": "2510.13154", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13154", "abs": "https://arxiv.org/abs/2510.13154", "authors": ["Pardis Sadat Zahraei", "Ehsaneddin Asgari"], "title": "I Am Aligned, But With Whom? MENA Values Benchmark for Evaluating Cultural Alignment and Multilingual Bias in LLMs", "comment": null, "summary": "We introduce MENAValues, a novel benchmark designed to evaluate the cultural\nalignment and multilingual biases of large language models (LLMs) with respect\nto the beliefs and values of the Middle East and North Africa (MENA) region, an\nunderrepresented area in current AI evaluation efforts. Drawing from\nlarge-scale, authoritative human surveys, we curate a structured dataset that\ncaptures the sociocultural landscape of MENA with population-level response\ndistributions from 16 countries. To probe LLM behavior, we evaluate diverse\nmodels across multiple conditions formed by crossing three perspective framings\n(neutral, personalized, and third-person/cultural observer) with two language\nmodes (English and localized native languages: Arabic, Persian, Turkish). Our\nanalysis reveals three critical phenomena: \"Cross-Lingual Value Shifts\" where\nidentical questions yield drastically different responses based on language,\n\"Reasoning-Induced Degradation\" where prompting models to explain their\nreasoning worsens cultural alignment, and \"Logit Leakage\" where models refuse\nsensitive questions while internal probabilities reveal strong hidden\npreferences. We further demonstrate that models collapse into simplistic\nlinguistic categories when operating in native languages, treating diverse\nnations as monolithic entities. MENAValues offers a scalable framework for\ndiagnosing cultural misalignment, providing both empirical insights and\nmethodological tools for developing more culturally inclusive AI.", "AI": {"tldr": "MENAValues\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e2d\u4e1c\u548c\u5317\u975e\u5730\u533a\u6587\u5316\u5bf9\u9f50\u548c\u504f\u89c1\u7684\u65b0\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u8de8\u8bed\u8a00\u4ef7\u503c\u504f\u79fb\u3001\u63a8\u7406\u8bf1\u5bfc\u9000\u5316\u7b49\u5173\u952e\u73b0\u8c61\u3002", "motivation": "\u4e2d\u4e1c\u548c\u5317\u975e\u5730\u533a\u5728\u5f53\u524dAI\u8bc4\u4f30\u5de5\u4f5c\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u4e13\u95e8\u5de5\u5177\u6765\u8bc4\u4f30LLMs\u4e0e\u8be5\u5730\u533a\u6587\u5316\u548c\u4ef7\u503c\u89c2\u7684\u5bf9\u9f50\u7a0b\u5ea6\u3002", "method": "\u57fa\u4e8e\u5927\u89c4\u6a21\u4eba\u7c7b\u8c03\u67e5\u6784\u5efa\u7ed3\u6784\u5316\u6570\u636e\u96c6\uff0c\u5728\u4e09\u79cd\u89c6\u89d2\u6846\u67b6\uff08\u4e2d\u6027\u3001\u4e2a\u6027\u5316\u3001\u7b2c\u4e09\u4eba\u79f0\uff09\u548c\u4e24\u79cd\u8bed\u8a00\u6a21\u5f0f\uff08\u82f1\u8bed\u548c\u672c\u5730\u8bed\u8a00\uff09\u4e0b\u8bc4\u4f30\u591a\u79cd\u6a21\u578b\u3002", "result": "\u53d1\u73b0\u8de8\u8bed\u8a00\u4ef7\u503c\u504f\u79fb\u3001\u63a8\u7406\u8bf1\u5bfc\u9000\u5316\u548cLogit\u6cc4\u6f0f\u4e09\u4e2a\u5173\u952e\u73b0\u8c61\uff0c\u6a21\u578b\u5728\u672c\u5730\u8bed\u8a00\u4e2d\u4f1a\u5c06\u591a\u6837\u56fd\u5bb6\u89c6\u4e3a\u5355\u4e00\u5b9e\u4f53\u3002", "conclusion": "MENAValues\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u8bca\u65ad\u6587\u5316\u9519\u4f4d\u6846\u67b6\uff0c\u4e3a\u5f00\u53d1\u66f4\u5177\u6587\u5316\u5305\u5bb9\u6027\u7684AI\u63d0\u4f9b\u5b9e\u8bc1\u89c1\u89e3\u548c\u65b9\u6cd5\u5de5\u5177\u3002"}}
{"id": "2510.13161", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13161", "abs": "https://arxiv.org/abs/2510.13161", "authors": ["Nikhil Bhendawade", "Kumari Nishu", "Arnav Kundu", "Chris Bartels", "Minsik Cho", "Irina Belousova"], "title": "Mirror Speculative Decoding: Breaking the Serial Barrier in LLM Inference", "comment": null, "summary": "Speculative decoding accelerates LLM inference by using a draft model to look\nahead, but gains are capped by the cost of autoregressive draft generation:\nincreasing draft size elevates acceptance rates but introduces additional\nlatency overhead exacerbating the speed-accuracy tradeoff. Prior methods\n(Medusa, Hydra, EAGLE) partially reduce draft cost but either degrade\nacceptance or introduce overheads that limit scaling. We present Mirror\nSpeculative Decoding (Mirror-SD), an inference algorithm that breaks the\nlatency-acceptance tradeoff. Mirror-SD launches branch-complete rollouts from\nearly-exit signals in parallel with the target model's suffix and explicitly\nmaps computation across heterogeneous accelerators (GPU and NPU) to exploit\ncross-device parallelism. The draft speculates forward continuations for the\ntarget to verify, while the target simultaneously speculates correction paths\nfor the draft, converting speculation into two complementary execution\npipelines. To further cut draft latency without weakening acceptance semantics,\nwe add speculative streaming so the draft emits multiple tokens per step. This\ndual strategy of parallel heterogeneous execution plus multi-token speculative\nstreaming pushes speculative decoding toward its ideal regime of high\nacceptance with low overhead. On SpecBench with server-scale models from 14B to\n66B parameters, Mirror-SD delivers consistent end-to-end gains, achieving\n2.8x-5.8x wall-time speedups across diverse tasks and a 30% average relative\nimprovement over the strongest baseline, EAGLE3.", "AI": {"tldr": "Mirror-SD\u662f\u4e00\u79cd\u63a8\u7406\u7b97\u6cd5\uff0c\u901a\u8fc7\u5e76\u884c\u5f02\u6784\u6267\u884c\u548c\u591a\u4ee4\u724c\u63a8\u6d4b\u6d41\u5f0f\u5904\u7406\uff0c\u6253\u7834\u5ef6\u8fdf-\u63a5\u53d7\u7387\u7684\u6743\u8861\uff0c\u5728\u4fdd\u6301\u9ad8\u63a5\u53d7\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u4f20\u7edf\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u5728\u589e\u52a0\u8349\u6848\u5927\u5c0f\u65f6\u4f1a\u63d0\u9ad8\u63a5\u53d7\u7387\u4f46\u5f15\u5165\u989d\u5916\u5ef6\u8fdf\uff0c\u73b0\u6709\u65b9\u6cd5\uff08Medusa\u3001Hydra\u3001EAGLE\uff09\u8981\u4e48\u964d\u4f4e\u63a5\u53d7\u7387\uff0c\u8981\u4e48\u5f15\u5165\u9650\u5236\u6269\u5c55\u7684\u5f00\u9500\u3002", "method": "\u4f7f\u7528\u5206\u652f\u5b8c\u6574\u5c55\u5f00\u4ece\u65e9\u671f\u9000\u51fa\u4fe1\u53f7\u5e76\u884c\u542f\u52a8\uff0c\u5728\u5f02\u6784\u52a0\u901f\u5668\uff08GPU\u548cNPU\uff09\u4e0a\u663e\u5f0f\u6620\u5c04\u8ba1\u7b97\u4ee5\u5229\u7528\u8de8\u8bbe\u5907\u5e76\u884c\u6027\uff1b\u8349\u6848\u63a8\u6d4b\u76ee\u6807\u6a21\u578b\u7684\u524d\u5411\u5ef6\u7eed\uff0c\u76ee\u6807\u6a21\u578b\u540c\u65f6\u63a8\u6d4b\u8349\u6848\u7684\u4fee\u6b63\u8def\u5f84\uff1b\u6dfb\u52a0\u63a8\u6d4b\u6d41\u5f0f\u5904\u7406\u4f7f\u8349\u6848\u6bcf\u6b65\u751f\u6210\u591a\u4e2a\u4ee4\u724c\u3002", "result": "\u5728SpecBench\u4e0a\uff0c\u4f7f\u752814B\u523066B\u53c2\u6570\u7684\u670d\u52a1\u5668\u89c4\u6a21\u6a21\u578b\uff0cMirror-SD\u5b9e\u73b0\u4e862.8\u500d\u52305.8\u500d\u7684\u7aef\u5230\u7aef\u52a0\u901f\uff0c\u76f8\u6bd4\u6700\u5f3a\u57fa\u7ebfEAGLE3\u5e73\u5747\u76f8\u5bf9\u63d0\u534730%\u3002", "conclusion": "Mirror-SD\u901a\u8fc7\u5e76\u884c\u5f02\u6784\u6267\u884c\u548c\u591a\u4ee4\u724c\u63a8\u6d4b\u6d41\u5f0f\u5904\u7406\u7684\u53cc\u91cd\u7b56\u7565\uff0c\u5c06\u63a8\u6d4b\u89e3\u7801\u63a8\u5411\u9ad8\u63a5\u53d7\u7387\u548c\u4f4e\u5f00\u9500\u7684\u7406\u60f3\u72b6\u6001\u3002"}}
{"id": "2510.13163", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13163", "abs": "https://arxiv.org/abs/2510.13163", "authors": ["Nyx Iskandar", "Hisham Bedri", "Andy Tsen"], "title": "A Matter of Representation: Towards Graph-Based Abstract Code Generation", "comment": null, "summary": "Most large language models (LLMs) today excel at generating raw, sequential\ncode with minimal abstractions and custom structures. However, there has been\nlittle work on graph-based abstract code generation, where significant logic is\nencapsulated in predefined nodes and execution flow is determined by edges.\nThis is relevant for visual programming languages, and in cases where raw\nsource code is inaccessible to users and LLM training sets. In this work, we\npropose and evaluate JSON representations for graphs to enable high accuracy\ngraph-based abstract code generation. We evaluate these representations on\nScratchTest, a mini-benchmark based on our custom Python re-implementation of\nScratch, which tests the LLM in code graph space. Our findings demonstrate that\nLLMs can indeed perform the aforementioned generation task in a single pass\nwithout relying on specialized or complex pipelines, given the correct graph\nrepresentations. We also show that different representations induce\nsignificantly different accuracies, highlighting the instrumental role of\nrepresentations in this generation task. All in all, this work establishes the\nfirst steps towards representation learning for graph-based abstract code\ngeneration.", "AI": {"tldr": "LLMs can generate graph-based abstract code using JSON representations, achieving high accuracy without complex pipelines, with representation choice significantly impacting performance.", "motivation": "Most LLMs focus on sequential code generation, but there's little work on graph-based abstract code generation where logic is encapsulated in nodes and execution flow by edges, which is important for visual programming languages and when source code is inaccessible.", "method": "Proposed and evaluated JSON representations for graphs, tested on ScratchTest benchmark based on custom Python re-implementation of Scratch to assess LLM performance in code graph space.", "result": "LLMs can perform graph-based abstract code generation in single pass without specialized pipelines using correct graph representations, with different representations inducing significantly different accuracies.", "conclusion": "This work establishes initial steps towards representation learning for graph-based abstract code generation, highlighting the critical role of graph representations in this task."}}
{"id": "2510.13166", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13166", "abs": "https://arxiv.org/abs/2510.13166", "authors": ["Kehua Feng", "Keyan Ding", "Zhihui Zhu", "Lei Liang", "Qiang Zhang", "Huajun Chen"], "title": "CoT-Evo: Evolutionary Distillation of Chain-of-Thought for Scientific Reasoning", "comment": "28 pages, 3 figures", "summary": "While chain-of-thought (CoT) distillation from advanced large language models\n(LLMs) has proven effective in general reasoning tasks, it struggles in\nscientific domains where even advanced models often produce incorrect or\nsuperficial reasoning due to high complexity and specialized knowledge\nrequirements. Directly distilling from such flawed outputs results in\nlow-quality training data and limits the performance of smaller student models.\nTo overcome this, we propose CoT-Evo, an evolutionary CoT distillation\nframework. It begins by constructing a diverse pool of reasoning trajectories\nfrom multiple LLM thinkers, enriches them with automatically retrieved domain\nknowledge, and iteratively refines the trajectories using novelty-driven\nselection, reflective recombination and mutation. The refinement is guided by a\nfitness function that evaluates answer correctness, coherence, and effective\nknowledge utilization. This results in a high-quality CoT dataset tailored for\nscientific reasoning. We employ this evolved dataset to fine-tune a compact\nmodel, which achieves state-of-the-art performance on scientific reasoning\nbenchmarks. Our work establishes a scalable approach to synthesizing\nhigh-fidelity scientific reasoning data from diverse and fallible LLMs.", "AI": {"tldr": "CoT-Evo\uff1a\u4e00\u79cd\u8fdb\u5316\u5f0f\u601d\u7ef4\u94fe\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u591aLLM\u601d\u8003\u8005\u6784\u5efa\u591a\u6837\u5316\u63a8\u7406\u8f68\u8ff9\uff0c\u7ed3\u5408\u77e5\u8bc6\u68c0\u7d22\u548c\u8fdb\u5316\u7b97\u6cd5\u8fed\u4ee3\u4f18\u5316\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u79d1\u5b66\u63a8\u7406\u6570\u636e\u96c6\uff0c\u4f7f\u5c0f\u578b\u6a21\u578b\u5728\u79d1\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u601d\u7ef4\u94fe\u84b8\u998f\u5728\u79d1\u5b66\u9886\u57df\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5373\u4f7f\u662f\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u4e5f\u7ecf\u5e38\u4ea7\u751f\u9519\u8bef\u6216\u80a4\u6d45\u7684\u63a8\u7406\uff0c\u76f4\u63a5\u84b8\u998f\u4f1a\u4ea7\u751f\u4f4e\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u9650\u5236\u5b66\u751f\u6a21\u578b\u6027\u80fd\u3002", "method": "\u6784\u5efa\u591a\u6837\u5316\u63a8\u7406\u8f68\u8ff9\u6c60\uff0c\u81ea\u52a8\u68c0\u7d22\u9886\u57df\u77e5\u8bc6\u8fdb\u884c\u4e30\u5bcc\uff0c\u901a\u8fc7\u65b0\u9896\u6027\u9a71\u52a8\u9009\u62e9\u3001\u53cd\u601d\u6027\u91cd\u7ec4\u548c\u53d8\u5f02\u8fed\u4ee3\u4f18\u5316\u63a8\u7406\u8f68\u8ff9\uff0c\u4f7f\u7528\u8bc4\u4f30\u7b54\u6848\u6b63\u786e\u6027\u3001\u8fde\u8d2f\u6027\u548c\u77e5\u8bc6\u5229\u7528\u6548\u7387\u7684\u9002\u5e94\u5ea6\u51fd\u6570\u6307\u5bfc\u4f18\u5316\u8fc7\u7a0b\u3002", "result": "\u751f\u6210\u9ad8\u8d28\u91cf\u79d1\u5b66\u63a8\u7406\u601d\u7ef4\u94fe\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u5fae\u8c03\u7684\u7d27\u51d1\u6a21\u578b\u5728\u79d1\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "CoT-Evo\u5efa\u7acb\u4e86\u4ece\u591a\u6837\u5316\u548c\u6613\u51fa\u9519\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u5408\u6210\u9ad8\u4fdd\u771f\u79d1\u5b66\u63a8\u7406\u6570\u636e\u7684\u53ef\u6269\u5c55\u65b9\u6cd5\u3002"}}
{"id": "2510.13170", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13170", "abs": "https://arxiv.org/abs/2510.13170", "authors": ["Xiaoshu Chen", "Sihang Zhou", "Ke Liang", "Duanyang Yuan", "Haoyuan Chen", "Xiaoyu Sun", "Linyuan Meng", "Xinwang Liu"], "title": "Putting on the Thinking Hats: A Survey on Chain of Thought Fine-tuning from the Perspective of Human Reasoning Mechanism", "comment": null, "summary": "Chain of thought (CoT) fine-tuning aims to endow large language models (LLMs)\nwith reasoning capabilities by training them on curated reasoning traces. It\nleverages both supervised and reinforced fine-tuning to cultivate human-like\nreasoning skills in LLMs, including detailed planning, divergent thinking,\nintuitive judgment, timely reflection, internal thinking, and fact perception,\netc. As CoT fine-tuning has advanced, LLMs have demonstrated substantial\nimprovements in tasks such as mathematical reasoning and code generation.\nHowever, existing surveys about CoT fine-tuning primarily focus on technical\naspects and overlook a systematic analysis from the perspective of human\nreasoning mechanisms. Given that the ultimate goal of CoT fine-tuning is to\nenable LLMs to reason like humans, it is crucial to investigate this technique\nthrough the lens of human cognition. To fill this gap, we present the first\ncomprehensive survey of CoT fine-tuning grounded in human reasoning theory.\nSpecifically, inspired by the well-known Six Thinking Hats framework, which\nsystematically characterizes common human thinking modes using six metaphorical\nhats, we classify and examine CoT fine-tuning methods through this lens.\nFurthermore, building upon this theory, we outline potential directions for\nfuture research in CoT fine-tuning. In addition, we compile a comprehensive\noverview of existing datasets and model performances, and a real-time GitHub\nrepository \\footnote{https://github.com/AI-Chen/Awesome-CoT-Finetuning} that\ncontinuously tracks recent advances in this area is maintained. We hope this\nsurvey will serve as a valuable resource to inspire innovation and foster\nprogress in this rapidly evolving field.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u4ece\u4eba\u7c7b\u63a8\u7406\u7406\u8bba\u7684\u89d2\u5ea6\u5bf9CoT\u5fae\u8c03\u8fdb\u884c\u4e86\u7cfb\u7edf\u7efc\u8ff0\uff0c\u57fa\u4e8e\u516d\u9876\u601d\u8003\u5e3d\u6846\u67b6\u5bf9CoT\u5fae\u8c03\u65b9\u6cd5\u8fdb\u884c\u5206\u7c7b\u5206\u6790\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u73b0\u6709\u7684CoT\u5fae\u8c03\u7efc\u8ff0\u4e3b\u8981\u5173\u6ce8\u6280\u672f\u5c42\u9762\uff0c\u800c\u5ffd\u7565\u4e86\u4ece\u4eba\u7c7b\u63a8\u7406\u673a\u5236\u89d2\u5ea6\u7684\u7cfb\u7edf\u5206\u6790\u3002\u9274\u4e8eCoT\u5fae\u8c03\u7684\u6700\u7ec8\u76ee\u6807\u662f\u8ba9LLM\u50cf\u4eba\u7c7b\u4e00\u6837\u63a8\u7406\uff0c\u4ece\u4eba\u7c7b\u8ba4\u77e5\u89d2\u5ea6\u7814\u7a76\u8fd9\u4e00\u6280\u672f\u81f3\u5173\u91cd\u8981\u3002", "method": "\u57fa\u4e8e\u8457\u540d\u7684\u516d\u9876\u601d\u8003\u5e3d\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u516d\u79cd\u9690\u55bb\u6027\u5e3d\u5b50\u7cfb\u7edf\u5316\u5730\u8868\u5f81\u5e38\u89c1\u7684\u4eba\u7c7b\u601d\u7ef4\u6a21\u5f0f\uff0c\u4f5c\u8005\u901a\u8fc7\u8fd9\u4e00\u89c6\u89d2\u5bf9CoT\u5fae\u8c03\u65b9\u6cd5\u8fdb\u884c\u5206\u7c7b\u548c\u68c0\u9a8c\u3002", "result": "\u672c\u6587\u7f16\u5236\u4e86\u73b0\u6709\u6570\u636e\u96c6\u548c\u6a21\u578b\u6027\u80fd\u7684\u5168\u9762\u6982\u8ff0\uff0c\u5e76\u7ef4\u62a4\u4e86\u4e00\u4e2a\u5b9e\u65f6GitHub\u4ed3\u5e93\u6765\u6301\u7eed\u8ddf\u8e2a\u8be5\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u3002", "conclusion": "\u8fd9\u9879\u8c03\u67e5\u6709\u671b\u6210\u4e3a\u6709\u4ef7\u503c\u7684\u8d44\u6e90\uff0c\u6fc0\u53d1\u8fd9\u4e00\u5feb\u901f\u6f14\u8fdb\u9886\u57df\u7684\u521b\u65b0\u5e76\u4fc3\u8fdb\u8fdb\u5c55\u3002"}}
{"id": "2510.13183", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13183", "abs": "https://arxiv.org/abs/2510.13183", "authors": ["Ming Dong", "Jinkui Zhang", "Bolong Zheng", "Xinhui Tu", "Po Hu", "Tingting He"], "title": "DSCD: Large Language Model Detoxification with Self-Constrained Decoding", "comment": "Accepted at EMNLP 2025 MainConference", "summary": "Detoxification in large language models (LLMs) remains a significant research\nchallenge. Existing decoding detoxification methods are all based on external\nconstraints, which require additional resource overhead and lose generation\nfluency. This work proposes Detoxification with Self-Constrained Decoding\n(DSCD), a novel method for LLM detoxification without parameter fine-tuning.\nDSCD strengthens the inner next-token distribution of the safety layer while\nweakening that of hallucination and toxic layers during output generation. This\neffectively diminishes toxicity and enhances output safety. DSCD offers\nlightweight, high compatibility, and plug-and-play capabilities, readily\nintegrating with existing detoxification methods for further performance\nimprovement. Extensive experiments on representative open-source LLMs and\npublic datasets validate DSCD's effectiveness, demonstrating state-of-the-art\n(SOTA) performance in both detoxification and generation fluency, with superior\nefficiency compared to existing methods. These results highlight DSCD's\npotential as a practical and scalable solution for safer LLM deployments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u53c2\u6570\u5fae\u8c03\u7684\u81ea\u7ea6\u675f\u89e3\u7801\u65b9\u6cd5DSCD\uff0c\u901a\u8fc7\u589e\u5f3a\u5b89\u5168\u5c42\u3001\u524a\u5f31\u5e7b\u89c9\u548c\u6bd2\u6027\u5c42\u7684\u5185\u90e8token\u5206\u5e03\u6765\u5b9e\u73b0LLM\u53bb\u6bd2\uff0c\u5177\u6709\u8f7b\u91cf\u3001\u517c\u5bb9\u6027\u5f3a\u548c\u5373\u63d2\u5373\u7528\u7279\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5916\u90e8\u7ea6\u675f\u7684\u89e3\u7801\u53bb\u6bd2\u65b9\u6cd5\u9700\u8981\u989d\u5916\u8d44\u6e90\u5f00\u9500\u4e14\u4f1a\u635f\u5931\u751f\u6210\u6d41\u7545\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u53bb\u6bd2\u65b9\u6848\u3002", "method": "DSCD\u65b9\u6cd5\u5728\u8f93\u51fa\u751f\u6210\u8fc7\u7a0b\u4e2d\u589e\u5f3a\u5b89\u5168\u5c42\u7684\u5185\u90e8\u4e0b\u4e00\u4e2atoken\u5206\u5e03\uff0c\u540c\u65f6\u524a\u5f31\u5e7b\u89c9\u548c\u6bd2\u6027\u5c42\u7684\u5206\u5e03\uff0c\u4ece\u800c\u6709\u6548\u964d\u4f4e\u6bd2\u6027\u5e76\u63d0\u9ad8\u8f93\u51fa\u5b89\u5168\u6027\u3002", "result": "\u5728\u4ee3\u8868\u6027\u5f00\u6e90LLM\u548c\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DSCD\u7684\u6709\u6548\u6027\uff0c\u5728\u53bb\u6bd2\u548c\u751f\u6210\u6d41\u7545\u6027\u65b9\u9762\u5747\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u4e14\u6548\u7387\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DSCD\u4f5c\u4e3a\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u66f4\u5b89\u5168\u7684LLM\u90e8\u7f72\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2510.13190", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13190", "abs": "https://arxiv.org/abs/2510.13190", "authors": ["Juan Ren", "Mark Dras", "Usman Naseem"], "title": "SHIELD: Classifier-Guided Prompting for Robust and Safer LVLMs", "comment": "Preprint", "summary": "Large Vision-Language Models (LVLMs) unlock powerful multimodal reasoning but\nalso expand the attack surface, particularly through adversarial inputs that\nconceal harmful goals in benign prompts. We propose SHIELD, a lightweight,\nmodel-agnostic preprocessing framework that couples fine-grained safety\nclassification with category-specific guidance and explicit actions (Block,\nReframe, Forward). Unlike binary moderators, SHIELD composes tailored safety\nprompts that enforce nuanced refusals or safe redirection without retraining.\nAcross five benchmarks and five representative LVLMs, SHIELD consistently\nlowers jailbreak and non-following rates while preserving utility. Our method\nis plug-and-play, incurs negligible overhead, and is easily extendable to new\nattack types -- serving as a practical safety patch for both weakly and\nstrongly aligned LVLMs.", "AI": {"tldr": "SHIELD\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u6a21\u578b\u65e0\u5173\u7684\u9884\u5904\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5b89\u5168\u5206\u7c7b\u548c\u7c7b\u522b\u7279\u5b9a\u6307\u5bfc\u6765\u9632\u5fa1\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u7684\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5177\u6709\u5f3a\u5927\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u4e5f\u6269\u5927\u4e86\u653b\u51fb\u9762\uff0c\u7279\u522b\u662f\u901a\u8fc7\u5c06\u6709\u5bb3\u76ee\u6807\u9690\u85cf\u5728\u826f\u6027\u63d0\u793a\u4e2d\u7684\u5bf9\u6297\u6027\u8f93\u5165\u3002", "method": "SHIELD\u7ed3\u5408\u7ec6\u7c92\u5ea6\u5b89\u5168\u5206\u7c7b\u4e0e\u7c7b\u522b\u7279\u5b9a\u6307\u5bfc\uff0c\u901a\u8fc7\u660e\u786e\u7684\u884c\u52a8\uff08\u963b\u6b62\u3001\u91cd\u6784\u3001\u8f6c\u53d1\uff09\u6765\u7ec4\u6210\u5b9a\u5236\u7684\u5b89\u5168\u63d0\u793a\uff0c\u5b9e\u73b0\u7ec6\u5fae\u7684\u62d2\u7edd\u6216\u5b89\u5168\u91cd\u5b9a\u5411\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e94\u4e2a\u4ee3\u8868\u6027LVLM\u4e0a\uff0cSHIELD\u6301\u7eed\u964d\u4f4e\u4e86\u8d8a\u72f1\u548c\u4e0d\u9075\u5faa\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b9e\u7528\u6027\uff0c\u5177\u6709\u5373\u63d2\u5373\u7528\u3001\u5f00\u9500\u53ef\u5ffd\u7565\u7684\u7279\u70b9\u3002", "conclusion": "SHIELD\u4f5c\u4e3a\u4e00\u4e2a\u5b9e\u7528\u7684\u5b89\u5168\u8865\u4e01\uff0c\u9002\u7528\u4e8e\u5f31\u5bf9\u9f50\u548c\u5f3a\u5bf9\u9f50\u7684LVLM\uff0c\u6613\u4e8e\u6269\u5c55\u5230\u65b0\u7684\u653b\u51fb\u7c7b\u578b\u3002"}}
{"id": "2510.13191", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13191", "abs": "https://arxiv.org/abs/2510.13191", "authors": ["Jiamin Chen", "Yuchen Li", "Xinyu Ma", "Xinran Chen", "Xiaokun Zhang", "Shuaiqiang Wang", "Chen Ma", "Dawei Yin"], "title": "Grounding Long-Context Reasoning with Contextual Normalization for Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has become an essential approach for\nextending the reasoning and knowledge capacity of large language models (LLMs).\nWhile prior research has primarily focused on retrieval quality and prompting\nstrategies, the influence of how the retrieved documents are framed, i.e.,\ncontext format, remains underexplored. We show that seemingly superficial\nchoices, such as delimiters or structural markers in key-value extraction, can\ninduce substantial shifts in accuracy and stability, even when semantic content\nis identical. To systematically investigate this effect, we design controlled\nexperiments that vary context density, delimiter styles, and positional\nplacement, revealing the underlying factors that govern performance\ndifferences. Building on these insights, we introduce Contextual Normalization,\na lightweight strategy that adaptively standardizes context representations\nbefore generation. Extensive experiments on both controlled and real-world RAG\nbenchmarks across diverse settings demonstrate that the proposed strategy\nconsistently improves robustness to order variation and strengthens\nlong-context utilization. These findings underscore that reliable RAG depends\nnot only on retrieving the right content, but also on how that content is\npresented, offering both new empirical evidence and a practical technique for\nbetter long-context reasoning.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\u4e0a\u4e0b\u6587\u683c\u5f0f\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5373\u4f7f\u662f\u8868\u9762\u4e0a\u7684\u683c\u5f0f\u9009\u62e9\uff08\u5982\u5206\u9694\u7b26\u3001\u7ed3\u6784\u6807\u8bb0\uff09\u4e5f\u4f1a\u663e\u8457\u5f71\u54cd\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e0a\u4e0b\u6587\u5f52\u4e00\u5316\u7b56\u7565\u6765\u6807\u51c6\u5316\u4e0a\u4e0b\u6587\u8868\u793a\u3002", "motivation": "\u867d\u7136\u5148\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u68c0\u7d22\u8d28\u91cf\u548c\u63d0\u793a\u7b56\u7565\uff0c\u4f46\u68c0\u7d22\u6587\u6863\u5982\u4f55\u88ab\u683c\u5f0f\u5316\uff08\u5373\u4e0a\u4e0b\u6587\u683c\u5f0f\uff09\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u4f5c\u8005\u53d1\u73b0\u770b\u4f3c\u8868\u9762\u7684\u683c\u5f0f\u9009\u62e9\u53ef\u80fd\u5bfc\u81f4\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u7684\u663e\u8457\u53d8\u5316\u3002", "method": "\u8bbe\u8ba1\u4e86\u63a7\u5236\u5b9e\u9a8c\u6765\u53d8\u5316\u4e0a\u4e0b\u6587\u5bc6\u5ea6\u3001\u5206\u9694\u7b26\u6837\u5f0f\u548c\u4f4d\u7f6e\u653e\u7f6e\uff0c\u63ed\u793a\u6027\u80fd\u5dee\u5f02\u7684\u6f5c\u5728\u56e0\u7d20\u3002\u57fa\u4e8e\u8fd9\u4e9b\u89c1\u89e3\uff0c\u63d0\u51fa\u4e86\u4e0a\u4e0b\u6587\u5f52\u4e00\u5316\u7b56\u7565\uff0c\u5728\u751f\u6210\u524d\u81ea\u9002\u5e94\u5730\u6807\u51c6\u5316\u4e0a\u4e0b\u6587\u8868\u793a\u3002", "result": "\u5728\u53d7\u63a7\u548c\u771f\u5b9e\u4e16\u754cRAG\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b56\u7565\u6301\u7eed\u63d0\u9ad8\u4e86\u5bf9\u987a\u5e8f\u53d8\u5316\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u589e\u5f3a\u4e86\u957f\u4e0a\u4e0b\u6587\u5229\u7528\u80fd\u529b\u3002", "conclusion": "\u53ef\u9760\u7684RAG\u4e0d\u4ec5\u53d6\u51b3\u4e8e\u68c0\u7d22\u6b63\u786e\u7684\u5185\u5bb9\uff0c\u8fd8\u53d6\u51b3\u4e8e\u8fd9\u4e9b\u5185\u5bb9\u7684\u5448\u73b0\u65b9\u5f0f\u3002\u7814\u7a76\u4e3a\u66f4\u597d\u7684\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u5b9e\u8bc1\u8bc1\u636e\u548c\u5b9e\u7528\u6280\u672f\u3002"}}
{"id": "2510.13194", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13194", "abs": "https://arxiv.org/abs/2510.13194", "authors": ["Xi Chen", "Yuchen Song", "Satoshi Nakamura"], "title": "StressTransfer: Stress-Aware Speech-to-Speech Translation with Emphasis Preservation", "comment": null, "summary": "We propose a stress-aware speech-to-speech translation (S2ST) system that\npreserves word-level emphasis by leveraging LLMs for cross-lingual emphasis\nconversion. Our method translates source-language stress into target-language\ntags that guide a controllable TTS model. To overcome data scarcity, we\ndeveloped a pipeline to automatically generate aligned training data and\nintroduce the \"LLM-as-Judge\" for evaluation. Experiments show our approach\nsubstantially outperforms baselines in preserving emphasis while maintaining\ncomparable translation quality, speaker intent, and naturalness. Our work\nhighlights the importance of prosody in translation and provides an effective,\ndata-efficient solution for preserving paralinguistic cues in S2ST.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u8de8\u8bed\u8a00\u91cd\u97f3\u8f6c\u6362\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6e90\u8bed\u8a00\u91cd\u97f3\u8f6c\u6362\u4e3a\u76ee\u6807\u8bed\u8a00\u6807\u7b7e\u6765\u6307\u5bfc\u53ef\u63a7TTS\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u7ffb\u8bd1\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u91cd\u97f3\u4fdd\u7559\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u5230\u8bed\u97f3\u7ffb\u8bd1\u7cfb\u7edf\u5728\u4fdd\u7559\u8bcd\u7ea7\u91cd\u97f3\u7b49\u97f5\u5f8b\u7279\u5f81\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u800c\u91cd\u97f3\u5bf9\u4e8e\u4f20\u8fbe\u8bf4\u8bdd\u8005\u610f\u56fe\u548c\u60c5\u611f\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528LLM\u8fdb\u884c\u8de8\u8bed\u8a00\u91cd\u97f3\u8f6c\u6362\uff0c\u5c06\u6e90\u8bed\u8a00\u91cd\u97f3\u6620\u5c04\u4e3a\u76ee\u6807\u8bed\u8a00\u6807\u7b7e\uff0c\u6307\u5bfc\u53ef\u63a7TTS\u6a21\u578b\u751f\u6210\uff1b\u5f00\u53d1\u81ea\u52a8\u5bf9\u9f50\u8bad\u7ec3\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u5e76\u4f7f\u7528\"LLM-as-Judge\"\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4fdd\u7559\u91cd\u97f3\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u5f53\u7684\u7ffb\u8bd1\u8d28\u91cf\u3001\u8bf4\u8bdd\u8005\u610f\u56fe\u548c\u81ea\u7136\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u97f5\u5f8b\u5728\u7ffb\u8bd1\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3aS2ST\u4e2d\u4fdd\u7559\u526f\u8bed\u8a00\u7ebf\u7d22\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u6570\u636e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.13197", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13197", "abs": "https://arxiv.org/abs/2510.13197", "authors": ["Yang Cao", "Sikun Yang", "Yujiu Yang", "Lianyong Qi", "Ming Liu"], "title": "Text Anomaly Detection with Simplified Isolation Kernel", "comment": "EMNLP Findings 2025", "summary": "Two-step approaches combining pre-trained large language model embeddings and\nanomaly detectors demonstrate strong performance in text anomaly detection by\nleveraging rich semantic representations. However, high-dimensional dense\nembeddings extracted by large language models pose challenges due to\nsubstantial memory requirements and high computation time. To address this\nchallenge, we introduce the Simplified Isolation Kernel (SIK), which maps\nhigh-dimensional dense embeddings to lower-dimensional sparse representations\nwhile preserving crucial anomaly characteristics. SIK has linear time\ncomplexity and significantly reduces space complexity through its innovative\nboundary-focused feature mapping. Experiments across 7 datasets demonstrate\nthat SIK achieves better detection performance than 11 state-of-the-art (SOTA)\nanomaly detection algorithms while maintaining computational efficiency and low\nmemory cost. All code and demonstrations are available at\nhttps://github.com/charles-cao/SIK.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7b80\u5316\u9694\u79bb\u6838(SIK)\u65b9\u6cd5\uff0c\u5c06\u9ad8\u7ef4\u5bc6\u96c6\u5d4c\u5165\u6620\u5c04\u5230\u4f4e\u7ef4\u7a00\u758f\u8868\u793a\uff0c\u5728\u4fdd\u6301\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u8fdb\u884c\u6587\u672c\u5f02\u5e38\u68c0\u6d4b\u65f6\u9762\u4e34\u9ad8\u7ef4\u5bc6\u96c6\u5d4c\u5165\u5e26\u6765\u7684\u5185\u5b58\u9700\u6c42\u5927\u548c\u8ba1\u7b97\u65f6\u95f4\u957f\u7684\u6311\u6218\u3002", "method": "\u5f15\u5165\u7b80\u5316\u9694\u79bb\u6838(SIK)\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8fb9\u754c\u805a\u7126\u7279\u5f81\u6620\u5c04\u5c06\u9ad8\u7ef4\u5bc6\u96c6\u5d4c\u5165\u8f6c\u6362\u4e3a\u4f4e\u7ef4\u7a00\u758f\u8868\u793a\uff0c\u5177\u6709\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "result": "\u57287\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSIK\u5728\u68c0\u6d4b\u6027\u80fd\u4e0a\u4f18\u4e8e11\u79cd\u6700\u5148\u8fdb\u7684\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u548c\u4f4e\u5185\u5b58\u6210\u672c\u3002", "conclusion": "SIK\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u7ef4\u5d4c\u5165\u5e26\u6765\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6311\u6218\uff0c\u5728\u6587\u672c\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u4e0e\u6548\u7387\u5e73\u8861\u3002"}}
{"id": "2510.13202", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13202", "abs": "https://arxiv.org/abs/2510.13202", "authors": ["Sai Suhruth Reddy Karri", "Yashwanth Sai Nallapuneni", "Laxmi Narasimha Reddy Mallireddy", "Gopichand G"], "title": "LLM-Guided Synthetic Augmentation (LGSA) for Mitigating Bias in AI Systems", "comment": "11 pages, 4 figures, 1 Table, submitted to an international\n  conference", "summary": "Bias in AI systems, especially those relying on natural language data, raises\nethical and practical concerns. Underrepresentation of certain groups often\nleads to uneven performance across demographics. Traditional fairness methods,\nsuch as pre-processing, in-processing, and post-processing, depend on\nprotected-attribute labels, involve accuracy-fairness trade-offs, and may not\ngeneralize across datasets. To address these challenges, we propose LLM-Guided\nSynthetic Augmentation (LGSA), which uses large language models to generate\ncounterfactual examples for underrepresented groups while preserving label\nintegrity. We evaluated LGSA on a controlled dataset of short English sentences\nwith gendered pronouns, professions, and binary classification labels.\nStructured prompts were used to produce gender-swapped paraphrases, followed by\nquality control including semantic similarity checks, attribute verification,\ntoxicity screening, and human spot checks. The augmented dataset expanded\ntraining coverage and was used to train a classifier under consistent\nconditions. Results show that LGSA reduces performance disparities without\ncompromising accuracy. The baseline model achieved 96.7 percent accuracy with a\n7.2 percent gender bias gap. Simple swap augmentation reduced the gap to 0.7\npercent but lowered accuracy to 95.6 percent. LGSA achieved 99.1 percent\naccuracy with a 1.9 percent bias gap, improving performance on female-labeled\nexamples. These findings demonstrate that LGSA is an effective strategy for\nbias mitigation, enhancing subgroup balance while maintaining high task\naccuracy and label fidelity.", "AI": {"tldr": "LGSA\u65b9\u6cd5\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u53cd\u4e8b\u5b9e\u6837\u672c\u6765\u7f13\u89e3AI\u504f\u89c1\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u6027\u522b\u504f\u89c1\u5dee\u8ddd\u3002", "motivation": "AI\u7cfb\u7edf\u4e2d\u7684\u504f\u89c1\u5f15\u53d1\u4f26\u7406\u548c\u5b9e\u8df5\u95ee\u9898\uff0c\u4f20\u7edf\u516c\u5e73\u6027\u65b9\u6cd5\u4f9d\u8d56\u53d7\u4fdd\u62a4\u5c5e\u6027\u6807\u7b7e\u3001\u6d89\u53ca\u51c6\u786e\u6027\u4e0e\u516c\u5e73\u6027\u6743\u8861\uff0c\u4e14\u96be\u4ee5\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u3002", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u53cd\u4e8b\u5b9e\u793a\u4f8b\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u521b\u5efa\u6027\u522b\u8f6c\u6362\u7684\u6539\u5199\u53e5\uff0c\u5e76\u8fdb\u884c\u8bed\u4e49\u76f8\u4f3c\u6027\u68c0\u67e5\u3001\u5c5e\u6027\u9a8c\u8bc1\u3001\u6bd2\u6027\u7b5b\u9009\u548c\u4eba\u5de5\u62bd\u67e5\u7b49\u8d28\u91cf\u63a7\u5236\u3002", "result": "\u57fa\u7ebf\u6a21\u578b\u51c6\u786e\u738796.7%\uff0c\u6027\u522b\u504f\u89c1\u5dee\u8ddd7.2%\uff1b\u7b80\u5355\u4ea4\u6362\u65b9\u6cd5\u5c06\u5dee\u8ddd\u964d\u81f30.7%\u4f46\u51c6\u786e\u7387\u964d\u81f395.6%\uff1bLGSA\u5b9e\u73b099.1%\u51c6\u786e\u7387\uff0c\u504f\u89c1\u5dee\u8ddd1.9%\uff0c\u6539\u5584\u4e86\u5973\u6027\u6807\u7b7e\u793a\u4f8b\u7684\u6027\u80fd\u3002", "conclusion": "LGSA\u662f\u6709\u6548\u7684\u504f\u89c1\u7f13\u89e3\u7b56\u7565\uff0c\u80fd\u5728\u4fdd\u6301\u9ad8\u4efb\u52a1\u51c6\u786e\u6027\u548c\u6807\u7b7e\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u589e\u5f3a\u5b50\u7fa4\u4f53\u5e73\u8861\u3002"}}
{"id": "2510.13211", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13211", "abs": "https://arxiv.org/abs/2510.13211", "authors": ["Prawaal Sharma", "Navneet Goyal", "Poonam Goyal", "Vishnupriyan R"], "title": "A fully automated and scalable Parallel Data Augmentation for Low Resource Languages using Image and Text Analytics", "comment": "4 Pages, Parallel Data Augmentation", "summary": "Linguistic diversity across the world creates a disparity with the\navailability of good quality digital language resources thereby restricting the\ntechnological benefits to majority of human population. The lack or absence of\ndata resources makes it difficult to perform NLP tasks for low-resource\nlanguages. This paper presents a novel scalable and fully automated methodology\nto extract bilingual parallel corpora from newspaper articles using image and\ntext analytics. We validate our approach by building parallel data corpus for\ntwo different language combinations and demonstrate the value of this dataset\nthrough a downstream task of machine translation and improve over the current\nbaseline by close to 3 BLEU points.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u62a5\u7eb8\u6587\u7ae0\u4e2d\u81ea\u52a8\u63d0\u53d6\u53cc\u8bed\u5e73\u884c\u8bed\u6599\u5e93\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u57fa\u7ebf\u63d0\u5347\u4e86\u8fd13\u4e2aBLEU\u5206\u6570\u3002", "motivation": "\u5168\u7403\u8bed\u8a00\u591a\u6837\u6027\u5bfc\u81f4\u9ad8\u8d28\u91cf\u6570\u5b57\u8bed\u8a00\u8d44\u6e90\u5206\u5e03\u4e0d\u5747\uff0c\u9650\u5236\u4e86\u5927\u591a\u6570\u4eba\u53e3\u83b7\u5f97\u6280\u672f\u6548\u76ca\u3002\u4f4e\u8d44\u6e90\u8bed\u8a00\u7f3a\u4e4f\u6570\u636e\u8d44\u6e90\u4f7f\u5f97NLP\u4efb\u52a1\u96be\u4ee5\u8fdb\u884c\u3002", "method": "\u4f7f\u7528\u56fe\u50cf\u548c\u6587\u672c\u5206\u6790\u4ece\u62a5\u7eb8\u6587\u7ae0\u4e2d\u63d0\u53d6\u53cc\u8bed\u5e73\u884c\u8bed\u6599\u5e93\u7684\u53ef\u6269\u5c55\u5168\u81ea\u52a8\u65b9\u6cd5\u3002", "result": "\u4e3a\u4e24\u79cd\u4e0d\u540c\u8bed\u8a00\u7ec4\u5408\u6784\u5efa\u4e86\u5e73\u884c\u6570\u636e\u8bed\u6599\u5e93\uff0c\u901a\u8fc7\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u5f53\u524d\u57fa\u7ebf\u63d0\u5347\u4e86\u8fd13\u4e2aBLEU\u5206\u6570\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4e3aNLP\u4efb\u52a1\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u53cc\u8bed\u8d44\u6e90\u3002"}}
{"id": "2510.13255", "categories": ["cs.CL", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.13255", "abs": "https://arxiv.org/abs/2510.13255", "authors": ["Jingmin An", "Yilong Song", "Ruolin Yang", "Nai Ding", "Lingxi Lu", "Yuxuan Wang", "Wei Wang", "Chu Zhuang", "Qian Wang", "Fang Fang"], "title": "Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain", "comment": null, "summary": "Large Language Models (LLMs) demonstrate human-level or even superior\nlanguage abilities, effectively modeling syntactic structures, yet the specific\ncomputational modules responsible remain unclear. A key question is whether LLM\nbehavioral capabilities stem from mechanisms akin to those in the human brain.\nTo address these questions, we introduce the Hierarchical Frequency Tagging\nProbe (HFTP), a tool that utilizes frequency-domain analysis to identify\nneuron-wise components of LLMs (e.g., individual Multilayer Perceptron (MLP)\nneurons) and cortical regions (via intracranial recordings) encoding syntactic\nstructures. Our results show that models such as GPT-2, Gemma, Gemma 2, Llama\n2, Llama 3.1, and GLM-4 process syntax in analogous layers, while the human\nbrain relies on distinct cortical regions for different syntactic levels.\nRepresentational similarity analysis reveals a stronger alignment between LLM\nrepresentations and the left hemisphere of the brain (dominant in language\nprocessing). Notably, upgraded models exhibit divergent trends: Gemma 2 shows\ngreater brain similarity than Gemma, while Llama 3.1 shows less alignment with\nthe brain compared to Llama 2. These findings offer new insights into the\ninterpretability of LLM behavioral improvements, raising questions about\nwhether these advancements are driven by human-like or non-human-like\nmechanisms, and establish HFTP as a valuable tool bridging computational\nlinguistics and cognitive neuroscience. This project is available at\nhttps://github.com/LilTiger/HFTP.", "AI": {"tldr": "HFTP\u5de5\u5177\u901a\u8fc7\u9891\u57df\u5206\u6790\u53d1\u73b0LLMs\u548c\u4eba\u7c7b\u5927\u8111\u5728\u53e5\u6cd5\u5904\u7406\u4e0a\u7684\u76f8\u4f3c\u6027\u4e0e\u5dee\u5f02\uff0c\u663e\u793aLLMs\u4e0e\u5927\u8111\u5de6\u534a\u7403\u8868\u5f81\u66f4\u76f8\u4f3c\uff0c\u4f46\u6a21\u578b\u5347\u7ea7\u5448\u73b0\u4e0d\u540c\u8d8b\u52bf\u3002", "motivation": "\u63a2\u7a76LLMs\u662f\u5426\u4f7f\u7528\u7c7b\u4f3c\u4eba\u8111\u7684\u673a\u5236\u6765\u5904\u7406\u53e5\u6cd5\u7ed3\u6784\uff0c\u4ee5\u53ca\u8bc6\u522b\u8d1f\u8d23\u53e5\u6cd5\u5904\u7406\u7684\u7279\u5b9a\u8ba1\u7b97\u6a21\u5757\u3002", "method": "\u5f15\u5165\u5206\u5c42\u9891\u7387\u6807\u8bb0\u63a2\u9488\uff08HFTP\uff09\uff0c\u5229\u7528\u9891\u57df\u5206\u6790\u8bc6\u522bLLMs\u4e2d\u7f16\u7801\u53e5\u6cd5\u7ed3\u6784\u7684\u795e\u7ecf\u5143\u7ec4\u4ef6\u548c\u5927\u8111\u76ae\u5c42\u533a\u57df\u3002", "result": "GPT-2\u3001Gemma\u7b49\u6a21\u578b\u5728\u7c7b\u4f3c\u5c42\u7ea7\u5904\u7406\u53e5\u6cd5\uff0c\u800c\u4eba\u8111\u4f9d\u8d56\u4e0d\u540c\u76ae\u5c42\u533a\u57df\uff1bLLMs\u8868\u5f81\u4e0e\u5927\u8111\u5de6\u534a\u7403\u66f4\u76f8\u4f3c\uff1b\u6a21\u578b\u5347\u7ea7\u8d8b\u52bf\u4e0d\u540c\uff1aGemma 2\u6bd4Gemma\u66f4\u63a5\u8fd1\u5927\u8111\uff0cLlama 3.1\u6bd4Llama 2\u66f4\u504f\u79bb\u5927\u8111\u3002", "conclusion": "HFTP\u4e3a\u8ba1\u7b97\u8bed\u8a00\u5b66\u548c\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u642d\u5efa\u4e86\u6865\u6881\uff0c\u5bf9LLMs\u884c\u4e3a\u6539\u8fdb\u7684\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u8d28\u7591\u8fd9\u4e9b\u8fdb\u6b65\u662f\u7531\u7c7b\u4eba\u8fd8\u662f\u975e\u7c7b\u4eba\u673a\u5236\u9a71\u52a8\u3002"}}
{"id": "2510.13271", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13271", "abs": "https://arxiv.org/abs/2510.13271", "authors": ["Ine Gevers", "Walter Daelemans"], "title": "Do You Get the Hint? Benchmarking LLMs on the Board Game Concept", "comment": null, "summary": "Large language models (LLMs) have achieved striking successes on many\nbenchmarks, yet recent studies continue to expose fundamental weaknesses. In\nparticular, tasks that require abstract reasoning remain challenging, often\nbecause they use representations such as grids, symbols, or visual patterns\nthat differ from the natural language data LLMs are trained on. In this paper,\nwe introduce Concept, a simple word-guessing board game, as a benchmark for\nprobing abductive reasoning in a representation that is much closer to LLM\npre-training data: natural language. Our results show that this game, easily\nsolved by humans (with a success rate of over 90\\%), is still very challenging\nfor state-of-the-art LLMs (no model exceeds 40\\% success rate). Specifically,\nwe observe that LLMs struggle with interpreting other players' strategic\nintents, and with correcting initial hypotheses given sequential information\nupdates. In addition, we extend the evaluation across multiple languages, and\nfind that the LLM performance drops further in lower-resource languages (Dutch,\nFrench, and Spanish) compared to English.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Concept\u6e38\u620f\u4f5c\u4e3a\u8bc4\u4f30LLM\u6eaf\u56e0\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u53d1\u73b0LLM\u5728\u6b64\u7c7b\u9700\u8981\u62bd\u8c61\u63a8\u7406\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u6210\u529f\u7387\u4f4e\u4e8e40%\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u768490%\u4ee5\u4e0a\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524dLLM\u867d\u7136\u5728\u8bb8\u591a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9700\u8981\u62bd\u8c61\u63a8\u7406\u7684\u4efb\u52a1\u4e0a\u4ecd\u5b58\u5728\u6839\u672c\u6027\u5f31\u70b9\uff0c\u7279\u522b\u662f\u6d89\u53ca\u7f51\u683c\u3001\u7b26\u53f7\u6216\u89c6\u89c9\u6a21\u5f0f\u7b49\u4e0d\u540c\u4e8e\u81ea\u7136\u8bed\u8a00\u8bad\u7ec3\u6570\u636e\u7684\u8868\u793a\u5f62\u5f0f\u3002", "method": "\u5f15\u5165Concept\u5355\u8bcd\u731c\u8c1c\u68cb\u76d8\u6e38\u620f\u4f5c\u4e3a\u57fa\u51c6\uff0c\u8be5\u6e38\u620f\u4f7f\u7528\u66f4\u63a5\u8fd1LLM\u9884\u8bad\u7ec3\u6570\u636e\u7684\u81ea\u7136\u8bed\u8a00\u8868\u793a\uff0c\u8bc4\u4f30LLM\u5728\u591a\u79cd\u8bed\u8a00\uff08\u82f1\u8bed\u3001\u8377\u5170\u8bed\u3001\u6cd5\u8bed\u3001\u897f\u73ed\u7259\u8bed\uff09\u4e2d\u7684\u6eaf\u56e0\u63a8\u7406\u80fd\u529b\u3002", "result": "LLM\u5728Concept\u6e38\u620f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u6210\u529f\u7387\u4e0d\u8d85\u8fc740%\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u768490%\u4ee5\u4e0a\u6210\u529f\u7387\u3002LLM\u5728\u89e3\u91ca\u5176\u4ed6\u73a9\u5bb6\u7684\u6218\u7565\u610f\u56fe\u548c\u6839\u636e\u5e8f\u5217\u4fe1\u606f\u66f4\u65b0\u4fee\u6b63\u521d\u59cb\u5047\u8bbe\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u4e14\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u8868\u73b0\u66f4\u5dee\u3002", "conclusion": "Concept\u6e38\u620f\u662f\u4e00\u4e2a\u6709\u6548\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86LLM\u5728\u62bd\u8c61\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u7406\u89e3\u6218\u7565\u610f\u56fe\u548c\u52a8\u6001\u4fe1\u606f\u66f4\u65b0\u65b9\u9762\u7684\u6311\u6218\uff0c\u4e14\u591a\u8bed\u8a00\u8bc4\u4f30\u663e\u793a\u6027\u80fd\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u8fdb\u4e00\u6b65\u4e0b\u964d\u3002"}}
{"id": "2510.13272", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13272", "abs": "https://arxiv.org/abs/2510.13272", "authors": ["Zhichao Xu", "Zongyu Wu", "Yun Zhou", "Aosong Feng", "Kang Zhou", "Sangmin Woo", "Kiran Ramnath", "Yijun Tian", "Xuan Qi", "Weikang Qiu", "Lin Lee Cheong", "Haibo Ding"], "title": "Beyond Correctness: Rewarding Faithful Reasoning in Retrieval-Augmented Generation", "comment": null, "summary": "Inspired by the success of reinforcement learning (RL) in Large Language\nModel (LLM) training for domains like math and code, recent works have begun\nexploring how to train LLMs to use search engines more effectively as tools for\nretrieval-augmented generation. Although these methods achieve performance\nimprovement across QA benchmarks, many prioritize final answer correctness\nwhile overlooking the quality of intermediate reasoning steps, which may lead\nto chain-of-thought unfaithfulness. In this paper, we first introduce a\ncomprehensive evaluation framework for evaluating RL-based search agents,\ncovering three distinct faithfulness metrics: information-think faithfulness,\nthink-answer faithfulness, and think-search faithfulness. Our evaluations\nreveal that a prototypical RL-based search agent, Search-R1, has significant\nroom for improvement in this regard. To foster faithful reasoning, we introduce\nVERITAS (Verifying Entailed Reasoning through Intermediate Traceability in\nAgentic Search), a novel framework that integrates fine-grained faithfulness\nrewards into the reinforcement learning process. Our experiments show that\nmodels trained with VERITAS not only significantly improve reasoning\nfaithfulness, but also achieve comparable task performance across seven QA\nbenchmarks.", "AI": {"tldr": "VERITAS\u6846\u67b6\u901a\u8fc7\u5c06\u7ec6\u7c92\u5ea6\u7684\u5fe0\u5b9e\u6027\u5956\u52b1\u6574\u5408\u5230\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8eRL\u7684\u641c\u7d22\u4ee3\u7406\u7684\u63a8\u7406\u5fe0\u5b9e\u6027\uff0c\u540c\u65f6\u5728\u591a\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u53ef\u6bd4\u8f83\u7684\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u641c\u7d22\u4ee3\u7406\u65b9\u6cd5\u867d\u7136\u63d0\u5347\u4e86\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u7684\u6027\u80fd\uff0c\u4f46\u5f80\u5f80\u5ffd\u89c6\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u7684\u8d28\u91cf\uff0c\u5bfc\u81f4\u601d\u7ef4\u94fe\u4e0d\u5fe0\u5b9e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86VERITAS\u6846\u67b6\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u6574\u5408\u4e86\u4e09\u79cd\u7ec6\u7c92\u5ea6\u7684\u5fe0\u5b9e\u6027\u5956\u52b1\uff1a\u4fe1\u606f-\u601d\u7ef4\u5fe0\u5b9e\u6027\u3001\u601d\u7ef4-\u7b54\u6848\u5fe0\u5b9e\u6027\u548c\u601d\u7ef4-\u641c\u7d22\u5fe0\u5b9e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528VERITAS\u8bad\u7ec3\u7684\u6a21\u578b\u4e0d\u4ec5\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u5fe0\u5b9e\u6027\uff0c\u8fd8\u5728\u4e03\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u53ef\u6bd4\u8f83\u7684\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "VERITAS\u6846\u67b6\u6709\u6548\u5730\u89e3\u51b3\u4e86RL-based\u641c\u7d22\u4ee3\u7406\u7684\u63a8\u7406\u5fe0\u5b9e\u6027\u95ee\u9898\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.13285", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13285", "abs": "https://arxiv.org/abs/2510.13285", "authors": ["Arthur Vogels", "Benjamin Wong", "Yann Choho", "Annabelle Blangero", "Milan Bhan"], "title": "In-Distribution Steering: Balancing Control and Coherence in Language Model Generation", "comment": null, "summary": "Activation steering methods control large language model (LLM) behavior by\nmodifying internal activations at inference time. However, most existing\nactivation steering methods rely on a fixed steering strength, leading to\neither insufficient control or unadapted intervention that degrades text\nplausibility and coherence. We introduce In-Distribution Steering (IDS), a\nnovel method that adapts steering strength based on the input data distribution\nin representation space. IDS dynamically adjusts interventions according to how\nfar a given input lies within the distribution, enabling adaptive intervention\nand generation stability during text generation. Experiments demonstrate that\nIDS achieves strong accuracy on classification tasks while producing coherent\ntext without collapse, making IDS particularly well suited for real-world\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6fc0\u6d3b\u5f15\u5bfc\u65b9\u6cd5IDS\uff0c\u6839\u636e\u8f93\u5165\u6570\u636e\u5728\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u5206\u5e03\u52a8\u6001\u8c03\u6574\u5f15\u5bfc\u5f3a\u5ea6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u56fa\u5b9a\u5f3a\u5ea6\u65b9\u6cd5\u5bfc\u81f4\u7684\u63a7\u5236\u4e0d\u8db3\u6216\u6587\u672c\u8d28\u91cf\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6fc0\u6d3b\u5f15\u5bfc\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u5f15\u5bfc\u5f3a\u5ea6\uff0c\u5bfc\u81f4\u8981\u4e48\u63a7\u5236\u4e0d\u8db3\uff0c\u8981\u4e48\u5e72\u9884\u8fc7\u5ea6\u800c\u964d\u4f4e\u6587\u672c\u5408\u7406\u6027\u548c\u8fde\u8d2f\u6027\u3002", "method": "IDS\u65b9\u6cd5\u57fa\u4e8e\u8f93\u5165\u6570\u636e\u5728\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u5206\u5e03\u81ea\u9002\u5e94\u8c03\u6574\u5f15\u5bfc\u5f3a\u5ea6\uff0c\u6839\u636e\u8f93\u5165\u4e0e\u5206\u5e03\u7684\u8ddd\u79bb\u52a8\u6001\u8c03\u8282\u5e72\u9884\u7a0b\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660eIDS\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fbe\u5230\u9ad8\u51c6\u786e\u7387\uff0c\u540c\u65f6\u751f\u6210\u8fde\u8d2f\u6587\u672c\u4e14\u907f\u514d\u5d29\u6e83\uff0c\u7279\u522b\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "IDS\u901a\u8fc7\u81ea\u9002\u5e94\u5f15\u5bfc\u5f3a\u5ea6\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u63a7\u5236\u6548\u679c\u548c\u6587\u672c\u8d28\u91cf\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6fc0\u6d3b\u5f15\u5bfc\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.13291", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13291", "abs": "https://arxiv.org/abs/2510.13291", "authors": ["Xuxin Cheng", "Ke Zeng", "Zhiquan Cao", "Linyi Dai", "Wenxuan Gao", "Fei Han", "Ai Jian", "Feng Hong", "Wenxing Hu", "Zihe Huang", "Dejian Kong", "Jia Leng", "Zhuoyuan Liao", "Pei Liu", "Jiaye Lin", "Xing Ma", "Jingqing Ruan", "Jiaxing Song", "Xiaoyu Tan", "Ruixuan Xiao", "Wenhui Yu", "Wenyu Zhan", "Haoxing Zhang", "Chao Zhou", "Hao Zhou", "Shaodong Zheng", "Ruinian Chen", "Siyuan Chen", "Ziyang Chen", "Yiwen Dong", "Yaoyou Fan", "Yangyi Fang", "Yang Gan", "Shiguang Guo", "Qi He", "Chaowen Hu", "Binghui Li", "Dailin Li", "Xiangyu Li", "Yan Li", "Chengjian Liu", "Xiangfeng Liu", "Jiahui Lv", "Qiao Ma", "Jiang Pan", "Cong Qin", "Chenxing Sun", "Wen Sun", "Zhonghui Wang", "Abudukelimu Wuerkaixi", "Xin Yang", "Fangyi Yuan", "Yawen Zhu", "Tianyi Zhai", "Jie Zhang", "Runlai Zhang", "Yao Xu", "Yiran Zhao", "Yifan Wang", "Xunliang Cai", "Yangen Hu", "Cao Liu", "Lu Pan", "Xiaoli Wang", "Bo Xiao", "Wenyuan Yao", "Qianlin Zhou", "Benchang Zhu"], "title": "Higher Satisfaction, Lower Cost: A Technical Report on How LLMs Revolutionize Meituan's Intelligent Interaction Systems", "comment": "36 pages, 14 figures", "summary": "Enhancing customer experience is essential for business success, particularly\nas service demands grow in scale and complexity. Generative artificial\nintelligence and Large Language Models (LLMs) have empowered intelligent\ninteraction systems to deliver efficient, personalized, and 24/7 support. In\npractice, intelligent interaction systems encounter several challenges: (1)\nConstructing high-quality data for cold-start training is difficult, hindering\nself-evolution and raising labor costs. (2) Multi-turn dialogue performance\nremains suboptimal due to inadequate intent understanding, rule compliance, and\nsolution extraction. (3) Frequent evolution of business rules affects system\noperability and transferability, constraining low-cost expansion and\nadaptability. (4) Reliance on a single LLM is insufficient in complex\nscenarios, where the absence of multi-agent frameworks and effective\ncollaboration undermines process completeness and service quality. (5) The\nopen-domain nature of multi-turn dialogues, lacking unified golden answers,\nhampers quantitative evaluation and continuous optimization. To address these\nchallenges, we introduce WOWService, an intelligent interaction system tailored\nfor industrial applications. With the integration of LLMs and multi-agent\narchitectures, WOWService enables autonomous task management and collaborative\nproblem-solving. Specifically, WOWService focuses on core modules including\ndata construction, general capability enhancement, business scenario\nadaptation, multi-agent coordination, and automated evaluation. Currently,\nWOWService is deployed on the Meituan App, achieving significant gains in key\nmetrics, e.g., User Satisfaction Metric 1 (USM 1) -27.53% and User Satisfaction\nMetric 2 (USM 2) +25.51%, demonstrating its effectiveness in capturing user\nneeds and advancing personalized service.", "AI": {"tldr": "WOWService\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u548c\u591a\u667a\u80fd\u4f53\u67b6\u6784\u7684\u667a\u80fd\u4ea4\u4e92\u7cfb\u7edf\uff0c\u9488\u5bf9\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u51b7\u542f\u52a8\u8bad\u7ec3\u3001\u591a\u8f6e\u5bf9\u8bdd\u3001\u4e1a\u52a1\u89c4\u5219\u6f14\u5316\u3001\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u8bc4\u4f30\u7b49\u6311\u6218\uff0c\u901a\u8fc7\u5728\u7f8e\u56e2App\u7684\u90e8\u7f72\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u6ee1\u610f\u5ea6\u6307\u6807\u3002", "motivation": "\u968f\u7740\u670d\u52a1\u9700\u6c42\u89c4\u6a21\u548c\u590d\u6742\u5ea6\u7684\u589e\u957f\uff0c\u667a\u80fd\u4ea4\u4e92\u7cfb\u7edf\u9762\u4e34\u591a\u4e2a\u6311\u6218\uff1a\u51b7\u542f\u52a8\u8bad\u7ec3\u6570\u636e\u6784\u5efa\u56f0\u96be\u3001\u591a\u8f6e\u5bf9\u8bdd\u6027\u80fd\u4e0d\u4f73\u3001\u4e1a\u52a1\u89c4\u5219\u9891\u7e41\u6f14\u5316\u5f71\u54cd\u7cfb\u7edf\u53ef\u64cd\u4f5c\u6027\u3001\u5355\u4e00LLM\u5728\u590d\u6742\u573a\u666f\u4e2d\u4e0d\u8db3\u3001\u4ee5\u53ca\u7f3a\u4e4f\u7edf\u4e00\u8bc4\u4f30\u6807\u51c6\u963b\u788d\u6301\u7eed\u4f18\u5316\u3002", "method": "WOWService\u96c6\u6210\u4e86LLMs\u548c\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u4e13\u6ce8\u4e8e\u6570\u636e\u6784\u5efa\u3001\u901a\u7528\u80fd\u529b\u589e\u5f3a\u3001\u4e1a\u52a1\u573a\u666f\u9002\u914d\u3001\u591a\u667a\u80fd\u4f53\u534f\u8c03\u548c\u81ea\u52a8\u5316\u8bc4\u4f30\u7b49\u6838\u5fc3\u6a21\u5757\uff0c\u5b9e\u73b0\u81ea\u4e3b\u4efb\u52a1\u7ba1\u7406\u548c\u534f\u4f5c\u95ee\u9898\u89e3\u51b3\u3002", "result": "WOWService\u5df2\u5728\u7f8e\u56e2App\u90e8\u7f72\uff0c\u5173\u952e\u6307\u6807\u663e\u8457\u6539\u5584\uff1a\u7528\u6237\u6ee1\u610f\u5ea6\u6307\u68071\uff08USM 1\uff09\u964d\u4f4e27.53%\uff0c\u7528\u6237\u6ee1\u610f\u5ea6\u6307\u68072\uff08USM 2\uff09\u63d0\u534725.51%\uff0c\u6709\u6548\u6355\u6349\u7528\u6237\u9700\u6c42\u5e76\u63a8\u8fdb\u4e2a\u6027\u5316\u670d\u52a1\u3002", "conclusion": "WOWService\u901a\u8fc7\u6574\u5408LLMs\u548c\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u667a\u80fd\u4ea4\u4e92\u7cfb\u7edf\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u548c\u7cfb\u7edf\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.13293", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13293", "abs": "https://arxiv.org/abs/2510.13293", "authors": ["Yizhou Peng", "Yukun Ma", "Chong Zhang", "Yi-Wen Chao", "Chongjia Ni", "Bin Ma"], "title": "Mismatch Aware Guidance for Robust Emotion Control in Auto-Regressive TTS Models", "comment": "Submitted to ICASSP 2026", "summary": "While Text-to-Speech (TTS) systems can achieve fine-grained control over\nemotional expression via natural language prompts, a significant challenge\nemerges when the desired emotion (style prompt) conflicts with the semantic\ncontent of the text. This mismatch often results in unnatural-sounding speech,\nundermining the goal of achieving fine-grained emotional control.\nClassifier-Free Guidance (CFG) is a key technique for enhancing prompt\nalignment; however, its application to auto-regressive (AR) TTS models remains\nunderexplored, which can lead to degraded audio quality. This paper directly\naddresses the challenge of style-content mismatch in AR TTS models by proposing\nan adaptive CFG scheme that adjusts to different levels of the detected\nmismatch, as measured using large language models or natural language inference\nmodels. This solution is based on a comprehensive analysis of CFG's impact on\nemotional expressiveness in state-of-the-art AR TTS models. Our results\ndemonstrate that the proposed adaptive CFG scheme improves the emotional\nexpressiveness of the AR TTS model while maintaining audio quality and\nintelligibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\uff08CFG\uff09\u65b9\u6848\uff0c\u7528\u4e8e\u89e3\u51b3\u81ea\u56de\u5f52TTS\u6a21\u578b\u4e2d\u98ce\u683c\u63d0\u793a\u4e0e\u6587\u672c\u8bed\u4e49\u5185\u5bb9\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u97f3\u9891\u8d28\u91cf\u548c\u53ef\u7406\u89e3\u6027\u7684\u540c\u65f6\u63d0\u5347\u60c5\u611f\u8868\u8fbe\u80fd\u529b\u3002", "motivation": "\u5f53TTS\u7cfb\u7edf\u4e2d\u671f\u671b\u7684\u60c5\u611f\uff08\u98ce\u683c\u63d0\u793a\uff09\u4e0e\u6587\u672c\u8bed\u4e49\u5185\u5bb9\u51b2\u7a81\u65f6\uff0c\u4f1a\u5bfc\u81f4\u8bed\u97f3\u542c\u8d77\u6765\u4e0d\u81ea\u7136\uff0c\u8fd9\u7834\u574f\u4e86\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u60c5\u611f\u63a7\u5236\u7684\u76ee\u6807\u3002\u867d\u7136CFG\u662f\u589e\u5f3a\u63d0\u793a\u5bf9\u9f50\u7684\u5173\u952e\u6280\u672f\uff0c\u4f46\u5728\u81ea\u56de\u5f52TTS\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u53ef\u80fd\u5bfc\u81f4\u97f3\u9891\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94CFG\u65b9\u6848\uff0c\u6839\u636e\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6216\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u6a21\u578b\u68c0\u6d4b\u5230\u7684\u4e0d\u540c\u4e0d\u5339\u914d\u7a0b\u5ea6\u8fdb\u884c\u8c03\u6574\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u5bf9CFG\u5728\u6700\u5148\u8fdb\u81ea\u56de\u5f52TTS\u6a21\u578b\u4e2d\u60c5\u611f\u8868\u8fbe\u80fd\u529b\u5f71\u54cd\u7684\u7efc\u5408\u5206\u6790\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u81ea\u9002\u5e94CFG\u65b9\u6848\u5728\u4fdd\u6301\u97f3\u9891\u8d28\u91cf\u548c\u53ef\u7406\u89e3\u6027\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u81ea\u56de\u5f52TTS\u6a21\u578b\u7684\u60c5\u611f\u8868\u8fbe\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u81ea\u9002\u5e94CFG\u65b9\u6848\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u56de\u5f52TTS\u6a21\u578b\u4e2d\u7684\u98ce\u683c-\u5185\u5bb9\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u60c5\u611f\u63a7\u5236\u6548\u679c\u3002"}}
{"id": "2510.13302", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13302", "abs": "https://arxiv.org/abs/2510.13302", "authors": ["Pablo Miralles-Gonz\u00e1lez", "Javier Huertas-Tato", "Alejandro Mart\u00edn", "David Camacho"], "title": "LLM one-shot style transfer for Authorship Attribution and Verification", "comment": null, "summary": "Computational stylometry analyzes writing style through quantitative patterns\nin text, supporting applications from forensic tasks such as identity linking\nand plagiarism detection to literary attribution in the humanities. Supervised\nand contrastive approaches rely on data with spurious correlations and often\nconfuse style with topic. Despite their natural use in AI-generated text\ndetection, the CLM pre-training of modern LLMs has been scarcely leveraged for\ngeneral authorship problems. We propose a novel unsupervised approach based on\nthis extensive pre-training and the in-context learning capabilities of LLMs,\nemploying the log-probabilities of an LLM to measure style transferability from\none text to another. Our method significantly outperforms LLM prompting\napproaches of comparable scale and achieves higher accuracy than contrastively\ntrained baselines when controlling for topical correlations. Moreover,\nperformance scales fairly consistently with the size of the base model and, in\nthe case of authorship verification, with an additional mechanism that\nincreases test-time computation; enabling flexible trade-offs between\ncomputational cost and accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u9884\u8bad\u7ec3\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u4f7f\u7528LLM\u7684\u5bf9\u6570\u6982\u7387\u6765\u6d4b\u91cf\u6587\u672c\u95f4\u7684\u98ce\u683c\u53ef\u8f6c\u79fb\u6027\uff0c\u5728\u63a7\u5236\u4e3b\u9898\u76f8\u5173\u6027\u65f6\u4f18\u4e8e\u5bf9\u6bd4\u8bad\u7ec3\u57fa\u7ebf\u3002", "motivation": "\u8ba1\u7b97\u98ce\u683c\u5b66\u901a\u8fc7\u6587\u672c\u4e2d\u7684\u91cf\u5316\u6a21\u5f0f\u5206\u6790\u5199\u4f5c\u98ce\u683c\uff0c\u4f46\u76d1\u7763\u548c\u5bf9\u6bd4\u65b9\u6cd5\u4f9d\u8d56\u5177\u6709\u865a\u5047\u76f8\u5173\u6027\u7684\u6570\u636e\uff0c\u7ecf\u5e38\u6df7\u6dc6\u98ce\u683c\u4e0e\u4e3b\u9898\u3002\u5c3d\u7ba1CLM\u9884\u8bad\u7ec3\u5728AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u4e2d\u81ea\u7136\u4f7f\u7528\uff0c\u4f46\u5728\u4e00\u822c\u4f5c\u8005\u8eab\u4efd\u95ee\u9898\u4e2d\u5f88\u5c11\u88ab\u5229\u7528\u3002", "method": "\u57fa\u4e8eLLM\u7684\u5e7f\u6cdb\u9884\u8bad\u7ec3\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u4f7f\u7528LLM\u7684\u5bf9\u6570\u6982\u7387\u6765\u6d4b\u91cf\u4ece\u4e00\u4e2a\u6587\u672c\u5230\u53e6\u4e00\u4e2a\u6587\u672c\u7684\u98ce\u683c\u53ef\u8f6c\u79fb\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u540c\u7b49\u89c4\u6a21\u7684LLM\u63d0\u793a\u65b9\u6cd5\uff0c\u5728\u63a7\u5236\u4e3b\u9898\u76f8\u5173\u6027\u65f6\u6bd4\u5bf9\u6bd4\u8bad\u7ec3\u57fa\u7ebf\u83b7\u5f97\u66f4\u9ad8\u51c6\u786e\u7387\u3002\u6027\u80fd\u4e0e\u57fa\u7840\u6a21\u578b\u5927\u5c0f\u76f8\u5f53\u4e00\u81f4\u5730\u6269\u5c55\uff0c\u5728\u4f5c\u8005\u8eab\u4efd\u9a8c\u8bc1\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u589e\u52a0\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u5b9e\u73b0\u8ba1\u7b97\u6210\u672c\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u7075\u6d3b\u6743\u8861\u3002", "conclusion": "LLM\u9884\u8bad\u7ec3\u4e3a\u8ba1\u7b97\u98ce\u683c\u5b66\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u533a\u5206\u98ce\u683c\u4e0e\u4e3b\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.13329", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13329", "abs": "https://arxiv.org/abs/2510.13329", "authors": ["Ye Yuan", "Mohammad Amin Shabani", "Siqi Liu"], "title": "Embedding-Based Context-Aware Reranker", "comment": "Under Review", "summary": "Retrieval-Augmented Generation (RAG) systems rely on retrieving relevant\nevidence from a corpus to support downstream generation. The common practice of\nsplitting a long document into multiple shorter passages enables finer-grained\nand targeted information retrieval. However, it also introduces challenges when\na correct retrieval would require inference across passages, such as resolving\ncoreference, disambiguating entities, and aggregating evidence scattered across\nmultiple sources. Many state-of-the-art (SOTA) reranking methods, despite\nutilizing powerful large pretrained language models with potentially high\ninference costs, still neglect the aforementioned challenges. Therefore, we\npropose Embedding-Based Context-Aware Reranker (EBCAR), a lightweight reranking\nframework operating directly on embeddings of retrieved passages with enhanced\ncross-passage understandings through the structural information of the passages\nand a hybrid attention mechanism, which captures both high-level interactions\nacross documents and low-level relationships within each document. We evaluate\nEBCAR against SOTA rerankers on the ConTEB benchmark, demonstrating its\neffectiveness for information retrieval requiring cross-passage inference and\nits advantages in both accuracy and efficiency.", "AI": {"tldr": "\u63d0\u51faEBCAR\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u91cd\u6392\u5e8f\u6846\u67b6\uff0c\u901a\u8fc7\u5d4c\u5165\u548c\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u8de8\u6bb5\u843d\u7406\u89e3\uff0c\u5728\u9700\u8981\u8de8\u6bb5\u843d\u63a8\u7406\u7684\u4fe1\u606f\u68c0\u7d22\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "RAG\u7cfb\u7edf\u5c06\u957f\u6587\u6863\u5206\u5272\u6210\u77ed\u6bb5\u843d\u8fdb\u884c\u68c0\u7d22\uff0c\u4f46\u5f53\u6b63\u786e\u7b54\u6848\u9700\u8981\u8de8\u6bb5\u843d\u63a8\u7406\uff08\u5982\u89e3\u51b3\u5171\u6307\u3001\u6d88\u6b67\u5b9e\u4f53\u3001\u805a\u5408\u5206\u6563\u8bc1\u636e\uff09\u65f6\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u91cd\u6392\u5e8f\u65b9\u6cd5\u5ffd\u89c6\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "EBCAR\u76f4\u63a5\u5728\u68c0\u7d22\u6bb5\u843d\u7684\u5d4c\u5165\u4e0a\u64cd\u4f5c\uff0c\u901a\u8fc7\u6bb5\u843d\u7ed3\u6784\u4fe1\u606f\u548c\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u8de8\u6bb5\u843d\u7406\u89e3\uff0c\u6355\u6349\u6587\u6863\u95f4\u9ad8\u5c42\u4ea4\u4e92\u548c\u6587\u6863\u5185\u4f4e\u5c42\u5173\u7cfb\u3002", "result": "\u5728ConTEB\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEBCAR\u5728\u9700\u8981\u8de8\u6bb5\u843d\u63a8\u7406\u7684\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u91cd\u6392\u5e8f\u65b9\u6cd5\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u6709\u4f18\u52bf\u3002", "conclusion": "EBCAR\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86RAG\u7cfb\u7edf\u4e2d\u8de8\u6bb5\u843d\u63a8\u7406\u7684\u6311\u6218\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u4e86\u68c0\u7d22\u51c6\u786e\u6027\u3002"}}
{"id": "2510.13334", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13334", "abs": "https://arxiv.org/abs/2510.13334", "authors": ["Yuan Feng", "Haoyu Guo", "JunLin Lv", "S. Kevin Zhou", "Xike Xie"], "title": "Taming the Fragility of KV Cache Eviction in LLM Inference", "comment": null, "summary": "Large language models have revolutionized natural language processing, yet\ntheir deployment remains hampered by the substantial memory and runtime\noverhead of the transformer's Key-Value cache. To mitigate this, recent methods\nemploy a scoring-aggregation framework to evict unimportant cache entries,\nbased on the stability assumption-that a fixed subset of entries remains\nconsistently important during generation. However, prior work has largely\nfocused on refining importance indicators for scoring, while defaulting to mean\naggregation due to a faithful trust in the stability assumption. In this work,\nwe argue that this underlying assumption is inherently fragile, making mean\naggregation highly vulnerable in extreme cases. To counter this, we propose a\nsimple yet elegant defensive aggregation strategy: a two-step, linear-time\napproach that controls worst-case risk, thereby defending against extreme cases\nwith negligible computational overhead. Embodying this strategy, we propose a\nnovel cache eviction method, DefensiveKV and its extension, Layer-DefensiveKV,\nwhich incorporates layer-wise budget allocation. Across seven task domains (18\ndatasets), our methods reduce generation quality loss by 2.3x and 4.3x\nrespectively, versus the strongest baseline under a 20% cache size. These\nresults set new performance benchmarks and pioneer a promising direction for\noptimizing cache eviction against underlying fragility through worst-case risk\nmanagement. Our code is available at https://github.com/FFY0/DefensiveKV.", "AI": {"tldr": "\u63d0\u51fa\u4e86DefensiveKV\u548cLayer-DefensiveKV\u4e24\u79cd\u65b0\u7684KV\u7f13\u5b58\u9a71\u9010\u65b9\u6cd5\uff0c\u901a\u8fc7\u9632\u5fa1\u6027\u805a\u5408\u7b56\u7565\u63a7\u5236\u6700\u574f\u60c5\u51b5\u98ce\u9669\uff0c\u572818\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u51cf\u5c11\u751f\u6210\u8d28\u91cf\u635f\u5931\u3002", "motivation": "\u73b0\u6709KV\u7f13\u5b58\u9a71\u9010\u65b9\u6cd5\u57fa\u4e8e\u7a33\u5b9a\u6027\u5047\u8bbe\uff0c\u8ba4\u4e3a\u56fa\u5b9a\u7684\u7f13\u5b58\u6761\u76ee\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u59cb\u7ec8\u91cd\u8981\uff0c\u4f46\u8fd9\u79cd\u5047\u8bbe\u672c\u8d28\u4e0a\u662f\u8106\u5f31\u7684\uff0c\u4f7f\u5747\u503c\u805a\u5408\u5728\u6781\u7aef\u60c5\u51b5\u4e0b\u5bb9\u6613\u5931\u6548\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u9aa4\u7ebf\u6027\u65f6\u95f4\u9632\u5fa1\u6027\u805a\u5408\u7b56\u7565\uff0c\u63a7\u5236\u6700\u574f\u60c5\u51b5\u98ce\u9669\u3002DefensiveKV\u53ca\u5176\u6269\u5c55Layer-DefensiveKV\u7ed3\u5408\u4e86\u5206\u5c42\u9884\u7b97\u5206\u914d\u3002", "result": "\u57287\u4e2a\u4efb\u52a1\u9886\u57df\uff0818\u4e2a\u6570\u636e\u96c6\uff09\u4e0a\uff0c\u5f53\u7f13\u5b58\u5927\u5c0f\u4e3a20%\u65f6\uff0c\u76f8\u6bd4\u6700\u5f3a\u57fa\u7ebf\uff0c\u4e24\u79cd\u65b9\u6cd5\u5206\u522b\u5c06\u751f\u6210\u8d28\u91cf\u635f\u5931\u51cf\u5c11\u4e862.3\u500d\u548c4.3\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u901a\u8fc7\u6700\u574f\u60c5\u51b5\u98ce\u9669\u7ba1\u7406\u4f18\u5316\u7f13\u5b58\u9a71\u9010\u5f00\u8f9f\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u5e76\u5efa\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u57fa\u51c6\u3002"}}
{"id": "2510.13341", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13341", "abs": "https://arxiv.org/abs/2510.13341", "authors": ["Katerina Korre", "John Pavlopoulos"], "title": "Are Proverbs the New Pythian Oracles? Exploring Sentiment in Greek Sayings", "comment": null, "summary": "Proverbs are among the most fascinating linguistic phenomena that transcend\ncultural and linguistic boundaries. Yet, much of the global landscape of\nproverbs remains underexplored, as many cultures preserve their traditional\nwisdom within their own communities due to the oral tradition of the\nphenomenon. Taking advantage of the current advances in Natural Language\nProcessing (NLP), we focus on Greek proverbs, analyzing their sentiment.\nDeparting from an annotated dataset of Greek proverbs, we expand it to include\nlocal dialects, effectively mapping the annotated sentiment. We present (1) a\nway to exploit LLMs in order to perform sentiment classification of proverbs,\n(2) a map of Greece that provides an overview of the distribution of sentiment,\n(3) a combinatory analysis in terms of the geographic position, dialect, and\ntopic of proverbs. Our findings show that LLMs can provide us with an accurate\nenough picture of the sentiment of proverbs, especially when approached as a\nnon-conventional sentiment polarity task. Moreover, in most areas of Greece\nnegative sentiment is more prevalent.", "AI": {"tldr": "\u5229\u7528NLP\u6280\u672f\u5206\u6790\u5e0c\u814a\u8c1a\u8bed\u7684\u60c5\u611f\u5206\u5e03\uff0c\u901a\u8fc7LLM\u8fdb\u884c\u60c5\u611f\u5206\u7c7b\uff0c\u5e76\u7ed8\u5236\u5e0c\u814a\u5404\u5730\u533a\u60c5\u611f\u5206\u5e03\u5730\u56fe\uff0c\u53d1\u73b0\u8d1f\u9762\u60c5\u611f\u5728\u5927\u591a\u6570\u5730\u533a\u66f4\u4e3a\u666e\u904d\u3002", "motivation": "\u8c1a\u8bed\u662f\u8de8\u8d8a\u6587\u5316\u548c\u8bed\u8a00\u8fb9\u754c\u7684\u6709\u8da3\u8bed\u8a00\u73b0\u8c61\uff0c\u4f46\u8bb8\u591a\u6587\u5316\u56e0\u53e3\u5934\u4f20\u7edf\u800c\u5c06\u5176\u667a\u6167\u4fdd\u7559\u5728\u793e\u533a\u5185\u90e8\uff0c\u5168\u7403\u8c1a\u8bed\u666f\u89c2\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5229\u7528NLP\u6280\u672f\uff0c\u57fa\u4e8e\u6807\u6ce8\u7684\u5e0c\u814a\u8c1a\u8bed\u6570\u636e\u96c6\uff0c\u6269\u5c55\u5230\u5305\u62ec\u5f53\u5730\u65b9\u8a00\uff0c\u4f7f\u7528LLM\u8fdb\u884c\u8c1a\u8bed\u60c5\u611f\u5206\u7c7b\uff0c\u5e76\u7ed3\u5408\u5730\u7406\u4f4d\u7f6e\u3001\u65b9\u8a00\u548c\u4e3b\u9898\u8fdb\u884c\u7ec4\u5408\u5206\u6790\u3002", "result": "LLM\u80fd\u591f\u63d0\u4f9b\u76f8\u5f53\u51c6\u786e\u7684\u8c1a\u8bed\u60c5\u611f\u5206\u6790\uff0c\u5c24\u5176\u662f\u5728\u4f5c\u4e3a\u975e\u5e38\u89c4\u60c5\u611f\u6781\u6027\u4efb\u52a1\u5904\u7406\u65f6\uff1b\u5728\u5e0c\u814a\u5927\u90e8\u5206\u5730\u533a\uff0c\u8d1f\u9762\u60c5\u611f\u66f4\u4e3a\u666e\u904d\u3002", "conclusion": "LLM\u53ef\u4ee5\u6709\u6548\u5206\u6790\u8c1a\u8bed\u60c5\u611f\uff0c\u5e0c\u814a\u8c1a\u8bed\u6574\u4f53\u5448\u73b0\u8d1f\u9762\u60c5\u611f\u503e\u5411\uff0c\u8fd9\u4e3a\u8de8\u6587\u5316\u8bed\u8a00\u73b0\u8c61\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2510.13351", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13351", "abs": "https://arxiv.org/abs/2510.13351", "authors": ["Karthik Avinash", "Nikhil Pareek", "Rishav Hada"], "title": "Protect: Towards Robust Guardrailing Stack for Trustworthy Enterprise LLM Systems", "comment": null, "summary": "The increasing deployment of Large Language Models (LLMs) across enterprise\nand mission-critical domains has underscored the urgent need for robust\nguardrailing systems that ensure safety, reliability, and compliance. Existing\nsolutions often struggle with real-time oversight, multi-modal data handling,\nand explainability -- limitations that hinder their adoption in regulated\nenvironments. Existing guardrails largely operate in isolation, focused on text\nalone making them inadequate for multi-modal, production-scale environments. We\nintroduce Protect, natively multi-modal guardrailing model designed to operate\nseamlessly across text, image, and audio inputs, designed for enterprise-grade\ndeployment. Protect integrates fine-tuned, category-specific adapters trained\nvia Low-Rank Adaptation (LoRA) on an extensive, multi-modal dataset covering\nfour safety dimensions: toxicity, sexism, data privacy, and prompt injection.\nOur teacher-assisted annotation pipeline leverages reasoning and explanation\ntraces to generate high-fidelity, context-aware labels across modalities.\nExperimental results demonstrate state-of-the-art performance across all safety\ndimensions, surpassing existing open and proprietary models such as WildGuard,\nLlamaGuard-4, and GPT-4.1. Protect establishes a strong foundation for\ntrustworthy, auditable, and production-ready safety systems capable of\noperating across text, image, and audio modalities.", "AI": {"tldr": "Protect\u662f\u4e00\u4e2a\u539f\u751f\u591a\u6a21\u6001\u62a4\u680f\u6a21\u578b\uff0c\u4e13\u4e3a\u4f01\u4e1a\u7ea7\u90e8\u7f72\u8bbe\u8ba1\uff0c\u80fd\u591f\u65e0\u7f1d\u5904\u7406\u6587\u672c\u3001\u56fe\u50cf\u548c\u97f3\u9891\u8f93\u5165\uff0c\u5728\u6bd2\u6027\u3001\u6027\u522b\u6b67\u89c6\u3001\u6570\u636e\u9690\u79c1\u548c\u63d0\u793a\u6ce8\u5165\u56db\u4e2a\u5b89\u5168\u7ef4\u5ea6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f01\u4e1a\u5173\u952e\u9886\u57df\u7684\u90e8\u7f72\u589e\u52a0\uff0c\u73b0\u6709\u62a4\u680f\u7cfb\u7edf\u5728\u5b9e\u65f6\u76d1\u7763\u3001\u591a\u6a21\u6001\u6570\u636e\u5904\u7406\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6ee1\u8db3\u53d7\u76d1\u7ba1\u73af\u5883\u7684\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u4f4e\u79e9\u9002\u5e94(LoRA)\u5728\u5e7f\u6cdb\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7279\u5b9a\u7c7b\u522b\u7684\u9002\u914d\u5668\uff0c\u91c7\u7528\u6559\u5e08\u8f85\u52a9\u6807\u6ce8\u6d41\u7a0b\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6807\u7b7e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5728\u6240\u6709\u5b89\u5168\u7ef4\u5ea6\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86WildGuard\u3001LlamaGuard-4\u548cGPT-4.1\u7b49\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "Protect\u4e3a\u53ef\u4fe1\u8d56\u3001\u53ef\u5ba1\u8ba1\u4e14\u751f\u4ea7\u5c31\u7eea\u7684\u5b89\u5168\u7cfb\u7edf\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u80fd\u591f\u8de8\u6587\u672c\u3001\u56fe\u50cf\u548c\u97f3\u9891\u6a21\u6001\u8fd0\u884c\u3002"}}
{"id": "2510.13357", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13357", "abs": "https://arxiv.org/abs/2510.13357", "authors": ["Hamdan Al-Ali", "Ali Reza Ghavamipour", "Tommaso Caselli", "Fatih Turkmen", "Zeerak Talat", "Hanan Aldarmaki"], "title": "Personal Attribute Leakage in Federated Speech Models", "comment": "5 pages, 4 figures, 2 tables", "summary": "Federated learning is a common method for privacy-preserving training of\nmachine learning models. In this paper, we analyze the vulnerability of ASR\nmodels to attribute inference attacks in the federated setting. We test a\nnon-parametric white-box attack method under a passive threat model on three\nASR models: Wav2Vec2, HuBERT, and Whisper. The attack operates solely on weight\ndifferentials without access to raw speech from target speakers. We demonstrate\nattack feasibility on sensitive demographic and clinical attributes: gender,\nage, accent, emotion, and dysarthria. Our findings indicate that attributes\nthat are underrepresented or absent in the pre-training data are more\nvulnerable to such inference attacks. In particular, information about accents\ncan be reliably inferred from all models. Our findings expose previously\nundocumented vulnerabilities in federated ASR models and offer insights towards\nimproved security.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e0bASR\u6a21\u578b\u5bf9\u5c5e\u6027\u63a8\u65ad\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u6d4b\u8bd5\u4e86\u4e09\u79cdASR\u6a21\u578b\u5728\u88ab\u52a8\u5a01\u80c1\u6a21\u578b\u4e0b\u7684\u975e\u53c2\u6570\u767d\u76d2\u653b\u51fb\u65b9\u6cd5\uff0c\u53d1\u73b0\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u5c5e\u6027\u66f4\u5bb9\u6613\u53d7\u5230\u653b\u51fb\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4f5c\u4e3a\u9690\u79c1\u4fdd\u62a4\u7684\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5176\u5b89\u5168\u6027\u9700\u8981\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002\u672c\u6587\u65e8\u5728\u5206\u6790ASR\u6a21\u578b\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e0b\u5bf9\u5c5e\u6027\u63a8\u65ad\u653b\u51fb\u7684\u8106\u5f31\u6027\u3002", "method": "\u91c7\u7528\u975e\u53c2\u6570\u767d\u76d2\u653b\u51fb\u65b9\u6cd5\uff0c\u5728\u88ab\u52a8\u5a01\u80c1\u6a21\u578b\u4e0b\u6d4b\u8bd5Wav2Vec2\u3001HuBERT\u548cWhisper\u4e09\u79cdASR\u6a21\u578b\uff0c\u653b\u51fb\u4ec5\u57fa\u4e8e\u6743\u91cd\u5dee\u5f02\u800c\u4e0d\u8bbf\u95ee\u539f\u59cb\u8bed\u97f3\u6570\u636e\u3002", "result": "\u653b\u51fb\u80fd\u591f\u6210\u529f\u63a8\u65ad\u654f\u611f\u7684\u4eba\u53e3\u7edf\u8ba1\u548c\u4e34\u5e8a\u5c5e\u6027\uff08\u6027\u522b\u3001\u5e74\u9f84\u3001\u53e3\u97f3\u3001\u60c5\u7eea\u3001\u6784\u97f3\u969c\u788d\uff09\uff0c\u7279\u522b\u662f\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\u6216\u7f3a\u5931\u7684\u5c5e\u6027\u66f4\u5bb9\u6613\u53d7\u5230\u653b\u51fb\uff0c\u53e3\u97f3\u4fe1\u606f\u53ef\u4ee5\u4ece\u6240\u6709\u6a21\u578b\u4e2d\u53ef\u9760\u63a8\u65ad\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u8054\u90a6ASR\u6a21\u578b\u4e2d\u5148\u524d\u672a\u8bb0\u5f55\u7684\u8106\u5f31\u6027\uff0c\u4e3a\u6539\u8fdb\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2510.13363", "categories": ["cs.CL", "68T50, 68T30", "I.2.7; I.2.4"], "pdf": "https://arxiv.org/pdf/2510.13363", "abs": "https://arxiv.org/abs/2510.13363", "authors": ["Xiang Lei", "Qin Li", "Min Zhang", "Min Zhang"], "title": "D-SMART: Enhancing LLM Dialogue Consistency via Dynamic Structured Memory And Reasoning Tree", "comment": "8 pages, 6 figures (main content); 25 pages, 18 figures (total)", "summary": "Large Language Models (LLMs) often exhibit factual inconsistencies and\nlogical decay in extended, multi-turn dialogues, a challenge stemming from\ntheir reliance on static, pre-trained knowledge and an inability to reason\nadaptively over the dialogue history. Prevailing mitigation strategies, such as\nRetrieval-Augmented Generation (RAG) and agentic working memories, improve\ninformation recall but still engage with fundamentally static knowledge sources\nand follow pre-defined single reasoning path. This hinders their ability to\npreserve factual and logical consistency of their responses in multi-turn\ndialogues while the context evolves over time. To address this issue, we\npropose D-SMART, a model-agnostic framework designed to maintain multi-turn\ndialogue consistency by enabling LLMs to build and reason over a dynamic,\nstructured representation of the conversational context. This is achieved via\ntwo synergistic components: (1) a Dynamic Structured Memory (DSM), which\nincrementally constructs and maintains an authoritative, OWL-compliant\nknowledge graph of the conversation; and (2) a Reasoning Tree (RT), which\nexecutes inferences as an explicit and traceable multi-step search over the\ngraph. As the popular-used quality score (judged by GPT-4) can overlook logical\nflaws, we introduce new NLI-based metrics to better measure multi-turn dialogue\nconsistency. Comprehensive experiments on the MT-Bench-101 benchmark show that\nD-SMART significantly outperforms state-of-the-art baselines, elevating the\ndialogue consistency score by over 48\\% for both proprietary and open-source\nmodels, and notably improves the quality score of the latter by up to 10.1\\%.", "AI": {"tldr": "D-SMART\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u7ed3\u6784\u5316\u8bb0\u5fc6\u548c\u63a8\u7406\u6811\u6765\u7ef4\u62a4\u591a\u8f6e\u5bf9\u8bdd\u7684\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u8bdd\u8d28\u91cf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7ecf\u5e38\u51fa\u73b0\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u548c\u903b\u8f91\u8870\u9000\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5982RAG\u548c\u5de5\u4f5c\u8bb0\u5fc6\u867d\u7136\u6539\u5584\u4e86\u4fe1\u606f\u53ec\u56de\uff0c\u4f46\u4ecd\u4f9d\u8d56\u9759\u6001\u77e5\u8bc6\u6e90\u548c\u5355\u4e00\u63a8\u7406\u8def\u5f84\uff0c\u65e0\u6cd5\u5728\u52a8\u6001\u53d8\u5316\u7684\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u4e2d\u4fdd\u6301\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faD-SMART\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u534f\u540c\u7ec4\u4ef6\uff1a(1) \u52a8\u6001\u7ed3\u6784\u5316\u8bb0\u5fc6(DSM)\uff0c\u589e\u91cf\u6784\u5efa\u548c\u7ef4\u62a4\u7b26\u5408OWL\u6807\u51c6\u7684\u77e5\u8bc6\u56fe\u8c31\uff1b(2) \u63a8\u7406\u6811(RT)\uff0c\u5728\u77e5\u8bc6\u56fe\u8c31\u4e0a\u6267\u884c\u663e\u5f0f\u3001\u53ef\u8ffd\u6eaf\u7684\u591a\u6b65\u63a8\u7406\u641c\u7d22\u3002", "result": "\u5728MT-Bench-101\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cD-SMART\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c06\u5bf9\u8bdd\u4e00\u81f4\u6027\u5f97\u5206\u63d0\u9ad8\u4e8648%\u4ee5\u4e0a\uff0c\u5f00\u6e90\u6a21\u578b\u7684\u8d28\u91cf\u5f97\u5206\u63d0\u5347\u4e8610.1%\u3002", "conclusion": "D-SMART\u901a\u8fc7\u52a8\u6001\u7ed3\u6784\u5316\u8868\u793a\u548c\u663e\u5f0f\u63a8\u7406\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4e3aLLM\u7684\u5bf9\u8bdd\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u6539\u8fdb\u3002"}}
{"id": "2510.13366", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13366", "abs": "https://arxiv.org/abs/2510.13366", "authors": ["Weishi Wang", "Hengchang Hu", "Zhijie Zhang", "Zhaochen Li", "Hongxin Shao", "Daniel Dahlmeier"], "title": "Document Intelligence in the Era of Large Language Models: A Survey", "comment": null, "summary": "Document AI (DAI) has emerged as a vital application area, and is\nsignificantly transformed by the advent of large language models (LLMs). While\nearlier approaches relied on encoder-decoder architectures, decoder-only LLMs\nhave revolutionized DAI, bringing remarkable advancements in understanding and\ngeneration. This survey provides a comprehensive overview of DAI's evolution,\nhighlighting current research attempts and future prospects of LLMs in this\nfield. We explore key advancements and challenges in multimodal, multilingual,\nand retrieval-augmented DAI, while also suggesting future research directions,\nincluding agent-based approaches and document-specific foundation models. This\npaper aims to provide a structured analysis of the state-of-the-art in DAI and\nits implications for both academic and practical applications.", "AI": {"tldr": "\u672c\u8c03\u67e5\u8bba\u6587\u5168\u9762\u56de\u987e\u4e86\u6587\u6863AI\uff08DAI\uff09\u7684\u53d1\u5c55\u5386\u7a0b\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8be5\u9886\u57df\u7684\u5f53\u524d\u7814\u7a76\u8fdb\u5c55\u548c\u672a\u6765\u524d\u666f\uff0c\u6db5\u76d6\u591a\u6a21\u6001\u3001\u591a\u8bed\u8a00\u548c\u68c0\u7d22\u589e\u5f3a\u7b49\u5173\u952e\u65b9\u5411\u3002", "motivation": "\u6587\u6863AI\u4f5c\u4e3a\u4e00\u4e2a\u91cd\u8981\u5e94\u7528\u9886\u57df\uff0c\u6b63\u88ab\u5927\u578b\u8bed\u8a00\u6a21\u578b\u663e\u8457\u6539\u53d8\u3002\u65e9\u671f\u65b9\u6cd5\u4f9d\u8d56\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u800c\u4ec5\u89e3\u7801\u5668\u7684LLMs\u4e3aDAI\u5e26\u6765\u4e86\u9769\u547d\u6027\u8fdb\u5c55\uff0c\u5728\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u6b65\u3002", "method": "\u672c\u8c03\u67e5\u91c7\u7528\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5168\u9762\u68b3\u7406\u6587\u6863AI\u7684\u6f14\u8fdb\u5386\u7a0b\uff0c\u5206\u6790LLMs\u5728DAI\u4e2d\u7684\u7814\u7a76\u5c1d\u8bd5\u548c\u53d1\u5c55\u524d\u666f\uff0c\u63a2\u7d22\u591a\u6a21\u6001\u3001\u591a\u8bed\u8a00\u548c\u68c0\u7d22\u589e\u5f3a\u7b49\u5173\u952e\u65b9\u5411\u3002", "result": "\u8bba\u6587\u63d0\u4f9b\u4e86\u6587\u6863AI\u9886\u57df\u6700\u65b0\u6280\u672f\u7684\u7ed3\u6784\u5316\u5206\u6790\uff0c\u8bc6\u522b\u4e86\u5173\u952e\u8fdb\u5c55\u548c\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u591a\u6a21\u6001\u7406\u89e3\u3001\u8de8\u8bed\u8a00\u5904\u7406\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7b49\u65b9\u9762\u3002", "conclusion": "\u6587\u6863AI\u9886\u57df\u6b63\u7ecf\u5386\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u6df1\u523b\u53d8\u9769\uff0c\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u57fa\u4e8e\u4ee3\u7406\u7684\u65b9\u6cd5\u548c\u6587\u6863\u7279\u5b9a\u57fa\u7840\u6a21\u578b\uff0c\u5bf9\u5b66\u672f\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u90fd\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.13387", "categories": ["cs.CL", "cs.GT"], "pdf": "https://arxiv.org/pdf/2510.13387", "abs": "https://arxiv.org/abs/2510.13387", "authors": ["Buwei He", "Yang Liu", "Zhaowei Zhang", "Zixia Jia", "Huijia Wu", "Zhaofeng He", "Zilong Zheng", "Yipeng Kang"], "title": "Make an Offer They Can't Refuse: Grounding Bayesian Persuasion in Real-World Dialogues without Pre-Commitment", "comment": null, "summary": "Persuasion, a fundamental social capability for humans, remains a challenge\nfor AI systems such as large language models (LLMs). Current studies often\noverlook the strategic use of information asymmetry in message design or rely\non strong assumptions regarding pre-commitment. In this work, we explore the\napplication of Bayesian Persuasion (BP) in natural language within single-turn\ndialogue settings, to enhance the strategic persuasion capabilities of LLMs.\nOur framework incorporates a commitment-communication mechanism, where the\npersuader explicitly outlines an information schema by narrating their\npotential types (e.g., honest or dishonest), thereby guiding the persuadee in\nperforming the intended Bayesian belief update. We evaluate two variants of our\napproach: Semi-Formal-Natural-Language (SFNL) BP and Fully-Natural-Language\n(FNL) BP, benchmarking them against both naive and strong non-BP (NBP)\nbaselines within a comprehensive evaluation framework. This framework covers a\ndiverse set of persuadees -- including LLM instances with varying prompts and\nfine-tuning and human participants -- across tasks ranging from specially\ndesigned persuasion scenarios to general everyday situations. Experimental\nresults on LLM-based agents reveal three main findings: (1) LLMs guided by BP\nstrategies consistently achieve higher persuasion success rates than NBP\nbaselines; (2) SFNL exhibits greater credibility and logical coherence, while\nFNL shows stronger emotional resonance and robustness in naturalistic\nconversations; (3) with supervised fine-tuning, smaller models can attain BP\nperformance comparable to that of larger models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u8bf4\u670d\u6846\u67b6\u7684\u81ea\u7136\u8bed\u8a00\u8bf4\u670d\u65b9\u6cd5\uff0c\u901a\u8fc7\u627f\u8bfa-\u6c9f\u901a\u673a\u5236\u589e\u5f3aLLMs\u7684\u6218\u7565\u8bf4\u670d\u80fd\u529b\uff0c\u5728\u5355\u8f6e\u5bf9\u8bdd\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u8bf4\u670d\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u5728\u8bf4\u670d\u80fd\u529b\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u5ffd\u89c6\u4e86\u4fe1\u606f\u4e0d\u5bf9\u79f0\u7684\u6218\u7565\u8fd0\u7528\uff0c\u4e14\u4f9d\u8d56\u5f3a\u5047\u8bbe\u7684\u9884\u627f\u8bfa\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u8d1d\u53f6\u65af\u8bf4\u670d\u53d8\u4f53\uff1a\u534a\u6b63\u5f0f\u81ea\u7136\u8bed\u8a00(SFNL)\u548c\u5168\u81ea\u7136\u8bed\u8a00(FNL)\uff0c\u901a\u8fc7\u627f\u8bfa-\u6c9f\u901a\u673a\u5236\u8ba9\u8bf4\u670d\u8005\u660e\u786e\u63cf\u8ff0\u4fe1\u606f\u6a21\u5f0f\uff0c\u5f15\u5bfc\u88ab\u8bf4\u670d\u8005\u8fdb\u884c\u8d1d\u53f6\u65af\u4fe1\u5ff5\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a(1)\u57fa\u4e8eBP\u7b56\u7565\u7684LLMs\u8bf4\u670d\u6210\u529f\u7387\u663e\u8457\u9ad8\u4e8e\u975eBP\u57fa\u7ebf\uff1b(2)SFNL\u5177\u6709\u66f4\u597d\u7684\u53ef\u4fe1\u5ea6\u548c\u903b\u8f91\u4e00\u81f4\u6027\uff0cFNL\u5728\u81ea\u7136\u5bf9\u8bdd\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u60c5\u7eea\u5171\u9e23\u548c\u9c81\u68d2\u6027\uff1b(3)\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\uff0c\u5c0f\u6a21\u578b\u53ef\u4ee5\u8fbe\u5230\u4e0e\u5927\u6a21\u578b\u76f8\u5f53\u7684BP\u6027\u80fd\u3002", "conclusion": "\u8d1d\u53f6\u65af\u8bf4\u670d\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347LLMs\u7684\u6218\u7565\u8bf4\u670d\u80fd\u529b\uff0cSFNL\u548cFNL\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u5404\u6709\u4f18\u52bf\uff0c\u4e14\u6a21\u578b\u89c4\u6a21\u4e0d\u662f\u6027\u80fd\u7684\u51b3\u5b9a\u56e0\u7d20\u3002"}}
{"id": "2510.13395", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13395", "abs": "https://arxiv.org/abs/2510.13395", "authors": ["Agnese Lombardi", "Alessandro Lenci"], "title": "Doing Things with Words: Rethinking Theory of Mind Simulation in Large Language Models", "comment": null, "summary": "Language is fundamental to human cooperation, facilitating not only the\nexchange of information but also the coordination of actions through shared\ninterpretations of situational contexts. This study explores whether the\nGenerative Agent-Based Model (GABM) Concordia can effectively model Theory of\nMind (ToM) within simulated real-world environments. Specifically, we assess\nwhether this framework successfully simulates ToM abilities and whether GPT-4\ncan perform tasks by making genuine inferences from social context, rather than\nrelying on linguistic memorization. Our findings reveal a critical limitation:\nGPT-4 frequently fails to select actions based on belief attribution,\nsuggesting that apparent ToM-like abilities observed in previous studies may\nstem from shallow statistical associations rather than true reasoning.\nAdditionally, the model struggles to generate coherent causal effects from\nagent actions, exposing difficulties in processing complex social interactions.\nThese results challenge current statements about emergent ToM-like capabilities\nin LLMs and highlight the need for more rigorous, action-based evaluation\nframeworks.", "AI": {"tldr": "GPT-4\u5728\u6a21\u62df\u5fc3\u7406\u7406\u8bba\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u5c40\u9650\uff0c\u5176\u770b\u4f3c\u7684\u5fc3\u667a\u63a8\u7406\u80fd\u529b\u53ef\u80fd\u6e90\u4e8e\u6d45\u5c42\u7edf\u8ba1\u5173\u8054\u800c\u975e\u771f\u6b63\u63a8\u7406\uff0c\u65e0\u6cd5\u57fa\u4e8e\u4fe1\u5ff5\u5f52\u56e0\u9009\u62e9\u884c\u52a8\uff0c\u4e14\u5728\u590d\u6742\u793e\u4ea4\u4e92\u52a8\u4e2d\u96be\u4ee5\u4ea7\u751f\u8fde\u8d2f\u56e0\u679c\u6548\u5e94\u3002", "motivation": "\u63a2\u7d22\u751f\u6210\u4ee3\u7406\u6a21\u578bConcordia\u80fd\u5426\u6709\u6548\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u5fc3\u7406\u7406\u8bba\uff0c\u8bc4\u4f30GPT-4\u662f\u5426\u80fd\u4ece\u793e\u4f1a\u60c5\u5883\u8fdb\u884c\u771f\u6b63\u63a8\u7406\u800c\u975e\u4f9d\u8d56\u8bed\u8a00\u8bb0\u5fc6\u3002", "method": "\u4f7f\u7528\u751f\u6210\u4ee3\u7406\u6a21\u578bConcordia\u6846\u67b6\uff0c\u8bc4\u4f30GPT-4\u5728\u6a21\u62df\u73af\u5883\u4e2d\u7684\u5fc3\u7406\u7406\u8bba\u8868\u73b0\uff0c\u7279\u522b\u662f\u57fa\u4e8e\u4fe1\u5ff5\u5f52\u56e0\u7684\u884c\u52a8\u9009\u62e9\u548c\u793e\u4ea4\u4e92\u52a8\u5904\u7406\u80fd\u529b\u3002", "result": "GPT-4\u7ecf\u5e38\u65e0\u6cd5\u57fa\u4e8e\u4fe1\u5ff5\u5f52\u56e0\u9009\u62e9\u884c\u52a8\uff0c\u96be\u4ee5\u4ece\u4ee3\u7406\u884c\u52a8\u751f\u6210\u8fde\u8d2f\u56e0\u679c\u6548\u5e94\uff0c\u8868\u660e\u5176\u5fc3\u667a\u63a8\u7406\u80fd\u529b\u6709\u9650\u4e14\u53ef\u80fd\u6e90\u4e8e\u6d45\u5c42\u7edf\u8ba1\u5173\u8054\u3002", "conclusion": "\u5f53\u524dLLMs\u4e2d\u6240\u8c13\u7684\u5fc3\u667a\u7406\u8bba\u80fd\u529b\u53ef\u80fd\u88ab\u5938\u5927\uff0c\u9700\u8981\u66f4\u4e25\u683c\u7684\u57fa\u4e8e\u884c\u52a8\u7684\u8bc4\u4ef7\u6846\u67b6\u6765\u51c6\u786e\u8bc4\u4f30\u5176\u771f\u5b9e\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2510.13407", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13407", "abs": "https://arxiv.org/abs/2510.13407", "authors": ["Kim Gfeller", "Sabine Stoll", "Chundra Cathcart", "Paul Widmer"], "title": "Investigating Lexical Change through Cross-Linguistic Colexification Patterns", "comment": null, "summary": "One of the most intriguing features of language is its constant change, with\nongoing shifts in how meaning is expressed. Despite decades of research, the\nfactors that determine how and why meanings evolve remain only partly\nunderstood. Colexification -- the phenomenon of expressing multiple distinct\nconcepts using the same word form -- serves as a valuable window onto the\ndynamics of meaning change across languages. Here, we apply phylogenetic\ncomparative models to dictionary data from three language families,\nAustronesian, Indo-European, and Uralic, in order to shed light on the\nevolutionary dynamics underlying the colexification of concept pairs. We assess\nthe effects of three predictors: associativity, borrowability, and usage\nfrequency. Our results show that more closely related concept pairs are\ncolexified across a larger portion of the family tree and exhibit slower rates\nof change. In contrast, concept pairs that are more frequent and more prone to\nborrowing tend to change more rapidly and are less often colexified. We also\nfind considerable differences between the language families under study,\nsuggesting that areal and cultural factors may play a role.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u7cfb\u7edf\u53d1\u80b2\u6bd4\u8f83\u6a21\u578b\u5206\u6790\u4e09\u4e2a\u8bed\u7cfb\uff08\u5357\u5c9b\u8bed\u7cfb\u3001\u5370\u6b27\u8bed\u7cfb\u548c\u4e4c\u62c9\u5c14\u8bed\u7cfb\uff09\u7684\u8bcd\u5178\u6570\u636e\uff0c\u53d1\u73b0\u6982\u5ff5\u5173\u8054\u6027\u8d8a\u5f3a\u8d8a\u5bb9\u6613\u5171\u8bcd\u5316\u4e14\u53d8\u5316\u66f4\u6162\uff0c\u800c\u4f7f\u7528\u9891\u7387\u9ad8\u548c\u6613\u501f\u7528\u7684\u6982\u5ff5\u5bf9\u53d8\u5316\u66f4\u5feb\u4e14\u8f83\u5c11\u5171\u8bcd\u5316\u3002", "motivation": "\u8bed\u8a00\u610f\u4e49\u5982\u4f55\u6f14\u53d8\u53ca\u5176\u51b3\u5b9a\u56e0\u7d20\u5c1a\u4e0d\u5b8c\u5168\u6e05\u695a\uff0c\u5171\u8bcd\u5316\u73b0\u8c61\u4e3a\u7814\u7a76\u8de8\u8bed\u8a00\u610f\u4e49\u53d8\u5316\u52a8\u6001\u63d0\u4f9b\u4e86\u91cd\u8981\u7a97\u53e3\u3002", "method": "\u5e94\u7528\u7cfb\u7edf\u53d1\u80b2\u6bd4\u8f83\u6a21\u578b\u5206\u6790\u4e09\u4e2a\u8bed\u7cfb\u7684\u8bcd\u5178\u6570\u636e\uff0c\u8bc4\u4f30\u5173\u8054\u6027\u3001\u501f\u7528\u6027\u548c\u4f7f\u7528\u9891\u7387\u4e09\u4e2a\u9884\u6d4b\u56e0\u5b50\u7684\u5f71\u54cd\u3002", "result": "\u5173\u8054\u6027\u5f3a\u7684\u6982\u5ff5\u5bf9\u5728\u8bed\u7cfb\u6811\u4e2d\u66f4\u5e7f\u6cdb\u5171\u8bcd\u5316\u4e14\u53d8\u5316\u66f4\u6162\uff1b\u9ad8\u9891\u548c\u6613\u501f\u7528\u6982\u5ff5\u5bf9\u53d8\u5316\u66f4\u5feb\u4e14\u8f83\u5c11\u5171\u8bcd\u5316\uff1b\u4e0d\u540c\u8bed\u7cfb\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u6982\u5ff5\u5173\u8054\u6027\u4fc3\u8fdb\u5171\u8bcd\u5316\u7a33\u5b9a\u6027\uff0c\u800c\u4f7f\u7528\u9891\u7387\u548c\u501f\u7528\u6027\u52a0\u901f\u53d8\u5316\uff0c\u8bed\u8a00\u533a\u57df\u548c\u6587\u5316\u56e0\u7d20\u5728\u610f\u4e49\u6f14\u53d8\u4e2d\u8d77\u91cd\u8981\u4f5c\u7528\u3002"}}
{"id": "2510.13430", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13430", "abs": "https://arxiv.org/abs/2510.13430", "authors": ["Ahmed Alzubaidi", "Shaikha Alsuwaidi", "Basma El Amel Boussaha", "Leen AlQadi", "Omar Alkaabi", "Mohammed Alyafeai", "Hamza Alobeidli", "Hakim Hacid"], "title": "Evaluating Arabic Large Language Models: A Survey of Benchmarks, Methods, and Gaps", "comment": null, "summary": "This survey provides the first systematic review of Arabic LLM benchmarks,\nanalyzing 40+ evaluation benchmarks across NLP tasks, knowledge domains,\ncultural understanding, and specialized capabilities. We propose a taxonomy\norganizing benchmarks into four categories: Knowledge, NLP Tasks, Culture and\nDialects, and Target-Specific evaluations. Our analysis reveals significant\nprogress in benchmark diversity while identifying critical gaps: limited\ntemporal evaluation, insufficient multi-turn dialogue assessment, and cultural\nmisalignment in translated datasets. We examine three primary approaches:\nnative collection, translation, and synthetic generation discussing their\ntrade-offs regarding authenticity, scale, and cost. This work serves as a\ncomprehensive reference for Arabic NLP researchers, providing insights into\nbenchmark methodologies, reproducibility standards, and evaluation metrics\nwhile offering recommendations for future development.", "AI": {"tldr": "\u8fd9\u662f\u9996\u4e2a\u7cfb\u7edf\u6027\u7684\u963f\u62c9\u4f2f\u8bed\u5927\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u8bc4\u6d4b\u7efc\u8ff0\uff0c\u5206\u6790\u4e8640\u591a\u4e2a\u8bc4\u6d4b\u57fa\u51c6\uff0c\u6db5\u76d6NLP\u4efb\u52a1\u3001\u77e5\u8bc6\u9886\u57df\u3001\u6587\u5316\u7406\u89e3\u548c\u4e13\u4e1a\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u56db\u5206\u7c7b\u6cd5\u5e76\u8bc6\u522b\u4e86\u5173\u952e\u5dee\u8ddd\u3002", "motivation": "\u7f3a\u4e4f\u5bf9\u963f\u62c9\u4f2f\u8bedLLM\u57fa\u51c6\u8bc4\u6d4b\u7684\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u9700\u8981\u4e3a\u963f\u62c9\u4f2f\u8bedNLP\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u5168\u9762\u7684\u53c2\u8003\u6307\u5357\uff0c\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u56db\u5206\u7c7b\u6cd5\uff08\u77e5\u8bc6\u3001NLP\u4efb\u52a1\u3001\u6587\u5316\u4e0e\u65b9\u8a00\u3001\u76ee\u6807\u7279\u5b9a\u8bc4\u6d4b\uff09\uff0c\u5206\u6790\u4e09\u79cd\u4e3b\u8981\u65b9\u6cd5\uff1a\u539f\u751f\u6536\u96c6\u3001\u7ffb\u8bd1\u548c\u5408\u6210\u751f\u6210\uff0c\u8ba8\u8bba\u5176\u5728\u771f\u5b9e\u6027\u3001\u89c4\u6a21\u548c\u6210\u672c\u65b9\u9762\u7684\u6743\u8861\u3002", "result": "\u53d1\u73b0\u57fa\u51c6\u591a\u6837\u6027\u663e\u8457\u8fdb\u6b65\uff0c\u4f46\u8bc6\u522b\u51fa\u5173\u952e\u5dee\u8ddd\uff1a\u65f6\u95f4\u8bc4\u4f30\u6709\u9650\u3001\u591a\u8f6e\u5bf9\u8bdd\u8bc4\u4f30\u4e0d\u8db3\u3001\u7ffb\u8bd1\u6570\u636e\u96c6\u7684\u6587\u5316\u9519\u4f4d\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u963f\u62c9\u4f2f\u8bedNLP\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u53c2\u8003\uff0c\u63d0\u4f9b\u4e86\u57fa\u51c6\u65b9\u6cd5\u5b66\u3001\u53ef\u590d\u73b0\u6027\u6807\u51c6\u548c\u8bc4\u4f30\u6307\u6807\u7684\u89c1\u89e3\uff0c\u5e76\u4e3a\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u5efa\u8bae\u3002"}}
{"id": "2510.13434", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13434", "abs": "https://arxiv.org/abs/2510.13434", "authors": ["Hao Wang", "Linlong Xu", "Heng Liu", "Yangyang Liu", "Xiaohu Zhao", "Bo Zeng", "Liangying Shao", "Longyue Wang", "Weihua Luo", "Kaifu Zhang"], "title": "Beyond Single-Reward: Multi-Pair, Multi-Perspective Preference Optimization for Machine Translation", "comment": null, "summary": "Direct Preference Optimization (DPO) is a powerful paradigm for aligning\nLarge Language Models (LLMs) to human preferences in Machine Translation (MT),\nbut current methods are hindered by two fundamental challenges: (1) flawed\nreward signals from Quality Estimation (QE) models that overlook critical\nerrors like translation hallucination, and (2) inefficient data utilization\nthat discards valuable learning signals by selecting only a single win-loss\npair. To address these limitations, we introduce M^2PO: Multi-Pair,\nMulti-Perspective Preference Optimization. Our framework integrates a\nmulti-perspective reward engine that creates a more robust signal by combining\ntwo key viewpoints: a new hallucination penalty for factuality, and an\ninnovative dynamic quality score that adaptively fuses external evaluations\nwith the model's own evolving judgment. This is synergistically paired with a\nmulti-pair construction strategy that systematically creates a comprehensive\nset of preference pairs from the entire pool of translation candidates. This\nsynergistic approach ensures the model learns from a richer spectrum of quality\ntrade-offs, leading to more robust and faithful translations. On challenging\nWMT21-22 benchmarks, M^2PO substantially outperforms existing preference\noptimization methods and demonstrates highly competitive performance against\nleading proprietary LLMs.", "AI": {"tldr": "M\u00b2PO\u662f\u4e00\u4e2a\u7528\u4e8e\u673a\u5668\u7ffb\u8bd1\u7684\u591a\u5bf9\u591a\u89c6\u89d2\u504f\u597d\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5e7b\u89c9\u60e9\u7f5a\u548c\u52a8\u6001\u8d28\u91cf\u8bc4\u5206\u6765\u589e\u5f3a\u5956\u52b1\u4fe1\u53f7\uff0c\u5e76\u5229\u7528\u591a\u5bf9\u6784\u9020\u7b56\u7565\u4ece\u6240\u6709\u7ffb\u8bd1\u5019\u9009\u4e2d\u7cfb\u7edf\u521b\u5efa\u504f\u597d\u5bf9\uff0c\u663e\u8457\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eDPO\u7684\u673a\u5668\u7ffb\u8bd1\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u6839\u672c\u6027\u6311\u6218\uff1a(1) \u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\u63d0\u4f9b\u7684\u5956\u52b1\u4fe1\u53f7\u5b58\u5728\u7f3a\u9677\uff0c\u5ffd\u7565\u4e86\u7ffb\u8bd1\u5e7b\u89c9\u7b49\u5173\u952e\u9519\u8bef\uff1b(2) \u6570\u636e\u5229\u7528\u6548\u7387\u4f4e\u4e0b\uff0c\u4ec5\u9009\u62e9\u5355\u4e00\u80dc\u8d1f\u5bf9\u800c\u4e22\u5f03\u4e86\u6709\u4ef7\u503c\u7684\u5b66\u4e60\u4fe1\u53f7\u3002", "method": "\u63d0\u51faM\u00b2PO\u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u591a\u89c6\u89d2\u5956\u52b1\u5f15\u64ce\uff0c\u7ed3\u5408\u5e7b\u89c9\u60e9\u7f5a\u548c\u52a8\u6001\u8d28\u91cf\u8bc4\u5206\uff1b2\uff09\u591a\u5bf9\u6784\u9020\u7b56\u7565\uff0c\u4ece\u6240\u6709\u7ffb\u8bd1\u5019\u9009\u4e2d\u7cfb\u7edf\u521b\u5efa\u5168\u9762\u7684\u504f\u597d\u5bf9\u96c6\u5408\u3002", "result": "\u5728WMT21-22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cM\u00b2PO\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff0c\u5e76\u4e0e\u9886\u5148\u7684\u4e13\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u51fa\u9ad8\u5ea6\u7ade\u4e89\u529b\u3002", "conclusion": "M\u00b2PO\u901a\u8fc7\u591a\u89c6\u89d2\u5956\u52b1\u548c\u591a\u5bf9\u6784\u9020\u7684\u534f\u540c\u65b9\u6cd5\uff0c\u80fd\u591f\u4ece\u66f4\u4e30\u5bcc\u7684\u8d28\u91cf\u6743\u8861\u8c31\u4e2d\u5b66\u4e60\uff0c\u4ea7\u751f\u66f4\u9c81\u68d2\u548c\u5fe0\u5b9e\u7684\u7ffb\u8bd1\u3002"}}
{"id": "2510.13494", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13494", "abs": "https://arxiv.org/abs/2510.13494", "authors": ["Tommaso Bonomo", "Luca Gioffr\u00e9", "Roberto Navigli"], "title": "LiteraryQA: Towards Effective Evaluation of Long-document Narrative QA", "comment": "Accepted to EMNLP 2025 Main Conference. 22 pages", "summary": "Question Answering (QA) on narrative text poses a unique challenge to current\nsystems, requiring a deep understanding of long, complex documents. However,\nthe reliability of NarrativeQA, the most widely used benchmark in this domain,\nis hindered by noisy documents and flawed QA pairs. In this work, we introduce\nLiteraryQA, a high-quality subset of NarrativeQA focused on literary works.\nUsing a human- and LLM-validated pipeline, we identify and correct low-quality\nQA samples while removing extraneous text from source documents. We then carry\nout a meta-evaluation of automatic metrics to clarify how systems should be\nevaluated on LiteraryQA. This analysis reveals that all n-gram-based metrics\nhave a low system-level correlation to human judgment, while LLM-as-a-Judge\nevaluations, even with small open-weight models, can strongly agree with the\nranking identified by humans. Finally, we benchmark a set of long-context LLMs\non LiteraryQA. We release our code and data at\nhttps://github.com/SapienzaNLP/LiteraryQA.", "AI": {"tldr": "LiteraryQA\u662f\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684NarrativeQA\u5b50\u96c6\uff0c\u4e13\u6ce8\u4e8e\u6587\u5b66\u4f5c\u54c1\uff0c\u901a\u8fc7\u4eba\u5de5\u548cLLM\u9a8c\u8bc1\u7684\u6d41\u7a0b\u6e05\u7406\u4e86\u4f4e\u8d28\u91cfQA\u6837\u672c\u548c\u6587\u6863\u566a\u97f3\uff0c\u5e76\u53d1\u73b0\u57fa\u4e8en-gram\u7684\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u4e0e\u4eba\u7c7b\u5224\u65ad\u76f8\u5173\u6027\u4f4e\uff0c\u800cLLM\u8bc4\u4f30\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u4e0e\u4eba\u7c7b\u6392\u540d\u4e00\u81f4\u3002", "motivation": "\u5f53\u524d\u5728\u53d9\u4e8b\u6587\u672c\u4e0a\u7684\u95ee\u7b54\u7cfb\u7edf\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u7406\u89e3\u957f\u800c\u590d\u6742\u7684\u6587\u6863\uff0c\u4f46\u6700\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6NarrativeQA\u5b58\u5728\u6587\u6863\u566a\u97f3\u548c\u6709\u7f3a\u9677\u7684QA\u5bf9\uff0c\u5f71\u54cd\u4e86\u5176\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528\u4eba\u5de5\u548cLLM\u9a8c\u8bc1\u7684\u6d41\u7a0b\uff0c\u8bc6\u522b\u548c\u4fee\u6b63\u4f4e\u8d28\u91cfQA\u6837\u672c\uff0c\u540c\u65f6\u4ece\u6e90\u6587\u6863\u4e2d\u79fb\u9664\u65e0\u5173\u6587\u672c\uff0c\u7136\u540e\u8fdb\u884c\u81ea\u52a8\u6307\u6807\u7684\u5143\u8bc4\u4f30\uff0c\u5e76\u5728\u4e00\u7ec4\u957f\u4e0a\u4e0b\u6587LLM\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5206\u6790\u663e\u793a\u6240\u6709\u57fa\u4e8en-gram\u7684\u6307\u6807\u5728\u7cfb\u7edf\u7ea7\u522b\u4e0a\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u76f8\u5173\u6027\u8f83\u4f4e\uff0c\u800cLLM-as-a-Judge\u8bc4\u4f30\u65b9\u6cd5\uff08\u5373\u4f7f\u662f\u5c0f\u578b\u5f00\u6e90\u6a21\u578b\uff09\u80fd\u4e0e\u4eba\u7c7b\u8bc6\u522b\u7684\u6392\u540d\u5f3a\u4e00\u81f4\u3002", "conclusion": "LiteraryQA\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u53ef\u9760\u7684\u53d9\u4e8b\u6587\u672c\u95ee\u7b54\u57fa\u51c6\uff0cLLM\u8bc4\u4f30\u65b9\u6cd5\u6bd4\u4f20\u7edfn-gram\u6307\u6807\u66f4\u9002\u5408\u8bc4\u4f30\u6b64\u7c7b\u4efb\u52a1\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2510.13499", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13499", "abs": "https://arxiv.org/abs/2510.13499", "authors": ["Xiaozhe Li", "TianYi Lyu", "Siyi Yang", "Yuxi Gong", "Yizhao Yang", "Jinxuan Huang", "Ligao Zhang", "Zhuoyi Huang", "Qingwen Liu"], "title": "ConsintBench: Evaluating Language Models on Real-World Consumer Intent Understanding", "comment": null, "summary": "Understanding human intent is a complex, high-level task for large language\nmodels (LLMs), requiring analytical reasoning, contextual interpretation,\ndynamic information aggregation, and decision-making under uncertainty.\nReal-world public discussions, such as consumer product discussions, are rarely\nlinear or involve a single user. Instead, they are characterized by interwoven\nand often conflicting perspectives, divergent concerns, goals, emotional\ntendencies, as well as implicit assumptions and background knowledge about\nusage scenarios. To accurately understand such explicit public intent, an LLM\nmust go beyond parsing individual sentences; it must integrate multi-source\nsignals, reason over inconsistencies, and adapt to evolving discourse, similar\nto how experts in fields like politics, economics, or finance approach complex,\nuncertain environments. Despite the importance of this capability, no\nlarge-scale benchmark currently exists for evaluating LLMs on real-world human\nintent understanding, primarily due to the challenges of collecting real-world\npublic discussion data and constructing a robust evaluation pipeline. To bridge\nthis gap, we introduce \\bench, the first dynamic, live evaluation benchmark\nspecifically designed for intent understanding, particularly in the consumer\ndomain. \\bench is the largest and most diverse benchmark of its kind,\nsupporting real-time updates while preventing data contamination through an\nautomated curation pipeline.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u4e13\u95e8\u7528\u4e8e\u610f\u56fe\u7406\u89e3\uff08\u7279\u522b\u662f\u6d88\u8d39\u9886\u57df\uff09\u7684\u52a8\u6001\u5b9e\u65f6\u8bc4\u4f30\u57fa\u51c6\\bench\uff0c\u8be5\u57fa\u51c6\u652f\u6301\u5b9e\u65f6\u66f4\u65b0\u5e76\u901a\u8fc7\u81ea\u52a8\u5316\u6d41\u7a0b\u9632\u6b62\u6570\u636e\u6c61\u67d3\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u9488\u5bf9\u771f\u5b9e\u4e16\u754c\u4eba\u7c7b\u610f\u56fe\u7406\u89e3\u7684\u5927\u89c4\u6a21\u8bc4\u4f30\u57fa\u51c6\uff0c\u4e3b\u8981\u7531\u4e8e\u6536\u96c6\u771f\u5b9e\u516c\u5171\u8ba8\u8bba\u6570\u636e\u548c\u6784\u5efa\u7a33\u5065\u8bc4\u4f30\u6d41\u7a0b\u7684\u6311\u6218\u3002\u771f\u5b9e\u4e16\u754c\u516c\u5171\u8ba8\u8bba\u5177\u6709\u4ea4\u7ec7\u51b2\u7a81\u89c2\u70b9\u3001\u4e0d\u540c\u5173\u5207\u3001\u76ee\u6807\u3001\u60c5\u611f\u503e\u5411\u4ee5\u53ca\u9690\u542b\u5047\u8bbe\u7b49\u7279\u70b9\u3002", "method": "\u6784\u5efa\\bench\u57fa\u51c6\uff0c\u8fd9\u662f\u540c\u7c7b\u4e2d\u6700\u5927\u6700\u591a\u6837\u5316\u7684\u57fa\u51c6\uff0c\u652f\u6301\u5b9e\u65f6\u66f4\u65b0\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u7b56\u5212\u6d41\u7a0b\u9632\u6b62\u6570\u636e\u6c61\u67d3\u3002", "result": "\u6210\u529f\u521b\u5efa\u4e86\u9996\u4e2a\u52a8\u6001\u3001\u5b9e\u65f6\u7684\u610f\u56fe\u7406\u89e3\u8bc4\u4f30\u57fa\u51c6\\bench\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u8bc4\u4f30\u5de5\u5177\u7684\u7a7a\u767d\u3002", "conclusion": "\\bench\u57fa\u51c6\u4e3a\u8bc4\u4f30LLMs\u5728\u771f\u5b9e\u4e16\u754c\u4eba\u7c7b\u610f\u56fe\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u7279\u522b\u662f\u5728\u6d88\u8d39\u9886\u57df\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6\u7f3a\u5931\u7684\u95ee\u9898\u3002"}}
{"id": "2510.13500", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13500", "abs": "https://arxiv.org/abs/2510.13500", "authors": ["Shujun Xia", "Haokun Lin", "Yichen Wu", "Yinan Zhou", "Zixuan Li", "Zhongwei Wan", "Xingrun Xing", "Yefeng Zheng", "Xiang Li", "Caifeng Shan", "Zhenan Sun", "Quanzheng Li"], "title": "MedREK: Retrieval-Based Editing for Medical LLMs with Key-Aware Prompts", "comment": "Preprint, work in progress", "summary": "LLMs hold great promise for healthcare applications, but the rapid evolution\nof medical knowledge and errors in training data often cause them to generate\noutdated or inaccurate information, limiting their applicability in high-stakes\nclinical practice. Model editing has emerged as a potential remedy without full\nretraining. While parameter-based editing often compromises locality and is\nthus ill-suited for the medical domain, retrieval-based editing offers a more\nviable alternative. However, it still faces two critical challenges: (1)\nrepresentation overlap within the medical knowledge space often causes\ninaccurate retrieval and reduces editing accuracy; (2) existing methods are\nrestricted to single-sample edits, while batch-editing remains largely\nunexplored despite its importance for real-world medical applications. To\naddress these challenges, we first construct MedVersa, \\hk{an enhanced\nbenchmark with broader coverage of medical subjects, designed to evaluate both\nsingle and batch edits under strict locality constraints}. We then propose\nMedREK, a retrieval-based editing framework that integrates a shared query-key\nmodule for precise matching with an attention-based prompt encoder for\ninformative guidance. Experimental results on various medical benchmarks\ndemonstrate that our MedREK achieves superior performance across different core\nmetrics and provides the first validated solution for batch-editing in medical\nLLMs. Our code and dataset are available at\nhttps://github.com/mylittleriver/MedREK.", "AI": {"tldr": "\u63d0\u51fa\u4e86MedREK\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u5f0f\u7f16\u8f91\u89e3\u51b3\u533b\u7597LLMs\u77e5\u8bc6\u8fc7\u65f6\u95ee\u9898\uff0c\u652f\u6301\u6279\u91cf\u7f16\u8f91\uff0c\u5728MedVersa\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02", "motivation": "\u533b\u7597LLMs\u9762\u4e34\u77e5\u8bc6\u5feb\u901f\u66f4\u65b0\u548c\u8bad\u7ec3\u6570\u636e\u9519\u8bef\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u751f\u6210\u8fc7\u65f6\u6216\u4e0d\u51c6\u786e\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u5e94\u7528\u3002\u53c2\u6570\u7f16\u8f91\u65b9\u6cd5\u4f1a\u635f\u5bb3\u5c40\u90e8\u6027\uff0c\u800c\u73b0\u6709\u68c0\u7d22\u7f16\u8f91\u65b9\u6cd5\u5b58\u5728\u8868\u793a\u91cd\u53e0\u548c\u4ec5\u652f\u6301\u5355\u6837\u672c\u7f16\u8f91\u7684\u5c40\u9650", "method": "\u6784\u5efaMedVersa\u57fa\u51c6\u8bc4\u4f30\u5355\u6837\u672c\u548c\u6279\u91cf\u7f16\u8f91\uff0c\u63d0\u51faMedREK\u6846\u67b6\uff1a\u96c6\u6210\u5171\u4eab\u67e5\u8be2-\u952e\u6a21\u5757\u8fdb\u884c\u7cbe\u786e\u5339\u914d\uff0c\u7ed3\u5408\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u63d0\u793a\u7f16\u7801\u5668\u63d0\u4f9b\u4fe1\u606f\u6307\u5bfc", "result": "\u5728\u5404\u79cd\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMedREK\u5728\u4e0d\u540c\u6838\u5fc3\u6307\u6807\u4e0a\u5747\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\uff0c\u9996\u6b21\u4e3a\u533b\u7597LLMs\u63d0\u4f9b\u4e86\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u6279\u91cf\u7f16\u8f91\u89e3\u51b3\u65b9\u6848", "conclusion": "MedREK\u901a\u8fc7\u68c0\u7d22\u5f0f\u7f16\u8f91\u6709\u6548\u89e3\u51b3\u4e86\u533b\u7597LLMs\u7684\u77e5\u8bc6\u66f4\u65b0\u95ee\u9898\uff0c\u652f\u6301\u6279\u91cf\u7f16\u8f91\uff0c\u4e3a\u4e34\u5e8a\u5b9e\u8df5\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.13554", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13554", "abs": "https://arxiv.org/abs/2510.13554", "authors": ["Yang Li", "Zhichen Dong", "Yuhan Sun", "Weixun Wang", "Shaopan Xiong", "Yijia Luo", "Jiashun Liu", "Han Lu", "Jiamang Wang", "Wenbo Su", "Bo Zheng", "Junchi Yan"], "title": "Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm Enables Fine-Grained Policy Optimization", "comment": "23 pages, 8 figures, 5 tables", "summary": "The reasoning pattern of Large language models (LLMs) remains opaque, and\nReinforcement learning (RL) typically applies uniform credit across an entire\ngeneration, blurring the distinction between pivotal and routine steps. This\nwork positions attention as a privileged substrate that renders the internal\nlogic of LLMs legible, not merely as a byproduct of computation, but as a\nmechanistic blueprint of reasoning itself. We first distinguish attention heads\nbetween locally and globally focused information processing and reveal that\nlocally focused heads produce a sawtooth pattern near the diagonal indicating\nphrasal chunks, while globally focused heads expose tokens that exert broad\ndownstream influence over future tokens. We formalize these with two metrics:\n1) Windowed Average Attention Distance, which measures the extent of backward\nattention within a clipped window; 2) Future Attention Influence, which\nquantifies a token's global importance as the average attention it receives\nfrom subsequent tokens. Taken together, these signals reveal a recurring\npreplan-and-anchor mechanism, where the model first performs a long-range\ncontextual reference to generate an introductory token, which is immediately\nfollowed by or coincides with a semantic anchor token that organizes subsequent\nreasoning. Leveraging these insights, we introduce three novel RL strategies\nthat dynamically perform targeted credit assignment to critical nodes (preplan\ntokens, anchor tokens, and their temporal coupling) and show consistent\nperformance gains across various reasoning tasks. By aligning optimization with\nthe model's intrinsic reasoning rhythm, we aim to transform opaque optimization\ninto an actionable structure-aware process, hoping to offer a potential step\ntoward more transparent and effective optimization of LLM reasoning.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5206\u6790\u6ce8\u610f\u529b\u673a\u5236\u63ed\u793a\u4e86LLM\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u91cf\u5316\u6307\u6807\u6765\u8bc6\u522b\u5173\u952e\u63a8\u7406\u6b65\u9aa4\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u4e09\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5728\u591a\u4e2a\u63a8\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6a21\u5f0f\u4e0d\u900f\u660e\uff0c\u4f20\u7edf\u7684\u5f3a\u5316\u5b66\u4e60\u901a\u5e38\u5bf9\u6574\u4e2a\u751f\u6210\u8fc7\u7a0b\u5e94\u7528\u7edf\u4e00\u7684\u4fe1\u7528\u5206\u914d\uff0c\u65e0\u6cd5\u533a\u5206\u5173\u952e\u6b65\u9aa4\u548c\u5e38\u89c4\u6b65\u9aa4\u3002", "method": "1) \u533a\u5206\u5c40\u90e8\u548c\u5168\u5c40\u6ce8\u610f\u529b\u5934\uff0c\u53d1\u73b0\u5c40\u90e8\u5934\u4ea7\u751f\u77ed\u8bed\u5757\u7684\u5bf9\u89d2\u952f\u9f7f\u6a21\u5f0f\uff0c\u5168\u5c40\u5934\u63ed\u793a\u5bf9\u672a\u6765\u6807\u8bb0\u6709\u5e7f\u6cdb\u5f71\u54cd\u7684\u6807\u8bb0\uff1b2) \u63d0\u51fa\u4e24\u4e2a\u91cf\u5316\u6307\u6807\uff1a\u7a97\u53e3\u5e73\u5747\u6ce8\u610f\u529b\u8ddd\u79bb\u548c\u672a\u6765\u6ce8\u610f\u529b\u5f71\u54cd\uff1b3) \u57fa\u4e8e\u53d1\u73b0\u7684\u9884\u89c4\u5212-\u951a\u5b9a\u673a\u5236\uff0c\u5f00\u53d1\u4e09\u79cd\u65b0\u7684RL\u7b56\u7565\u8fdb\u884c\u9488\u5bf9\u6027\u4fe1\u7528\u5206\u914d\u3002", "result": "\u63ed\u793a\u4e86LLM\u63a8\u7406\u4e2d\u53cd\u590d\u51fa\u73b0\u7684\u9884\u89c4\u5212-\u951a\u5b9a\u673a\u5236\uff0c\u63d0\u51fa\u7684\u4e09\u79cdRL\u7b56\u7565\u5728\u5404\u79cd\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u5c06\u4f18\u5316\u4e0e\u6a21\u578b\u5185\u5728\u63a8\u7406\u8282\u594f\u5bf9\u9f50\uff0c\u5c06\u4e0d\u900f\u660e\u7684\u4f18\u5316\u8f6c\u53d8\u4e3a\u53ef\u64cd\u4f5c\u7684\u7ed3\u6784\u611f\u77e5\u8fc7\u7a0b\uff0c\u4e3aLLM\u63a8\u7406\u7684\u900f\u660e\u548c\u6709\u6548\u4f18\u5316\u63d0\u4f9b\u4e86\u6f5c\u5728\u6b65\u9aa4\u3002"}}
{"id": "2510.13580", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13580", "abs": "https://arxiv.org/abs/2510.13580", "authors": ["Daniil Gurgurov", "Josef van Genabith", "Simon Ostermann"], "title": "Sparse Subnetwork Enhancement for Underrepresented Languages in Large Language Models", "comment": "preprint", "summary": "Large language models exhibit uneven performance across languages, with\nsubstantial gaps between high- and low-resource languages. We present a\nframework for enhancing monolingual capabilities of LLMs in underrepresented\nlanguages while preserving their general-purpose performance through targeted\nfine-tuning of language-specific subnetworks. Our approach identifies\nlanguage-specific neurons using Language Activation Probability Entropy and\nfine-tunes only the weights associated with these neurons, a dedicated\nsubnetwork, on target-language data. Experiments on Llama-3.1-8B and\nMistral-Nemo-12B across 12 mid- and low-resource languages demonstrate that our\nmethod consistently outperforms full fine-tuning, FFN-only fine-tuning, LoRA\nadaptation, and random subset fine-tuning baselines while efficiently updating\nonly up to 1% of model parameters. Beyond performance improvements, we observe\nenhanced favorable training dynamics, cross-lingual representational alignment,\nand systematic weight update changes. To facilitate future research, we release\nlanguage-specific neuron identifications for over 100 languages as well as our\nadaptation pipeline, offering a cost-effective pathway for adapting\nstate-of-the-art models to underrepresented languages.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u8bc6\u522b\u8bed\u8a00\u7279\u5b9a\u5b50\u7f51\u7edc\u8fdb\u884c\u9488\u5bf9\u6027\u5fae\u8c03\u7684\u65b9\u6cd5\uff0c\u5728\u4ec5\u66f4\u65b0\u6700\u591a1%\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u63d0\u5347LLM\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u5355\u8bed\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u8a00\u95f4\u8868\u73b0\u4e0d\u5747\uff0c\u9ad8\u8d44\u6e90\u8bed\u8a00\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u589e\u5f3aLLM\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u8bed\u8a00\u6fc0\u6d3b\u6982\u7387\u71b5\u8bc6\u522b\u8bed\u8a00\u7279\u5b9a\u795e\u7ecf\u5143\uff0c\u4ec5\u5fae\u8c03\u4e0e\u8fd9\u4e9b\u795e\u7ecf\u5143\u76f8\u5173\u7684\u6743\u91cd\uff08\u4e13\u7528\u5b50\u7f51\u7edc\uff09\u5728\u76ee\u6807\u8bed\u8a00\u6570\u636e\u4e0a\u3002", "result": "\u572812\u79cd\u4e2d\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4ec5\u66f4\u65b0\u6700\u591a1%\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u6301\u7eed\u4f18\u4e8e\u5168\u5fae\u8c03\u3001\u4ec5FFN\u5fae\u8c03\u3001LoRA\u9002\u914d\u548c\u968f\u673a\u5b50\u96c6\u5fae\u8c03\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5c06\u6700\u5148\u8fdb\u6a21\u578b\u9002\u914d\u5230\u4f4e\u8d44\u6e90\u8bed\u8a00\u63d0\u4f9b\u4e86\u4e00\u6761\u6210\u672c\u6548\u76ca\u9ad8\u7684\u9014\u5f84\uff0c\u540c\u65f6\u89c2\u5bdf\u5230\u6539\u8fdb\u7684\u8bad\u7ec3\u52a8\u6001\u3001\u8de8\u8bed\u8a00\u8868\u793a\u5bf9\u9f50\u548c\u7cfb\u7edf\u6027\u7684\u6743\u91cd\u66f4\u65b0\u53d8\u5316\u3002"}}
{"id": "2510.13586", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13586", "abs": "https://arxiv.org/abs/2510.13586", "authors": ["Pasin Buakhaw", "Kun Kerdthaisong", "Phuree Phenhiran", "Pitikorn Khlaisamniang", "Supasate Vorathammathorn", "Piyalitt Ittichaiwong", "Nutchanon Yongsatianchot"], "title": "Deflanderization for Game Dialogue: Balancing Character Authenticity with Task Execution in LLM-based NPCs", "comment": null, "summary": "The emergence of large language models (LLMs) has opened new opportunities\nfor cre- ating dynamic non-player characters (NPCs) in gaming environments,\nenabling both func- tional task execution and persona-consistent dialogue\ngeneration. In this paper, we (Tu_Character_lab) report our participation in\nthe Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025 Round 2, which\neval- uates agents across three tracks: task-oriented dialogue, context-aware\ndialogue, and their integration. Our approach combines two complementary\nstrategies: (i) lightweight prompting techniques in the API track, including a\nDeflanderization prompting method to suppress excessive role-play and improve\ntask fidelity, and (ii) fine-tuned large models in the GPU track, leveraging\nQwen3-14B with supervisedfinetuning (SFT) and Low-Rank Adaptation(LoRA). Our\nbest submissions ranked 2nd on Task 1, 2nd on Task 3 (API track), and 4th on\nTask 3 (GPU track).", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u5728CPDC 2025 Round 2\u7ade\u8d5b\u4e2d\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u63d0\u793a\u6280\u672f\u548c\u5fae\u8c03\u5927\u6a21\u578b\u7684\u65b9\u6cd5\u6765\u521b\u5efa\u52a8\u6001NPC\uff0c\u5728\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u6210\u7ee9\u3002", "motivation": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u521b\u5efa\u6e38\u620f\u4e2d\u7684\u52a8\u6001\u975e\u73a9\u5bb6\u89d2\u8272\uff0c\u65e2\u80fd\u6267\u884c\u529f\u80fd\u6027\u4efb\u52a1\uff0c\u53c8\u80fd\u751f\u6210\u7b26\u5408\u89d2\u8272\u8bbe\u5b9a\u7684\u5bf9\u8bdd\uff0c\u4e3a\u6e38\u620f\u73af\u5883\u5e26\u6765\u65b0\u7684\u53ef\u80fd\u6027\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u4e92\u8865\u7b56\u7565\uff1a(1) API\u8d5b\u9053\u4f7f\u7528\u8f7b\u91cf\u7ea7\u63d0\u793a\u6280\u672f\uff0c\u5305\u62ecDeflanderization\u63d0\u793a\u65b9\u6cd5\u6765\u6291\u5236\u8fc7\u5ea6\u89d2\u8272\u626e\u6f14\u5e76\u63d0\u9ad8\u4efb\u52a1\u4fdd\u771f\u5ea6\uff1b(2) GPU\u8d5b\u9053\u4f7f\u7528\u5fae\u8c03\u7684\u5927\u6a21\u578b\uff0c\u57fa\u4e8eQwen3-14B\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u548c\u4f4e\u79e9\u9002\u914d\u3002", "result": "\u5728\u7ade\u8d5b\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u6210\u7ee9\uff1a\u4efb\u52a11\u6392\u540d\u7b2c2\uff0c\u4efb\u52a13\uff08API\u8d5b\u9053\uff09\u6392\u540d\u7b2c2\uff0c\u4efb\u52a13\uff08GPU\u8d5b\u9053\uff09\u6392\u540d\u7b2c4\u3002", "conclusion": "\u7ed3\u5408\u8f7b\u91cf\u7ea7\u63d0\u793a\u6280\u672f\u548c\u5fae\u8c03\u5927\u6a21\u578b\u7684\u6df7\u5408\u65b9\u6cd5\u5728\u521b\u5efa\u52a8\u6001NPC\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.13598", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13598", "abs": "https://arxiv.org/abs/2510.13598", "authors": ["Krist\u00fdna Onderkov\u00e1", "Ond\u0159ej Pl\u00e1tek", "Zden\u011bk Kasner", "Ond\u0159ej Du\u0161ek"], "title": "FreshTab: Sourcing Fresh Data for Table-to-Text Generation Evaluation", "comment": "To be published in INLG 2025", "summary": "Table-to-text generation (insight generation from tables) is a challenging\ntask that requires precision in analyzing the data. In addition, the evaluation\nof existing benchmarks is affected by contamination of Large Language Model\n(LLM) training data as well as domain imbalance. We introduce FreshTab, an\non-the-fly table-to-text benchmark generation from Wikipedia, to combat the LLM\ndata contamination problem and enable domain-sensitive evaluation. While\nnon-English table-to-text datasets are limited, FreshTab collects datasets in\ndifferent languages on demand (we experiment with German, Russian and French in\naddition to English). We find that insights generated by LLMs from recent\ntables collected by our method appear clearly worse by automatic metrics, but\nthis does not translate into LLM and human evaluations. Domain effects are\nvisible in all evaluations, showing that a~domain-balanced benchmark is more\nchallenging.", "AI": {"tldr": "FreshTab\u662f\u4e00\u4e2a\u52a8\u6001\u751f\u6210\u7684\u8868\u683c\u5230\u6587\u672c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ece\u7ef4\u57fa\u767e\u79d1\u5b9e\u65f6\u6536\u96c6\u6570\u636e\u4ee5\u89e3\u51b3LLM\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\u95ee\u9898\uff0c\u652f\u6301\u591a\u8bed\u8a00\u548c\u9886\u57df\u654f\u611f\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u8868\u683c\u5230\u6587\u672c\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728LLM\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\u548c\u9886\u57df\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u4e14\u975e\u82f1\u8bed\u6570\u636e\u96c6\u6709\u9650\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4ece\u7ef4\u57fa\u767e\u79d1\u52a8\u6001\u6536\u96c6\u6700\u65b0\u8868\u683c\u6570\u636e\uff0c\u652f\u6301\u6309\u9700\u751f\u6210\u82f1\u8bed\u3001\u5fb7\u8bed\u3001\u4fc4\u8bed\u548c\u6cd5\u8bed\u7b49\u591a\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u9886\u57df\u5e73\u8861\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u81ea\u52a8\u6307\u6807\u663e\u793aLLM\u4ece\u65b0\u8868\u683c\u751f\u6210\u7684\u89c1\u89e3\u660e\u663e\u8f83\u5dee\uff0c\u4f46LLM\u548c\u4eba\u5de5\u8bc4\u4f30\u672a\u4f53\u73b0\u6b64\u5dee\u5f02\uff1b\u9886\u57df\u6548\u5e94\u5728\u6240\u6709\u8bc4\u4f30\u4e2d\u90fd\u5f88\u660e\u663e\uff0c\u8868\u660e\u9886\u57df\u5e73\u8861\u7684\u57fa\u51c6\u66f4\u5177\u6311\u6218\u6027\u3002", "conclusion": "FreshTab\u901a\u8fc7\u5b9e\u65f6\u6570\u636e\u6536\u96c6\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u6c61\u67d3\u95ee\u9898\uff0c\u9886\u57df\u5e73\u8861\u7684\u57fa\u51c6\u6d4b\u8bd5\u80fd\u66f4\u597d\u5730\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u591a\u8bed\u8a00\u652f\u6301\u6269\u5c55\u4e86\u8bc4\u4f30\u8303\u56f4\u3002"}}
{"id": "2510.13602", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13602", "abs": "https://arxiv.org/abs/2510.13602", "authors": ["Yuxiang Huang", "Chaojun Xiao", "Xu Han", "Zhiyuan Liu"], "title": "NOSA: Native and Offloadable Sparse Attention", "comment": "Preprint", "summary": "Trainable sparse attention has emerged as a promising solution to address the\ndecoding efficiency bottleneck of LLMs in long-context processing,\nsignificantly saving memory accesses while minimally impacting task\nperformance. However, existing sparse attention methods leave a crucial\nlimitation unresolved: the size of the key-value (KV) cache remains unreduced,\nwhich constrains on-GPU batch sizes and throttles decoding throughput,\nespecially in large-scale batched inference. In this paper, we show that\ntrainable sparse attention naturally exhibits strong locality in token\nselection across adjacent decoding steps, thereby enabling KV cache offloading\nwithout altering the underlying attention computation. However, the inherent\nlocality remains insufficient to achieve efficient offloading, as the transfer\nof selected KV pairs between the CPU and GPU continues to dominate the overall\ndecoding cost. Building on this insight, we present NOSA, a trainable sparse\nattention framework designed to natively support KV cache offloading. NOSA\nintroduces explicit locality constraints by decomposing token selection into\nquery-aware and query-agnostic components, thereby reducing KV transfers while\npreserving the same attention computation as used during training. We pretrain\na 1B-parameter model with NOSA and conduct extensive benchmarks, showing that\nit preserves near-lossless performance while achieving up to a 2.3x improvement\nin decoding throughput compared with the vanilla trainable sparse attention\nbaseline (InfLLM-V2).", "AI": {"tldr": "NOSA\u662f\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u7a00\u758f\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u663e\u5f0f\u5c40\u90e8\u6027\u7ea6\u675f\u6765\u51cf\u5c11KV\u7f13\u5b58\u4f20\u8f93\uff0c\u5728\u4fdd\u6301\u8bad\u7ec3\u65f6\u6ce8\u610f\u529b\u8ba1\u7b97\u4e0d\u53d8\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u89e3\u7801\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u867d\u7136\u51cf\u5c11\u4e86\u5185\u5b58\u8bbf\u95ee\uff0c\u4f46KV\u7f13\u5b58\u5927\u5c0f\u672a\u51cf\u5c11\uff0c\u9650\u5236\u4e86GPU\u6279\u91cf\u5927\u5c0f\u548c\u89e3\u7801\u541e\u5410\u91cf\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u6279\u91cf\u63a8\u7406\u4e2d\u3002", "method": "NOSA\u901a\u8fc7\u5c06token\u9009\u62e9\u5206\u89e3\u4e3a\u67e5\u8be2\u611f\u77e5\u548c\u67e5\u8be2\u65e0\u5173\u7ec4\u4ef6\uff0c\u5f15\u5165\u663e\u5f0f\u5c40\u90e8\u6027\u7ea6\u675f\uff0c\u51cf\u5c11KV\u4f20\u8f93\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u8bad\u7ec3\u65f6\u76f8\u540c\u7684\u6ce8\u610f\u529b\u8ba1\u7b97\u3002", "result": "\u9884\u8bad\u7ec3\u76841B\u53c2\u6570\u6a21\u578b\u5728\u4fdd\u6301\u8fd1\u4e4e\u65e0\u635f\u6027\u80fd\u7684\u540c\u65f6\uff0c\u76f8\u6bd4\u57fa\u51c6\u65b9\u6cd5(InfLLM-V2)\u5b9e\u73b0\u4e86\u6700\u9ad82.3\u500d\u7684\u89e3\u7801\u541e\u5410\u91cf\u63d0\u5347\u3002", "conclusion": "NOSA\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86KV\u7f13\u5b58\u5378\u8f7d\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u89e3\u7801\u6548\u7387\uff0c\u4e3a\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.13614", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13614", "abs": "https://arxiv.org/abs/2510.13614", "authors": ["Xingyu Tan", "Xiaoyang Wang", "Xiwei Xu", "Xin Yuan", "Liming Zhu", "Wenjie Zhang"], "title": "MemoTime: Memory-Augmented Temporal Knowledge Graph Enhanced Large Language Model Reasoning", "comment": null, "summary": "Large Language Models (LLMs) have achieved impressive reasoning abilities,\nbut struggle with temporal understanding, especially when questions involve\nmultiple entities, compound operators, and evolving event sequences. Temporal\nKnowledge Graphs (TKGs), which capture vast amounts of temporal facts in a\nstructured format, offer a reliable source for temporal reasoning. However,\nexisting TKG-based LLM reasoning methods still struggle with four major\nchallenges: maintaining temporal faithfulness in multi-hop reasoning, achieving\nmulti-entity temporal synchronization, adapting retrieval to diverse temporal\noperators, and reusing prior reasoning experience for stability and efficiency.\nTo address these issues, we propose MemoTime, a memory-augmented temporal\nknowledge graph framework that enhances LLM reasoning through structured\ngrounding, recursive reasoning, and continual experience learning. MemoTime\ndecomposes complex temporal questions into a hierarchical Tree of Time,\nenabling operator-aware reasoning that enforces monotonic timestamps and\nco-constrains multiple entities under unified temporal bounds. A dynamic\nevidence retrieval layer adaptively selects operator-specific retrieval\nstrategies, while a self-evolving experience memory stores verified reasoning\ntraces, toolkit decisions, and sub-question embeddings for cross-type reuse.\nComprehensive experiments on multiple temporal QA benchmarks show that MemoTime\nachieves overall state-of-the-art results, outperforming the strong baseline by\nup to 24.0%. Furthermore, MemoTime enables smaller models (e.g., Qwen3-4B) to\nachieve reasoning performance comparable to that of GPT-4-Turbo.", "AI": {"tldr": "MemoTime\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bb0\u5fc6\u589e\u5f3a\u7684\u65f6\u95f4\u77e5\u8bc6\u56fe\u8c31\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u57fa\u7840\u3001\u9012\u5f52\u63a8\u7406\u548c\u6301\u7eed\u7ecf\u9a8c\u5b66\u4e60\u6765\u589e\u5f3aLLM\u7684\u65f6\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u65f6\u95f4QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u7406\u89e3\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u6d89\u53ca\u591a\u4e2a\u5b9e\u4f53\u3001\u590d\u5408\u64cd\u4f5c\u7b26\u548c\u6f14\u5316\u4e8b\u4ef6\u5e8f\u5217\u7684\u95ee\u9898\u65f6\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u65f6\u95f4\u77e5\u8bc6\u56fe\u8c31\u7684LLM\u63a8\u7406\u65b9\u6cd5\u9762\u4e34\u56db\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u591a\u8df3\u63a8\u7406\u4e2d\u7684\u65f6\u95f4\u5fe0\u5b9e\u6027\u3001\u591a\u5b9e\u4f53\u65f6\u95f4\u540c\u6b65\u3001\u9002\u5e94\u4e0d\u540c\u65f6\u95f4\u64cd\u4f5c\u7b26\u7684\u68c0\u7d22\uff0c\u4ee5\u53ca\u91cd\u7528\u5148\u524d\u63a8\u7406\u7ecf\u9a8c\u4ee5\u63d0\u9ad8\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002", "method": "\u63d0\u51faMemoTime\u6846\u67b6\uff0c\u5c06\u590d\u6742\u65f6\u95f4\u95ee\u9898\u5206\u89e3\u4e3a\u65f6\u95f4\u5c42\u6b21\u6811\uff0c\u5b9e\u73b0\u64cd\u4f5c\u7b26\u611f\u77e5\u63a8\u7406\uff0c\u5f3a\u5236\u5355\u8c03\u65f6\u95f4\u6233\u5e76\u5728\u7edf\u4e00\u65f6\u95f4\u8fb9\u754c\u4e0b\u5171\u540c\u7ea6\u675f\u591a\u4e2a\u5b9e\u4f53\u3002\u5305\u542b\u52a8\u6001\u8bc1\u636e\u68c0\u7d22\u5c42\u81ea\u9002\u5e94\u9009\u62e9\u64cd\u4f5c\u7b26\u7279\u5b9a\u7684\u68c0\u7d22\u7b56\u7565\uff0c\u4ee5\u53ca\u81ea\u6f14\u5316\u7ecf\u9a8c\u8bb0\u5fc6\u5b58\u50a8\u5df2\u9a8c\u8bc1\u7684\u63a8\u7406\u8f68\u8ff9\u3001\u5de5\u5177\u5305\u51b3\u7b56\u548c\u5b50\u95ee\u9898\u5d4c\u5165\u4ee5\u5b9e\u73b0\u8de8\u7c7b\u578b\u91cd\u7528\u3002", "result": "\u5728\u591a\u4e2a\u65f6\u95f4QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMemoTime\u5b9e\u73b0\u4e86\u6574\u4f53\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u6bd4\u5f3a\u57fa\u7ebf\u9ad8\u51fa24.0%\u3002\u6b64\u5916\uff0cMemoTime\u4f7f\u8f83\u5c0f\u6a21\u578b\uff08\u5982Qwen3-4B\uff09\u80fd\u591f\u8fbe\u5230\u4e0eGPT-4-Turbo\u76f8\u5f53\u7684\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "MemoTime\u901a\u8fc7\u7ed3\u6784\u5316\u57fa\u7840\u3001\u9012\u5f52\u63a8\u7406\u548c\u6301\u7eed\u7ecf\u9a8c\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u65f6\u95f4\u63a8\u7406\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u4f7f\u8f83\u5c0f\u6a21\u578b\u80fd\u591f\u8fbe\u5230\u5927\u578b\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2510.13624", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13624", "abs": "https://arxiv.org/abs/2510.13624", "authors": ["Stefan Lenz", "Lakisha Ortiz Rosario", "Georg Vollmar", "Arsenij Ustjanzew", "Fatma Alickovic", "Thomas Kindler", "Torsten Panholzer"], "title": "Unlocking Public Catalogues: Instruction-Tuning LLMs for ICD Coding of German Tumor Diagnoses", "comment": "19 pages, 4 figures", "summary": "Accurate coding of tumor diagnoses with ICD-10-GM and ICD-O-3 is essential\nfor structured cancer documentation in Germany. Smaller open-weight LLMs are\nappealing for privacy-preserving automation but often struggle with coding\naccuracy in German-language contexts. This study investigates whether\ninstruction-based fine-tuning on public datasets improves the coding accuracy\nof open-weight LLMs for German tumor diagnosis texts. The evaluation uses coded\ndiagnoses from the local tumor documentation system as test data. In a\nsystematic data quality assessment, the upper limit for ICD-10 coding\nperformance was estimated at 60-79% for exact and 81-94% for partial\n(three-character codes only) derivation. As training data, over 500,000\nquestion-answer pairs were created based on the ICD-10-GM, ICD-O-3, and OPS\ncatalogues. Eight open-weight models from the Qwen, Llama, and Mistral families\n(7-70 B parameters) were fine-tuned. ICD-10-GM accuracy rose from 1.4-24% to\n41-58%, and partial accuracy from 31-74% to 73-83%. The accuracy of ICD-O-3\ntopography coding also improved but started and remained considerably lower\nwith an exact accuracy of 22-40% and a partial accuracy of 56-67% after\nfine-tuning. Malformed code outputs dropped to 0% for all models.\nTumor-diagnosis recognition reached 99%. Accuracy correlated positively with\nmodel size, but gaps between small and large models narrowed after fine-tuning.\nThe reasoning mode in Qwen3 generally yielded a lower performance than\nfine-tuning and was over 100 times slower. Our findings highlight the potential\nof leveraging public catalogues to build instruction datasets that improve LLMs\nin medical documentation tasks. The complete training dataset and the\nbest-performing checkpoints of the fine-tuned models are available from\nhttps://huggingface.co/datasets/stefan-m-lenz/ICDOPS-QA-2024.", "AI": {"tldr": "\u6307\u4ee4\u5fae\u8c03\u663e\u8457\u63d0\u5347\u4e86\u5f00\u6e90LLM\u5728\u5fb7\u8bed\u80bf\u7624\u8bca\u65adICD\u7f16\u7801\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\uff0cICD-10-GM\u51c6\u786e\u7387\u4ece1.4-24%\u63d0\u5347\u81f341-58%\uff0c\u6a21\u578b\u5927\u5c0f\u4e0e\u6027\u80fd\u6b63\u76f8\u5173\u4f46\u5dee\u8ddd\u7f29\u5c0f\uff0c\u63a8\u7406\u6a21\u5f0f\u6027\u80fd\u8f83\u4f4e\u4e14\u901f\u5ea6\u6162100\u500d\u3002", "motivation": "\u5fb7\u56fd\u80bf\u7624\u8bca\u65ad\u7684ICD-10-GM\u548cICD-O-3\u51c6\u786e\u7f16\u7801\u5bf9\u7ed3\u6784\u5316\u764c\u75c7\u6587\u6863\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5c0f\u578b\u5f00\u6e90LLM\u5728\u5fb7\u8bed\u8bed\u5883\u4e0b\u7f16\u7801\u51c6\u786e\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u7814\u7a76\u6307\u4ee4\u5fae\u8c03\u662f\u5426\u80fd\u63d0\u5347\u5176\u6027\u80fd\u3002", "method": "\u57fa\u4e8eICD-10-GM\u3001ICD-O-3\u548cOPS\u76ee\u5f55\u521b\u5efa\u4e86\u8d85\u8fc750\u4e07\u4e2a\u95ee\u7b54\u5bf9\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\uff0c\u5bf9Qwen\u3001Llama\u548cMistral\u5bb6\u65cf\u76848\u4e2a\u5f00\u6e90\u6a21\u578b\uff087-70B\u53c2\u6570\uff09\u8fdb\u884c\u6307\u4ee4\u5fae\u8c03\uff0c\u4f7f\u7528\u672c\u5730\u80bf\u7624\u6587\u6863\u7cfb\u7edf\u7684\u7f16\u7801\u8bca\u65ad\u4f5c\u4e3a\u6d4b\u8bd5\u6570\u636e\u3002", "result": "ICD-10-GM\u51c6\u786e\u7387\u4ece1.4-24%\u63d0\u5347\u81f341-58%\uff0c\u90e8\u5206\u51c6\u786e\u7387\u4ece31-74%\u63d0\u5347\u81f373-83%\uff1bICD-O-3\u5730\u5f62\u7f16\u7801\u51c6\u786e\u7387\u4ece\u8f83\u4f4e\u6c34\u5e73\u63d0\u5347\u81f322-40%\uff08\u7cbe\u786e\uff09\u548c56-67%\uff08\u90e8\u5206\uff09\uff1b\u7578\u5f62\u4ee3\u7801\u8f93\u51fa\u964d\u81f30%\uff0c\u80bf\u7624\u8bca\u65ad\u8bc6\u522b\u7387\u8fbe99%\u3002", "conclusion": "\u5229\u7528\u516c\u5171\u76ee\u5f55\u6784\u5efa\u6307\u4ee4\u6570\u636e\u96c6\u53ef\u4ee5\u6709\u6548\u63d0\u5347LLM\u5728\u533b\u7597\u6587\u6863\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5f00\u6e90LLM\u901a\u8fc7\u6307\u4ee4\u5fae\u8c03\u5728\u5fb7\u8bed\u533b\u7597\u7f16\u7801\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.13632", "categories": ["cs.CL", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.13632", "abs": "https://arxiv.org/abs/2510.13632", "authors": ["Santiago Cuervo", "Skyler Seto", "Maureen de Seyssel", "Richard He Bai", "Zijin Gu", "Tatiana Likhomanenko", "Navdeep Jaitly", "Zakaria Aldeneh"], "title": "Closing the Gap Between Text and Speech Understanding in LLMs", "comment": null, "summary": "Large Language Models (LLMs) can be adapted to extend their text capabilities\nto speech inputs. However, these speech-adapted LLMs consistently underperform\ntheir text-based counterparts--and even cascaded pipelines--on language\nunderstanding tasks. We term this shortfall the text-speech understanding gap:\nthe performance drop observed when a speech-adapted LLM processes spoken inputs\nrelative to when the original text-based LLM processes the equivalent text.\nRecent approaches to narrowing this gap either rely on large-scale speech\nsynthesis of text corpora, which is costly and heavily dependent on synthetic\ndata, or on large-scale proprietary speech datasets, which are not\nreproducible. As a result, there remains a need for more data-efficient\nalternatives for closing the text-speech understanding gap. In this work, we\nanalyze the gap as driven by two factors: (i) forgetting of text capabilities\nduring adaptation, and (ii) cross-modal misalignment between speech and text.\nBased on this analysis, we introduce SALAD--Sample-efficient Alignment with\nLearning through Active selection and cross-modal Distillation--which combines\ncross-modal distillation with targeted synthetic data to improve alignment\nwhile mitigating forgetting. Applied to 3B and 7B LLMs, SALAD achieves\ncompetitive performance with a strong open-weight model across broad-domain\nbenchmarks in knowledge, language understanding, and reasoning, while training\non over an order of magnitude less speech data from public corpora.", "AI": {"tldr": "SALAD\u65b9\u6cd5\u901a\u8fc7\u8de8\u6a21\u6001\u84b8\u998f\u548c\u9488\u5bf9\u6027\u5408\u6210\u6570\u636e\uff0c\u5728\u51cf\u5c11\u8bed\u97f3\u6570\u636e\u4f7f\u7528\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u7f29\u5c0f\u6587\u672c-\u8bed\u97f3\u7406\u89e3\u5dee\u8ddd\uff0c\u5728\u77e5\u8bc6\u3001\u8bed\u8a00\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e0e\u5f3a\u57fa\u7ebf\u6a21\u578b\u7ade\u4e89\u7684\u6027\u80fd\u3002", "motivation": "\u8bed\u97f3\u9002\u914d\u7684LLMs\u5728\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u4e0a\u6301\u7eed\u8868\u73b0\u4e0d\u5982\u57fa\u4e8e\u6587\u672c\u7684\u5bf9\u5e94\u6a21\u578b\uff0c\u5b58\u5728\u6587\u672c-\u8bed\u97f3\u7406\u89e3\u5dee\u8ddd\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6602\u8d35\u7684\u8bed\u97f3\u5408\u6210\uff0c\u8981\u4e48\u4f9d\u8d56\u5927\u89c4\u6a21\u4e13\u6709\u6570\u636e\u96c6\uff0c\u9700\u8981\u66f4\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "SALAD\u65b9\u6cd5\u7ed3\u5408\u8de8\u6a21\u6001\u84b8\u998f\u4e0e\u9488\u5bf9\u6027\u5408\u6210\u6570\u636e\uff0c\u901a\u8fc7\u4e3b\u52a8\u9009\u62e9\u548c\u5b66\u4e60\u6765\u6539\u5584\u5bf9\u9f50\u540c\u65f6\u51cf\u8f7b\u9057\u5fd8\u3002\u8be5\u65b9\u6cd5\u8bc6\u522b\u6587\u672c\u80fd\u529b\u9057\u5fd8\u548c\u8de8\u6a21\u6001\u4e0d\u5bf9\u9f50\u662f\u5bfc\u81f4\u5dee\u8ddd\u7684\u4e24\u4e2a\u4e3b\u8981\u56e0\u7d20\u3002", "result": "\u57283B\u548c7B LLMs\u4e0a\u5e94\u7528SALAD\uff0c\u5728\u5e7f\u6cdb\u9886\u57df\u7684\u77e5\u8bc6\u3001\u8bed\u8a00\u7406\u89e3\u548c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528\u6bd4\u516c\u5171\u8bed\u6599\u5e93\u5c11\u4e00\u4e2a\u6570\u91cf\u7ea7\u7684\u8bed\u97f3\u6570\u636e\uff0c\u8fbe\u5230\u4e86\u4e0e\u5f3a\u5f00\u6e90\u6743\u91cd\u6a21\u578b\u7ade\u4e89\u7684\u6027\u80fd\u3002", "conclusion": "SALAD\u63d0\u4f9b\u4e86\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u7f29\u5c0f\u6587\u672c-\u8bed\u97f3\u7406\u89e3\u5dee\u8ddd\uff0c\u901a\u8fc7\u540c\u65f6\u89e3\u51b3\u6587\u672c\u80fd\u529b\u9057\u5fd8\u548c\u8de8\u6a21\u6001\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u5728\u51cf\u5c11\u6570\u636e\u4f9d\u8d56\u7684\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u6027\u80fd\u3002"}}
{"id": "2510.13681", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13681", "abs": "https://arxiv.org/abs/2510.13681", "authors": ["Matthieu Dubois", "Fran\u00e7ois Yvon", "Pablo Piantanida"], "title": "How Sampling Affects the Detectability of Machine-written texts: A Comprehensive Study", "comment": "EMNLP 2025 Findings", "summary": "As texts generated by Large Language Models (LLMs) are ever more common and\noften indistinguishable from human-written content, research on automatic text\ndetection has attracted growing attention. Many recent detectors report\nnear-perfect accuracy, often boasting AUROC scores above 99\\%. However, these\nclaims typically assume fixed generation settings, leaving open the question of\nhow robust such systems are to changes in decoding strategies. In this work, we\nsystematically examine how sampling-based decoding impacts detectability, with\na focus on how subtle variations in a model's (sub)word-level distribution\naffect detection performance. We find that even minor adjustments to decoding\nparameters - such as temperature, top-p, or nucleus sampling - can severely\nimpair detector accuracy, with AUROC dropping from near-perfect levels to 1\\%\nin some settings. Our findings expose critical blind spots in current detection\nmethods and emphasize the need for more comprehensive evaluation protocols. To\nfacilitate future research, we release a large-scale dataset encompassing 37\ndecoding configurations, along with our code and evaluation framework\nhttps://github.com/BaggerOfWords/Sampling-and-Detection", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u662f\u5fae\u5c0f\u7684\u89e3\u7801\u53c2\u6570\u8c03\u6574\uff08\u5982\u6e29\u5ea6\u3001top-p\u7b49\uff09\u4e5f\u4f1a\u4e25\u91cd\u635f\u5bb3LLM\u6587\u672c\u68c0\u6d4b\u5668\u7684\u51c6\u786e\u6027\uff0cAUROC\u4ece\u63a5\u8fd1\u5b8c\u7f8e\u964d\u81f31%\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u68c0\u6d4b\u65b9\u6cd5\u7684\u76f2\u70b9\u3002", "motivation": "\u968f\u7740LLM\u751f\u6210\u6587\u672c\u65e5\u76ca\u666e\u904d\u4e14\u96be\u4ee5\u4e0e\u4eba\u7c7b\u5199\u4f5c\u533a\u5206\uff0c\u81ea\u52a8\u6587\u672c\u68c0\u6d4b\u7814\u7a76\u53d7\u5230\u5173\u6ce8\u3002\u8bb8\u591a\u68c0\u6d4b\u5668\u58f0\u79f0\u63a5\u8fd1\u5b8c\u7f8e\u7684\u51c6\u786e\u7387\uff0c\u4f46\u8fd9\u4e9b\u8bc4\u4f30\u901a\u5e38\u57fa\u4e8e\u56fa\u5b9a\u7684\u751f\u6210\u8bbe\u7f6e\uff0c\u672a\u8003\u8651\u89e3\u7801\u7b56\u7565\u53d8\u5316\u5bf9\u68c0\u6d4b\u9c81\u68d2\u6027\u7684\u5f71\u54cd\u3002", "method": "\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u4e86\u57fa\u4e8e\u91c7\u6837\u7684\u89e3\u7801\u5bf9\u53ef\u68c0\u6d4b\u6027\u7684\u5f71\u54cd\uff0c\u91cd\u70b9\u5173\u6ce8\u6a21\u578b\uff08\u5b50\uff09\u8bcd\u7ea7\u5206\u5e03\u7684\u7ec6\u5fae\u53d8\u5316\u5982\u4f55\u5f71\u54cd\u68c0\u6d4b\u6027\u80fd\u3002\u6784\u5efa\u4e86\u5305\u542b37\u79cd\u89e3\u7801\u914d\u7f6e\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "result": "\u53d1\u73b0\u5373\u4f7f\u5bf9\u89e3\u7801\u53c2\u6570\u8fdb\u884c\u5fae\u5c0f\u8c03\u6574\uff08\u5982\u6e29\u5ea6\u3001top-p\u6216\u6838\u91c7\u6837\uff09\uff0c\u4e5f\u4f1a\u4e25\u91cd\u635f\u5bb3\u68c0\u6d4b\u5668\u51c6\u786e\u6027\uff0c\u5728\u67d0\u4e9b\u8bbe\u7f6e\u4e0bAUROC\u4ece\u63a5\u8fd1\u5b8c\u7f8e\u6c34\u5e73\u964d\u81f31%\u3002", "conclusion": "\u5f53\u524d\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u5173\u952e\u76f2\u70b9\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u534f\u8bae\u3002\u4e3a\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\uff0c\u53d1\u5e03\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3001\u4ee3\u7801\u548c\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2510.13721", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.13721", "abs": "https://arxiv.org/abs/2510.13721", "authors": ["Run Luo", "Xiaobo Xia", "Lu Wang", "Longze Chen", "Renke Shan", "Jing Luo", "Min Yang", "Tat-Seng Chua"], "title": "NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching", "comment": null, "summary": "Next-generation multimodal foundation models capable of any-to-any\ncross-modal generation and multi-turn interaction will serve as core components\nof artificial general intelligence systems, playing a pivotal role in\nhuman-machine interaction. However, most existing multimodal models remain\nconstrained by autoregressive architectures, whose inherent limitations prevent\na balanced integration of understanding and generation capabilities. Although\nhybrid and decoupling strategies have been explored to address these tasks\nwithin unified frameworks separately, their redundant, non-integrated designs\nlimit their applicability to broader scenarios, such as cross-modal\nretrieval.In this work, we introduce NExT-OMNI, an open-source omnimodal\nfoundation model that achieves unified modeling through discrete flow\nparadigms. By leveraging metric-induced probability paths and kinetic optimal\nvelocities, NExT-OMNI natively supports any-to-any understanding and generation\nwith enhanced response efficiency, while enabling broader application scenarios\nthrough concise unified representations rather than task-decoupled designs.\nTrained on large-scale interleaved text, image, video, and audio data,\nNExT-OMNI delivers competitive performance on multimodal generation and\nunderstanding benchmarks, while outperforming prior unified models in\nmulti-turn multimodal interaction and cross-modal retrieval, highlighting its\narchitectural advantages as a next-generation multimodal foundation model. To\nadvance further research, we release training details, data protocols, and\nopen-source both the code and model checkpoints.", "AI": {"tldr": "NExT-OMNI\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5168\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u79bb\u6563\u6d41\u8303\u5f0f\u5b9e\u73b0\u7edf\u4e00\u5efa\u6a21\uff0c\u652f\u6301\u4efb\u610f\u6a21\u6001\u95f4\u7684\u7406\u89e3\u548c\u751f\u6210\uff0c\u5728\u591a\u8f6e\u591a\u6a21\u6001\u4ea4\u4e92\u548c\u8de8\u6a21\u6001\u68c0\u7d22\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7edf\u4e00\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5927\u591a\u6570\u591a\u6a21\u6001\u6a21\u578b\u53d7\u9650\u4e8e\u81ea\u56de\u5f52\u67b6\u6784\uff0c\u65e0\u6cd5\u5e73\u8861\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\uff0c\u4e14\u6df7\u5408\u548c\u89e3\u8026\u7b56\u7565\u7684\u8bbe\u8ba1\u5197\u4f59\uff0c\u9650\u5236\u4e86\u5728\u66f4\u5e7f\u6cdb\u573a\u666f\uff08\u5982\u8de8\u6a21\u6001\u68c0\u7d22\uff09\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5229\u7528\u5ea6\u91cf\u8bf1\u5bfc\u6982\u7387\u8def\u5f84\u548c\u52a8\u529b\u5b66\u6700\u4f18\u901f\u5ea6\u7684\u79bb\u6563\u6d41\u8303\u5f0f\uff0c\u901a\u8fc7\u7b80\u6d01\u7684\u7edf\u4e00\u8868\u793a\u800c\u975e\u4efb\u52a1\u89e3\u8026\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4efb\u610f\u6a21\u6001\u95f4\u7684\u7406\u89e3\u548c\u751f\u6210\u3002", "result": "\u5728\u5927\u89c4\u6a21\u4ea4\u9519\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u548c\u97f3\u9891\u6570\u636e\u4e0a\u8bad\u7ec3\uff0cNExT-OMNI\u5728\u591a\u6a21\u6001\u751f\u6210\u548c\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u591a\u8f6e\u591a\u6a21\u6001\u4ea4\u4e92\u548c\u8de8\u6a21\u6001\u68c0\u7d22\u65b9\u9762\u4f18\u4e8e\u5148\u524d\u7684\u7edf\u4e00\u6a21\u578b\u3002", "conclusion": "NExT-OMNI\u4f5c\u4e3a\u4e0b\u4e00\u4ee3\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u79bb\u6563\u6d41\u8303\u5f0f\u5b9e\u73b0\u4e86\u7edf\u4e00\u5efa\u6a21\uff0c\u5728\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u5177\u6709\u67b6\u6784\u4f18\u52bf\uff0c\u5e76\u5f00\u6e90\u4e86\u8bad\u7ec3\u7ec6\u8282\u3001\u6570\u636e\u534f\u8bae\u3001\u4ee3\u7801\u548c\u6a21\u578b\u68c0\u67e5\u70b9\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2510.13734", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13734", "abs": "https://arxiv.org/abs/2510.13734", "authors": ["Xiuyuan Chen", "Tao Sun", "Dexin Su", "Ailing Yu", "Junwei Liu", "Zhe Chen", "Gangzeng Jin", "Xin Wang", "Jingnan Liu", "Hansong Xiao", "Hualei Zhou", "Dongjie Tao", "Chunxiao Guo", "Minghui Yang", "Yuan Xia", "Jing Zhao", "Qianrui Fan", "Yanyun Wang", "Shuai Zhen", "Kezhong Chen", "Jun Wang", "Zewen Sun", "Heng Zhao", "Tian Guan", "Shaodong Wang", "Geyun Chang", "Jiaming Deng", "Hongchengcheng Chen", "Kexin Feng", "Ruzhen Li", "Jiayi Geng", "Changtai Zhao", "Jun Wang", "Guihu Lin", "Peihao Li", "Liqi Liu", "Peng Wei", "Jian Wang", "Jinjie Gu", "Ping Wang", "Fan Yang"], "title": "GAPS: A Clinically Grounded, Automated Benchmark for Evaluating AI Clinicians", "comment": null, "summary": "Current benchmarks for AI clinician systems, often based on multiple-choice\nexams or manual rubrics, fail to capture the depth, robustness, and safety\nrequired for real-world clinical practice. To address this, we introduce the\nGAPS framework, a multidimensional paradigm for evaluating \\textbf{G}rounding\n(cognitive depth), \\textbf{A}dequacy (answer completeness),\n\\textbf{P}erturbation (robustness), and \\textbf{S}afety. Critically, we\ndeveloped a fully automated, guideline-anchored pipeline to construct a\nGAPS-aligned benchmark end-to-end, overcoming the scalability and subjectivity\nlimitations of prior work. Our pipeline assembles an evidence neighborhood,\ncreates dual graph and tree representations, and automatically generates\nquestions across G-levels. Rubrics are synthesized by a DeepResearch agent that\nmimics GRADE-consistent, PICO-driven evidence review in a ReAct loop. Scoring\nis performed by an ensemble of large language model (LLM) judges. Validation\nconfirmed our automated questions are high-quality and align with clinician\njudgment. Evaluating state-of-the-art models on the benchmark revealed key\nfailure modes: performance degrades sharply with increased reasoning depth\n(G-axis), models struggle with answer completeness (A-axis), and they are\nhighly vulnerable to adversarial perturbations (P-axis) as well as certain\nsafety issues (S-axis). This automated, clinically-grounded approach provides a\nreproducible and scalable method for rigorously evaluating AI clinician systems\nand guiding their development toward safer, more reliable clinical practice.", "AI": {"tldr": "\u63d0\u51fa\u4e86GAPS\u6846\u67b6\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30AI\u4e34\u5e8a\u533b\u751f\u7cfb\u7edf\u7684\u591a\u7ef4\u8bc4\u4f30\u8303\u5f0f\uff0c\u5305\u62ec\u8ba4\u77e5\u6df1\u5ea6(G)\u3001\u7b54\u6848\u5b8c\u6574\u6027(A)\u3001\u9c81\u68d2\u6027(P)\u548c\u5b89\u5168\u6027(S)\u3002\u5f00\u53d1\u4e86\u5168\u81ea\u52a8\u7684\u6307\u5357\u951a\u5b9a\u6d41\u6c34\u7ebf\u6765\u6784\u5efaGAPS\u5bf9\u9f50\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u89e3\u51b3\u4e86\u4e4b\u524d\u5de5\u4f5c\u7684\u53ef\u6269\u5c55\u6027\u548c\u4e3b\u89c2\u6027\u9650\u5236\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u591a\u9879\u9009\u62e9\u9898\u6216\u624b\u52a8\u8bc4\u5206\u7684AI\u4e34\u5e8a\u533b\u751f\u7cfb\u7edf\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u4e16\u754c\u4e34\u5e8a\u5b9e\u8df5\u6240\u9700\u7684\u6df1\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u5168\u81ea\u52a8\u7684\u6307\u5357\u951a\u5b9a\u6d41\u6c34\u7ebf\uff0c\u5305\u62ec\u6784\u5efa\u8bc1\u636e\u90bb\u57df\u3001\u521b\u5efa\u53cc\u56fe\u548c\u6811\u8868\u793a\u3001\u81ea\u52a8\u751f\u6210\u8de8G\u7ea7\u522b\u7684\u95ee\u9898\u3002\u4f7f\u7528DeepResearch\u4ee3\u7406\u5408\u6210\u8bc4\u5206\u6807\u51c6\uff0c\u6a21\u62dfGRADE\u4e00\u81f4\u3001PICO\u9a71\u52a8\u7684\u8bc1\u636e\u5ba1\u67e5\u3002\u901a\u8fc7LLM\u8bc4\u59d4\u96c6\u5408\u8fdb\u884c\u8bc4\u5206\u3002", "result": "\u9a8c\u8bc1\u786e\u8ba4\u81ea\u52a8\u751f\u6210\u7684\u95ee\u9898\u8d28\u91cf\u9ad8\u4e14\u4e0e\u4e34\u5e8a\u533b\u751f\u5224\u65ad\u4e00\u81f4\u3002\u8bc4\u4f30\u6700\u5148\u8fdb\u6a21\u578b\u53d1\u73b0\u5173\u952e\u5931\u8d25\u6a21\u5f0f\uff1a\u6027\u80fd\u968f\u63a8\u7406\u6df1\u5ea6\u589e\u52a0\u800c\u6025\u5267\u4e0b\u964d\uff0c\u6a21\u578b\u5728\u7b54\u6848\u5b8c\u6574\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5bf9\u5bf9\u6297\u6027\u6270\u52a8\u548c\u67d0\u4e9b\u5b89\u5168\u95ee\u9898\u9ad8\u5ea6\u8106\u5f31\u3002", "conclusion": "\u8fd9\u79cd\u81ea\u52a8\u5316\u7684\u3001\u4e34\u5e8a\u57fa\u7840\u7684\u65b9\u6cd5\u4e3a\u4e25\u683c\u8bc4\u4f30AI\u4e34\u5e8a\u533b\u751f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u548c\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u6307\u5bfc\u5176\u5411\u66f4\u5b89\u5168\u3001\u66f4\u53ef\u9760\u7684\u4e34\u5e8a\u5b9e\u8df5\u53d1\u5c55\u3002"}}
{"id": "2510.13749", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13749", "abs": "https://arxiv.org/abs/2510.13749", "authors": ["Ivan Vykopal", "Mat\u00fa\u0161 Pikuliak", "Simon Ostermann", "Mari\u00e1n \u0160imko"], "title": "Assessing Web Search Credibility and Response Groundedness in Chat Assistants", "comment": null, "summary": "Chat assistants increasingly integrate web search functionality, enabling\nthem to retrieve and cite external sources. While this promises more reliable\nanswers, it also raises the risk of amplifying misinformation from\nlow-credibility sources. In this paper, we introduce a novel methodology for\nevaluating assistants' web search behavior, focusing on source credibility and\nthe groundedness of responses with respect to cited sources. Using 100 claims\nacross five misinformation-prone topics, we assess GPT-4o, GPT-5, Perplexity,\nand Qwen Chat. Our findings reveal differences between the assistants, with\nPerplexity achieving the highest source credibility, whereas GPT-4o exhibits\nelevated citation of non-credibility sources on sensitive topics. This work\nprovides the first systematic comparison of commonly used chat assistants for\nfact-checking behavior, offering a foundation for evaluating AI systems in\nhigh-stakes information environments.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u56db\u79cd\u804a\u5929\u52a9\u624b\uff08GPT-4o\u3001GPT-5\u3001Perplexity\u3001Qwen Chat\uff09\u5728\u7f51\u7edc\u641c\u7d22\u4e2d\u7684\u53ef\u4fe1\u5ea6\u548c\u5f15\u7528\u53ef\u9760\u6027\uff0c\u53d1\u73b0Perplexity\u5728\u6e90\u53ef\u4fe1\u5ea6\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u800cGPT-4o\u5728\u654f\u611f\u8bdd\u9898\u4e0a\u66f4\u503e\u5411\u4e8e\u5f15\u7528\u975e\u53ef\u4fe1\u6765\u6e90\u3002", "motivation": "\u968f\u7740\u804a\u5929\u52a9\u624b\u96c6\u6210\u7f51\u7edc\u641c\u7d22\u529f\u80fd\uff0c\u867d\u7136\u80fd\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u7b54\u6848\uff0c\u4f46\u4e5f\u5b58\u5728\u653e\u5927\u4f4e\u53ef\u4fe1\u5ea6\u6765\u6e90\u9519\u8bef\u4fe1\u606f\u7684\u98ce\u9669\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u5176\u641c\u7d22\u884c\u4e3a\u3002", "method": "\u4f7f\u7528100\u4e2a\u6d89\u53ca\u4e94\u4e2a\u6613\u51fa\u73b0\u9519\u8bef\u4fe1\u606f\u8bdd\u9898\u7684\u58f0\u660e\uff0c\u8bc4\u4f30\u56db\u79cd\u804a\u5929\u52a9\u624b\u7684\u7f51\u7edc\u641c\u7d22\u884c\u4e3a\uff0c\u91cd\u70b9\u5173\u6ce8\u6e90\u53ef\u4fe1\u5ea6\u548c\u56de\u7b54\u4e0e\u5f15\u7528\u6765\u6e90\u7684\u5173\u8054\u6027\u3002", "result": "\u4e0d\u540c\u52a9\u624b\u4e4b\u95f4\u5b58\u5728\u5dee\u5f02\uff0cPerplexity\u83b7\u5f97\u6700\u9ad8\u7684\u6e90\u53ef\u4fe1\u5ea6\uff0c\u800cGPT-4o\u5728\u654f\u611f\u8bdd\u9898\u4e0a\u5bf9\u975e\u53ef\u4fe1\u6765\u6e90\u7684\u5f15\u7528\u7387\u8f83\u9ad8\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u9996\u6b21\u7cfb\u7edf\u6bd4\u8f83\u4e86\u5e38\u7528\u804a\u5929\u52a9\u624b\u7684\u4e8b\u5b9e\u6838\u67e5\u884c\u4e3a\uff0c\u4e3a\u8bc4\u4f30\u9ad8\u98ce\u9669\u4fe1\u606f\u73af\u5883\u4e2d\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.13750", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13750", "abs": "https://arxiv.org/abs/2510.13750", "authors": ["Zhiqi Huang", "Vivek Datla", "Chenyang Zhu", "Alfy Samuel", "Daben Liu", "Anoop Kumar", "Ritesh Soni"], "title": "Confidence-Based Response Abstinence: Improving LLM Trustworthiness via Activation-Based Uncertainty Estimation", "comment": "UncertaiNLP at EMNLP 2025", "summary": "We propose a method for confidence estimation in retrieval-augmented\ngeneration (RAG) systems that aligns closely with the correctness of large\nlanguage model (LLM) outputs. Confidence estimation is especially critical in\nhigh-stakes domains such as finance and healthcare, where the cost of an\nincorrect answer outweighs that of not answering the question. Our approach\nextends prior uncertainty quantification methods by leveraging raw feed-forward\nnetwork (FFN) activations as auto-regressive signals, avoiding the information\nloss inherent in token logits and probabilities after projection and softmax\nnormalization. We model confidence prediction as a sequence classification\ntask, and regularize training with a Huber loss term to improve robustness\nagainst noisy supervision. Applied in a real-world financial industry\ncustomer-support setting with complex knowledge bases, our method outperforms\nstrong baselines and maintains high accuracy under strict latency constraints.\nExperiments on Llama 3.1 8B model show that using activations from only the\n16th layer preserves accuracy while reducing response latency. Our results\ndemonstrate that activation-based confidence modeling offers a scalable,\narchitecture-aware path toward trustworthy RAG deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u524d\u9988\u7f51\u7edc\u6fc0\u6d3b\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\uff0c\u5728\u91d1\u878d\u5ba2\u670d\u573a\u666f\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u80fd\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u5728\u91d1\u878d\u548c\u533b\u7597\u7b49\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u9519\u8bef\u7b54\u6848\u7684\u6210\u672c\u8fdc\u9ad8\u4e8e\u4e0d\u56de\u7b54\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4fe1\u606f\u635f\u5931\u95ee\u9898\uff0c\u9700\u8981\u66f4\u51c6\u786e\u7684\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u3002", "method": "\u5229\u7528\u539f\u59cb\u524d\u9988\u7f51\u7edc\u6fc0\u6d3b\u4f5c\u4e3a\u81ea\u56de\u5f52\u4fe1\u53f7\uff0c\u907f\u514d\u6295\u5f71\u548csoftmax\u5f52\u4e00\u5316\u9020\u6210\u7684\u4fe1\u606f\u635f\u5931\uff1b\u5c06\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u5efa\u6a21\u4e3a\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\uff0c\u4f7f\u7528Huber\u635f\u5931\u8fdb\u884c\u6b63\u5219\u5316\u8bad\u7ec3\u4ee5\u63d0\u9ad8\u5bf9\u566a\u58f0\u76d1\u7763\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u771f\u5b9e\u91d1\u878d\u884c\u4e1a\u5ba2\u670d\u573a\u666f\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u4e25\u683c\u5ef6\u8fdf\u7ea6\u675f\u4e0b\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u3002\u5728Llama 3.1 8B\u6a21\u578b\u4e0a\uff0c\u4ec5\u4f7f\u7528\u7b2c16\u5c42\u6fc0\u6d3b\u5373\u53ef\u4fdd\u6301\u51c6\u786e\u6027\u540c\u65f6\u51cf\u5c11\u54cd\u5e94\u5ef6\u8fdf\u3002", "conclusion": "\u57fa\u4e8e\u6fc0\u6d3b\u7684\u7f6e\u4fe1\u5ea6\u5efa\u6a21\u4e3a\u53ef\u4fe1\u8d56\u7684RAG\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u3001\u67b6\u6784\u611f\u77e5\u7684\u8def\u5f84\u3002"}}
{"id": "2510.13796", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13796", "abs": "https://arxiv.org/abs/2510.13796", "authors": ["Shuyu Wu", "Ziqiao Ma", "Xiaoxi Luo", "Yidong Huang", "Josue Torres-Fonseca", "Freda Shi", "Joyce Chai"], "title": "The Mechanistic Emergence of Symbol Grounding in Language Models", "comment": null, "summary": "Symbol grounding (Harnad, 1990) describes how symbols such as words acquire\ntheir meanings by connecting to real-world sensorimotor experiences. Recent\nwork has shown preliminary evidence that grounding may emerge in\n(vision-)language models trained at scale without using explicit grounding\nobjectives. Yet, the specific loci of this emergence and the mechanisms that\ndrive it remain largely unexplored. To address this problem, we introduce a\ncontrolled evaluation framework that systematically traces how symbol grounding\narises within the internal computations through mechanistic and causal\nanalysis. Our findings show that grounding concentrates in middle-layer\ncomputations and is implemented through the aggregate mechanism, where\nattention heads aggregate the environmental ground to support the prediction of\nlinguistic forms. This phenomenon replicates in multimodal dialogue and across\narchitectures (Transformers and state-space models), but not in unidirectional\nLSTMs. Our results provide behavioral and mechanistic evidence that symbol\ngrounding can emerge in language models, with practical implications for\npredicting and potentially controlling the reliability of generation.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u673a\u5236\u6027\u548c\u56e0\u679c\u5206\u6790\u6846\u67b6\uff0c\u53d1\u73b0\u7b26\u53f7\u63a5\u5730\u73b0\u8c61\u4e3b\u8981\u51fa\u73b0\u5728\u8bed\u8a00\u6a21\u578b\u7684\u4e2d\u95f4\u5c42\u8ba1\u7b97\u4e2d\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u5934\u805a\u5408\u73af\u5883\u4fe1\u606f\u6765\u652f\u6301\u8bed\u8a00\u5f62\u5f0f\u9884\u6d4b\uff0c\u8fd9\u4e00\u73b0\u8c61\u5728Transformer\u548c\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4e2d\u666e\u904d\u5b58\u5728\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u6709\u521d\u6b65\u8bc1\u636e\u8868\u660e\u5927\u89c4\u6a21\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u81ea\u53d1\u51fa\u73b0\u7b26\u53f7\u63a5\u5730\u73b0\u8c61\uff0c\u4f46\u8fd9\u79cd\u73b0\u8c61\u7684\u5177\u4f53\u53d1\u751f\u4f4d\u7f6e\u548c\u9a71\u52a8\u673a\u5236\u4ecd\u4e0d\u660e\u786e\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u63a2\u7d22\u3002", "method": "\u5f15\u5165\u53d7\u63a7\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u673a\u5236\u6027\u548c\u56e0\u679c\u5206\u6790\u7cfb\u7edf\u8ffd\u8e2a\u7b26\u53f7\u63a5\u5730\u5728\u5185\u90e8\u8ba1\u7b97\u4e2d\u7684\u4ea7\u751f\u8fc7\u7a0b\u3002", "result": "\u53d1\u73b0\u63a5\u5730\u73b0\u8c61\u96c6\u4e2d\u5728\u4e2d\u95f4\u5c42\u8ba1\u7b97\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u5934\u805a\u5408\u73af\u5883\u4fe1\u606f\u5b9e\u73b0\uff1b\u8be5\u73b0\u8c61\u5728\u591a\u6a21\u6001\u5bf9\u8bdd\u548c\u4e0d\u540c\u67b6\u6784\uff08Transformer\u3001\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff09\u4e2d\u53ef\u590d\u73b0\uff0c\u4f46\u5728\u5355\u5411LSTM\u4e2d\u4e0d\u5b58\u5728\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u884c\u4e3a\u548c\u673a\u5236\u8bc1\u636e\uff0c\u8868\u660e\u7b26\u53f7\u63a5\u5730\u53ef\u4ee5\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u81ea\u53d1\u51fa\u73b0\uff0c\u8fd9\u5bf9\u9884\u6d4b\u548c\u63a7\u5236\u751f\u6210\u53ef\u9760\u6027\u5177\u6709\u5b9e\u9645\u610f\u4e49\u3002"}}
{"id": "2510.13797", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13797", "abs": "https://arxiv.org/abs/2510.13797", "authors": ["Giovanni Monea", "Yair Feldman", "Shankar Padmanabhan", "Kiant\u00e9 Brantley", "Yoav Artzi"], "title": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression Beacons", "comment": null, "summary": "The scalability of large language models for long-context reasoning is\nseverely constrained by the linear growth of their Transformer key-value cache,\nwhich incurs significant memory and computational costs. We posit that as a\nmodel generates reasoning tokens, the informational value of past generated\ntokens diminishes, creating an opportunity for compression. In this work, we\npropose to periodically compress the generation KV cache with a learned,\nspecial-purpose token and evict compressed entries. We train the model to\nperform this compression via a modified joint distillation and reinforcement\nlearning (RL) framework. Our training method minimizes overhead over the\nconventional RL process, as it leverages RL outputs for distillation.\nEmpirically, our method achieves a superior memory-accuracy Pareto frontier\ncompared to both the model without cache compression and training-free\ncompression techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u5468\u671f\u6027\u538b\u7f29\u751f\u6210KV\u7f13\u5b58\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u5b66\u4e60\u5230\u7684\u4e13\u7528\u4ee4\u724c\u6765\u538b\u7f29\u5e76\u6dd8\u6c70\u7f13\u5b58\u6761\u76ee\uff0c\u4ece\u800c\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2dTransformer\u952e\u503c\u7f13\u5b58\u7ebf\u6027\u589e\u957f\u5e26\u6765\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u53d7\u5230Transformer\u952e\u503c\u7f13\u5b58\u7ebf\u6027\u589e\u957f\u7684\u4e25\u91cd\u9650\u5236\uff0c\u8fd9\u4f1a\u5e26\u6765\u663e\u8457\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u3002\u968f\u7740\u6a21\u578b\u751f\u6210\u63a8\u7406\u4ee4\u724c\uff0c\u8fc7\u53bb\u751f\u6210\u7684\u4ee4\u724c\u4fe1\u606f\u4ef7\u503c\u4f1a\u9010\u6e10\u51cf\u5c11\uff0c\u8fd9\u4e3a\u538b\u7f29\u521b\u9020\u4e86\u673a\u4f1a\u3002", "method": "\u63d0\u51fa\u5468\u671f\u6027\u538b\u7f29\u751f\u6210KV\u7f13\u5b58\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u5b66\u4e60\u5230\u7684\u4e13\u7528\u4ee4\u724c\u6765\u538b\u7f29\u7f13\u5b58\u6761\u76ee\u3002\u901a\u8fc7\u6539\u8fdb\u7684\u8054\u5408\u84b8\u998f\u548c\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3\u6a21\u578b\u6267\u884c\u8fd9\u79cd\u538b\u7f29\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u8f93\u51fa\u8fdb\u884c\u84b8\u998f\uff0c\u6700\u5c0f\u5316\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\u7684\u5f00\u9500\u3002", "result": "\u7ecf\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5185\u5b58-\u51c6\u786e\u7387\u5e15\u7d2f\u6258\u8fb9\u754c\u4e0a\u4f18\u4e8e\u6ca1\u6709\u7f13\u5b58\u538b\u7f29\u7684\u6a21\u578b\u548c\u65e0\u8bad\u7ec3\u7684\u538b\u7f29\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2dKV\u7f13\u5b58\u7ebf\u6027\u589e\u957f\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u538b\u7f29\u7f13\u5b58\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5185\u5b58\u6548\u7387\u548c\u51c6\u786e\u7387\u5e73\u8861\u3002"}}
{"id": "2510.13799", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13799", "abs": "https://arxiv.org/abs/2510.13799", "authors": ["Jia-Chen Gu", "Junyi Zhang", "Di Wu", "Yuankai Li", "Kai-Wei Chang", "Nanyun Peng"], "title": "BRIEF-Pro: Universal Context Compression with Short-to-Long Synthesis for Fast and Accurate Multi-Hop Reasoning", "comment": "Code and data: https://github.com/JasonForJoy/BRIEF", "summary": "As retrieval-augmented generation (RAG) tackles complex tasks, increasingly\nexpanded contexts offer richer information, but at the cost of higher latency\nand increased cognitive load on the model. To mitigate this bottleneck,\nespecially for intricate multi-hop questions, we introduce BRIEF-Pro. It is a\nuniversal, lightweight compressor that distills relevant evidence for a given\nquery from retrieved documents into a concise summary for seamless integration\ninto in-context RAG. Using seed data consisting of relatively short contexts\n(fewer than 1k words), BRIEF-Pro is trained to perform abstractive compression\nof extended contexts exceeding 10k words across a wide range of scenarios.\nFurthermore, BRIEF-Pro offers flexible user control over summary length by\nallowing users to specify the desired number of sentences. Experiments on four\nopen-domain multi-hop question-answering datasets show that BRIEF-Pro generates\nmore concise and relevant summaries, enhancing performance across small, large,\nand proprietary language models. With the 70B reader model, 32x compression by\nBRIEF-Pro improves QA performance by 4.67% on average over LongLLMLingua's 9x,\nwhile requiring only 23% of its computational overhead.", "AI": {"tldr": "BRIEF-Pro\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u538b\u7f29\u5668\uff0c\u53ef\u5c06\u68c0\u7d22\u5230\u7684\u6587\u6863\u538b\u7f29\u4e3a\u7b80\u6d01\u6458\u8981\uff0c\u7528\u4e8e\u589e\u5f3a\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u591a\u8df3\u95ee\u9898\u65f6\u63d0\u9ad8\u6027\u80fd\u5e76\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u968f\u7740RAG\u5904\u7406\u590d\u6742\u4efb\u52a1\uff0c\u6269\u5c55\u7684\u4e0a\u4e0b\u6587\u867d\u7136\u63d0\u4f9b\u66f4\u4e30\u5bcc\u4fe1\u606f\uff0c\u4f46\u5bfc\u81f4\u66f4\u9ad8\u5ef6\u8fdf\u548c\u6a21\u578b\u8ba4\u77e5\u8d1f\u62c5\u589e\u52a0\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u590d\u6742\u7684\u591a\u8df3\u95ee\u9898\u3002", "method": "BRIEF-Pro\u4f7f\u7528\u76f8\u5bf9\u8f83\u77ed\u7684\u4e0a\u4e0b\u6587\uff08\u5c11\u4e8e1k\u8bcd\uff09\u4f5c\u4e3a\u79cd\u5b50\u6570\u636e\u8bad\u7ec3\uff0c\u80fd\u591f\u5bf9\u8d85\u8fc710k\u8bcd\u7684\u6269\u5c55\u4e0a\u4e0b\u6587\u8fdb\u884c\u62bd\u8c61\u538b\u7f29\uff0c\u5e76\u5141\u8bb8\u7528\u6237\u63a7\u5236\u6458\u8981\u957f\u5ea6\u3002", "result": "\u5728\u56db\u4e2a\u5f00\u653e\u57df\u591a\u8df3\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBRIEF-Pro\u751f\u6210\u66f4\u7b80\u6d01\u76f8\u5173\u7684\u6458\u8981\uff0c\u63d0\u5347\u5c0f\u578b\u3001\u5927\u578b\u548c\u4e13\u6709\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002\u4f7f\u752870B\u9605\u8bfb\u5668\u6a21\u578b\u65f6\uff0c32\u500d\u538b\u7f29\u6bd4LongLLMLingua\u76849\u500d\u538b\u7f29\u6027\u80fd\u63d0\u53474.67%\uff0c\u8ba1\u7b97\u5f00\u9500\u4ec5\u970023%\u3002", "conclusion": "BRIEF-Pro\u662f\u4e00\u79cd\u901a\u7528\u8f7b\u91cf\u7ea7\u538b\u7f29\u5668\uff0c\u80fd\u6709\u6548\u7f13\u89e3RAG\u7cfb\u7edf\u4e2d\u957f\u4e0a\u4e0b\u6587\u5e26\u6765\u7684\u5ef6\u8fdf\u548c\u8ba4\u77e5\u8d1f\u62c5\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u591a\u8df3\u95ee\u7b54\u6027\u80fd\u3002"}}
