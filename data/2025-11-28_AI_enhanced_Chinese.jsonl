{"id": "2511.21610", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.21610", "abs": "https://arxiv.org/abs/2511.21610", "authors": ["Yixiu Zhao", "Xiaozhi Wang", "Zijun Yao", "Lei Hou", "Juanzi Li"], "title": "Auxiliary Metrics Help Decoding Skill Neurons in the Wild", "comment": "7 pages, 7 figures. Includes additional appendix", "summary": "Large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, yet their internal mechanisms remain largely opaque. In this paper, we introduce a simple, lightweight, and broadly applicable method with a focus on isolating neurons that encode specific skills. Building upon prior work that identified \"skill neurons\" via soft prompt training on classification tasks, our approach extends the analysis to complex scenarios involving multiple skills. We correlate neuron activations with auxiliary metrics -- such as external labels and the model's own confidence score -- thereby uncovering interpretable and task-specific behaviors without the need for manual token aggregation. We empirically validate our method on tasks spanning open-ended text generation and natural language inference, demonstrating its ability to detect neurons that not only drive known skills but also reveal previously unidentified shortcuts in arithmetic reasoning on BigBench.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u8f7b\u91cf\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7f16\u7801\u7279\u5b9a\u6280\u80fd\u7684\u795e\u7ecf\u5143\uff0c\u901a\u8fc7\u5c06\u795e\u7ecf\u5143\u6fc0\u6d3b\u4e0e\u5916\u90e8\u6807\u7b7e\u548c\u6a21\u578b\u7f6e\u4fe1\u5ea6\u7b49\u8f85\u52a9\u6307\u6807\u5173\u8054\uff0c\u5728\u6587\u672c\u751f\u6210\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5185\u90e8\u673a\u5236\u4ecd\u7136\u4e0d\u900f\u660e\uff0c\u9700\u8981\u5f00\u53d1\u65b9\u6cd5\u6765\u7406\u89e3\u6a21\u578b\u5982\u4f55\u7f16\u7801\u7279\u5b9a\u6280\u80fd\u3002", "method": "\u57fa\u4e8e\u5148\u524d\u901a\u8fc7\u8f6f\u63d0\u793a\u8bad\u7ec3\u8bc6\u522b\u6280\u80fd\u795e\u7ecf\u5143\u7684\u5de5\u4f5c\uff0c\u6269\u5c55\u5230\u590d\u6742\u591a\u6280\u80fd\u573a\u666f\uff0c\u5c06\u795e\u7ecf\u5143\u6fc0\u6d3b\u4e0e\u5916\u90e8\u6807\u7b7e\u548c\u6a21\u578b\u7f6e\u4fe1\u5ea6\u7b49\u8f85\u52a9\u6307\u6807\u76f8\u5173\u8054\uff0c\u65e0\u9700\u624b\u52a8\u6807\u8bb0\u805a\u5408\u3002", "result": "\u5728\u5f00\u653e\u6587\u672c\u751f\u6210\u548c\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u4e0d\u4ec5\u80fd\u68c0\u6d4b\u9a71\u52a8\u5df2\u77e5\u6280\u80fd\u7684\u795e\u7ecf\u5143\uff0c\u8fd8\u53d1\u73b0\u4e86BigBench\u7b97\u672f\u63a8\u7406\u4e2d\u4ee5\u524d\u672a\u8bc6\u522b\u7684\u6377\u5f84\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u9014\u5f84\u6765\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u5de5\u4f5c\u673a\u5236\uff0c\u6709\u52a9\u4e8e\u63ed\u793a\u6a21\u578b\u6280\u80fd\u7f16\u7801\u548c\u63a8\u7406\u7b56\u7565\u3002"}}
{"id": "2511.21613", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21613", "abs": "https://arxiv.org/abs/2511.21613", "authors": ["Dongyang Fan", "Diba Hashemi", "Sai Praneeth Karimireddy", "Martin Jaggi"], "title": "Beyond URLs: Metadata Diversity and Position for Efficient LLM Pretraining", "comment": null, "summary": "Incorporating metadata in Large Language Models (LLMs) pretraining has recently emerged as a promising approach to accelerate training. However prior work highlighted only one useful signal-URLs, leaving open the question of whether other forms of metadata could yield greater benefits. In this study, we investigate a wider range of metadata types and find other types of metadata, such as fine-grained indicators of document quality that can also accelerate pretraining when prepended. We identify a common feature among effective metadata: they encode information at a finer granularity. We further introduce metadata appending as a means of improving training efficiency, where predicting an appropriate metadata as auxiliary task can help speed up pretraining. In addition, learnable meta-tokens trained with masked loss can recover part of the speedup by inducing quality-aware latent structure. Using probing, we analyze latent representations to understand how metadata shapes learning. Together, these results yield practical guidelines for integrating metadata to improve both the efficiency and effectiveness of LLM pretraining.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u9664URL\u5916\uff0c\u5176\u4ed6\u5143\u6570\u636e\u7c7b\u578b\uff08\u5982\u6587\u6863\u8d28\u91cf\u6307\u6807\uff09\u4e5f\u80fd\u52a0\u901fLLM\u9884\u8bad\u7ec3\uff0c\u6709\u6548\u5143\u6570\u636e\u7684\u5171\u540c\u7279\u5f81\u662f\u7ec6\u7c92\u5ea6\u4fe1\u606f\u7f16\u7801\u3002\u63d0\u51fa\u4e86\u5143\u6570\u636e\u8ffd\u52a0\u548c\u53ef\u5b66\u4e60\u5143\u4ee4\u724c\u4e24\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u6f5c\u5728\u8868\u5f81\u63ed\u793a\u4e86\u5143\u6570\u636e\u5982\u4f55\u5851\u9020\u5b66\u4e60\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4ec5\u53d1\u73b0URL\u5bf9\u52a0\u901fLLM\u9884\u8bad\u7ec3\u6709\u6548\uff0c\u4f46\u5176\u4ed6\u5143\u6570\u636e\u7c7b\u578b\u662f\u5426\u5177\u6709\u66f4\u5927\u6f5c\u529b\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u66f4\u5e7f\u6cdb\u7684\u5143\u6570\u636e\u7c7b\u578b\u53ca\u5176\u5bf9\u9884\u8bad\u7ec3\u6548\u7387\u7684\u5f71\u54cd\u3002", "method": "1) \u8c03\u67e5\u591a\u79cd\u5143\u6570\u636e\u7c7b\u578b\u5728\u9884\u8bad\u7ec3\u4e2d\u7684\u6548\u679c\uff1b2) \u5f15\u5165\u5143\u6570\u636e\u8ffd\u52a0\u65b9\u6cd5\uff0c\u5c06\u9884\u6d4b\u9002\u5f53\u5143\u6570\u636e\u4f5c\u4e3a\u8f85\u52a9\u4efb\u52a1\uff1b3) \u4f7f\u7528\u63a9\u7801\u635f\u5931\u8bad\u7ec3\u53ef\u5b66\u4e60\u5143\u4ee4\u724c\uff1b4) \u901a\u8fc7\u63a2\u9488\u5206\u6790\u6f5c\u5728\u8868\u5f81\u3002", "result": "\u53d1\u73b0\u7ec6\u7c92\u5ea6\u7684\u6587\u6863\u8d28\u91cf\u6307\u6807\u7b49\u5143\u6570\u636e\u4e5f\u80fd\u52a0\u901f\u9884\u8bad\u7ec3\uff0c\u6709\u6548\u5143\u6570\u636e\u7684\u5171\u540c\u7279\u5f81\u662f\u7ec6\u7c92\u5ea6\u4fe1\u606f\u7f16\u7801\u3002\u5143\u6570\u636e\u8ffd\u52a0\u548c\u53ef\u5b66\u4e60\u5143\u4ee4\u724c\u65b9\u6cd5\u90fd\u80fd\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\uff0c\u540e\u8005\u901a\u8fc7\u8bf1\u5bfc\u8d28\u91cf\u611f\u77e5\u7684\u6f5c\u5728\u7ed3\u6784\u6062\u590d\u90e8\u5206\u52a0\u901f\u6548\u679c\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6574\u5408\u5143\u6570\u636e\u4ee5\u63d0\u5347LLM\u9884\u8bad\u7ec3\u6548\u7387\u548c\u6548\u679c\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\uff0c\u8bc1\u660e\u4e86\u591a\u79cd\u5143\u6570\u636e\u7c7b\u578b\u7684\u4ef7\u503c\uff0c\u5e76\u63ed\u793a\u4e86\u5143\u6570\u636e\u901a\u8fc7\u5851\u9020\u6f5c\u5728\u8868\u5f81\u6765\u5f71\u54cd\u5b66\u4e60\u8fc7\u7a0b\u7684\u673a\u5236\u3002"}}
{"id": "2511.21629", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.21629", "abs": "https://arxiv.org/abs/2511.21629", "authors": ["Anna Marklov\u00e1", "Ond\u0159ej Vin\u0161", "Martina Vok\u00e1\u010dov\u00e1", "Ji\u0159\u00ed Mili\u010dka"], "title": "The author is dead, but what if they never lived? A reception experiment on Czech AI- and human-authored poetry", "comment": null, "summary": "Large language models are increasingly capable of producing creative texts, yet most studies on AI-generated poetry focus on English -- a language that dominates training data. In this paper, we examine the perception of AI- and human-written Czech poetry. We ask if Czech native speakers are able to identify it and how they aesthetically judge it. Participants performed at chance level when guessing authorship (45.8\\% correct on average), indicating that Czech AI-generated poems were largely indistinguishable from human-written ones. Aesthetic evaluations revealed a strong authorship bias: when participants believed a poem was AI-generated, they rated it as less favorably, even though AI poems were in fact rated equally or more favorably than human ones on average. The logistic regression model uncovered that the more the people liked a poem, the less probable was that they accurately assign the authorship. Familiarity with poetry or literary background had no effect on recognition accuracy. Our findings show that AI can convincingly produce poetry even in a morphologically complex, low-resource (with respect of the training data of AI models) Slavic language such as Czech. The results suggest that readers' beliefs about authorship and the aesthetic evaluation of the poem are interconnected.", "AI": {"tldr": "\u6377\u514b\u8bedAI\u751f\u6210\u8bd7\u6b4c\u4e0e\u4eba\u7c7b\u8bd7\u6b4c\u96be\u4ee5\u533a\u5206\uff0c\u53c2\u4e0e\u8005\u8bc6\u522b\u51c6\u786e\u7387\u4ec545.8%\uff0c\u4e14\u5ba1\u7f8e\u8bc4\u4ef7\u5b58\u5728\u4f5c\u8005\u504f\u89c1\u2014\u2014\u5f53\u8ba4\u4e3a\u8bd7\u6b4c\u662fAI\u751f\u6210\u65f6\u8bc4\u5206\u66f4\u4f4e\uff0c\u5c3d\u7ba1AI\u8bd7\u6b4c\u5b9e\u9645\u8bc4\u5206\u4e0e\u4eba\u7c7b\u8bd7\u6b4c\u76f8\u5f53\u6216\u66f4\u9ad8\u3002", "motivation": "\u5927\u591a\u6570\u5173\u4e8eAI\u751f\u6210\u8bd7\u6b4c\u7684\u7814\u7a76\u96c6\u4e2d\u5728\u82f1\u8bed\u4e0a\uff0c\u800c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u6377\u514b\u8bed\uff08\u4e00\u79cd\u5f62\u6001\u590d\u6742\u3001\u8bad\u7ec3\u6570\u636e\u8f83\u5c11\u7684\u65af\u62c9\u592b\u8bed\u8a00\uff09\u4e2dAI\u751f\u6210\u8bd7\u6b4c\u7684\u611f\u77e5\u548c\u5ba1\u7f8e\u8bc4\u4ef7\u3002", "method": "\u8ba9\u6377\u514b\u6bcd\u8bed\u8005\u8bc6\u522b\u548c\u5ba1\u7f8e\u8bc4\u4ef7AI\u751f\u6210\u4e0e\u4eba\u7c7b\u521b\u4f5c\u7684\u8bd7\u6b4c\uff0c\u4f7f\u7528\u903b\u8f91\u56de\u5f52\u6a21\u578b\u5206\u6790\u8bc6\u522b\u51c6\u786e\u7387\u4e0e\u5ba1\u7f8e\u8bc4\u4ef7\u7684\u5173\u7cfb\u3002", "result": "\u53c2\u4e0e\u8005\u8bc6\u522b\u51c6\u786e\u7387\u63a5\u8fd1\u968f\u673a\u6c34\u5e73\uff0845.8%\uff09\uff0c\u8868\u660eAI\u8bd7\u6b4c\u96be\u4ee5\u4e0e\u4eba\u7c7b\u8bd7\u6b4c\u533a\u5206\u3002\u5ba1\u7f8e\u8bc4\u4ef7\u663e\u793a\u5f3a\u70c8\u7684\u4f5c\u8005\u504f\u89c1\u2014\u2014\u5f53\u8ba4\u4e3a\u8bd7\u6b4c\u662fAI\u751f\u6210\u65f6\u8bc4\u5206\u66f4\u4f4e\uff0c\u5c3d\u7ba1AI\u8bd7\u6b4c\u5b9e\u9645\u8bc4\u5206\u4e0e\u4eba\u7c7b\u8bd7\u6b4c\u76f8\u5f53\u6216\u66f4\u9ad8\u3002\u903b\u8f91\u56de\u5f52\u663e\u793a\u8bd7\u6b4c\u8d8a\u53d7\u6b22\u8fce\uff0c\u51c6\u786e\u8bc6\u522b\u4f5c\u8005\u7684\u53ef\u80fd\u6027\u8d8a\u4f4e\u3002", "conclusion": "\u5373\u4f7f\u5728\u5f62\u6001\u590d\u6742\u3001\u8bad\u7ec3\u6570\u636e\u8f83\u5c11\u7684\u6377\u514b\u8bed\u4e2d\uff0cAI\u4e5f\u80fd\u751f\u6210\u4ee4\u4eba\u4fe1\u670d\u7684\u8bd7\u6b4c\u3002\u8bfb\u8005\u7684\u4f5c\u8005\u4fe1\u5ff5\u4e0e\u8bd7\u6b4c\u5ba1\u7f8e\u8bc4\u4ef7\u5bc6\u5207\u76f8\u5173\uff0c\u5b58\u5728\u660e\u663e\u7684\u4f5c\u8005\u504f\u89c1\u3002"}}
{"id": "2511.21686", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21686", "abs": "https://arxiv.org/abs/2511.21686", "authors": ["Dong Wang", "Yang Li", "Ansong Ni", "Ching-Feng Yeh", "Youssef Emad", "Xinjie Lei", "Liam Robbins", "Karthik Padthe", "Hu Xu", "Xian Li", "Asli Celikyilmaz", "Ramya Raghavendra", "Lifei Huang", "Carole-Jean Wu", "Shang-Wen Li"], "title": "Matrix: Peer-to-Peer Multi-Agent Synthetic Data Generation Framework", "comment": null, "summary": "Synthetic data has become increasingly important for training large language models, especially when real data is scarce, expensive, or privacy-sensitive. Many such generation tasks require coordinated multi-agent workflows, where specialized agents collaborate to produce data that is higher quality, more diverse, and structurally richer. However, existing frameworks for multi-agent synthesis often depend on a centralized orchestrator, creating scalability bottlenecks, or are hardcoded for specific domains, limiting flexibility. We present \\textbf{Matrix}, a decentralized framework that represents both control and data flow as serialized messages passed through distributed queues. This peer-to-peer design eliminates the central orchestrator. Each task progresses independently through lightweight agents, while compute-intensive operations, such as LLM inference or containerized environments, are handled by distributed services. Built on Ray, Matrix scales to tens of thousands of concurrent agentic workflows and provides a modular, configurable design that enables easy adaptation to a wide range of data generation workflows. We evaluate Matrix across diverse synthesis scenarios, such as multi-agent collaborative dialogue, web-based reasoning data extraction, and tool-use trajectory generation in customer service environments. In all cases, Matrix achieves $2$--$15\\times$ higher data generation throughput under identical hardware resources, without compromising output quality.", "AI": {"tldr": "Matrix\u662f\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u7684\u591a\u667a\u80fd\u4f53\u5408\u6210\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u961f\u5217\u4f20\u9012\u5e8f\u5217\u5316\u6d88\u606f\u6765\u66ff\u4ee3\u4e2d\u592e\u7f16\u6392\u5668\uff0c\u5728\u76f8\u540c\u786c\u4ef6\u8d44\u6e90\u4e0b\u5b9e\u73b02-15\u500d\u7684\u6570\u636e\u751f\u6210\u541e\u5410\u91cf\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u5408\u6210\u6846\u67b6\u4f9d\u8d56\u4e2d\u592e\u7f16\u6392\u5668\u5bfc\u81f4\u53ef\u6269\u5c55\u6027\u74f6\u9888\uff0c\u6216\u9488\u5bf9\u7279\u5b9a\u9886\u57df\u786c\u7f16\u7801\u9650\u5236\u4e86\u7075\u6d3b\u6027\u3002", "method": "\u57fa\u4e8eRay\u6784\u5efa\u7684\u53bb\u4e2d\u5fc3\u5316\u6846\u67b6\uff0c\u5c06\u63a7\u5236\u548c\u6570\u636e\u6d41\u8868\u793a\u4e3a\u901a\u8fc7\u5206\u5e03\u5f0f\u961f\u5217\u4f20\u9012\u7684\u5e8f\u5217\u5316\u6d88\u606f\uff0c\u8f7b\u91cf\u7ea7\u667a\u80fd\u4f53\u72ec\u7acb\u5904\u7406\u4efb\u52a1\uff0c\u8ba1\u7b97\u5bc6\u96c6\u578b\u64cd\u4f5c\u7531\u5206\u5e03\u5f0f\u670d\u52a1\u5904\u7406\u3002", "result": "\u5728\u591a\u79cd\u5408\u6210\u573a\u666f\uff08\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5bf9\u8bdd\u3001\u57fa\u4e8e\u7f51\u7edc\u7684\u63a8\u7406\u6570\u636e\u63d0\u53d6\u3001\u5ba2\u6237\u670d\u52a1\u73af\u5883\u4e2d\u7684\u5de5\u5177\u4f7f\u7528\u8f68\u8ff9\u751f\u6210\uff09\u4e2d\uff0cMatrix\u5728\u76f8\u540c\u786c\u4ef6\u8d44\u6e90\u4e0b\u5b9e\u73b02-15\u500d\u7684\u6570\u636e\u751f\u6210\u541e\u5410\u91cf\u63d0\u5347\uff0c\u4e14\u4e0d\u635f\u5bb3\u8f93\u51fa\u8d28\u91cf\u3002", "conclusion": "Matrix\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u914d\u7f6e\u7684\u53bb\u4e2d\u5fc3\u5316\u6846\u67b6\uff0c\u80fd\u591f\u8f7b\u677e\u9002\u5e94\u5e7f\u6cdb\u7684\u6570\u636e\u751f\u6210\u5de5\u4f5c\u6d41\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u5408\u6210\u6570\u636e\u751f\u6210\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2511.21689", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.21689", "abs": "https://arxiv.org/abs/2511.21689", "authors": ["Hongjin Su", "Shizhe Diao", "Ximing Lu", "Mingjie Liu", "Jiacheng Xu", "Xin Dong", "Yonggan Fu", "Peter Belcak", "Hanrong Ye", "Hongxu Yin", "Yi Dong", "Evelina Bakhturina", "Tao Yu", "Yejin Choi", "Jan Kautz", "Pavlo Molchanov"], "title": "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration", "comment": "21 pages, 6 figures", "summary": "Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.", "AI": {"tldr": "ToolOrchestra\u8bad\u7ec3\u7684\u5c0f\u578b\u7f16\u6392\u5668Orchestrator\uff088B\u53c2\u6570\uff09\u5728\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u65f6\uff0c\u4ee5\u66f4\u4f4e\u7684\u6210\u672c\u8d85\u8d8aGPT-5\u7b49\u5927\u578b\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0e\u6548\u7387\u7684\u6700\u4f73\u5e73\u8861\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u529f\u80fd\u5f3a\u5927\uff0c\u4f46\u5728\u89e3\u51b3\u50cfHumanity's Last Exam\u8fd9\u6837\u7684\u6df1\u5ea6\u590d\u6742\u95ee\u9898\u65f6\uff0c\u4ecd\u9762\u4e34\u6982\u5ff5\u6311\u6218\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u95ee\u9898\u3002\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u534f\u8c03\u667a\u80fd\u5de5\u5177\u3002", "method": "\u63d0\u51faToolOrchestra\u65b9\u6cd5\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5c0f\u578b\u7f16\u6392\u5668\uff0c\u5956\u52b1\u673a\u5236\u8003\u8651\u7ed3\u679c\u8d28\u91cf\u3001\u6548\u7387\u548c\u7528\u6237\u504f\u597d\uff0c\u534f\u8c03\u591a\u79cd\u667a\u80fd\u5de5\u5177\u3002", "result": "Orchestrator\u5728HLE\u4e0a\u83b7\u5f9737.1%\u7684\u5206\u6570\uff0c\u8d85\u8d8aGPT-5\uff0835.1%\uff09\u4e14\u6548\u7387\u63d0\u9ad82.5\u500d\uff1b\u5728tau2-Bench\u548cFRAMES\u4e0a\u5927\u5e45\u8d85\u8d8aGPT-5\uff0c\u4ec5\u4f7f\u7528\u7ea630%\u7684\u6210\u672c\u3002", "conclusion": "\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7f16\u6392\u6a21\u578b\u7ec4\u5408\u591a\u6837\u5316\u5de5\u5177\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u9ad8\u6548\u6709\u6548\uff0c\u4e3a\u5b9e\u7528\u548c\u53ef\u6269\u5c55\u7684\u5de5\u5177\u589e\u5f3a\u63a8\u7406\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2511.21692", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21692", "abs": "https://arxiv.org/abs/2511.21692", "authors": ["Yeganeh Kordi", "Nihal V. Nayak", "Max Zuo", "Ilana Nguyen", "Stephen H. Bach"], "title": "Revisiting Generalization Across Difficulty Levels: It's Not So Easy", "comment": null, "summary": "We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation. Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address this question by conducting a systematic evaluation of LLMs' generalization across models, datasets, and fine-grained groups of example difficulty. We rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory (IRT), a well-established difficulty metric in educational testing. Unlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs, excluding human opinions of difficulty. With a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties. These results show the importance of having a range of difficulties in both training and evaluation data for LLMs, and that taking shortcuts with respect to difficulty is risky.", "AI": {"tldr": "LLMs\u5728\u8de8\u96be\u5ea6\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u8bad\u7ec3\u4f7f\u7528\u7b80\u5355\u6216\u56f0\u96be\u6570\u636e\u90fd\u65e0\u6cd5\u5728\u6240\u6709\u96be\u5ea6\u8303\u56f4\u5185\u5b9e\u73b0\u4e00\u81f4\u6539\u8fdb\uff0c\u8868\u660e\u8bad\u7ec3\u548c\u8bc4\u4f30\u6570\u636e\u9700\u8981\u5305\u542b\u4e0d\u540c\u96be\u5ea6\u7ea7\u522b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9\u4e8e\u8bad\u7ec3\u4f7f\u7528\u7b80\u5355\u8fd8\u662f\u56f0\u96be\u6570\u636e\u6548\u679c\u66f4\u597d\u5b58\u5728\u4e89\u8bae\uff0c\u4e14\u4e0d\u6e05\u695a\u8fd9\u4e9b\u6539\u8fdb\u662f\u5426\u4f53\u73b0\u5728\u7b80\u5355\u8fd8\u662f\u56f0\u96be\u7684\u6d4b\u8bd5\u6570\u636e\u4e0a\u3002", "method": "\u4f7f\u7528\u6570\u5343\u4e2a\u4e0d\u540cLLM\u7684\u8f93\u51fa\u548c\u9879\u76ee\u53cd\u5e94\u7406\u8bba(IRT)\u5bf9\u516d\u4e2a\u6570\u636e\u96c6\u4e2d\u7684\u793a\u4f8b\u8fdb\u884c\u96be\u5ea6\u6392\u5e8f\uff0c\u4ec5\u57fa\u4e8eLLM\u80fd\u529b\u786e\u5b9a\u96be\u5ea6\u8bc4\u7ea7\uff0c\u6392\u9664\u4eba\u7c7b\u4e3b\u89c2\u5224\u65ad\u3002", "result": "\u8de8\u96be\u5ea6\u6cdb\u5316\u901a\u5e38\u6709\u9650\uff0c\u8bad\u7ec3\u4f7f\u7528\u7b80\u5355\u6216\u56f0\u96be\u6570\u636e\u90fd\u65e0\u6cd5\u5728\u6240\u6709\u96be\u5ea6\u8303\u56f4\u5185\u5b9e\u73b0\u4e00\u81f4\u6539\u8fdb\u3002", "conclusion": "LLM\u8bad\u7ec3\u548c\u8bc4\u4f30\u6570\u636e\u9700\u8981\u5305\u542b\u4e0d\u540c\u96be\u5ea6\u7ea7\u522b\uff0c\u5728\u96be\u5ea6\u65b9\u9762\u8d70\u6377\u5f84\u662f\u6709\u98ce\u9669\u7684\u3002"}}
