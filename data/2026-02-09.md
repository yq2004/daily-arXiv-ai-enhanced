<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 55]
- [cs.IR](#cs.IR) [Total: 5]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Recontextualizing Famous Quotes for Brand Slogan Generation](https://arxiv.org/abs/2602.06049)
*Ziao Yang,Zizhang Chen,Lei Zhang,Hongfu Liu*

Main category: cs.CL

TL;DR: 提出一种基于名人名言重构的广告口号生成新范式，通过模块化框架实现，相比现有LLM方法在多样性、新颖性、情感影响和人类偏好方面有边际提升


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成广告口号存在风格冗余、缺乏明确品牌形象、过于机器化的问题。广告疲劳降低了重复口号的有效性，市场需要新颖、有创意、有洞察力的口号生成方法

Method: 提出基于名人名言重构的新范式，采用模块化框架将口号生成分解为可解释的子任务：名言匹配、结构分解、词汇替换和混音生成

Result: 通过自动和人工评估，相比三种最先进的LLM基线方法，在多样性、新颖性、情感影响和人类偏好方面取得了边际改进

Conclusion: 将名人名言重新语境化为品牌口号是一种有效的生成方法，能够平衡新颖性与熟悉性，产生更具创意和洞察力的广告口号

Abstract: Slogans are concise and memorable catchphrases that play a crucial role in advertising by conveying brand identity and shaping public perception. However, advertising fatigue reduces the effectiveness of repeated slogans, creating a growing demand for novel, creative, and insightful slogan generation. While recent work leverages large language models (LLMs) for this task, existing approaches often produce stylistically redundant outputs that lack a clear brand persona and appear overtly machine-generated. We argue that effective slogans should balance novelty with familiarity and propose a new paradigm that recontextualizes persona-related famous quotes for slogan generation. Well-known quotes naturally align with slogan-length text, employ rich rhetorical devices, and offer depth and insight, making them a powerful resource for creative generation. Technically, we introduce a modular framework that decomposes slogan generation into interpretable subtasks, including quote matching, structural decomposition, vocabulary replacement, and remix generation. Extensive automatic and human evaluations demonstrate marginal improvements in diversity, novelty, emotional impact, and human preference over three state-of-the-art LLM baselines.

</details>


### [2] [Relevance-aware Multi-context Contrastive Decoding for Retrieval-augmented Visual Question Answering](https://arxiv.org/abs/2602.06050)
*Jongha Kim,Byungoh Ko,Jeehye Na,Jinsung Yoon,Hyunwoo J. Kim*

Main category: cs.CL

TL;DR: RMCD是一种新的解码方法，通过基于上下文相关性加权聚合多个上下文预测，提升大视觉语言模型在检索增强生成中的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大视觉语言模型（LVLMs）能力显著，但缺乏对特定实体的详细知识。检索增强生成（RAG）通过外部知识库提供额外上下文来增强LVLMs，但现有解码方法未能充分利用多个相关上下文并抑制无关上下文的负面影响。

Method: 提出相关性感知的多上下文对比解码（RMCD），通过加权组合基于每个上下文预测的输出，权重基于上下文与问题的相关性，有效聚合相关信息并抵消无关信息的影响。

Result: 在多个知识密集型视觉问答基准测试中，RMCD一致优于其他解码方法，在不同强度的检索结果下均表现最佳，且无需额外训练即可应用。

Conclusion: RMCD是一种简单有效的解码方法，能显著提升大视觉语言模型在检索增强生成中的性能，具有鲁棒性和无需训练的优势。

Abstract: Despite the remarkable capabilities of Large Vision Language Models (LVLMs), they still lack detailed knowledge about specific entities. Retrieval-augmented Generation (RAG) is a widely adopted solution that enhances LVLMs by providing additional contexts from an external Knowledge Base. However, we observe that previous decoding methods for RAG are sub-optimal as they fail to sufficiently leverage multiple relevant contexts and suppress the negative effects of irrelevant contexts. To this end, we propose Relevance-aware Multi-context Contrastive Decoding (RMCD), a novel decoding method for RAG. RMCD outputs a final prediction by combining outputs predicted with each context, where each output is weighted based on its relevance to the question. By doing so, RMCD effectively aggregates useful information from multiple relevant contexts while also counteracting the negative effects of irrelevant ones. Experiments show that RMCD consistently outperforms other decoding methods across multiple LVLMs, achieving the best performance on three knowledge-intensive visual question-answering benchmarks. Also, RMCD can be simply applied by replacing the decoding method of LVLMs without additional training. Analyses also show that RMCD is robust to the retrieval results, consistently performing the best across the weakest to the strongest retrieval results. Code is available at https://github.com/mlvlab/RMCD.

</details>


### [3] [CAST: Character-and-Scene Episodic Memory for Agents](https://arxiv.org/abs/2602.06051)
*Kexin Ma,Bojun Li,Yuhua Tang,Ruochun Jin,Liting Sun*

Main category: cs.CL

TL;DR: CAST：一种基于戏剧理论的记忆架构，通过构建3D场景（时间/地点/主题）并将其组织成角色档案来表示情景记忆，结合基于图的语义记忆形成双重记忆设计，显著提升了开放性和时间敏感性对话问题的性能。


<details>
  <summary>Details</summary>
Motivation: 现有智能体记忆系统主要强调语义回忆，将经验视为键值、向量或图等结构，难以表示和检索连贯的事件。人类的情景记忆能够回忆基于谁、何时、何地的连贯事件，而现有系统在这方面存在不足。

Method: 受到戏剧理论启发，提出CAST（Character-and-Scene based memory architecture）架构：1）构建3D场景（时间/地点/主题）并将其组织成角色档案，总结角色的所有事件来表示情景记忆；2）结合基于图的语义记忆，形成双重记忆设计。

Result: 实验表明，CAST在各种数据集上平均比基线方法提升了8.11%的F1分数和10.21%的J(LLM-as-a-Judge)分数，特别是在开放性和时间敏感性的对话问题上表现尤为突出。

Conclusion: CAST通过戏剧理论启发的角色和场景架构有效解决了智能体情景记忆表示和检索的挑战，双重记忆设计提供了更强大和全面的记忆能力，在对话系统中具有重要应用价值。

Abstract: Episodic memory is a central component of human memory, which refers to the ability to recall coherent events grounded in who, when, and where. However, most agent memory systems only emphasize semantic recall and treat experience as structures such as key-value, vector, or graph, which makes them struggle to represent and retrieve coherent events. To address this challenge, we propose a Character-and-Scene based memory architecture(CAST) inspired by dramatic theory. Specifically, CAST constructs 3D scenes (time/place/topic) and organizes them into character profiles that summarize the events of a character to represent episodic memory. Moreover, CAST complements this episodic memory with a graph-based semantic memory, which yields a robust dual memory design. Experiments demonstrate that CAST has averagely improved 8.11% F1 and 10.21% J(LLM-as-a-Judge) than baselines on various datasets, especially on open and time-sensitive conversational questions.

</details>


### [4] [Rethinking Memory Mechanisms of Foundation Agents in the Second Half](https://arxiv.org/abs/2602.06052)
*Wei-Chieh Huang,Weizhi Zhang,Yueqing Liang,Yuanchen Bei,Yankai Chen,Tao Feng,Xinyu Pan,Zhen Tan,Yu Wang,Tianxin Wei,Shanglin Wu,Ruiyao Xu,Liangwei Yang,Rui Yang,Wooseong Yang,Chin-Yuan Yeh,Hanrong Zhang,Haozhen Zhang,Siqi Zhu,Henry Peng Zou,Wanjia Zhao,Song Wang,Wujiang Xu,Zixuan Ke,Zheng Hui,Dawei Li,Yaozu Wu,Langzhou He,Chen Wang,Xiongxiao Xu,Baixiang Huang,Juntao Tan,Shelby Heinecke,Huan Wang,Caiming Xiong,Ahmed A. Metwally,Jun Yan,Chen-Yu Lee,Hanqing Zeng,Yinglong Xia,Xiaokai Wei,Ali Payani,Yu Wang,Haitong Ma,Wenya Wang,Chengguang Wang,Yu Zhang,Xin Wang,Yongfeng Zhang,Jiaxuan You,Hanghang Tong,Xiao Luo,Yizhou Sun,Wei Wang,Julian McAuley,James Zou,Jiawei Han,Philip S. Yu,Kai Shu*

Main category: cs.CL

TL;DR: 该论文是一篇关于智能体记忆系统的综述，提出了统一的三维框架（记忆基质、认知机制、记忆主体），分析了不同智能体拓扑中的记忆实现，并探讨了记忆效用的评估方法。


<details>
  <summary>Details</summary>
Motivation: 人工智能研究正从注重模型创新和基准分数转向强调问题定义和严格的现实世界评估。在"下半场"中，核心挑战是智能体在长周期、动态、用户依赖环境中的实际效用，这需要智能体能够持续积累、管理和选择性重用大量信息，而记忆系统是填补这一效用鸿沟的关键解决方案。

Method: 1. 提出统一的三维框架分析基础智能体记忆：记忆基质（内部和外部）、认知机制（情景、语义、感官、工作、程序）、记忆主体（智能体中心vs用户中心）。2. 分析不同智能体拓扑中的记忆实现和操作。3. 研究记忆操作的学习策略。4. 综述记忆效用的评估基准和指标。

Result: 1. 建立了智能体记忆的统一分析框架。2. 系统分析了记忆在不同智能体架构中的实现方式。3. 梳理了记忆操作的学习机制。4. 总结了现有的记忆评估方法和基准。5. 识别了当前记忆系统面临的关键挑战和研究空白。

Conclusion: 记忆系统是推动智能体在复杂现实环境中实现实际效用的关键。论文提出的三维框架为理解和发展智能体记忆提供了系统性指导，同时指出了记忆评估、跨模态整合、可扩展性等方面的未来研究方向，对推动智能体向更高层次的认知能力发展具有重要意义。

Abstract: The research of artificial intelligence is undergoing a paradigm shift from prioritizing model innovations over benchmark scores towards emphasizing problem definition and rigorous real-world evaluation. As the field enters the "second half," the central challenge becomes real utility in long-horizon, dynamic, and user-dependent environments, where agents face context explosion and must continuously accumulate, manage, and selectively reuse large volumes of information across extended interactions. Memory, with hundreds of papers released this year, therefore emerges as the critical solution to fill the utility gap. In this survey, we provide a unified view of foundation agent memory along three dimensions: memory substrate (internal and external), cognitive mechanism (episodic, semantic, sensory, working, and procedural), and memory subject (agent- and user-centric). We then analyze how memory is instantiated and operated under different agent topologies and highlight learning policies over memory operations. Finally, we review evaluation benchmarks and metrics for assessing memory utility, and outline various open challenges and future directions.

</details>


### [5] [PersonaPlex: Voice and Role Control for Full Duplex Conversational Speech Models](https://arxiv.org/abs/2602.06053)
*Rajarshi Roy,Jonathan Raiman,Sang-gil Lee,Teodor-Dumitru Ene,Robert Kirby,Sungwon Kim,Jaehyeon Kim,Bryan Catanzaro*

Main category: cs.CL

TL;DR: PersonaPlex是一个双工对话语音模型，通过混合系统提示（角色条件+文本提示）和语音克隆技术，支持多角色和个性化语音交互，在角色遵循、说话人相似度、延迟和自然度方面超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有双工语音模型局限于固定角色和声音，无法支持结构化、角色驱动的实际应用和个性化交互，限制了其在真实场景中的应用。

Method: 1. 引入混合系统提示：结合角色条件与文本提示，以及通过语音样本进行语音克隆
2. 使用开源大语言模型和TTS模型生成大规模合成数据集（配对提示和用户-智能体对话）
3. 扩展Full-Duplex-Bench基准测试，从单一助手角色扩展到多角色客服场景

Result: PersonaPlex在角色条件行为、语音条件语音和自然对话响应性方面表现优异，在角色遵循、说话人相似度、延迟和自然度方面超越了最先进的双工语音模型和基于大语言模型的混合语音系统。

Conclusion: PersonaPlex通过混合系统提示和语音克隆技术，成功实现了支持多角色和个性化语音交互的双工对话语音模型，为结构化、角色驱动的实际应用提供了有效解决方案。

Abstract: Recent advances in duplex speech models have enabled natural, low-latency speech-to-speech interactions. However, existing models are restricted to a fixed role and voice, limiting their ability to support structured, role-driven real-world applications and personalized interactions. In this work, we introduce PersonaPlex, a duplex conversational speech model that incorporates hybrid system prompts, combining role conditioning with text prompts and voice cloning with speech samples. PersonaPlex is trained on a large-scale synthetic dataset of paired prompts and user-agent conversations, generated with open-source large language models (LLM) and text-to-speech (TTS) models. To evaluate role conditioning in real-world settings, we extend the Full-Duplex-Bench benchmark beyond a single assistant role to multi-role customer service scenarios. Experiments show that PersonaPlex achieves strong role-conditioned behavior, voice-conditioned speech, and natural conversational responsiveness, surpassing state-of-the-art duplex speech models and hybrid large language model-based speech systems in role adherence, speaker similarity, latency, and naturalness.

</details>


### [6] [What Is Novel? A Knowledge-Driven Framework for Bias-Aware Literature Originality Evaluation](https://arxiv.org/abs/2602.06054)
*Abeer Mostafa,Thi Huyen Nguyen,Zahra Ahmadi*

Main category: cs.CL

TL;DR: 基于80K同行评审数据，开发了一个文献感知的新颖性评估框架，通过结构化对比现有研究来学习人类评审员的新颖性判断


<details>
  <summary>Details</summary>
Motivation: 研究新颖性评估是同行评审的核心但高度主观的方面，通常基于隐式判断和对先前工作的不完全比较，需要更客观、一致的方法

Method: 使用80K新颖性标注的AI会议评审报告，微调大语言模型学习评审员的新颖性评估行为；提取研究论文的结构化表示（思想、方法、主张），检索语义相关论文，构建相似度图进行细粒度概念级比较

Result: 系统能够生成校准的新颖性分数和类人的解释性评估，相对于现有方法减少了高估并提高了评估一致性

Conclusion: 该框架通过结构化对比现有研究和学习人类评审行为，为研究新颖性评估提供了更客观、一致的方法，有望改进同行评审过程

Abstract: Assessing research novelty is a core yet highly subjective aspect of peer review, typically based on implicit judgment and incomplete comparison to prior work. We introduce a literature-aware novelty assessment framework that explicitly learns how humans judge novelty from peer-review reports and grounds these judgments in structured comparison to existing research. Using nearly 80K novelty-annotated reviews from top-tier AI conferences, we fine-tune a large language model to capture reviewer-aligned novelty evaluation behavior. For a given manuscript, the system extracts structured representations of its ideas, methods, and claims, retrieves semantically related papers, and constructs a similarity graph that enables fine-grained, concept-level comparison to prior work. Conditioning on this structured evidence, the model produces calibrated novelty scores and human-like explanatory assessments, reducing overestimation and improving consistency relative to existing approaches.

</details>


### [7] [Quantifying and Attributing Polarization to Annotator Groups](https://arxiv.org/abs/2602.06055)
*Dimitris Tsirmpas,John Pavlopoulos*

Main category: cs.CL

TL;DR: 提出了一种用于评估标注者群体间极化程度的量化指标及显著性检验方法，适用于不平衡群体和多标签场景，应用于仇恨言论和毒性检测数据集发现种族是主要极化因素。


<details>
  <summary>Details</summary>
Motivation: 现有标注一致性指标不适合群体间分析，对群体规模不平衡敏感，且仅限于单标注设置，无法满足毒性检测等主观任务的需求。

Method: 提出量化极化指标及统计显著性检验，能够直接比较不平衡的社会人口学和意识形态子群体，支持多标签设置分析。

Result: 应用于三个仇恨言论数据集和一个毒性检测数据集发现：1) 种族是仇恨言论任务中持续且强烈的极化因素；2) 宗教标注者内部基本一致但与其他标注者存在分歧；3) 教育程度较低的标注者更主观，高教育程度者内部一致性更高。

Conclusion: 该指标能够有效评估不同子群体间的标注极化模式，提供了获得稳健结果所需的最小标注者数量估计，并开源了Python实现库。

Abstract: Current annotation agreement metrics are not well-suited for inter-group analysis, are sensitive to group size imbalances and restricted to single-annotation settings. These restrictions render them insufficient for many subjective tasks such as toxicity and hate-speech detection. For this reason, we introduce a quantifiable metric, paired with a statistical significance test, that attributes polarization to various annotator groups. Our metric enables direct comparisons between heavily imbalanced sociodemographic and ideological subgroups across different datasets and tasks, while also enabling analysis on multi-label settings. We apply this metric to three datasets on hate speech, and one on toxicity detection, discovering that: (1) Polarization is strongly and persistently attributed to annotator race, especially on the hate speech task. (2) Religious annotators do not fundamentally disagree with each other, but do with other annotators, a trend that is gradually diminished and then reversed with irreligious annotators. (3) Less educated annotators are more subjective, while educated ones tend to broadly agree more between themselves. Overall, our results reflect current findings around annotation patterns for various subgroups. Finally, we estimate the minimum number of annotators needed to obtain robust results, and provide an open-source Python library that implements our metric.

</details>


### [8] [Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding](https://arxiv.org/abs/2602.06161)
*Yanzheng Xiang,Lan Wei,Yizhen Yao,Qinglin Zhu,Hanqi Yan,Chen Jin,Philip Alexander Teare,Dandan Zhang,Lin Gui,Amrutha Saseendran,Yulan He*

Main category: cs.CL

TL;DR: COVER提出了一种新的并行扩散解码验证方法，通过KV缓存覆盖在单次前向传播中同时进行验证和稳定生成，显著减少了不必要的修订并加速解码。


<details>
  <summary>Details</summary>
Motivation: 现有的可撤销解码方法存在"翻转振荡"问题，即令牌被重新掩码后又恢复原状，这削弱了并行生成的条件上下文，并消耗了修订预算却几乎没有实际进展。

Method: COVER采用KV缓存覆盖技术，在单次前向传播中构建两个注意力视图：对选定的种子位置进行掩码验证，同时将它们的KV状态注入其他查询以保留上下文信息，并通过闭式对角线校正防止自泄漏。此外，COVER使用稳定性感知分数来优先选择种子，并自适应调整每步验证的种子数量。

Result: 在多个基准测试中，COVER显著减少了不必要的修订，在保持输出质量的同时实现了更快的解码速度。

Conclusion: COVER通过KV缓存覆盖和稳定性感知种子选择，有效解决了并行扩散解码中的翻转振荡问题，实现了更高效的解码过程。

Abstract: Parallel diffusion decoding can accelerate diffusion language model inference by unmasking multiple tokens per step, but aggressive parallelism often harms quality. Revocable decoding mitigates this by rechecking earlier tokens, yet we observe that existing verification schemes frequently trigger flip-flop oscillations, where tokens are remasked and later restored unchanged. This behaviour slows inference in two ways: remasking verified positions weakens the conditioning context for parallel drafting, and repeated remask cycles consume the revision budget with little net progress. We propose COVER (Cache Override Verification for Efficient Revision), which performs leave-one-out verification and stable drafting within a single forward pass. COVER constructs two attention views via KV cache override: selected seeds are masked for verification, while their cached key value states are injected for all other queries to preserve contextual information, with a closed form diagonal correction preventing self leakage at the seed positions. COVER further prioritises seeds using a stability aware score that balances uncertainty, downstream influence, and cache drift, and it adapts the number of verified seeds per step. Across benchmarks, COVER markedly reduces unnecessary revisions and yields faster decoding while preserving output quality.

</details>


### [9] [Uncertainty Drives Social Bias Changes in Quantized Large Language Models](https://arxiv.org/abs/2602.06181)
*Stanley Z. Hua,Sanae Lotfi,Irene Y. Chen*

Main category: cs.CL

TL;DR: 量化后训练会改变大语言模型的社会偏见模式，即使总体指标不变，也会出现量化诱导的偏见翻转现象，对不同的社会群体产生不对称影响。


<details>
  <summary>Details</summary>
Motivation: 虽然后训练量化可以降低大语言模型的计算成本，但人们对其如何改变模型的社会偏见了解不足。现有研究主要依赖聚合指标，可能掩盖量化对偏见模式的复杂影响。

Method: 研究者对50个量化模型进行了大规模研究，使用PostTrainingBiasBench统一基准（包含13个封闭式和开放式偏见数据集）。他们分析了量化强度（4位和8位）、模型大小和模型系列的影响，特别关注了量化诱导的偏见翻转现象。

Result: 研究发现：1）量化会导致高达21%的响应在偏见和无偏见状态之间翻转，尽管总体偏见分数不变；2）这种翻转主要由模型不确定性驱动，高不确定性响应翻转可能性是低不确定性的3-11倍；3）4位量化模型的行为变化是8位模型的4-6倍；4）偏见变化对人口群体产生不对称影响，某些群体的偏见恶化高达18.6%，而其他群体改善达14.1%；5）更大模型没有一致的鲁棒性优势，群体特定变化在不同模型系列中不可预测。

Conclusion: 压缩从根本上改变了偏见模式，导致聚合指标具有误导性。为确保实际应用的可靠性，需要进行关键的后量化评估和干预措施。

Abstract: Post-training quantization reduces the computational cost of large language models but fundamentally alters their social biases in ways that aggregate metrics fail to capture. We present the first large-scale study of 50 quantized models evaluated on PostTrainingBiasBench, a unified benchmark of 13 closed- and open-ended bias datasets. We identify a phenomenon we term quantization-induced masked bias flipping, in which up to 21% of responses flip between biased and unbiased states after quantization, despite showing no change in aggregate bias scores. These flips are strongly driven by model uncertainty, where the responses with high uncertainty are 3-11x more likely to change than the confident ones. Quantization strength amplifies this effect, with 4-bit quantized models exhibiting 4-6x more behavioral changes than 8-bit quantized models. Critically, these changes create asymmetric impacts across demographic groups, where bias can worsen by up to 18.6% for some groups while improving by 14.1% for others, yielding misleadingly neutral aggregate outcomes. Larger models show no consistent robustness advantage, and group-specific shifts vary unpredictably across model families. Our findings demonstrate that compression fundamentally alters bias patterns, requiring crucial post-quantization evaluation and interventions to ensure reliability in practice.

</details>


### [10] [BenchMarker: An Education-Inspired Toolkit for Highlighting Flaws in Multiple-Choice Benchmarks](https://arxiv.org/abs/2602.06221)
*Nishant Balepur,Bhavya Rajasekaran,Jane Oh,Michael Xie,Atrey Desai,Vipul Gupta,Steven James Moore,Eunsol Choi,Rachel Rudinger,Jordan Lee Boyd-Graber*

Main category: cs.CL

TL;DR: 开发了BenchMarker工具包，使用LLM法官检测多选题中的三种常见缺陷：污染、捷径和写作错误，揭示了这些缺陷会影响NLP评估的准确性。


<details>
  <summary>Details</summary>
Motivation: NLP领域的多选题问答（MCQA）基准测试缺乏严格的质量控制，现有基准中存在多种缺陷，影响了评估的可靠性和有效性。

Method: 提出了BenchMarker工具包，灵感来源于教育研究，使用LLM法官来标记三种常见MCQ缺陷：1)污染 - 题目在网上完全重现；2)捷径 - 选项中的线索使猜测成为可能；3)写作错误 - 基于19条教育规则的语法/结构问题。

Result: 验证了BenchMarker的人工标注一致性，并审计了12个基准测试，发现：污染题目会虚高准确率，写作错误会降低准确率并改变排名；先前的基准修复虽然解决了目标问题，但意外引入了新的缺陷。

Conclusion: MCQ中的缺陷会降低NLP评估的质量，但教育研究为解决这些问题提供了途径。发布BenchMarker工具包以连接两个领域并改进MCQA基准设计。

Abstract: Multiple-choice question answering (MCQA) is standard in NLP, but benchmarks lack rigorous quality control. We present BenchMarker, an education-inspired toolkit using LLM judges to flag three common MCQ flaws: 1) contamination - items appearing exactly online; 2) shortcuts - cues in the choices that enable guessing; and 3) writing errors - structural/grammatical issues based on a 19-rule education rubric. We validate BenchMarker with human annotations, then run the tool to audit 12 benchmarks, revealing: 2) contaminated MCQs tend to inflate accuracy, while writing errors tend to lower it and change rankings beyond random; and 3) prior benchmark repairs address their targeted issues (i.e., lowering accuracy with LLM-written distractors), but inadvertently add new flaws (i.e. implausible distractors, many correct answers). Overall, flaws in MCQs degrade NLP evaluation, but education research offers a path forward. We release BenchMarker to bridge the fields and improve MCQA benchmark design.

</details>


### [11] [Can One-sided Arguments Lead to Response Change in Large Language Models?](https://arxiv.org/abs/2602.06260)
*Pedro Cisneros-Velarde*

Main category: cs.CL

TL;DR: LLMs可以通过仅提供单方面论点来被引导至特定观点，即使面对争议性问题


<details>
  <summary>Details</summary>
Motivation: LLMs在处理争议性问题时，通常会提供平衡答案、采取单一对齐观点或拒绝回答。研究者想知道是否可以通过简单直观的方式（仅提供支持特定观点的单方面论点）来引导LLM的初始响应。

Method: 构建小型数据集，系统研究三个维度：(i) LLM响应中被诱导的立场，(ii) 争议性问题的表述方式，(iii) 论点的呈现方式。考察不同模型、论点数量和主题下的意见引导效果。

Result: 研究发现意见引导在所有三个维度上都发生，适用于不同模型、论点数量和主题。切换到其他论点会一致地减少意见引导。

Conclusion: 仅提供单方面论点就能有效引导LLM在争议性问题上的立场，这表明LLM容易受到论证偏见的影响，需要注意其在敏感话题上的潜在操纵风险。

Abstract: Polemic questions need more than one viewpoint to express a balanced answer. Large Language Models (LLMs) can provide a balanced answer, but also take a single aligned viewpoint or refuse to answer. In this paper, we study if such initial responses can be steered to a specific viewpoint in a simple and intuitive way: by only providing one-sided arguments supporting the viewpoint. Our systematic study has three dimensions: (i) which stance is induced in the LLM response, (ii) how the polemic question is formulated, (iii) how the arguments are shown. We construct a small dataset and remarkably find that opinion steering occurs across (i)-(iii) for diverse models, number of arguments, and topics. Switching to other arguments consistently decreases opinion steering.

</details>


### [12] [Is my model "mind blurting"? Interpreting the dynamics of reasoning tokens with Recurrence Quantification Analysis (RQA)](https://arxiv.org/abs/2602.06266)
*Quoc Tuan Pham,Mehdi Jafari,Flora Salim*

Main category: cs.CL

TL;DR: 本文提出使用递归量化分析（RQA）作为分析大型推理模型测试时计算的非文本方法，通过将token生成视为动力系统，提取隐藏嵌入并分析其轨迹，以量化推理链中的重复和停滞模式。


<details>
  <summary>Details</summary>
Motivation: 当前分析大型推理模型推理行为主要依赖生成的文本，但这种方法越来越不实用且不可靠。响应长度常被用作推理努力的粗略代理，但无法捕捉思维链（CoT）或生成token的动态和有效性。

Method: 将token生成视为动力系统，在每个生成步骤提取隐藏嵌入，对得到的轨迹应用递归量化分析（RQA）。使用确定性、层流性等RQA指标来量化模型潜在表示中的重复和停滞模式。

Result: 在DeepSeek-R1-Distill的3,600个生成轨迹分析中，RQA捕获了响应长度无法反映的信号，并将任务复杂度预测能力提高了8%。

Conclusion: RQA可作为研究推理模型测试时扩展中潜在token生成动态的原则性工具，提供比传统文本分析更深入的理解。

Abstract: Test-time compute is central to large reasoning models, yet analysing their reasoning behaviour through generated text is increasingly impractical and unreliable. Response length is often used as a brute proxy for reasoning effort, but this metric fails to capture the dynamics and effectiveness of the Chain of Thoughts (CoT) or the generated tokens. We propose Recurrence Quantification Analysis (RQA) as a non-textual alternative for analysing model's reasoning chains at test time. By treating token generation as a dynamical system, we extract hidden embeddings at each generation step and apply RQA to the resulting trajectories. RQA metrics, including Determinism and Laminarity, quantify patterns of repetition and stalling in the model's latent representations. Analysing 3,600 generation traces from DeepSeek-R1-Distill, we show that RQA captures signals not reflected by response length, but also substantially improves prediction of task complexity by 8\%. These results help establish RQA as a principled tool for studying the latent token generation dynamics of test-time scaling in reasoning models.

</details>


### [13] [MPIB: A Benchmark for Medical Prompt Injection Attacks and Clinical Safety in LLMs](https://arxiv.org/abs/2602.06268)
*Junhyeok Lee,Han Jang,Kyu Sung Choi*

Main category: cs.CL

TL;DR: 该论文提出了医疗提示注入基准（MPIB），用于评估大型语言模型和检索增强生成系统在临床环境中的安全性，重点关注直接和间接提示注入攻击带来的临床风险。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型和检索增强生成系统在临床工作流程中的广泛应用，提示注入攻击可能导致临床不安全或误导性输出，但缺乏系统评估这些系统临床安全性的基准。

Method: 开发了包含9,697个精心构建实例的MPIB数据集，采用多阶段质量控制和临床安全检查。引入临床伤害事件率（CHER）来量化高严重性临床伤害事件，并与攻击成功率（ASR）一起报告，以区分指令遵从性和下游患者风险。

Result: 评估发现ASR和CHER可能存在显著差异，鲁棒性关键取决于对抗性指令出现在用户查询中还是检索上下文中。提供了全面的评估代码、对抗性基线和文档。

Conclusion: MPIB为临床提示注入提供了系统化、可重复的研究框架，强调了区分攻击成功率和实际临床风险的重要性，有助于提高临床AI系统的安全性评估。

Abstract: Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems are increasingly integrated into clinical workflows; however, prompt injection attacks can steer these systems toward clinically unsafe or misleading outputs. We introduce the Medical Prompt Injection Benchmark (MPIB), a dataset-and-benchmark suite for evaluating clinical safety under both direct prompt injection and indirect, RAG-mediated injection across clinically grounded tasks. MPIB emphasizes outcome-level risk via the Clinical Harm Event Rate (CHER), which measures high-severity clinical harm events under a clinically grounded taxonomy, and reports CHER alongside Attack Success Rate (ASR) to disentangle instruction compliance from downstream patient risk. The benchmark comprises 9,697 curated instances constructed through multi-stage quality gates and clinical safety linting. Evaluating MPIB across a diverse set of baseline LLMs and defense configurations, we find that ASR and CHER can diverge substantially, and that robustness depends critically on whether adversarial instructions appear in the user query or in retrieved context. We release MPIB with evaluation code, adversarial baselines, and comprehensive documentation to support reproducible and systematic research on clinical prompt injection. Code and data are available at GitHub (code) and Hugging Face (data).

</details>


### [14] [VowelPrompt: Hearing Speech Emotions from Text via Vowel-level Prosodic Augmentation](https://arxiv.org/abs/2602.06270)
*Yancheng Wang,Osama Hanna,Ruiming Xie,Xianfeng Rui,Maohao Shen,Xuedong Zhang,Christian Fuegen,Jilong Wu,Debjyoti Paul,Arthur Guo,Zhihong Lei,Ozlem Kalinli,Qing He,Yingzhen Yang*

Main category: cs.CL

TL;DR: VowelPrompt是一个基于语言学的框架，通过提取可解释的元音级韵律线索来增强LLM的情感识别能力，并在多领域评测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 语音情感识别需要同时理解语言内容和声学特征，但现有LLM主要关注文本转录，忽略了精细的韵律信息，这限制了其效果和可解释性。

Method: 1. 基于语音学证据提取元音段的音高、能量和时长描述符；2. 将这些特征转换为自然语言描述；3. 采用两阶段适应方法：监督微调后接基于可验证奖励的强化学习（RLVR），使用GRPO实现。

Result: 在多个基准数据集上的广泛评估表明，VowelPrompt在零样本、微调、跨领域和跨语言条件下均优于最先进的情感识别方法，并能生成基于上下文语义和精细韵律结构的可解释解释。

Conclusion: VowelPrompt通过将精细的元音级韵律信息整合到LLM推理中，显著提升了语音情感识别的性能和可解释性，为多模态情感理解提供了有效的语言学基础框架。

Abstract: Emotion recognition in speech presents a complex multimodal challenge, requiring comprehension of both linguistic content and vocal expressivity, particularly prosodic features such as fundamental frequency, intensity, and temporal dynamics. Although large language models (LLMs) have shown promise in reasoning over textual transcriptions for emotion recognition, they typically neglect fine-grained prosodic information, limiting their effectiveness and interpretability. In this work, we propose VowelPrompt, a linguistically grounded framework that augments LLM-based emotion recognition with interpretable, fine-grained vowel-level prosodic cues. Drawing on phonetic evidence that vowels serve as primary carriers of affective prosody, VowelPrompt extracts pitch-, energy-, and duration-based descriptors from time-aligned vowel segments, and converts these features into natural language descriptions for better interpretability. Such a design enables LLMs to jointly reason over lexical semantics and fine-grained prosodic variation. Moreover, we adopt a two-stage adaptation procedure comprising supervised fine-tuning (SFT) followed by Reinforcement Learning with Verifiable Reward (RLVR), implemented via Group Relative Policy Optimization (GRPO), to enhance reasoning capability, enforce structured output adherence, and improve generalization across domains and speaker variations. Extensive evaluations across diverse benchmark datasets demonstrate that VowelPrompt consistently outperforms state-of-the-art emotion recognition methods under zero-shot, fine-tuned, cross-domain, and cross-linguistic conditions, while enabling the generation of interpretable explanations that are jointly grounded in contextual semantics and fine-grained prosodic structure.

</details>


### [15] [RoPE-LIME: RoPE-Space Locality + Sparse-K Sampling for Efficient LLM Attribution](https://arxiv.org/abs/2602.06275)
*Isaac Picov,Ritesh Goru*

Main category: cs.CL

TL;DR: RoPE-LIME 是一个用于解释闭源大语言模型输出的方法，通过使用开源代理模型和概率目标来计算token级归因，同时引入RoPE嵌入空间的局部性核和高效的稀疏K采样策略来提升解释效果。


<details>
  <summary>Details</summary>
Motivation: 解释闭源LLM输出具有挑战性，因为API访问阻止了基于梯度的归因方法，而依赖重新生成文本的扰动方法既昂贵又嘈杂。

Method: 1. 使用较小的开源代理模型计算固定输出的token级归因；2. 基于RoPE嵌入空间的松弛词移动距离构建局部性核，确保掩码下的稳定相似性；3. 提出稀疏K采样策略，在有限预算下提高交互覆盖率。

Result: 在HotpotQA（句子特征）和手工标记的MMLU子集（单词特征）上的实验表明，RoPE-LIME比留一采样产生更多信息量的归因，优于gSMILE，同时显著减少了闭源模型的API调用。

Conclusion: RoPE-LIME提供了一种有效解释闭源LLM输出的方法，通过解耦推理和解释过程，结合新颖的相似性度量和采样策略，在保持解释质量的同时降低了计算成本。

Abstract: Explaining closed-source LLM outputs is challenging because API access prevents gradient-based attribution, while perturbation methods are costly and noisy when they depend on regenerated text. We introduce RoPE-LIME, an open-source extension of gSMILE that decouples reasoning from explanation: given a fixed output from a closed model, a smaller open-source surrogate computes token-level attributions from probability-based objectives (negative log-likelihood and divergence targets) under input perturbations. RoPE-LIME incorporates (i) a locality kernel based on Relaxed Word Mover's Distance computed in RoPE embedding space for stable similarity under masking, and (ii) Sparse-K sampling, an efficient perturbation strategy that improves interaction coverage under limited budgets. Experiments on HotpotQA (sentence features) and a hand-labeled MMLU subset (word features) show that RoPE-LIME produces more informative attributions than leave-one-out sampling and improves over gSMILE while substantially reducing closed-model API calls.

</details>


### [16] [Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math](https://arxiv.org/abs/2602.06291)
*Guijin Son,Donghun Yang,Hitesh Laxmichand Patel,Hyunwoo Ko,Amit Agarwal,Sunghee Ahn,Kyong-Ha Lee,Youngjae Yu*

Main category: cs.CL

TL;DR: 提出了一种基于后果效用的无监督评估方法，通过测试候选方案在解决相关可验证问题时的价值来评分，在数学推理任务中超越了传统奖励模型和LLM评判器。


<details>
  <summary>Details</summary>
Motivation: 当前推理模型能生成研究级数学问题的可能解法，但验证过程消耗专家时间，成为瓶颈。作者假设有意义的解决方案应包含足够的方法层面信息，当应用于相关问题时，应比错误方案带来更好的下游性能。

Method: 提出了Consequence-Based Utility（基于后果的效用）评估方法，这是一种无需人工标注的评估器。通过将候选方案作为上下文示例，测试其在解决相关可验证问题时的价值来评分。评估使用研究级数学问题数据集，每个问题包含一个专家编写的解决方案和九个LLM生成的解决方案。

Result: Consequence-Based Utility在排名质量上一致优于奖励模型、生成式奖励模型和LLM评判器。对于GPT-OSS-120B，将Acc@1从67.2提升到76.3，AUC从71.4提升到79.6；在GPT-OSS-20B上AUC从69.0提升到79.2。相比LLM评判器，它展现出更大的求解器-评估器差距，即使在底层求解器经常失败的实例上也能保持更强的正确-错误区分能力。

Conclusion: 基于后果的效用评估方法为研究级数学推理提供了一种有效的无监督评估方案，通过利用候选方案在相关任务上的实用性来评估其质量，显著超越了现有评估方法。

Abstract: Recent progress in reasoning models suggests that generating plausible attempts for research-level mathematics may be within reach, but verification remains a bottleneck, consuming scarce expert time. We hypothesize that a meaningful solution should contain enough method-level information that, when applied to a neighborhood of related questions, it should yield better downstream performance than incorrect solutions. Building on this idea, we propose \textbf{Consequence-Based Utility}, an oracle-free evaluator that scores each candidate by testing its value as an in-context exemplar in solving related yet verifiable questions. Our approach is evaluated on an original set of research-level math problems, each paired with one expert-written solution and nine LLM-generated solutions. Notably, Consequence-Based Utility consistently outperforms reward models, generative reward models, and LLM judges on ranking quality. Specifically, for GPT-OSS-120B, it improves Acc@1 from 67.2 to 76.3 and AUC from 71.4 to 79.6, with similarly large AUC gains on GPT-OSS-20B (69.0 to 79.2). Furthermore, compared to LLM-Judges, it also exhibits a larger solver-evaluator gap, maintaining a stronger correct-wrong separation even on instances where the underlying solver often fails to solve.

</details>


### [17] [Lost in Speech: Benchmarking, Evaluation, and Parsing of Spoken Code-Switching Beyond Standard UD Assumptions](https://arxiv.org/abs/2602.06307)
*Nemika Tyagi,Holly Hendrix,Nelvin Licona-Guevara,Justin Mackie,Phanos Kareen,Muhammad Imran,Megan Michelle Smith,Tatiana Gallego Hernande,Chitta Baral,Olga Kellert*

Main category: cs.CL

TL;DR: 针对口语语码转换的句法分析挑战，提出了一个系统导向的方法，包括新的评估基准和解析框架。


<details>
  <summary>Details</summary>
Motivation: 口语语码转换在句法分析上面临独特挑战，包括不流畅、重复、省略和话语驱动的结构，这些违反了通用依存关系的标准假设，导致现有解析器和大型语言模型在口语数据上表现不佳。

Method: 提出了一个系统导向的方法：1）引入基于语言学的口语语码转换现象分类法；2）创建专家标注的黄金基准SpokeBench；3）提出模糊感知的评估指标FLEX-UD；4）开发解耦的代理解析框架DECAP。

Result: DECAP框架在不重新训练的情况下产生了更鲁棒和可解释的解析结果，相比现有解析技术提高了52.6%。FLEX-UD评估显示了标准指标掩盖的定性改进。

Conclusion: 口语语码转换解析需要超越标准UD假设的新方法。提出的系统导向方法（包括新的基准、评估指标和解析框架）能够显著改善口语结构分析，为处理复杂口语现象提供了有效解决方案。

Abstract: Spoken code-switching (CSW) challenges syntactic parsing in ways not observed in written text. Disfluencies, repetition, ellipsis, and discourse-driven structure routinely violate standard Universal Dependencies (UD) assumptions, causing parsers and large language models (LLMs) to fail despite strong performance on written data. These failures are compounded by rigid evaluation metrics that conflate genuine structural errors with acceptable variation. In this work, we present a systems-oriented approach to spoken CSW parsing. We introduce a linguistically grounded taxonomy of spoken CSW phenomena and SpokeBench, an expert-annotated gold benchmark designed to test spoken-language structure beyond standard UD assumptions. We further propose FLEX-UD, an ambiguity-aware evaluation metric, which reveals that existing parsing techniques perform poorly on spoken CSW by penalizing linguistically plausible analyses as errors. We then propose DECAP, a decoupled agentic parsing framework that isolates spoken-phenomena handling from core syntactic analysis. Experiments show that DECAP produces more robust and interpretable parses without retraining and achieves up to 52.6% improvements over existing parsing techniques. FLEX-UD evaluations further reveal qualitative improvements that are masked by standard metrics.

</details>


### [18] [Can Post-Training Transform LLMs into Causal Reasoners?](https://arxiv.org/abs/2602.06337)
*Junqi Chen,Sirui Chen,Chaochao Lu*

Main category: cs.CL

TL;DR: 通过后训练增强大语言模型的因果推理能力，14B参数模型在CaLM基准上达到93.5%准确率，超越OpenAI o3的55.4%。


<details>
  <summary>Details</summary>
Motivation: 因果推理对决策至关重要，但非专家难以掌握。虽然大语言模型在该领域有潜力，但其精确因果估计能力有限，且后训练对这些能力的影响研究不足。

Method: 引入CauGym数据集，包含7个核心因果任务用于训练和5个多样化测试集。系统评估5种后训练方法：SFT、DPO、KTO、PPO和GRPO。

Result: 适当的后训练使较小的大语言模型在因果推理上具有竞争力，常超越更大模型。14B参数模型在CaLM基准上达到93.5%准确率，远超OpenAI o3的55.4%。后训练模型在分布偏移和噪声数据等现实条件下表现出强泛化性和鲁棒性。

Conclusion: 提供了首个系统性证据，表明有针对性的后训练可以产生可靠且鲁棒的大语言模型因果推理器。数据与GRPO模型已开源。

Abstract: Causal inference is essential for decision-making but remains challenging for non-experts. While large language models (LLMs) show promise in this domain, their precise causal estimation capabilities are still limited, and the impact of post-training on these abilities is insufficiently explored. This paper examines the extent to which post-training can enhance LLMs' capacity for causal inference. We introduce CauGym, a comprehensive dataset comprising seven core causal tasks for training and five diverse test sets. Using this dataset, we systematically evaluate five post-training approaches: SFT, DPO, KTO, PPO, and GRPO. Across five in-domain and four existing benchmarks, our experiments demonstrate that appropriate post-training enables smaller LLMs to perform causal inference competitively, often surpassing much larger models. Our 14B parameter model achieves 93.5% accuracy on the CaLM benchmark, compared to 55.4% by OpenAI o3. Furthermore, the post-trained LLMs exhibit strong generalization and robustness under real-world conditions such as distribution shifts and noisy data. Collectively, these findings provide the first systematic evidence that targeted post-training can produce reliable and robust LLM-based causal reasoners. Our data and GRPO-model are available at https://github.com/OpenCausaLab/CauGym.

</details>


### [19] [SHINE: A Scalable In-Context Hypernetwork for Mapping Context to LoRA in a Single Pass](https://arxiv.org/abs/2602.06358)
*Yewei Liu,Xiyuan Wang,Yansheng Mao,Yoav Gelbery,Haggai Maron,Muhan Zhang*

Main category: cs.CL

TL;DR: SHINE是一种可扩展的超网络，能够将不同的上下文映射为高质量LoRA适配器，通过单次前向传播将上下文知识转化为参数知识，显著节省时间、计算和内存成本。


<details>
  <summary>Details</summary>
Motivation: 现有的超网络方法在表达能力和可扩展性方面存在限制，需要更高效的方法将上下文知识转化为LLM参数知识，避免直接访问上下文进行复杂问答任务。

Method: 提出SHINE超网络架构，重用冻结LLM参数进行上下文超网络设计，引入架构创新，通过预训练和指令微调管道训练超网络，从不同上下文单次生成高质量LoRA适配器。

Result: 在各种任务上取得优异结果，相比基于SFT的LLM适配方法大大节省时间、计算和内存成本，显示出良好的扩展潜力。

Conclusion: SHINE通过创新的超网络设计实现了高效的上下文到参数知识转化，为LLM适配提供了可扩展且高效的解决方案，具有重要的实际应用价值。

Abstract: We propose SHINE (Scalable Hyper In-context NEtwork), a scalable hypernetwork that can map diverse meaningful contexts into high-quality LoRA adapters for large language models (LLM). By reusing the frozen LLM's own parameters in an in-context hypernetwork design and introducing architectural innovations, SHINE overcomes key limitations of prior hypernetworks and achieves strong expressive power with a relatively small number of parameters. We introduce a pretraining and instruction fine-tuning pipeline, and train our hypernetwork to generate high quality LoRA adapters from diverse meaningful contexts in a single forward pass. It updates LLM parameters without any fine-tuning, and immediately enables complex question answering tasks related to the context without directly accessing the context, effectively transforming in-context knowledge to in-parameter knowledge in one pass. Our work achieves outstanding results on various tasks, greatly saves time, computation and memory costs compared to SFT-based LLM adaptation, and shows great potential for scaling. Our code is available at https://github.com/Yewei-Liu/SHINE

</details>


### [20] [Cost-Aware Model Selection for Text Classification: Multi-Objective Trade-offs Between Fine-Tuned Encoders and LLM Prompting in Production](https://arxiv.org/abs/2602.06370)
*Alberto Andres Valdes Gonzalez*

Main category: cs.CL

TL;DR: 该研究系统比较了零/少样本提示的LLM与微调的编码器模型在文本分类任务中的表现，发现微调编码器在成本、延迟和性能方面通常优于LLMs


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在文本分类任务中被广泛采用，但模型选择往往只关注预测性能，忽略了生产系统中的操作约束（如推理延迟和成本）。本研究旨在系统比较不同方法在结构化文本分类中的综合表现。

Method: 使用四个经典基准数据集（IMDB、SST-2、AG News、DBPedia），对比零/少样本提示的LLMs（如GPT-4o、Claude Sonnet 4.5）与完全微调的编码器架构（BERT家族）。评估指标包括预测质量（宏F1）、推理延迟和货币成本，并将模型评估构建为多目标决策问题，使用帕累托前沿投影和参数化效用函数分析权衡。

Result: 微调的编码器模型在分类性能上具有竞争力甚至更优，同时成本比LLMs低1-2个数量级，延迟也更低。LLMs在标准文本分类工作负载中的不加区分使用可能导致次优的系统级结果。

Conclusion: 微调编码器是结构化NLP流程中稳健高效的组件，而LLMs更适合作为混合架构中的补充元素。研究建议根据实际部署需求进行模型选择，并发布了所有代码、数据集和评估协议以支持可复现性。

Abstract: Large language models (LLMs) such as GPT-4o and Claude Sonnet 4.5 have demonstrated strong capabilities in open-ended reasoning and generative language tasks, leading to their widespread adoption across a broad range of NLP applications. However, for structured text classification problems with fixed label spaces, model selection is often driven by predictive performance alone, overlooking operational constraints encountered in production systems.
  In this work, we present a systematic comparison of two contrasting paradigms for text classification: zero- and few-shot prompt-based large language models, and fully fine-tuned encoder-only architectures. We evaluate these approaches across four canonical benchmarks (IMDB, SST-2, AG News, and DBPedia), measuring predictive quality (macro F1), inference latency, and monetary cost.
  We frame model evaluation as a multi-objective decision problem and analyze trade-offs using Pareto frontier projections and a parameterized utility function reflecting different deployment regimes. Our results show that fine-tuned encoder-based models from the BERT family achieve competitive, and often superior, classification performance while operating at one to two orders of magnitude lower cost and latency compared to zero- and few-shot LLM prompting.
  Overall, our findings suggest that indiscriminate use of large language models for standard text classification workloads can lead to suboptimal system-level outcomes. Instead, fine-tuned encoders emerge as robust and efficient components for structured NLP pipelines, while LLMs are better positioned as complementary elements within hybrid architectures. We release all code, datasets, and evaluation protocols to support reproducibility and cost-aware NLP system design.

</details>


### [21] [ReBeCA: Unveiling Interpretable Behavior Hierarchy behind the Iterative Self-Reflection of Language Models with Causal Analysis](https://arxiv.org/abs/2602.06373)
*Tianqiang Yan,Sihan Shang,Yuheng Li,Song Qiu,Hao Peng,Wenjian Luo,Jue Xie,Lizhen Qu,Yuan Gao*

Main category: cs.CL

TL;DR: ReBeCA框架通过因果分析揭示自反思行为的可解释层次结构，发现只有少数语义行为真正影响自反思效果，且更多行为不一定更好。


<details>
  <summary>Details</summary>
Motivation: 当前对语言模型自反思机制的分析大多停留在相关性层面，缺乏可泛化的因果解释，需要更严谨的方法来揭示自反思行为的真正决定因素。

Method: 提出ReBeCA框架，将自反思轨迹建模为因果图，采用三阶段不变因果预测(ICP)流程来分离性能的真正决定因素。

Result: 1)发现自反思行为存在层次结构：语义行为以直接或间接方式影响最终结果；2)只有少数语义行为具有因果普遍性；3)看似积极的多个语义行为组合反而会损害自反思效果。ICP验证显示稀疏因果父节点可获得高达49.6%的结构似然增益。

Conclusion: ReBeCA为解耦自反思动态中的真实因果机制与虚假关联提供了严谨方法，揭示了自反思行为的可解释层次结构，对理解语言模型自反思机制有重要意义。

Abstract: While self-reflection can enhance language model reliability, its underlying mechanisms remain opaque, with existing analyses often yielding correlation-based insights that fail to generalize. To address this, we introduce \textbf{\texttt{ReBeCA}} (self-\textbf{\texttt{Re}}flection \textbf{\texttt{Be}}havior explained through \textbf{\texttt{C}}ausal \textbf{\texttt{A}}nalysis), a framework that unveils the interpretable behavioral hierarchy governing the self-reflection outcome. By modeling self-reflection trajectories as causal graphs, ReBeCA isolates genuine determinants of performance through a three-stage Invariant Causal Prediction (ICP) pipeline. We establish three critical findings: (1) \textbf{Behavioral hierarchy:} Semantic behaviors of the model influence final self-reflection results hierarchically: directly or indirectly; (2) \textbf{Causation matters:} Generalizability in self-reflection effects is limited to just a few semantic behaviors; (3) \textbf{More $\mathbf{\neq}$ better:} The confluence of seemingly positive semantic behaviors, even among direct causal factors, can impair the efficacy of self-reflection. ICP-based verification identifies sparse causal parents achieving up to $49.6\%$ structural likelihood gains, stable across tasks where correlation-based patterns fail. Intervention studies on novel datasets confirm these causal relationships hold out-of-distribution ($p = .013, η^2_\mathrm{p} = .071$). ReBeCA thus provides a rigorous methodology for disentangling genuine causal mechanisms from spurious associations in self-reflection dynamics.

</details>


### [22] [FMBench: Adaptive Large Language Model Output Formatting](https://arxiv.org/abs/2602.06384)
*Yaoting Wang,Yun Zhou,Henghui Ding*

Main category: cs.CL

TL;DR: FMBench：评估大语言模型Markdown格式输出能力的基准测试，通过监督微调和强化学习改进模型格式合规性


<details>
  <summary>Details</summary>
Motivation: 大语言模型在用户界面和系统集成工作流中需要同时满足语义意图和格式约束。Markdown格式在助手、文档和工具增强管道中普遍存在，但容易出现难以检测的细微错误（如破损列表、格式错误的表格、不一致的标题和无效的代码块），这些错误会显著降低下游可用性。

Method: 提出FMBench基准测试，评估模型在多样化指令遵循场景下的Markdown格式输出能力。采用轻量级对齐流程：首先在指令-响应对上进行监督微调（SFT），然后通过强化学习微调优化平衡语义保真度和结构正确性的复合目标。

Result: 在两个模型家族（OpenPangu和Qwen）上的实验表明：SFT持续改善语义对齐，而强化学习在从强SFT策略初始化时，对具有挑战性的Markdown指令提供额外的鲁棒性增益。结果揭示了语义目标和结构目标之间的固有权衡。

Conclusion: FMBench为评估和改进大语言模型的Markdown格式输出能力提供了有效基准。结合SFT和强化学习的对齐流程能够显著提升格式合规性，但需要精心设计奖励函数来平衡语义和结构目标。

Abstract: Producing outputs that satisfy both semantic intent and format constraints is essential for deploying large language models in user-facing and system-integrated workflows. In this work, we focus on Markdown formatting, which is ubiquitous in assistants, documentation, and tool-augmented pipelines but still prone to subtle, hard-to-detect errors (e.g., broken lists, malformed tables, inconsistent headings, and invalid code blocks) that can significantly degrade downstream usability. We present FMBench, a benchmark for adaptive Markdown output formatting that evaluates models under a wide range of instruction-following scenarios with diverse structural requirements. FMBench emphasizes real-world formatting behaviors such as multi-level organization, mixed content (natural language interleaved with lists/tables/code), and strict adherence to user-specified layout constraints. To improve Markdown compliance without relying on hard decoding constraints, we propose a lightweight alignment pipeline that combines supervised fine-tuning (SFT) with reinforcement learning fine-tuning. Starting from a base model, we first perform SFT on instruction-response pairs, and then optimize a composite objective that balances semantic fidelity with structural correctness. Experiments on two model families (OpenPangu and Qwen) show that SFT consistently improves semantic alignment, while reinforcement learning provides additional gains in robustness to challenging Markdown instructions when initialized from a strong SFT policy. Our results also reveal an inherent trade-off between semantic and structural objectives, highlighting the importance of carefully designed rewards for reliable formatted generation. Code is available at: https://github.com/FudanCVL/FMBench.

</details>


### [23] [Stopping Computation for Converged Tokens in Masked Diffusion-LM Decoding](https://arxiv.org/abs/2602.06412)
*Daisuke Oba,Danushka Bollegala,Masahiro Kaneko,Naoaki Okazaki*

Main category: cs.CL

TL;DR: SureLock通过锁定已稳定的未遮蔽位置来减少扩散语言模型的计算开销，将计算复杂度从O(N²d)降低到O(MNd)，在LLaDA-8B上减少30-50%的FLOPs同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的遮蔽扩散语言模型在生成过程中需要为每个token位置重新计算注意力机制和前馈网络，即使许多未遮蔽的token已经稳定不变，这造成了显著的计算浪费。

Method: 提出SureLock方法：当未遮蔽位置的分布在不同步骤间稳定时（sure条件），锁定该位置，跳过其查询投影和前馈子层，同时缓存其注意力键值供其他位置继续关注，从而减少计算量。

Result: 在LLaDA-8B模型上，SureLock相比未锁定采样器减少了30-50%的算法FLOPs，同时保持了可比的生成质量。理论分析表明监控锁定步骤的局部KL散度足以限制最终token概率的偏差。

Conclusion: SureLock是一种高效的扩散语言模型加速方法，通过智能锁定已稳定的token位置，显著减少计算开销而不牺牲生成质量，为大规模语言模型的实际部署提供了实用解决方案。

Abstract: Masked Diffusion Language Models generate sequences via iterative sampling that progressively unmasks tokens. However, they still recompute the attention and feed-forward blocks for every token position at every step -- even when many unmasked tokens are essentially fixed, resulting in substantial waste in compute. We propose SureLock: when the posterior at an unmasked position has stabilized across steps (our sure condition), we lock that position -- thereafter skipping its query projection and feed-forward sublayers -- while caching its attention keys and values so other positions can continue to attend to it. This reduces the dominant per-iteration computational cost from $O(N^2d)$ to $O(MNd)$ where $N$ is the sequence length, $M$ is the number of unlocked token positions, and $d$ is the model dimension. In practice, $M$ decreases as the iteration progresses, yielding substantial savings. On LLaDA-8B, SureLock reduces algorithmic FLOPs by 30--50% relative to the same sampler without locking, while maintaining comparable generation quality. We also provide a theoretical analysis to justify the design rationale of SureLock: monitoring only the local KL at the lock step suffices to bound the deviation in final token probabilities. Our code will be available at https://daioba.github.io/surelock .

</details>


### [24] [On the Wings of Imagination: Conflicting Script-based Multi-role Framework for Humor Caption Generation](https://arxiv.org/abs/2602.06423)
*Wenbo Shang,Yuxi Sun,Jing Ma,Xin Huang*

Main category: cs.CL

TL;DR: 提出HOMER框架，基于GTVH幽默理论，通过多角色LLM协作和幽默检索生成图像幽默字幕，在New Yorker卡通数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态幽默生成（如图像幽默字幕）对LLMs具有挑战性，需要视觉理解、幽默推理和创造性想象。现有基于LLM的方法依赖推理链或自我改进，存在创造力有限和可解释性不足的问题。

Method: 基于GTVH幽默理论，提出幽默理论驱动的多角色LLM协作框架HOMER，包含三个角色：冲突脚本提取器（识别关键脚本对立）、检索增强的分层想象器（识别幽默目标并构建想象树扩展创意空间）、字幕生成器（基于获取知识生成幽默字幕）。

Result: 在两个New Yorker卡通基准数据集上的实验表明，HOMER在多模态幽默字幕生成任务上优于最先进的基线方法和强大的LLM推理策略。

Conclusion: HOMER框架通过结合幽默理论、多角色LLM协作和检索增强，有效解决了多模态幽默生成的挑战，在生成幽默性和多样性方面表现优异。

Abstract: Humor is a commonly used and intricate human language in daily life. Humor generation, especially in multi-modal scenarios, is a challenging task for large language models (LLMs), which is typically as funny caption generation for images, requiring visual understanding, humor reasoning, creative imagination, and so on. Existing LLM-based approaches rely on reasoning chains or self-improvement, which suffer from limited creativity and interpretability. To address these bottlenecks, we develop a novel LLM-based humor generation mechanism based on a fundamental humor theory, GTVH. To produce funny and script-opposite captions, we introduce a humor-theory-driven multi-role LLM collaboration framework augmented with humor retrieval (HOMER). The framework consists of three LLM-based roles: (1) conflicting-script extractor that grounds humor in key script oppositions, forming the basis of caption generation; (2) retrieval-augmented hierarchical imaginator that identifies key humor targets and expands the creative space of them through diverse associations structured as imagination trees; and (3) caption generator that produces funny and diverse captions conditioned on the obtained knowledge. Extensive experiments on two New Yorker Cartoon benchmarking datasets show that HOMER outperforms state-of-the-art baselines and powerful LLM reasoning strategies on multi-modal humor captioning.

</details>


### [25] [Investigating the structure of emotions by analyzing similarity and association of emotion words](https://arxiv.org/abs/2602.06430)
*Fumitaka Iwaki,Tatsuji Takahashi*

Main category: cs.CL

TL;DR: 该研究通过构建和分析情感词语义网络，验证了Plutchik情感轮模型的局部有效性，发现网络结构与情感轮大体相似但存在局部差异。


<details>
  <summary>Details</summary>
Motivation: 在自然语言处理的情感分析研究中，Plutchik情感轮是常用的情感模型，但其有效性尚未得到充分验证。本研究旨在通过语义网络分析方法检验情感轮的结构有效性。

Method: 通过实验收集情感词对的相似性和关联性数据，构建语义网络，然后使用社区检测方法分析网络结构，并与Plutchik情感轮的结构进行比较。

Result: 研究发现，构建的情感语义网络结构与Plutchik情感轮在整体上相似，但在局部存在差异。

Conclusion: Plutchik情感轮模型在整体结构上具有一定有效性，但需要关注其局部差异，这为情感模型的改进提供了实证基础。

Abstract: In the field of natural language processing, some studies have attempted sentiment analysis on text by handling emotions as explanatory or response variables. One of the most popular emotion models used in this context is the wheel of emotion proposed by Plutchik. This model schematizes human emotions in a circular structure, and represents them in two or three dimensions. However, the validity of Plutchik's wheel of emotion has not been sufficiently examined. This study investigated the validity of the wheel by creating and analyzing a semantic networks of emotion words. Through our experiments, we collected data of similarity and association of ordered pairs of emotion words, and constructed networks using these data. We then analyzed the structure of the networks through community detection, and compared it with that of the wheel of emotion. The results showed that each network's structure was, for the most part, similar to that of the wheel of emotion, but locally different.

</details>


### [26] [TrailBlazer: History-Guided Reinforcement Learning for Black-Box LLM Jailbreaking](https://arxiv.org/abs/2602.06440)
*Sung-Hoon Yoon,Ruizhi Qian,Minda Zhao,Weiyue Li,Mengyu Wang*

Main category: cs.CL

TL;DR: 基于历史感知的强化学习越狱框架，通过分析并重加权先前交互中的漏洞信号来提升越狱成功率和查询效率。


<details>
  <summary>Details</summary>
Motivation: 现有越狱技术大多未能有效利用先前交互轮次中暴露的漏洞，导致攻击效率低下且不稳定。由于越狱涉及序列交互，强化学习为此提供了自然框架。

Method: 提出历史感知的RL越狱框架，分析并重加权先前步骤的漏洞信号来指导未来决策；引入基于注意力的重加权机制，突出交互历史中的关键漏洞，实现更高效的探索。

Result: 在AdvBench和HarmBench上的大量实验表明，该方法实现了最先进的越狱性能，同时显著提高了查询效率。

Conclusion: 历史漏洞信号在强化学习驱动的越狱策略中至关重要，为推进大语言模型安全防护的对抗性研究提供了原则性路径。

Abstract: Large Language Models (LLMs) have become integral to many domains, making their safety a critical priority. Prior jailbreaking research has explored diverse approaches, including prompt optimization, automated red teaming, obfuscation, and reinforcement learning (RL) based methods. However, most existing techniques fail to effectively leverage vulnerabilities revealed in earlier interaction turns, resulting in inefficient and unstable attacks. Since jailbreaking involves sequential interactions in which each response influences future actions, reinforcement learning provides a natural framework for this problem. Motivated by this, we propose a history-aware RL-based jailbreak framework that analyzes and reweights vulnerability signals from prior steps to guide future decisions. We show that incorporating historical information alone improves jailbreak success rates. Building on this insight, we introduce an attention-based reweighting mechanism that highlights critical vulnerabilities within the interaction history, enabling more efficient exploration with fewer queries. Extensive experiments on AdvBench and HarmBench demonstrate that our method achieves state-of-the-art jailbreak performance while significantly improving query efficiency. These results underscore the importance of historical vulnerability signals in reinforcement learning-driven jailbreak strategies and offer a principled pathway for advancing adversarial research on LLM safeguards.

</details>


### [27] [CORE: Comprehensive Ontological Relation Evaluation for Large Language Models](https://arxiv.org/abs/2602.06446)
*Satyam Dwivedi,Sanjukta Ghosh,Shivam Dwivedi,Nishi Kumari,Anil Thakur,Anurag Purushottam,Deepak Alok,Praveen Gatla,Manjuprasad B,Bipasha Patgiri*

Main category: cs.CL

TL;DR: CORE评估基准揭示大语言模型在区分相关与无关语义关系方面存在严重缺陷，尤其在无关关系判断上表现极差，存在系统性语义崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估很少测试模型区分有意义语义关系与真正无关关系的能力，这构成了LLM评估和安全的重要前沿问题。

Method: 构建CORE数据集，包含225K多选题覆盖74个学科，并创建203个经过严格验证的基准问题，涵盖24种语义关系类型，确保无关对等比例表示。收集1000+人类参与者的基准数据，测试29个SOTA LLM。

Result: 人类基线准确率92.6%（无关对上95.1%），而29个LLM总体准确率48.25-70.9%，在相关对上表现良好（86.5-100%），但在无关对上严重退化（0-41.35%）。无关对上的预期校准误差增加2-4倍，平均语义崩溃率37.6%。在225K数据集上准确率降至约2%。

Conclusion: 无关关系推理是LLM评估和安全的关键但未被充分评估的前沿领域，LLM在区分语义相关与无关方面存在系统性缺陷，需要新的评估方法和安全措施。

Abstract: Large Language Models (LLMs) perform well on many reasoning benchmarks, yet existing evaluations rarely assess their ability to distinguish between meaningful semantic relations and genuine unrelatedness. We introduce CORE (Comprehensive Ontological Relation Evaluation), a dataset of 225K multiple-choice questions spanning 74 disciplines, together with a general-domain open-source benchmark of 203 rigorously validated questions (Cohen's Kappa = 1.0) covering 24 semantic relation types with equal representation of unrelated pairs. A human baseline from 1,000+ participants achieves 92.6% accuracy (95.1% on unrelated pairs). In contrast, 29 state-of-the-art LLMs achieve 48.25-70.9% overall accuracy, with near-ceiling performance on related pairs (86.5-100%) but severe degradation on unrelated pairs (0-41.35%), despite assigning similar confidence (92-94%). Expected Calibration Error increases 2-4x on unrelated pairs, and a mean semantic collapse rate of 37.6% indicates systematic generation of spurious relations. On the CORE 225K MCQs dataset, accuracy further drops to approximately 2%, highlighting substantial challenges in domain-specific semantic reasoning. We identify unrelatedness reasoning as a critical, under-evaluated frontier for LLM evaluation and safety.

</details>


### [28] [Evaluating an evidence-guided reinforcement learning framework in aligning light-parameter large language models with decision-making cognition in psychiatric clinical reasoning](https://arxiv.org/abs/2602.06449)
*Xinxin Lin,Guangxin Dai,Yi Zhong,Xiang Li,Xue Xiao,Yixin Zhang,Zhengdong Wu,Yongbo Zheng,Runchuan Zhu,Ming Zhao,Huizi Yu,Shuo Wu,Jun Zhao,Lingming Hu,Yumei Wang,Ping Yin,Joey W. Y. Chan,Ngan Yin Chan,Sijing Chen,Yun Kwok Wing,Lin Lu,Xin Ma,Lizhou Fan*

Main category: cs.CL

TL;DR: ClinMPO：通过强化学习框架对齐轻量级LLM与精神科临床推理，在复杂病例诊断上超越医学生表现


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在精神科决策支持中存在幻觉和浅层推理问题，轻量级模型尤其严重。现有训练范式优先语言流畅性而非结构化临床逻辑，导致与专业诊断认知不匹配。

Method: 提出ClinMPO强化学习框架，使用专门奖励模型，该模型基于4,474篇精神科期刊文章构建的循证医学原则数据集独立训练，旨在对齐LLM内部推理与专业精神病学实践。

Result: ClinMPO调优的Qwen3-8B模型在复杂病例诊断准确率达到31.4%，超越人类基准（医学生平均30.8%），在领先大参数LLM持续失败的测试集上表现优异。

Conclusion: 医学证据引导的优化使轻量级LLM能够掌握复杂推理任务，显式认知对齐为可靠、安全的精神科决策支持提供了可扩展路径。

Abstract: Large language models (LLMs) hold transformative potential for medical decision support yet their application in psychiatry remains constrained by hallucinations and superficial reasoning. This limitation is particularly acute in light-parameter LLMs which are essential for privacy-preserving and efficient clinical deployment. Existing training paradigms prioritize linguistic fluency over structured clinical logic and result in a fundamental misalignment with professional diagnostic cognition. Here we introduce ClinMPO, a reinforcement learning framework designed to align the internal reasoning of LLMs with professional psychiatric practice. The framework employs a specialized reward model trained independently on a dataset derived from 4,474 psychiatry journal articles and structured according to evidence-based medicine principles. We evaluated ClinMPO on a unseen subset of the benchmark designed to isolate reasoning capabilities from rote memorization. This test set comprises items where leading large-parameter LLMs consistently fail. We compared the ClinMPO-aligned light LLM performance against a cohort of 300 medical students. The ClinMPO-tuned Qwen3-8B model achieved a diagnostic accuracy of 31.4% and surpassed the human benchmark of 30.8% on these complex cases. These results demonstrate that medical evidence-guided optimization enables light-parameter LLMs to master complex reasoning tasks. Our findings suggest that explicit cognitive alignment offers a scalable pathway to reliable and safe psychiatric decision support.

</details>


### [29] [RelayGen: Intra-Generation Model Switching for Efficient Reasoning](https://arxiv.org/abs/2602.06454)
*Jiwon Song,Yoongon Kim,Jae-Joon Kim*

Main category: cs.CL

TL;DR: RelayGen：一种无需训练的分段级运行时模型切换框架，通过利用长形式推理中的难度变化，动态将低难度段委托给小模型处理，在保持大模型精度的同时显著降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务上表现出色，但生成长推理轨迹的推理时间成本很高。现有效率优化方法要么忽略生成难度在单个输出内的变化，要么依赖监督的令牌级路由导致系统复杂度高。需要一种能够利用推理轨迹中难度变化、降低推理成本的方法。

Method: 1. 使用令牌概率边际分析生成不确定性，发现粗粒度的分段级控制足以捕捉推理轨迹中的难度转换；2. 识别模型特定的切换线索，当检测到向低难度段转换时，动态将后续生成委托给小模型；3. 保持高难度推理在大模型上进行，结合推测性解码进一步加速。

Result: 在多个推理基准测试中，RelayGen显著降低了推理延迟，同时保持了大型模型的大部分准确性。结合推测性解码时，实现了高达2.2倍的端到端加速，准确度下降小于2%，且无需额外训练或学习路由组件。

Conclusion: RelayGen通过利用推理生成中的难度变化，实现了有效的运行时模型切换，在保持性能的同时大幅提升推理效率，为大型推理模型的部署提供了实用的效率优化方案。

Abstract: Large reasoning models (LRMs) achieve strong performance on complex reasoning tasks by generating long, multi-step reasoning trajectories, but inference-time scaling incurs substantial deployment cost. A key challenge is that generation difficulty varies within a single output, whereas existing efficiency-oriented approaches either ignore this intra-generation variation or rely on supervised token-level routing with high system complexity. We present \textbf{RelayGen}, a training-free, segment-level runtime model switching framework that exploits difficulty variation in long-form reasoning. Through offline analysis of generation uncertainty using token probability margins, we show that coarse-grained segment-level control is sufficient to capture difficulty transitions within a reasoning trajectory. RelayGen identifies model-specific switch cues that signal transitions to lower-difficulty segments and dynamically delegates their continuation to a smaller model, while preserving high-difficulty reasoning on the large model. Across multiple reasoning benchmarks, RelayGen substantially reduces inference latency while preserving most of the accuracy of large models. When combined with speculative decoding, RelayGen achieves up to 2.2$\times$ end-to-end speedup with less than 2\% accuracy degradation, without requiring additional training or learned routing components.

</details>


### [30] [Diffusion-State Policy Optimization for Masked Diffusion Language Models](https://arxiv.org/abs/2602.06462)
*Daisuke Oba,Hiroki Furuta,Naoaki Okazaki*

Main category: cs.CL

TL;DR: DiSPO提出了一种针对掩码扩散语言模型的中间信用分配方法，通过分支采样和评分优化中间填充决策，无需额外扩散rollout。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散语言模型通过多步去噪迭代生成，仅基于最终完成度的终端奖励会导致对中间决策的信用分配粗糙，影响学习效率。

Method: 在选定的中间掩码状态，DiSPO通过从rollout缓存的logits中重新采样当前掩码位置的填充来分支，对生成的完成度进行评分，并仅更新新填充的token，无需额外的多步扩散rollout。

Result: 在LLaDA-8B-Instruct上，DiSPO在数学和规划基准测试中始终优于终端反馈的diffu-GRPO基线，在匹配的rollout计算和优化器步骤下表现更优。

Conclusion: DiSPO提供了一种有效的中间信用分配机制，能够直接优化扩散语言模型的中间填充决策，提高学习效率而不增加计算开销。

Abstract: Masked diffusion language models generate by iteratively filling masked tokens over multiple denoising steps, so learning only from a terminal reward on the final completion yields coarse credit assignment over intermediate decisions. We propose DiSPO (Diffusion-State Policy Optimization), a plug-in credit-assignment layer that directly optimizes intermediate filling decisions. At selected intermediate masked states, DiSPO branches by resampling fillings for the currently masked positions from rollout-cached logits, scores the resulting completions, and updates only the newly filled tokens -- without additional multi-step diffusion rollouts. We formalize a fixed-state objective for branched completions and derive a policy-gradient estimator that can be combined with terminal-feedback policy optimization using the same rollouts. On LLaDA-8B-Instruct, DiSPO consistently improves over the terminal-feedback diffu-GRPO baseline on math and planning benchmarks under matched rollout compute and optimizer steps. Our code will be available at https://daioba.github.io/dispo .

</details>


### [31] [Improve Large Language Model Systems with User Logs](https://arxiv.org/abs/2602.06470)
*Changyue Wang,Weihang Su,Qingyao Ai,Yiqun Liu*

Main category: cs.CL

TL;DR: UNO是一个从用户日志驱动优化LLM系统的统一框架，通过将非结构化日志转化为半结构化规则和偏好对，使用查询-反馈聚类管理数据异质性，并量化模型先验知识与日志数据之间的认知差距，从而自适应过滤噪声反馈并构建不同模块来改进LLM响应。


<details>
  <summary>Details</summary>
Motivation: 当前LLM发展面临高质量数据稀缺和计算成本收益递减的约束，而用户交互日志提供了丰富的真实人类反馈和过程知识。然而，从用户日志学习具有挑战性，因为日志是非结构化和嘈杂的，传统LLM系统难以区分有用反馈信号与噪声用户行为，且用户日志收集与模型优化之间的不匹配（如离策略优化问题）加剧了这一困难。

Method: UNO框架包含三个主要步骤：1) 将用户日志提炼为半结构化规则和偏好对；2) 使用查询和反馈驱动的聚类来管理数据异质性；3) 量化模型先验知识与日志数据之间的认知差距，指导LLM系统自适应过滤噪声反馈，并为用户日志中提取的主要经验和反思经验构建不同模块。

Result: 大量实验表明，UNO在有效性和效率方面达到了最先进水平，显著优于检索增强生成（RAG）和基于记忆的基线方法。

Conclusion: UNO提供了一个有效的框架，能够从嘈杂的用户日志中提取有价值的知识，通过结构化处理和认知差距评估来改进LLM系统，为持续学习提供了有前景的解决方案。

Abstract: Scaling training data and model parameters has long driven progress in large language models (LLMs), but this paradigm is increasingly constrained by the scarcity of high-quality data and diminishing returns from rising computational costs. As a result, recent work is increasing the focus on continual learning from real-world deployment, where user interaction logs provide a rich source of authentic human feedback and procedural knowledge. However, learning from user logs is challenging due to their unstructured and noisy nature. Vanilla LLM systems often struggle to distinguish useful feedback signals from noisy user behavior, and the disparity between user log collection and model optimization (e.g., the off-policy optimization problem) further strengthens the problem. To this end, we propose UNO (User log-driveN Optimization), a unified framework for improving LLM systems (LLMsys) with user logs. UNO first distills logs into semi-structured rules and preference pairs, then employs query-and-feedback-driven clustering to manage data heterogeneity, and finally quantifies the cognitive gap between the model's prior knowledge and the log data. This assessment guides the LLMsys to adaptively filter out noisy feedback and construct different modules for primary and reflective experiences extracted from user logs, thereby improving future responses. Extensive experiments show that UNO achieves state-of-the-art effectiveness and efficiency, significantly outperforming Retrieval Augmented Generation (RAG) and memory-based baselines. We have open-sourced our code at https://github.com/bebr2/UNO .

</details>


### [32] [Revisiting the Shape Convention of Transformer Language Models](https://arxiv.org/abs/2602.06471)
*Feng-Ting Liao,Meng-Hsi Chen,Guan-Ting Yi,Da-shan Shiu*

Main category: cs.CL

TL;DR: 该论文挑战了Transformer中窄-宽-窄MLP的传统设计，提出用更深但更轻的hourglass形状MLP替代传统FFN，在固定参数预算下提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer语言模型普遍采用窄-宽-窄MLP设计，但最近研究表明宽-窄-宽（hourglass）形状的MLP具有更好的函数逼近能力。作者旨在重新审视Transformer中MLP形状的长期惯例，挑战窄-宽-窄设计的必要性。

Method: 开发了一个Transformer变体，用更深层的hourglass形状FFN替代传统FFN。该hourglass FFN由多个hourglass子MLP通过残差连接堆叠而成。在固定参数预算下，通过减少FFN参数来增加模型隐藏维度或注意力参数。

Result: 在400M参数规模下，hourglass FFN优于传统FFN；在1B参数规模下，两者性能相当。在匹配预算下，减少FFN参数并增加注意力参数的hourglass变体相比传统配置有持续改进。

Conclusion: hourglass FFN可以作为传统FFN的竞争性替代方案，参数节省可用于更有效地利用模型容量。这些发现促使重新思考窄-宽-窄MLP惯例以及注意力与FFN之间的平衡，以构建更高效和表达能力更强的现代语言模型。

Abstract: Dense Transformer language models have largely adhered to one consistent architectural shape: each layer consists of an attention module followed by a feed-forward network (FFN) with a narrow-wide-narrow MLP, allocating most parameters to the MLP at expansion ratios between 2 and 4. Motivated by recent results that residual wide-narrow-wide (hourglass) MLPs offer superior function approximation capabilities, we revisit the long-standing MLP shape convention in Transformer, challenging the necessity of the narrow-wide-narrow design. To study this, we develop a Transformer variant that replaces the conventional FFN with a deeper hourglass-shaped FFN, comprising a stack of hourglass sub-MLPs connected by residual pathways. We posit that a deeper but lighter hourglass FFN can serve as a competitive alternative to the conventional FFN, and that parameters saved by using a lighter hourglass FFN can be more effectively utilized, such as by enlarging model hidden dimensions under fixed budgets. We confirm these through empirical validations across model scales: hourglass FFNs outperform conventional FFNs up to 400M and achieve comparable performance at larger scales to 1B parameters; hourglass FFN variants with reduced FFN and increased attention parameters show consistent improvements over conventional configurations at matched budgets. Together, these findings shed new light on recent work and prompt a rethinking of the narrow-wide-narrow MLP convention and the balance between attention and FFN towards efficient and expressive modern language models.

</details>


### [33] [Completing Missing Annotation: Multi-Agent Debate for Accurate and Scalable Relevant Assessment for IR Benchmarks](https://arxiv.org/abs/2602.06526)
*Minjeong Ban,Jeonghwan Choi,Hyangsuk Min,Nicole Hee-Yeon Kim,Minseok Kim,Jae-Gil Lee,Hwanjun Song*

Main category: cs.CL

TL;DR: 提出DREAM框架，通过多轮辩论的LLM代理进行相关性评估，显著提高标注准确性并减少人工参与，同时构建BRIDGE基准解决IR评估中的缺失相关块问题。


<details>
  <summary>Details</summary>
Motivation: 现有IR评估面临基准数据集不完整的问题，包含未标注的相关块。虽然LLM和LLM-人类混合策略减少了人工成本，但仍存在LLM过度自信和AI到人类升级无效的问题。

Method: 提出DREAM框架，基于对立初始立场和迭代相互批评的多轮辩论式相关性评估，通过基于共识的辩论为确定案例提供更准确标注，为不确定案例提供更可靠的AI到人类升级机制。

Result: DREAM实现了95.2%的标注准确率，仅需3.5%的人工参与。使用DREAM构建的BRIDGE基准发现了29,824个缺失的相关块，缓解了评估偏差，实现了更公平的检索器比较。

Conclusion: DREAM框架有效解决了IR评估中的标注挑战，BRIDGE基准揭示了未处理的相关块缺口不仅扭曲检索器排名，还导致检索-生成错位，为IR和RAG系统提供了更可靠的评估基础。

Abstract: Information retrieval (IR) evaluation remains challenging due to incomplete IR benchmark datasets that contain unlabeled relevant chunks. While LLMs and LLM-human hybrid strategies reduce costly human effort, they remain prone to LLM overconfidence and ineffective AI-to-human escalation. To address this, we propose DREAM, a multi-round debate-based relevance assessment framework with LLM agents, built on opposing initial stances and iterative reciprocal critique. Through our agreement-based debate, it yields more accurate labeling for certain cases and more reliable AI-to-human escalation for uncertain ones, achieving 95.2% labeling accuracy with only 3.5% human involvement. Using DREAM, we build BRIDGE, a refined benchmark that mitigates evaluation bias and enables fairer retriever comparison by uncovering 29,824 missing relevant chunks. We then re-benchmark IR systems and extend evaluation to RAG, showing that unaddressed holes not only distort retriever rankings but also drive retrieval-generation misalignment. The relevance assessment framework is available at https: //github.com/DISL-Lab/DREAM-ICLR-26; and the BRIDGE dataset is available at https://github.com/DISL-Lab/BRIDGE-Benchmark.

</details>


### [34] [MTQE.en-he: Machine Translation Quality Estimation for English-Hebrew](https://arxiv.org/abs/2602.06546)
*Andy Rosenbaum,Assaf Siani,Ilan Kernerman*

Main category: cs.CL

TL;DR: MTQE.en-he是首个公开的英希机器翻译质量评估基准，包含959个英希翻译对和人工评分，通过集成ChatGPT、TransQuest和CometKiwi模型取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 英希语言对在机器翻译质量评估研究方面资源匮乏，缺乏公开可用的基准数据集，限制了该语言对的研究进展。

Method: 1) 构建MTQE.en-he数据集：从WMT24++选取959个英语片段，配以机器翻译的希伯来语版本，并由三位专家进行直接评估打分；2) 基准测试：评估ChatGPT提示、TransQuest和CometKiwi三种方法；3) 集成方法：结合三种模型的预测；4) 微调实验：对TransQuest和CometKiwi进行全模型更新和参数高效方法（LoRA、BitFit、FTHead）的微调。

Result: 1) 集成三种模型的方法比最佳单模型（CometKiwi）在Pearson相关系数上提升6.4个百分点，Spearman相关系数上提升5.6个百分点；2) 全模型微调容易过拟合和分布塌陷，而参数高效方法（LoRA、BitFit、FTHead）训练稳定，能提升2-3个百分点；3) 公开发布了MTQE.en-he数据集和实验结果。

Conclusion: MTQE.en-he填补了英希机器翻译质量评估的基准空白，展示了集成方法和参数高效微调的有效性，为这一资源匮乏语言对的研究提供了重要基础。

Abstract: We release MTQE.en-he: to our knowledge, the first publicly available English-Hebrew benchmark for Machine Translation Quality Estimation. MTQE.en-he contains 959 English segments from WMT24++, each paired with a machine translation into Hebrew, and Direct Assessment scores of the translation quality annotated by three human experts. We benchmark ChatGPT prompting, TransQuest, and CometKiwi and show that ensembling the three models outperforms the best single model (CometKiwi) by 6.4 percentage points Pearson and 5.6 percentage points Spearman. Fine-tuning experiments with TransQuest and CometKiwi reveal that full-model updates are sensitive to overfitting and distribution collapse, yet parameter-efficient methods (LoRA, BitFit, and FTHead, i.e., fine-tuning only the classification head) train stably and yield improvements of 2-3 percentage points. MTQE.en-he and our experimental results enable future research on this under-resourced language pair.

</details>


### [35] [Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making](https://arxiv.org/abs/2602.06570)
*Baichuan-M3 Team,:,Chengfeng Dou,Fan Yang,Fei Li,Jiyuan Jia,Qiang Ju,Shuai Wang,Tianpeng Li,Xiangrong Zeng,Yijie Zhou,Hongda Zhang,Jinyang Tai,Linzhuang Sun,Peidong Guo,Yichuan Mo,Xiaochuan Wang,Hengfu Cui,Zhishou Zhang*

Main category: cs.CL

TL;DR: Baichuan-M3是一个医疗增强大语言模型，能够从被动问答转向主动的临床级决策支持，在多个医疗基准测试中表现优于GPT-5.2。


<details>
  <summary>Details</summary>
Motivation: 现有医疗AI系统在开放式咨询中存在局限性，缺乏主动信息获取、长期推理和幻觉抑制能力，无法提供临床级决策支持。

Method: 采用专门训练流程模拟医生系统工作流程，包括主动信息获取解决歧义、长期推理整合分散证据、自适应幻觉抑制确保事实可靠性。

Result: 在HealthBench、新推出的HealthBench-Hallu和ScanBench上取得最先进成果，在临床咨询、建议和安全性方面显著优于GPT-5.2。

Conclusion: Baichuan-M3成功实现了从被动问答到主动临床决策支持的范式转变，通过专门训练方法提升了医疗AI的可靠性和实用性。

Abstract: We introduce Baichuan-M3, a medical-enhanced large language model engineered to shift the paradigm from passive question-answering to active, clinical-grade decision support. Addressing the limitations of existing systems in open-ended consultations, Baichuan-M3 utilizes a specialized training pipeline to model the systematic workflow of a physician. Key capabilities include: (i) proactive information acquisition to resolve ambiguity; (ii) long-horizon reasoning that unifies scattered evidence into coherent diagnoses; and (iii) adaptive hallucination suppression to ensure factual reliability. Empirical evaluations demonstrate that Baichuan-M3 achieves state-of-the-art results on HealthBench, the newly introduced HealthBench-Hallu and ScanBench, significantly outperforming GPT-5.2 in clinical inquiry, advisory and safety. The models are publicly available at https://huggingface.co/collections/baichuan-inc/baichuan-m3.

</details>


### [36] [Inference-Time Rethinking with Latent Thought Vectors for Math Reasoning](https://arxiv.org/abs/2602.06584)
*Deqian Kong,Minglu Zhao,Aoyang Qin,Bo Pang,Chenxin Tao,David Hartmann,Edouardo Honig,Dehong Xu,Amit Kumar,Matt Sarte,Chuan Li,Jianwen Xie,Ying Nian Wu*

Main category: cs.CL

TL;DR: 提出Inference-Time Rethinking框架，通过解耦潜在思考向量和生成过程实现迭代式自我修正，在GSM8K上仅用0.2B参数就超越3B参数基线模型


<details>
  <summary>Details</summary>
Motivation: 标准思维链推理在单次前向传播中生成解决方案，一旦出现早期错误就无法修正。需要一种能够迭代自我修正的推理框架

Method: 将推理分解为连续潜在思考向量（推理什么）和解码器（如何推理）。使用先验模型将噪声映射到有效推理模式的学习流形，在测试时采用Gibbs风格过程，交替生成候选推理轨迹和优化潜在向量

Result: 在GSM8K上训练0.2B参数模型，经过30次重新思考迭代后，性能超越参数多10-15倍的基线模型，包括3B参数模型

Conclusion: 有效的数学推理可以通过复杂的推理时计算实现，而不仅仅依赖于大规模参数数量，展示了推理时优化在提升模型性能方面的潜力

Abstract: Standard chain-of-thought reasoning generates a solution in a single forward pass, committing irrevocably to each token and lacking a mechanism to recover from early errors. We introduce Inference-Time Rethinking, a generative framework that enables iterative self-correction by decoupling declarative latent thought vectors from procedural generation. We factorize reasoning into a continuous latent thought vector (what to reason about) and a decoder that verbalizes the trace conditioned on this vector (how to reason). Beyond serving as a declarative buffer, latent thought vectors compress the reasoning structure into a continuous representation that abstracts away surface-level token variability, making gradient-based optimization over reasoning strategies well-posed. Our prior model maps unstructured noise to a learned manifold of valid reasoning patterns, and at test time we employ a Gibbs-style procedure that alternates between generating a candidate trace and optimizing the latent vector to better explain that trace, effectively navigating the latent manifold to refine the reasoning strategy. Training a 0.2B-parameter model from scratch on GSM8K, our method with 30 rethinking iterations surpasses baselines with 10 to 15 times more parameters, including a 3B counterpart. This result demonstrates that effective mathematical reasoning can emerge from sophisticated inference-time computation rather than solely from massive parameter counts.

</details>


### [37] [Echoes as Anchors: Probabilistic Costs and Attention Refocusing in LLM Reasoning](https://arxiv.org/abs/2602.06600)
*Zhuoyuan Hao,Zhuo Li,Wu Li,Fangming Liu,Min Zhang,Jing Li*

Main category: cs.CL

TL;DR: 论文提出利用大型推理模型中的"提示回响"现象作为计算分配机制，通过回响蒸馏监督微调和回响提示技术提升模型推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在测试时计算分配上存在局限：要么注入任务无关的"思考标记"，要么采用启发式方法忽略了模型内部链中自发的重复现象。论文旨在分析和利用模型倾向于重述问题的"提示回响"现象作为计算分配机制。

Method: 1. 将回响移除形式化为基于拒绝的条件化，定义"回响似然间隙"作为可计算的代理指标；2. 开发回响蒸馏监督微调技术来植入"先回响后推理"模式；3. 提出回响提示技术在不训练的情况下重新锚定模型；4. 进行长度和后缀控制的似然分析以及层间注意力研究。

Result: 在GSM8K、MathQA、Hendrycks-MATH、AIME24和MATH-500等数学推理基准测试中，在相同的解码设置和计算预算下，相比基线方法获得了持续的性能提升。回响现象增加了中间层中答案对答案前缀的注意力，符合注意力重新聚焦机制。

Conclusion: 提示回响现象可以作为有效的计算分配机制，通过回响蒸馏监督微调和回响提示技术能够显著提升大型推理模型的性能，为理解模型内部推理过程提供了新的理论框架。

Abstract: Test-time compute allocation in large reasoning models (LRMs) is widely used and has applications in mathematical problem solving, code synthesis, and planning. Recent work has addressed this problem by scaling self-consistency and parallel thinking, adding generic ``thinking tokens'' and prompting models to re-read the question before answering. Unfortunately, these approaches either inject task-agnostic tokens or mandate heuristics that do not explain -- and often ignore -- the \emph{spontaneous} repetition that many LRMs exhibit at the head of their internal chains. In contrast, we analyze and harness the model's tendency to restate the question, which we term the \emph{Echo of Prompt (EOP)}, as a front-loaded, compute-shaping mechanism. We formalize its probabilistic cost by casting echo removal as rejection-based conditioning and defining the \emph{Echo Likelihood Gap} $Δ\mathcal{L}$ as a computable proxy. This provides the missing theoretical link that links early repetition to likelihood gains and downstream accuracy. However, it does not by itself specify how to exploit EOP. Consequently, we develop \emph{Echo-Distilled SFT (ED-SFT)} to instill an ``echo-then-reason'' pattern through supervised finetuning, and \emph{Echoic Prompting (EP)} to re-ground the model mid-trace without training. While promising, quantifying benefits beyond verbosity is non-trivial. Therefore, we conduct length and suffix-controlled likelihood analyses together with layer-wise attention studies, showing that EOP increases answer to answer-prefix attention in middle layers, consistent with an \emph{attention refocusing} mechanism. We evaluate on GSM8K, MathQA, Hendrycks-MATH, AIME24, and MATH-500 under identical decoding settings and budgets, and find consistent gains over baselines. Code is available at https://github.com/hhh2210/echoes-as-anchors.

</details>


### [38] [Do Prompts Guarantee Safety? Mitigating Toxicity from LLM Generations through Subspace Intervention](https://arxiv.org/abs/2602.06623)
*Himanshu Singh,Ziwei Xu,A. V. Subramanyam,Mohan Kankanhalli*

Main category: cs.CL

TL;DR: 提出一种针对LLM隐藏毒性模式的子空间干预策略，在保持生成流畅性的同时有效降低毒性


<details>
  <summary>Details</summary>
Motivation: LLM即使接收无害提示也可能生成有毒内容，现有方法在安全性与流畅性之间存在平衡问题，且毒性检测困难

Method: 采用针对性子空间干预策略，从底层模型表示中识别并抑制隐藏的毒性模式

Result: 在RealToxicityPrompts上相比基线方法取得显著降低毒性效果，毒性减少8-20%，同时保持推理复杂度最小化

Conclusion: 该方法能有效降低LLM毒性而不损害生成性能，在安全性和流畅性之间取得良好平衡

Abstract: Large Language Models (LLMs) are powerful text generators, yet they can produce toxic or harmful content even when given seemingly harmless prompts. This presents a serious safety challenge and can cause real-world harm. Toxicity is often subtle and context-dependent, making it difficult to detect at the token level or through coarse sentence-level signals. Moreover, efforts to mitigate toxicity often face a trade-off between safety and the coherence, or fluency of the generated text. In this work, we present a targeted subspace intervention strategy for identifying and suppressing hidden toxic patterns from underlying model representations, while preserving overall ability to generate safe fluent content. On the RealToxicityPrompts, our method achieves strong mitigation performance compared to existing baselines, with minimal impact on inference complexity. Across multiple LLMs, our approach reduces toxicity of state-of-the-art detoxification systems by 8-20%, while maintaining comparable fluency. Through extensive quantitative and qualitative analyses, we show that our approach achieves effective toxicity reduction without impairing generative performance, consistently outperforming existing baselines.

</details>


### [39] [FairJudge: An Adaptive, Debiased, and Consistent LLM-as-a-Judge](https://arxiv.org/abs/2602.06625)
*Bo Yang,Lanfei Feng,Yunkui Chen,Yu Zhang,Xiao Xu,Shijian Li*

Main category: cs.CL

TL;DR: FairJudge是一个自适应、去偏且一致的LLM-as-a-Judge系统，通过将评判行为建模为可学习的策略，使用课程学习训练范式，显著提升评判质量并减少系统性偏见。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-as-a-Judge系统存在三个根本性局限：1) 对任务和领域特定评估标准的适应性有限；2) 由位置、长度、格式和模型来源等非语义线索驱动的系统性偏见；3) 评估不一致性导致不同评估模式（如点式与成对）下的矛盾判断。

Method: 1) 将评判行为建模为可学习和正则化的策略；2) 从数据中心视角构建高信息密度的评判数据集，注入与评估行为对齐的监督信号；3) 采用课程式SFT-DPO-GRPO训练范式，逐步对齐评分标准遵守度、偏见缓解和跨模式一致性，同时避免灾难性遗忘。

Result: 在多个内部和公开基准测试中，FairJudge持续改善了一致性和F1分数，减少了非语义偏见，并且显著优于更大的指令调优LLMs。

Conclusion: FairJudge通过将评判行为建模为可学习策略的创新方法，有效解决了现有LLM评判系统的三大局限性。该方法不仅提升了评判质量，还减少了系统性偏见，为LLM评估提供了更可靠的工具。所有资源将在接受后公开发布以促进未来研究。

Abstract: Existing LLM-as-a-Judge systems suffer from three fundamental limitations: limited adaptivity to task- and domain-specific evaluation criteria, systematic biases driven by non-semantic cues such as position, length, format, and model provenance, and evaluation inconsistency that leads to contradictory judgments across different evaluation modes (e.g., pointwise versus pairwise). To address these issues, we propose FairJudge, an adaptive, debiased, and consistent LLM-as-a-Judge. Unlike prior approaches that treat the judge as a static evaluator, FairJudge models judging behavior itself as a learnable and regularized policy. From a data-centric perspective, we construct a high-information-density judging dataset that explicitly injects supervision signals aligned with evaluation behavior. Building on this dataset, we adopt a curriculum-style SFT-DPO-GRPO training paradigm that progressively aligns rubric adherence, bias mitigation, and cross-mode consistency, while avoiding catastrophic forgetting. Experimental results on multiple internal and public benchmarks show that FairJudge consistently improves agreement and F1, reduces non-semantic biases, and outperforms substantially larger instruction-tuned LLMs. All resources will be publicly released after acceptance to facilitate future research.

</details>


### [40] [Reading Between the Waves: Robust Topic Segmentation Using Inter-Sentence Audio Features](https://arxiv.org/abs/2602.06647)
*Steffen Freisinger,Philipp Seeberger,Tobias Bocklet,Korbinian Riedhammer*

Main category: cs.CL

TL;DR: 本文提出了一种多模态方法，通过联合微调文本编码器和孪生音频编码器，利用声学特征提升话题分割性能，在多个数据集上显著优于纯文本和多模态基线。


<details>
  <summary>Details</summary>
Motivation: 当前的话题分割方法未能充分利用声学特征，而口语内容（如在线视频和播客）通常包含多个话题，自动话题分割对于用户导航和下游应用至关重要。

Method: 提出多模态方法，同时微调文本编码器和孪生音频编码器，捕捉句子边界周围的声学线索。

Result: 在YouTube视频大规模数据集上，相比纯文本和多模态基线获得显著提升；模型对ASR噪声更具鲁棒性，并在葡萄牙语、德语和英语的额外数据集上优于更大的纯文本基线。

Conclusion: 学习到的声学特征对于鲁棒的话题分割具有重要价值，多模态方法能够有效提升话题分割性能。

Abstract: Spoken content, such as online videos and podcasts, often spans multiple topics, which makes automatic topic segmentation essential for user navigation and downstream applications. However, current methods do not fully leverage acoustic features, leaving room for improvement. We propose a multi-modal approach that fine-tunes both a text encoder and a Siamese audio encoder, capturing acoustic cues around sentence boundaries. Experiments on a large-scale dataset of YouTube videos show substantial gains over text-only and multi-modal baselines. Our model also proves more resilient to ASR noise and outperforms a larger text-only baseline on three additional datasets in Portuguese, German, and English, underscoring the value of learned acoustic features for robust topic segmentation.

</details>


### [41] [Beyond Static Alignment: Hierarchical Policy Control for LLM Safety via Risk-Aware Chain-of-Thought](https://arxiv.org/abs/2602.06650)
*Jianfeng Si,Lin Sun,Weihong Lin,Xiangzheng Zhang*

Main category: cs.CL

TL;DR: PACT是一个通过显式风险感知推理实现动态安全控制的框架，采用分层策略架构，在保持安全性的同时提高可控性和实用性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型面临安全性-实用性权衡问题，静态的安全策略缺乏运行时可控性，难以适应多样化的应用需求，导致模型可能过度拒绝良性请求或对有害请求约束不足。

Method: 提出PACT框架，采用分层策略架构：不可覆盖的全局安全策略为关键风险建立不可变边界，用户定义策略可引入领域特定的风险类别并指定标签到行为的映射。框架将安全决策分解为结构化的分类→行动路径，将查询路由到适当的行动（遵从、引导或拒绝），并使决策过程透明化。

Result: 大量实验表明，PACT在全局策略评估下实现了接近最先进的安全性能，同时在用户特定策略评估下获得了最佳的可控性，有效缓解了安全性-实用性权衡问题。

Conclusion: PACT通过动态安全控制和显式风险感知推理，为可控的安全对齐研究提供了可复现的框架，能够更好地适应实际部署场景中的多样化需求。

Abstract: Large Language Models (LLMs) face a fundamental safety-helpfulness trade-off due to static, one-size-fits-all safety policies that lack runtime controllabilityxf, making it difficult to tailor responses to diverse application needs. %As a result, models may over-refuse benign requests or under-constrain harmful ones. We present \textbf{PACT} (Prompt-configured Action via Chain-of-Thought), a framework for dynamic safety control through explicit, risk-aware reasoning. PACT operates under a hierarchical policy architecture: a non-overridable global safety policy establishes immutable boundaries for critical risks (e.g., child safety, violent extremism), while user-defined policies can introduce domain-specific (non-global) risk categories and specify label-to-action behaviors to improve utility in real-world deployment settings. The framework decomposes safety decisions into structured Classify$\rightarrow$Act paths that route queries to the appropriate action (comply, guide, or reject) and render the decision-making process transparent.
  Extensive experiments demonstrate that PACT achieves near state-of-the-art safety performance under global policy evaluation while attaining the best controllability under user-specific policy evaluation, effectively mitigating the safety-helpfulness trade-off. We will release the PACT model suite, training data, and evaluation protocols to facilitate reproducible research in controllable safety alignment.

</details>


### [42] [Not All Layers Need Tuning: Selective Layer Restoration Recovers Diversity](https://arxiv.org/abs/2602.06665)
*Bowen Zhang,Meiyi Wang,Harold Soh*

Main category: cs.CL

TL;DR: 通过选择性恢复特定层到预训练权重，SLR方法在保持输出质量的同时显著提升大语言模型生成多样性，解决微调后的模式坍塌问题。


<details>
  <summary>Details</summary>
Motivation: 后训练（微调）虽然提升了大语言模型的指令遵循和帮助性，但经常导致生成多样性下降，产生重复输出（模式坍塌）。作者观察到LLM各层承担不同功能角色，假设模式坍塌可以定位到特定层，恢复这些层到预训练权重可以恢复多样性同时保持高质量输出。

Method: 设计了约束随机字符（CRC）代理任务来验证假设并确定哪些层需要恢复。基于CRC发现，提出了选择性层恢复（SLR）方法：无需训练，将后训练模型中选定的层恢复到预训练权重，形成混合模型，保持相同架构和参数数量，不增加推理成本。

Result: 在三个任务（创意写作、开放问答、多步推理）和三个模型家族（Llama、Qwen、Gemma）上的实验表明，SLR能一致且显著提高输出多样性，同时保持高质量输出。

Conclusion: SLR是一种简单有效的训练免费方法，通过局部层恢复解决微调后的模式坍塌问题，在多样性和质量之间取得良好平衡，适用于多种任务和模型。

Abstract: Post-training improves instruction-following and helpfulness of large language models (LLMs) but often reduces generation diversity, which leads to repetitive outputs in open-ended settings, a phenomenon known as mode collapse. Motivated by evidence that LLM layers play distinct functional roles, we hypothesize that mode collapse can be localized to specific layers and that restoring a carefully chosen range of layers to their pre-trained weights can recover diversity while maintaining high output quality. To validate this hypothesis and decide which layers to restore, we design a proxy task -- Constrained Random Character(CRC) -- with an explicit validity set and a natural diversity objective. Results on CRC reveal a clear diversity-validity trade-off across restoration ranges and identify configurations that increase diversity with minimal quality loss. Based on these findings, we propose Selective Layer Restoration (SLR), a training-free method that restores selected layers in a post-trained model to their pre-trained weights, yielding a hybrid model with the same architecture and parameter count, incurring no additional inference cost. Across three different tasks (creative writing, open-ended question answering, and multi-step reasoning) and three different model families (Llama, Qwen, and Gemma), we find SLR can consistently and substantially improve output diversity while maintaining high output quality.

</details>


### [43] [compar:IA: The French Government's LLM arena to collect French-language human prompts and preference data](https://arxiv.org/abs/2602.06669)
*Lucie Termignon,Simonas Zilinskas,Hadrien Pélissier,Aurélien Barrot,Nicolas Chesnais,Elie Gavoty*

Main category: cs.CL

TL;DR: 法国政府开发的开源数字公共服务平台compar:IA，用于收集大规模法语人类偏好数据，以解决LLMs在非英语语言中性能下降、文化对齐和安全鲁棒性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在非英语语言中表现不佳，主要因为预训练数据和人类偏好对齐数据集以英语为主。RLHF和DPO等训练方法需要人类偏好数据，但许多非英语语言缺乏公开可用的此类数据。

Method: 开发compar:IA平台，采用盲对比较接口，从法语为主的普通受众收集大规模人类偏好数据。平台通过低参与门槛和隐私保护的自动过滤机制，收集自由形式的prompt和用户判断。

Result: 截至2026年2月7日，收集了超过600,000个自由形式prompt和250,000个偏好投票，其中约89%为法语数据。发布了三个互补数据集（对话、投票、反应），并进行了初步分析，包括法语模型排行榜和用户交互模式。

Conclusion: compar:IA不仅服务于法国语境，还正在发展成为国际数字公共产品，为多语言模型训练、评估和人机交互研究提供可重复使用的基础设施。

Abstract: Large Language Models (LLMs) often show reduced performance, cultural alignment, and safety robustness in non-English languages, partly because English dominates both pre-training data and human preference alignment datasets. Training methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) require human preference data, which remains scarce and largely non-public for many languages beyond English. To address this gap, we introduce compar:IA, an open-source digital public service developed inside the French government and designed to collect large-scale human preference data from a predominantly French-speaking general audience. The platform uses a blind pairwise comparison interface to capture unconstrained, real-world prompts and user judgments across a diverse set of language models, while maintaining low participation friction and privacy-preserving automated filtering. As of 2026-02-07, compar:IA has collected over 600,000 free-form prompts and 250,000 preference votes, with approximately 89% of the data in French. We release three complementary datasets -- conversations, votes, and reactions -- under open licenses, and present initial analyses, including a French-language model leaderboard and user interaction patterns. Beyond the French context, compar:IA is evolving toward an international digital public good, offering reusable infrastructure for multilingual model training, evaluation, and the study of human-AI interaction.

</details>


### [44] [Evaluating Prompt Engineering Strategies for Sentiment Control in AI-Generated Texts](https://arxiv.org/abs/2602.06692)
*Kerstin Sahler,Sophie Jentzsch*

Main category: cs.CL

TL;DR: 通过提示工程控制LLM生成文本的情感，提供比微调更经济实用的替代方案


<details>
  <summary>Details</summary>
Motivation: LLM为情感自适应AI提供了新机会，但控制这些系统中的情感仍然具有挑战性。需要资源敏感且易于访问的方法来控制LLM生成文本的情感。

Method: 使用Ekman的六种基本情感，研究各种提示技术，包括零样本提示、思维链提示（使用gpt-3.5-turbo），并与微调方法进行比较。特别评估了少样本提示（使用人工编写的示例）的效果。

Result: 提示工程能有效引导AI生成文本的情感，为数据受限场景提供了实用且经济有效的替代方案。少样本提示（使用人工编写的示例）在所有技术中效果最好，可能是因为提供了额外的任务特定指导。

Conclusion: 提示工程是控制LLM情感的有效方法，为开发情感自适应AI系统提供了有价值的见解，特别是在资源受限的情况下。

Abstract: The groundbreaking capabilities of Large Language Models (LLMs) offer new opportunities for enhancing human-computer interaction through emotion-adaptive Artificial Intelligence (AI). However, deliberately controlling the sentiment in these systems remains challenging. The present study investigates the potential of prompt engineering for controlling sentiment in LLM-generated text, providing a resource-sensitive and accessible alternative to existing methods. Using Ekman's six basic emotions (e.g., joy, disgust), we examine various prompting techniques, including Zero-Shot and Chain-of-Thought prompting using gpt-3.5-turbo, and compare it to fine-tuning. Our results indicate that prompt engineering effectively steers emotions in AI-generated texts, offering a practical and cost-effective alternative to fine-tuning, especially in data-constrained settings. In this regard, Few-Shot prompting with human-written examples was the most effective among other techniques, likely due to the additional task-specific guidance. The findings contribute valuable insights towards developing emotion-adaptive AI systems.

</details>


### [45] [Table-as-Search: Formulate Long-Horizon Agentic Information Seeking as Table Completion](https://arxiv.org/abs/2602.06724)
*Tian Lan,Felix Henry,Bin Zhu,Qianghuai Jia,Junyang Ren,Qihang Pu,Haijun Li,Longyue Wang,Zhao Xu,Weihua Luo*

Main category: cs.CL

TL;DR: 提出Table-as-Search框架，将信息搜索任务重构为表格填充任务，通过结构化表格管理搜索状态，显著提升长时程搜索的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 当前信息搜索代理在长时程探索中难以保持专注和连贯性，因为在一个纯文本上下文中跟踪搜索状态（包括规划过程和大量搜索结果）本质上很脆弱。

Method: 引入Table-as-Search框架，将每个查询映射到外部数据库维护的结构化表格模式中，行代表搜索候选，列表示约束或所需信息。已填充单元格严格记录历史和搜索结果，空单元格作为显式搜索计划。

Result: TaS在三种类型的基准测试中显著优于众多最先进的基线方法，包括多智能体框架和商业系统。分析验证了TaS在长时程信息搜索中的卓越鲁棒性、效率、可扩展性和灵活性。

Conclusion: TaS框架通过结构化表格管理搜索状态，有效解决了长时程信息搜索中的专注性和连贯性问题，统一了深度搜索、广度搜索和深度广度搜索三种任务，具有实用价值。

Abstract: Current Information Seeking (InfoSeeking) agents struggle to maintain focus and coherence during long-horizon exploration, as tracking search states, including planning procedure and massive search results, within one plain-text context is inherently fragile. To address this, we introduce \textbf{Table-as-Search (TaS)}, a structured planning framework that reformulates the InfoSeeking task as a Table Completion task. TaS maps each query into a structured table schema maintained in an external database, where rows represent search candidates and columns denote constraints or required information. This table precisely manages the search states: filled cells strictly record the history and search results, while empty cells serve as an explicit search plan. Crucially, TaS unifies three distinct InfoSeeking tasks: Deep Search, Wide Search, and the challenging DeepWide Search. Extensive experiments demonstrate that TaS significantly outperforms numerous state-of-the-art baselines across three kinds of benchmarks, including multi-agent framework and commercial systems. Furthermore, our analysis validates the TaS's superior robustness in long-horizon InfoSeeking, alongside its efficiency, scalability and flexibility. Code and datasets are publicly released at https://github.com/AIDC-AI/Marco-Search-Agent.

</details>


### [46] [R-Align: Enhancing Generative Reward Models through Rationale-Centric Meta-Judging](https://arxiv.org/abs/2602.06763)
*Yanlin Lai,Mitt Huang,Hangyu Guo,Xiangfeng Wang,Haodong Li,Shaoxiong Zhan,Liang Zhao,Chengyuan Yao,Yinmin Zhang,Qi Han,Chun Yuan,Zheng Ge,Xiangyu Zhang,Daxin Jiang*

Main category: cs.CL

TL;DR: 该论文提出了一种新的奖励模型评估指标S-Corr，揭示了现有生成式奖励模型中推理与偏好决策不一致的问题，并提出了R-Align方法来提升推理忠实度，从而改善RLHF下游性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成式奖励模型(GenRMs)在训练和评估中只关注结果标签，而忽略了推理质量。论文发现推理忠实度（奖励模型偏好决策与参考决策理由之间的一致性）对下游RLHF结果有重要预测作用，但当前实践缺乏对此的检查。

Method: 1. 提出Spurious Correctness(S-Corr)指标，衡量标签正确但推理与黄金判断不一致的决策比例；2. 重新利用现有奖励模型基准计算S-Corr；3. 提出Rationale-Centric Alignment (R-Align)方法，通过加入黄金判断和监督推理对齐来增强训练。

Result: 1. 实证评估显示竞争性GenRMs存在显著S-Corr问题；2. 更高的S-Corr与优化下的策略退化相关；3. R-Align在奖励模型基准上减少了S-Corr；4. 在STEM、编程、指令遵循和通用任务上，R-Align带来了actor性能的一致增益。

Conclusion: 推理忠实度是奖励模型质量的关键预测指标，仅仅关注标签准确性是不够的。R-Align通过显式监督推理对齐，有效减少了S-Corr问题，并提升了RLHF下游性能，为奖励模型训练提供了新的改进方向。

Abstract: Reinforcement Learning from Human Feedback (RLHF) remains indispensable for aligning large language models (LLMs) in subjective domains. To enhance robustness, recent work shifts toward Generative Reward Models (GenRMs) that generate rationales before predicting preferences. Yet in GenRM training and evaluation, practice remains outcome-label-only, leaving reasoning quality unchecked. We show that reasoning fidelity-the consistency between a GenRM's preference decision and reference decision rationales-is highly predictive of downstream RLHF outcomes, beyond standard label accuracy. Specifically, we repurpose existing reward-model benchmarks to compute Spurious Correctness (S-Corr)-the fraction of label-correct decisions with rationales misaligned with golden judgments. Our empirical evaluation reveals substantial S-Corr even for competitive GenRMs, and higher S-Corr is associated with policy degeneration under optimization. To improve fidelity, we propose Rationale-Centric Alignment, R-Align, which augments training with gold judgments and explicitly supervises rationale alignment. R-Align reduces S-Corr on RM benchmarks and yields consistent gains in actor performance across STEM, coding, instruction following, and general tasks.

</details>


### [47] [Generating Data-Driven Reasoning Rubrics for Domain-Adaptive Reward Modeling](https://arxiv.org/abs/2602.06795)
*Kate Sanders,Nathaniel Weir,Sapana Chaudhary,Kaj Bostrom,Huzefa Rangwala*

Main category: cs.CL

TL;DR: 使用数据驱动方法构建细粒度推理错误分类法，提升LLM在技术领域推理输出验证中的错误检测能力，并用于强化学习训练


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推理输出验证中存在困难，特别是在长输出、需要专业知识的领域以及没有可验证奖励的问题中，难以可靠识别思维轨迹中的错误

Method: 提出数据驱动方法自动构建高度细粒度的推理错误分类法（"评分标准"），利用这些分类法增强LLM在未见推理轨迹上的错误检测能力，并将其用于构建更强的LLM-as-judge奖励函数

Result: 基于错误分类法的分类方法在编码、数学和化学工程等技术领域表现出比基线方法更强的错误识别能力；使用这些奖励函数训练模型，在困难领域任务准确率比通用LLM-as-judge训练模型提高+45%，仅使用20%的黄金标签就能接近使用可验证奖励训练模型的性能

Conclusion: 该方法将奖励评分标准从评估定性模型行为扩展到评估定量模型正确性，为在没有完整黄金标签数据集的情况下教授模型解决复杂技术问题打开了大门

Abstract: An impediment to using Large Language Models (LLMs) for reasoning output verification is that LLMs struggle to reliably identify errors in thinking traces, particularly in long outputs, domains requiring expert knowledge, and problems without verifiable rewards. We propose a data-driven approach to automatically construct highly granular reasoning error taxonomies to enhance LLM-driven error detection on unseen reasoning traces. Our findings indicate that classification approaches that leverage these error taxonomies, or "rubrics", demonstrate strong error identification compared to baseline methods in technical domains like coding, math, and chemical engineering. These rubrics can be used to build stronger LLM-as-judge reward functions for reasoning model training via reinforcement learning. Experimental results show that these rewards have the potential to improve models' task accuracy on difficult domains over models trained by general LLMs-as-judges by +45%, and approach performance of models trained by verifiable rewards while using as little as 20% as many gold labels. Through our approach, we extend the usage of reward rubrics from assessing qualitative model behavior to assessing quantitative model correctness on tasks typically learned via RLVR rewards. This extension opens the door for teaching models to solve complex technical problems without a full dataset of gold labels, which are often highly costly to procure.

</details>


### [48] [Visual Word Sense Disambiguation with CLIP through Dual-Channel Text Prompting and Image Augmentations](https://arxiv.org/abs/2602.06799)
*Shamik Bhattacharya,Daniel Perkins,Yaren Dogan,Vineeth Konjeti,Sudarshan Srinivasan,Edmon Begoli*

Main category: cs.CL

TL;DR: 提出可解释的视觉词义消歧框架，通过CLIP将歧义文本和候选图像映射到共享多模态空间，使用双通道提示增强文本嵌入，通过图像增强改进图像嵌入，在SemEval-2023数据集上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 词汇歧义是大型语言模型在自然语言理解中持续面临的挑战，需要探索如何通过视觉领域来解决词汇歧义问题。

Method: 开发可解释的视觉词义消歧框架：1) 利用CLIP将歧义语言和候选图像投影到共享多模态空间；2) 使用基于语义和照片的双通道提示与WordNet同义词增强文本嵌入；3) 通过鲁棒的测试时增强技术改进图像嵌入；4) 使用余弦相似度确定与歧义文本最匹配的图像。

Result: 在SemEval-2023 VWSD数据集上，增强嵌入使MRR从0.7227提升到0.7590，命中率从0.5810提升到0.6220。消融研究表明：双通道提示提供强大低延迟性能，而激进的图像增强仅带来边际收益；使用WordNet定义和多语言提示的实验表明，噪声外部信号会稀释语义特异性。

Conclusion: 精确的CLIP对齐提示对于视觉词义消歧最为有效，双通道提示策略在性能和效率方面表现优越，而过度增强或噪声外部信号会降低模型性能。

Abstract: Ambiguity poses persistent challenges in natural language understanding for large language models (LLMs). To better understand how lexical ambiguity can be resolved through the visual domain, we develop an interpretable Visual Word Sense Disambiguation (VWSD) framework. The model leverages CLIP to project ambiguous language and candidate images into a shared multimodal space. We enrich textual embeddings using a dual-channel ensemble of semantic and photo-based prompts with WordNet synonyms, while image embeddings are refined through robust test-time augmentations. We then use cosine similarity to determine the image that best aligns with the ambiguous text. When evaluated on the SemEval-2023 VWSD dataset, enriching the embeddings raises the MRR from 0.7227 to 0.7590 and the Hit Rate from 0.5810 to 0.6220. Ablation studies reveal that dual-channel prompting provides strong, low-latency performance, whereas aggressive image augmentation yields only marginal gains. Additional experiments with WordNet definitions and multilingual prompt ensembles further suggest that noisy external signals tend to dilute semantic specificity, reinforcing the effectiveness of precise, CLIP-aligned prompts for visual word sense disambiguation.

</details>


### [49] [The Representational Geometry of Number](https://arxiv.org/abs/2602.06843)
*Zhimin Hu,Lanhao Niu,Sashank Varma*

Main category: cs.CL

TL;DR: 研究发现语言模型中数字概念的表示共享几何关系结构而非具体表征本身，任务特定表征位于不同子空间但可通过线性映射相互转换。


<details>
  <summary>Details</summary>
Motivation: 认知科学中的一个核心问题是概念表示是收敛到共享流形以支持泛化，还是发散到正交子空间以最小化任务干扰。先前研究发现了两种证据，但对于这些特性如何共存并在任务间转换的机制解释仍不清楚。

Method: 使用数字概念作为测试平台，语言模型作为高维计算基质，分析数字表示如何在任务间保持稳定的关系结构。通过线性代数方法研究任务特定表示在不同子空间中的嵌入方式。

Result: 数字表示在任务间保持稳定的关系结构；任务特定表示嵌入在截然不同的子空间中，低层特征如数量和奇偶性沿可分离的线性方向编码；这些子空间主要通过线性映射相互转换，表明表示共享关系结构。

Conclusion: 语言模型通过将任务特定变换应用于概念表示的共享底层关系结构来平衡数字表示的共享结构与功能灵活性，这为理解概念表示如何在保持共享结构的同时实现功能灵活性提供了机制性视角。

Abstract: A central question in cognitive science is whether conceptual representations converge onto a shared manifold to support generalization, or diverge into orthogonal subspaces to minimize task interference. While prior work has discovered evidence for both, a mechanistic account of how these properties coexist and transform across tasks remains elusive. We propose that representational sharing lies not in the concepts themselves, but in the geometric relations between them. Using number concepts as a testbed and language models as high-dimensional computational substrates, we show that number representations preserve a stable relational structure across tasks. Task-specific representations are embedded in distinct subspaces, with low-level features like magnitude and parity encoded along separable linear directions. Crucially, we find that these subspaces are largely transformable into one another via linear mappings, indicating that representations share relational structure despite being located in distinct subspaces. Together, these results provide a mechanistic lens of how language models balance the shared structure of number representation with functional flexibility. It suggests that understanding arises when task-specific transformations are applied to a shared underlying relational structure of conceptual representations.

</details>


### [50] [SEMA: Simple yet Effective Learning for Multi-Turn Jailbreak Attacks](https://arxiv.org/abs/2602.06854)
*Mingqian Feng,Xiaodong Liu,Weiwei Yang,Jialin Song,Xuekai Zhu,Chenliang Xu,Jianfeng Gao*

Main category: cs.CL

TL;DR: SEMA是一个用于多轮越狱攻击的简单有效框架，通过自调优预填充和意图漂移感知的强化学习训练攻击者，在多种数据集和模型上实现最先进的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 多轮越狱攻击更符合实际威胁模型，但现有方法面临探索复杂性和意图漂移问题。需要一种不依赖现有策略或外部数据的简单有效框架。

Method: SEMA包含两个阶段：1）预填充自调优：通过在自生成的多轮对抗提示上微调，获得可用的rollout；2）意图漂移感知强化学习：训练攻击者生成有效的多轮对抗提示，同时保持有害意图。采用开环攻击机制，不依赖受害者反馈。

Result: 在多个数据集、受害者模型和越狱评估中，SEMA实现了最先进的攻击成功率（ASR）。例如，在AdvBench上对三个闭源和开源受害者模型平均达到80.1%的ASR@1，比现有最好方法高出33.9%。

Conclusion: SEMA提供了一个紧凑、可复现且可跨目标迁移的多轮越狱攻击框架，为大语言模型安全提供了更强、更现实的压力测试，并支持自动红队测试以暴露和定位故障模式。

Abstract: Multi-turn jailbreaks capture the real threat model for safety-aligned chatbots, where single-turn attacks are merely a special case. Yet existing approaches break under exploration complexity and intent drift. We propose SEMA, a simple yet effective framework that trains a multi-turn attacker without relying on any existing strategies or external data. SEMA comprises two stages. Prefilling self-tuning enables usable rollouts by fine-tuning on non-refusal, well-structured, multi-turn adversarial prompts that are self-generated with a minimal prefix, thereby stabilizing subsequent learning. Reinforcement learning with intent-drift-aware reward trains the attacker to elicit valid multi-turn adversarial prompts while maintaining the same harmful objective. We anchor harmful intent in multi-turn jailbreaks via an intent-drift-aware reward that combines intent alignment, compliance risk, and level of detail. Our open-loop attack regime avoids dependence on victim feedback, unifies single- and multi-turn settings, and reduces exploration complexity. Across multiple datasets, victim models, and jailbreak judges, our method achieves state-of-the-art (SOTA) attack success rates (ASR), outperforming all single-turn baselines, manually scripted and template-driven multi-turn baselines, as well as our SFT (Supervised Fine-Tuning) and DPO (Direct Preference Optimization) variants. For instance, SEMA performs an average $80.1\%$ ASR@1 across three closed-source and open-source victim models on AdvBench, 33.9% over SOTA. The approach is compact, reproducible, and transfers across targets, providing a stronger and more realistic stress test for large language model (LLM) safety and enabling automatic redteaming to expose and localize failure modes. Our code is available at: https://github.com/fmmarkmq/SEMA.

</details>


### [51] [Uncovering Cross-Objective Interference in Multi-Objective Alignment](https://arxiv.org/abs/2602.06869)
*Yining Lu,Meng Jiang*

Main category: cs.CL

TL;DR: 本文研究大语言模型多目标对齐中的持续失败模式：训练只能改善部分目标性能，同时导致其他目标性能下降。作者提出协方差目标权重适应方法（CTWA）来缓解跨目标干扰。


<details>
  <summary>Details</summary>
Motivation: 多目标对齐中普遍存在跨目标干扰现象，即训练改善某些目标性能的同时会导致其他目标性能下降。这种现象在不同模型间表现出强烈依赖性，需要系统研究和解决方案。

Method: 首先形式化跨目标干扰现象，推导局部协方差定律，分析目标改进与奖励协方差的关系。将此分析扩展到现代对齐中使用的裁剪代理目标。基于此提出CTWA方法，通过保持目标奖励与训练信号的正协方差来缓解干扰。最后在Polyak-Łojasiewicz条件下进行全局收敛分析。

Result: 研究发现跨目标干扰普遍存在且具有强模型依赖性。提出的CTWA方法能有效缓解跨目标干扰。全局收敛分析确定了非凸标量化优化实现全局收敛的条件以及跨目标干扰如何依赖于特定模型的几何特性。

Conclusion: 跨目标干扰是大语言模型多目标对齐中的核心挑战，可以通过协方差定律理解。提出的CTWA方法为缓解这一干扰提供了有效的即插即用解决方案，全局收敛分析为理解优化过程提供了理论保证。

Abstract: We study a persistent failure mode in multi-objective alignment for large language models (LLMs): training improves performance on only a subset of objectives while causing others to degrade. We formalize this phenomenon as cross-objective interference and conduct the first systematic study across classic scalarization algorithms, showing that interference is pervasive and exhibits strong model dependence.
  To explain this phenomenon, we derive a local covariance law showing that an objective improves at first order when its reward exhibits positive covariance with the scalarized score. We extend this analysis to clipped surrogate objectives used in modern alignment, demonstrating that the covariance law remains valid under mild conditions despite clipping. Building on this analysis, we propose Covariance Targeted Weight Adaptation (CTWA), a plug-and-play method that maintains positive covariance between objective rewards and the training signal to effectively mitigate cross-objective interference. Finally, we complement these local improvement conditions with a global convergence analysis under the Polyak--Łojasiewicz condition, establishing when non-convex scalarized optimization achieves global convergence and how cross-objective interference depends on specific model geometric properties.

</details>


### [52] [Halluverse-M^3: A multitask multilingual benchmark for hallucination in LLMs](https://arxiv.org/abs/2602.06920)
*Samir Abdaljalil,Parichit Sharma,Erchin Serpedin,Hasan Kurban*

Main category: cs.CL

TL;DR: Halluverse-M^3是一个多语言、多任务、多幻觉类型的数据集，用于系统分析大语言模型在不同语言、任务和幻觉类别上的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多语言和生成式场景中的幻觉问题依然存在，但现有研究主要关注英语，缺乏对不同语言、任务和幻觉类型的系统分析。需要构建一个全面的数据集来深入理解模型在不同条件下的幻觉行为。

Method: 构建了Halluverse-M^3数据集，涵盖英语、阿拉伯语、印地语和土耳其语四种语言，支持问答和对话摘要两种生成任务。通过受控编辑过程构造幻觉输出，并由人工标注验证，确保原始内容与幻觉生成之间的清晰对齐。

Result: 评估显示：问答任务比对话摘要更容易；句子级幻觉对最强模型仍具挑战性；英语表现最佳，低资源语言（特别是印地语）检测准确率下降。数据集为多语言多任务幻觉研究提供了现实且具有挑战性的基准。

Conclusion: Halluverse-M^3为系统研究多语言、多任务场景中的幻觉问题提供了重要工具，揭示了模型在不同条件下的性能差异，有助于推动幻觉检测和缓解的进一步研究。

Abstract: Hallucinations in large language models remain a persistent challenge, particularly in multilingual and generative settings where factual consistency is difficult to maintain. While recent models show strong performance on English-centric benchmarks, their behavior across languages, tasks, and hallucination types is not yet well understood. In this work, we introduce Halluverse-M^3, a dataset designed to enable systematic analysis of hallucinations across multiple languages, multiple generation tasks, and multiple hallucination categories. Halluverse-M^3 covers four languages, English, Arabic, Hindi, and Turkish, and supports two generation tasks: question answering and dialogue summarization. The dataset explicitly distinguishes between entity-level, relation-level, and sentence-level hallucinations. Hallucinated outputs are constructed through a controlled editing process and validated by human annotators, ensuring clear alignment between original content and hallucinated generations. Using this dataset, we evaluate a diverse set of contemporary open-source and proprietary language models on fine-grained hallucination detection. Our results show that question answering is consistently easier than dialogue summarization, while sentence-level hallucinations remain challenging even for the strongest models. Performance is highest in English and degrades in lower-resource languages, with Hindi exhibiting the lowest detection accuracy. Overall, Halluverse-M^3 provides a realistic and challenging benchmark for studying hallucinations in multilingual, multi-task settings. We release the dataset to support future research on hallucination detection and mitigation\footnote{https://huggingface.co/datasets/sabdalja/HalluVerse-M3}.

</details>


### [53] [Optimal Turkish Subword Strategies at Scale: Systematic Evaluation of Data, Vocabulary, Morphology Interplay](https://arxiv.org/abs/2602.06942)
*Duygu Altinok*

Main category: cs.CL

TL;DR: 土耳其语等形态丰富语言中首个系统性的子词分词研究，通过控制词汇表大小和训练语料规模，比较不同分词方法，并建立形态感知的评估框架。


<details>
  <summary>Details</summary>
Motivation: 土耳其语等形态丰富语言中，多产粘着现象对词汇效率和形态保真度构成挑战。先前研究存在三个主要局限：未系统控制分词器训练语料、内在诊断有限、下游任务评估范围狭窄。

Method: 提出"子词宣言"研究框架：1)联合变化词汇表大小和训练语料规模（数据与词汇耦合）；2)在匹配参数预算下比较多种分词方法（WordPiece、形态级别、字符基线）；3)在语义、句法和形态敏感任务上全面评估；4)引入形态感知诊断工具包，包括边界级微/宏F1、词素原子性、过/欠分割指数、字符/词编辑距离等指标。

Result: 系统研究了词汇表-语料-成功三元关系，建立了统一的形态感知评估框架，识别了字符级和形态级分词的优势场景，为形态丰富语言构建有效分词器提供了可操作指导。

Conclusion: 这是首个针对形态丰富语言的系统性分词研究，为构建有效的分词器提供了可操作指导，并为未来研究建立了可复现的基础。研究强调了控制训练语料的重要性，并展示了形态感知评估框架的价值。

Abstract: Tokenization is a pivotal design choice for neural language modeling in morphologically rich languages (MRLs) such as Turkish, where productive agglutination challenges both vocabulary efficiency and morphological fidelity. Prior studies have explored tokenizer families and vocabulary sizes but typically (i) vary vocabulary without systematically controlling the tokenizer's training corpus, (ii) provide limited intrinsic diagnostics, and (iii) evaluate a narrow slice of downstream tasks. We present the first comprehensive, principled study of Turkish subword tokenization; a "subwords manifest", that jointly varies vocabulary size and tokenizer training corpus size (data and vocabulary coupling), compares multiple tokenizer families under matched parameter budgets (WordPiece, morphology level, and character baselines), and evaluates across semantic (NLI, STS, sentiment analysis, NER), syntactic (POS, dependency parsing), and morphology-sensitive probes. To explain why tokenizers succeed or fail, we introduce a morphology-aware diagnostic toolkit that goes beyond coarse aggregates to boundary-level micro/macro F1, decoupled lemma atomicity vs. surface boundary hits, over/under-segmentation indices, character/word edit distances (CER/WER), continuation rates, and affix-type coverage and token-level atomicity. Our contributions are fourfold: (i) a systematic investigation of the vocabulary-corpus-success triad; (ii) a unified, morphology-aware evaluation framework linking intrinsic diagnostics to extrinsic outcomes; (iii) controlled comparisons identifying when character-level and morphology-level tokenization pay off; and (iv) an open-source release of evaluation code, tokenizer pipelines, and models. As the first work of its kind, this "subwords manifest" delivers actionable guidance for building effective tokenizers in MRLs and establishes a reproducible foundation for future research.

</details>


### [54] [DAWN: Dependency-Aware Fast Inference for Diffusion LLMs](https://arxiv.org/abs/2602.06953)
*Lizhuo Luo,Zhuoran Shi,Jiajun Luo,Zhi Wang,Shen Ren,Wenya Wang,Tianwei Zhang*

Main category: cs.CL

TL;DR: DAWN提出了一种无需训练、依赖感知的解码方法，通过建模token间的依赖关系来选择更可靠的并行解码位置，在保持生成质量的同时显著提升扩散大语言模型的推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有扩散大语言模型的并行解码策略受限于质量-速度权衡，采用保守策略导致效率潜力未充分挖掘。核心挑战在于并行解码假设每个位置可独立填充，但token之间存在语义耦合，忽略这些依赖关系会导致输出质量下降。

Method: DAWN是一种无需训练的解码方法，通过提取token依赖关系构建依赖图，基于两个关键洞察：(1)依赖于已确定位置的token更可靠；(2)同时解码强耦合的不确定位置会引入错误。在每轮迭代中，DAWN利用依赖图选择更可靠的解码位置。

Result: 在多个模型和数据集上的实验表明，DAWN相比基线方法将推理速度提升了1.80-8.06倍，同时保持了生成质量。

Conclusion: DAWN通过建模token依赖关系，解决了扩散大语言模型并行解码中的质量-速度权衡问题，实现了高效且高质量的文本生成。

Abstract: Diffusion large language models (dLLMs) have shown advantages in text generation, particularly due to their inherent ability for parallel decoding. However, constrained by the quality--speed trade-off, existing inference solutions adopt conservative parallel strategies, leaving substantial efficiency potential underexplored. A core challenge is that parallel decoding assumes each position can be filled independently, but tokens are often semantically coupled. Thus, the correct choice at one position constrains valid choices at others. Without modeling these inter-token dependencies, parallel strategies produce deteriorated outputs. Motivated by this insight, we propose DAWN, a training-free, dependency-aware decoding method for fast dLLM inference. DAWN extracts token dependencies and leverages two key motivations: (1) positions dependent on unmasked certain positions become more reliable, (2) simultaneously unmasking strongly coupled uncertain positions induces errors. Given those findings, DAWN leverages a dependency graph to select more reliable unmasking positions at each iteration, achieving high parallelism with negligible loss in generation quality. Extensive experiments across multiple models and datasets demonstrate that DAWN speedups the inference by 1.80-8.06x over baselines while preserving the generation quality. Code is released at https://github.com/lizhuo-luo/DAWN.

</details>


### [55] [InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning](https://arxiv.org/abs/2602.06960)
*Yuchen Yan,Liang Jiang,Jin Jiang,Shuaicheng Li,Zujie Wen,Zhiqiang Zhang,Jun Zhou,Jian Shao,Yueting Zhuang,Yongliang Shen*

Main category: cs.CL

TL;DR: InftyThink+：一个通过强化学习优化迭代推理轨迹的框架，在减少推理成本的同时提升推理性能


<details>
  <summary>Details</summary>
Motivation: 传统的链式思维推理存在二次成本、上下文长度限制和中间信息丢失问题，而现有的迭代推理方法依赖于监督学习或固定启发式规则，无法优化何时总结、保留什么以及如何继续推理。

Method: 提出InftyThink+端到端强化学习框架，基于模型控制的迭代边界和显式总结，采用两阶段训练方案：监督冷启动后接轨迹级强化学习，让模型学习策略性总结和继续推理决策。

Result: 在DeepSeek-R1-Distill-Qwen-1.5B上，InftyThink+在AIME24上准确率提升21%，优于传统长链式思维强化学习，在分布外基准测试中泛化更好，同时显著降低推理延迟并加速强化学习训练。

Conclusion: InftyThink+通过强化学习优化迭代推理轨迹，在提升推理性能的同时提高了推理效率，为解决大规模推理模型的问题提供了有效方案。

Abstract: Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects. Iterative reasoning mitigates these issues by periodically summarizing intermediate thoughts, yet existing methods rely on supervised learning or fixed heuristics and fail to optimize when to summarize, what to preserve, and how to resume reasoning. We propose InftyThink+, an end-to-end reinforcement learning framework that optimizes the entire iterative reasoning trajectory, building on model-controlled iteration boundaries and explicit summarization. InftyThink+ adopts a two-stage training scheme with supervised cold-start followed by trajectory-level reinforcement learning, enabling the model to learn strategic summarization and continuation decisions. Experiments on DeepSeek-R1-Distill-Qwen-1.5B show that InftyThink+ improves accuracy by 21% on AIME24 and outperforms conventional long chain-of-thought reinforcement learning by a clear margin, while also generalizing better to out-of-distribution benchmarks. Moreover, InftyThink+ significantly reduces inference latency and accelerates reinforcement learning training, demonstrating improved reasoning efficiency alongside stronger performance.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [56] [MuCo: Multi-turn Contrastive Learning for Multimodal Embedding Model](https://arxiv.org/abs/2602.06393)
*Geonmo Gu,Byeongho Heo,Jaemyung Yu,Jaehui Hwang,Taekyung Kim,Sangmin Lee,HeeJae Jun,Yoohoon Kang,Sangdoo Yun,Dongyoon Han*

Main category: cs.IR

TL;DR: 提出多轮对比学习框架MuCo，利用多模态大语言模型的对话特性，在单次前向传播中处理多个相关查询-目标对，提高训练效率和表示一致性。


<details>
  <summary>Details</summary>
Motivation: 传统多模态嵌入模型使用对比学习，但基于"单轮"公式，每个查询-目标对被当作独立数据点。这导致计算效率低下（每个对需要单独前向传播），且忽略了同一上下文中多个查询之间的潜在关系。

Method: 引入多轮对比学习框架MuCo，利用MLLMs的对话特性，在单次前向传播中处理与单个图像相关的多个查询-目标对。同时提取多个查询和目标嵌入，基于共享上下文表示，有效扩大批次大小。

Result: 实验使用新构建的500万多模态多轮数据集M3T，在MMEB和M-BEIR基准测试中实现了最先进的检索性能，同时显著提高了训练效率和跨模态表示一致性。

Conclusion: MuCo通过多轮对比学习框架有效解决了传统对比学习的计算效率问题，在保持性能的同时显著提升了训练效率，为多模态嵌入模型提供了更高效的训练范式。

Abstract: Universal Multimodal embedding models built on Multimodal Large Language Models (MLLMs) have traditionally employed contrastive learning, which aligns representations of query-target pairs across different modalities. Yet, despite its empirical success, they are primarily built on a "single-turn" formulation where each query-target pair is treated as an independent data point. This paradigm leads to computational inefficiency when scaling, as it requires a separate forward pass for each pair and overlooks potential contextual relationships between multiple queries that can relate to the same context. In this work, we introduce Multi-Turn Contrastive Learning (MuCo), a dialogue-inspired framework that revisits this process. MuCo leverages the conversational nature of MLLMs to process multiple, related query-target pairs associated with a single image within a single forward pass. This allows us to extract a set of multiple query and target embeddings simultaneously, conditioned on a shared context representation, amplifying the effective batch size and overall training efficiency. Experiments exhibit MuCo with a newly curated 5M multimodal multi-turn dataset (M3T), which yields state-of-the-art retrieval performance on MMEB and M-BEIR benchmarks, while markedly enhancing both training efficiency and representation coherence across modalities. Code and M3T are available at https://github.com/naver-ai/muco

</details>


### [57] [TokenMixer-Large: Scaling Up Large Ranking Models in Industrial Recommenders](https://arxiv.org/abs/2602.06563)
*Yuchen Jiang,Jie Zhu,Xintian Han,Hui Lu,Kunmin Bai,Mingyu Yang,Shikang Wu,Ruihao Zhang,Wenlin Zhao,Shipeng Bai,Sijin Zhou,Huizhi Yang,Tianyi Liu,Wenda Liu,Ziyan Gong,Haoran Ding,Zheng Chai,Deping Xie,Zhe Chen,Yuchao Zheng,Peng Xu*

Main category: cs.IR

TL;DR: TokenMixer-Large是对基础TokenMixer架构的改进，通过解决残差设计、梯度更新、MoE稀疏化和可扩展性等核心问题，成功将参数规模扩展到70亿和150亿，并在字节跳动多个场景中部署获得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有大型推荐模型研究（如Wukong、HiFormer、DHEN）的实验规模有限，而基础TokenMixer架构存在残差设计不佳、深层模型梯度更新不足、MoE稀疏化不完全、可扩展性探索有限等设计局限性。

Method: 提出TokenMixer-Large架构，采用混合-还原操作、层间残差、辅助损失以及新颖的稀疏Pertoken MoE架构，系统解决基础TokenMixer的核心问题。

Result: 成功将参数规模扩展到在线流量70亿参数和离线实验150亿参数，在字节跳动多个场景中部署，获得了显著的离线和在线性能提升。

Conclusion: TokenMixer-Large通过系统改进解决了基础TokenMixer架构的局限性，实现了大规模推荐模型的有效扩展，在实际应用中验证了其性能优势。

Abstract: In recent years, the study of scaling laws for large recommendation models has gradually gained attention. Works such as Wukong, HiFormer, and DHEN have attempted to increase the complexity of interaction structures in ranking models and validate scaling laws between performance and parameters/FLOPs by stacking multiple layers. However, their experimental scale remains relatively limited. Our previous work introduced the TokenMixer architecture, an efficient variant of the standard Transformer where the self-attention mechanism is replaced by a simple reshape operation, and the feed-forward network is adapted to a pertoken FFN. The effectiveness of this architecture was demonstrated in the ranking stage by the model presented in the RankMixer paper. However, this foundational TokenMixer architecture itself has several design limitations. In this paper, we propose TokenMixer-Large, which systematically addresses these core issues: sub-optimal residual design, insufficient gradient updates in deep models, incomplete MoE sparsification, and limited exploration of scalability. By leveraging a mixing-and-reverting operation, inter-layer residuals, the auxiliary loss and a novel Sparse-Pertoken MoE architecture, TokenMixer-Large successfully scales its parameters to 7-billion and 15-billion on online traffic and offline experiments, respectively. Currently deployed in multiple scenarios at ByteDance, TokenMixer -Large has achieved significant offline and online performance gains.

</details>


### [58] [R2LED: Equipping Retrieval and Refinement in Lifelong User Modeling with Semantic IDs for CTR Prediction](https://arxiv.org/abs/2602.06622)
*Qidong Liu,Gengnan Wang,Zhichen Liu,Moranxin Wang,Zijian Zhang,Xiao Han,Ni Zhang,Tao Qin,Chen Li*

Main category: cs.IR

TL;DR: R2LED：一种基于语义ID的终身用户建模新范式，通过多路混合检索和双层融合精炼，在CTR预测中平衡效果与效率。


<details>
  <summary>Details</summary>
Motivation: 现有终身用户建模方法采用"检索-精炼"两阶段策略，但仍存在两个问题：(1) 由于数据分布倾斜导致的噪声检索；(2) 精炼阶段缺乏语义理解。虽然语义增强（如LLM建模或语义嵌入）提供了潜在解决方案，但这些方法面临推理成本过高或表示粒度不足的问题。

Method: 提出R2LED框架：1) 检索阶段：多路混合检索，通过多条并行召回路由从不同粒度捕获用户兴趣，同时提出混合检索机制从协同和语义视角高效检索候选，减少噪声；2) 精炼阶段：双层融合精炼，包括用于路由级融合的目标感知交叉注意力，以及用于语义ID级融合的门控机制，能够弥合语义空间与协同空间之间的差距。

Result: 在两个公共数据集上的综合实验结果表明，该方法在性能和效率方面都具有优越性。

Conclusion: R2LED通过利用语义ID的多粒度性和轻量性优势，有效解决了终身用户建模中的噪声检索和语义理解不足问题，实现了效果与效率的良好平衡，代码已开源供复现。

Abstract: Lifelong user modeling, which leverages users' long-term behavior sequences for CTR prediction, has been widely applied in personalized services. Existing methods generally adopted a two-stage "retrieval-refinement" strategy to balance effectiveness and efficiency. However, they still suffer from (i) noisy retrieval due to skewed data distribution and (ii) lack of semantic understanding in refinement. While semantic enhancement, e.g., LLMs modeling or semantic embeddings, offers potential solutions to these two challenges, these approaches face impractical inference costs or insufficient representation granularity. Obsorbing multi-granularity and lightness merits of semantic identity (SID), we propose a novel paradigm that equips retrieval and refinement in Lifelong User Modeling with SEmantic IDs (R2LED) to address these issues. First, we introduce a Multi-route Mixed Retrieval for the retrieval stage. On the one hand, it captures users' interests from various granularities by several parallel recall routes. On the other hand, a mixed retrieval mechanism is proposed to efficiently retrieve candidates from both collaborative and semantic views, reducing noise. Then, for refinement, we design a Bi-level Fusion Refinement, including a target-aware cross-attention for route-level fusion and a gate mechanism for SID-level fusion. It can bridge the gap between semantic and collaborative spaces, exerting the merits of SID. The comprehensive experimental results on two public datasets demonstrate the superiority of our method in both performance and efficiency. To facilitate the reproduction, we have released the code online https://github.com/abananbao/R2LED.

</details>


### [59] [Multimodal Generative Retrieval Model with Staged Pretraining for Food Delivery on Meituan](https://arxiv.org/abs/2602.06654)
*Boyu Chen,Tai Guo,Weiyu Cui,Yuqing Li,Xingxing Wang,Chuan Shi,Cheng Yang*

Main category: cs.IR

TL;DR: 提出分阶段预训练策略解决多模态检索中模态主导问题和训练速度不一致问题，通过生成和判别任务利用语义ID，在美团数据上取得显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 现有双塔架构多模态检索模型在联合优化时存在模态主导问题（某些模态主导训练，其他模态被忽视），以及训练速度不一致导致的"单轮次"问题。

Method: 1. 分阶段预训练策略：每个阶段专注于特定任务，使模型能有效关注和利用多模态特征，灵活控制各阶段训练过程；2. 设计生成和判别任务：利用压缩高维多模态嵌入的语义ID，帮助模型理解语义ID、查询和物品特征之间的关联。

Result: 在美团大规模真实数据上，R@5、R@10、R@20分别提升3.80%、2.64%、2.17%，N@5、N@10、N@20分别提升5.10%、4.22%、2.09%。在线A/B测试显示收入增加1.12%，点击率提升1.02%。

Conclusion: 分阶段预训练策略能有效解决多模态检索中的模态主导和训练不一致问题，结合语义ID的生成和判别任务能显著提升检索性能，在实际应用中验证了方法的有效性和优越性。

Abstract: Multimodal retrieval models are becoming increasingly important in scenarios such as food delivery, where rich multimodal features can meet diverse user needs and enable precise retrieval. Mainstream approaches typically employ a dual-tower architecture between queries and items, and perform joint optimization of intra-tower and inter-tower tasks. However, we observe that joint optimization often leads to certain modalities dominating the training process, while other modalities are neglected. In addition, inconsistent training speeds across modalities can easily result in the one-epoch problem. To address these challenges, we propose a staged pretraining strategy, which guides the model to focus on specialized tasks at each stage, enabling it to effectively attend to and utilize multimodal features, and allowing flexible control over the training process at each stage to avoid the one-epoch problem. Furthermore, to better utilize the semantic IDs that compress high-dimensional multimodal embeddings, we design both generative and discriminative tasks to help the model understand the associations between SIDs, queries, and item features, thereby improving overall performance. Extensive experiments on large-scale real-world Meituan data demonstrate that our method achieves improvements of 3.80%, 2.64%, and 2.17% on R@5, R@10, and R@20, and 5.10%, 4.22%, and 2.09% on N@5, N@10, and N@20 compared to mainstream baselines. Online A/B testing on the Meituan platform shows that our approach achieves a 1.12% increase in revenue and a 1.02% increase in click-through rate, validating the effectiveness and superiority of our method in practical applications.

</details>


### [60] [On the Efficiency of Sequentially Aware Recommender Systems: Cotten4Rec](https://arxiv.org/abs/2602.06935)
*Shankar Veludandi,Gulrukh Kurdistan,Uzma Mushtaque*

Main category: cs.IR

TL;DR: Cotten4Rec：一种使用线性时间余弦相似度注意力的高效序列推荐模型，通过优化的CUDA内核实现，显著减少计算资源消耗


<details>
  <summary>Details</summary>
Motivation: Transformer-based序列推荐模型（如BERT4Rec）虽然能有效捕捉用户行为模式，但因其基于Softmax的注意力机制需要大量中间计算，导致显著的计算开销。在实际大规模推荐场景中，计算资源是关键限制因素。

Method: 提出Cotten4Rec模型，采用线性时间余弦相似度注意力机制，通过单个优化的CUDA内核实现，最小化中间缓冲区使用和内核启动开销。

Result: 在三个基准数据集上的评估表明，Cotten4Rec相比BERT4Rec和线性注意力基准LinRec，在内存使用和运行时间上实现了显著降低，同时推荐准确率损失最小。特别在中等序列长度和词汇量的数据集上效果更明显。

Conclusion: Cotten4Rec证明了作为高效替代方案的可行性，适用于计算资源受限的大规模序列推荐场景，在保持推荐准确性的同时大幅提升计算效率。

Abstract: Sequential recommendation (SR) models predict a user's next interaction by modeling their historical behaviors. Transformer-based SR methods, notably BERT4Rec, effectively capture these patterns but incur significant computational overhead due to extensive intermediate computations associated with Softmax-based attention. We propose Cotten4Rec, a novel SR model utilizing linear-time cosine similarity attention, implemented through a single optimized compute unified device architecture (CUDA) kernel. By minimizing intermediate buffers and kernel-launch overhead, Cotten4Rec substantially reduces resource usage compared to BERT4Rec and the linear-attention baseline, LinRec, especially for datasets with moderate sequence lengths and vocabulary sizes. Evaluations across three benchmark datasets confirm that Cotten4Rec achieves considerable reductions in memory and runtime with minimal compromise in recommendation accuracy, demonstrating Cotten4Rec's viability as an efficient alternative for practical, large-scale sequential recommendation scenarios where computational resources are critical.

</details>
