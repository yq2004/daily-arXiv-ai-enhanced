<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 43]
- [cs.IR](#cs.IR) [Total: 15]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [References Improve LLM Alignment in Non-Verifiable Domains](https://arxiv.org/abs/2602.16802)
*Kejian Shi,Yixin Liu,Peifeng Wang,Alexander R. Fabbri,Shafiq Joty,Arman Cohan*

Main category: cs.CL

TL;DR: 本文提出使用参考输出指导的LLM评估器作为软"验证器"，以解决非可验证领域（如LLM对齐）中缺乏真实验证器的问题，并通过参考引导的自我改进在LLM对齐任务中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统的可验证奖励强化学习（RLVR）需要真实验证器，无法直接应用于缺乏真实验证器的非可验证领域（如LLM对齐）。本文旨在探索是否可以通过参考指导的LLM评估器作为软"验证器"来弥补这一差距。

Method: 1. 设计使用参考输出的评估协议来增强基于LLM的评估器；2. 通过实验表明参考引导方法能显著提升较弱LLM法官的准确性（使用前沿模型的参考输出），以及通过高质量（人类撰写）参考提升较强LLM法官；3. 基于改进的法官，在LLM对齐调优中使用参考引导的LLM作为法官进行自我改进。

Result: 参考引导的自我改进在Llama-3-8B-Instruct上AlpacaEval达到73.1%，Arena-Hard达到58.7%；在Qwen2.5-7B上AlpacaEval达到70.0%，Arena-Hard达到74.1%。相比SFT蒸馏平均绝对增益为+20.2/+17.1点，相比无参考自我改进增益为+5.3/+3.6点（AlpacaEval/Arena-Hard）。性能与使用ArmoRM（强大的微调奖励模型）训练相当。

Conclusion: 参考引导的LLM评估器可作为有效的软"验证器"，在非可验证领域实现有效的LLM后训练，为LLM对齐等缺乏真实验证器的任务提供了可行的解决方案。

Abstract: While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft "verifiers". First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve. We show that reference-guided self-improvement yields clear gains over both direct SFT on reference outputs and self-improvement with reference-free judges, achieving performance comparable to training with ArmoRM, a strong finetuned reward model. Specifically, our method achieves 73.1% and 58.7% on AlpacaEval and Arena-Hard with Llama-3-8B-Instruct, and 70.0% and 74.1% with Qwen2.5-7B, corresponding to average absolute gains of +20.2 / +17.1 points over SFT distillation and +5.3 / +3.6 points over reference-free self-improvement on AlpacaEval / Arena-Hard. These results highlight the potential of using reference-guided LLM-evaluators to enable effective LLM post-training in non-verifiable domains.

</details>


### [2] [Evaluating Monolingual and Multilingual Large Language Models for Greek Question Answering: The DemosQA Benchmark](https://arxiv.org/abs/2602.16811)
*Charalampos Mastrokostas,Nikolaos Giarelis,Nikos Karacapilidis*

Main category: cs.CL

TL;DR: 该研究针对希腊语问答任务，创建了DemosQA数据集并评估了11种单语和多语大语言模型，以解决多语模型对低资源语言社会文化表征不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型研究主要针对高资源语言（如英语），多语模型存在对少数流行语言的训练数据偏见，或依赖从高资源语言到低资源语言的迁移学习，这可能导致社会、文化和历史方面的表征失真。虽然已有针对低资源语言的单语模型，但其在语言特定任务上的有效性相比多语模型研究不足。

Method: 1. 构建DemosQA数据集：使用社交媒体用户问题和社区审核答案，以更好捕捉希腊社会文化特征；2. 开发内存高效的大语言模型评估框架，可适应不同问答数据集和语言；3. 使用3种不同提示策略，对11种单语和多语大语言模型在6个人工整理的希腊语问答数据集上进行广泛评估。

Result: 研究创建了DemosQA数据集，开发了可复用的评估框架，并对11种模型在希腊语问答任务上进行了系统评估。代码和数据已公开以促进可复现性。

Conclusion: 该研究填补了希腊语问答任务的研究空白，通过构建专门的数据集和评估框架，为低资源语言的大语言模型研究提供了重要参考，强调了考虑社会文化背景在语言模型开发中的重要性。

Abstract: Recent advancements in Natural Language Processing and Deep Learning have enabled the development of Large Language Models (LLMs), which have significantly advanced the state-of-the-art across a wide range of tasks, including Question Answering (QA). Despite these advancements, research on LLMs has primarily targeted high-resourced languages (e.g., English), and only recently has attention shifted toward multilingual models. However, these models demonstrate a training data bias towards a small number of popular languages or rely on transfer learning from high- to under-resourced languages; this may lead to a misrepresentation of social, cultural, and historical aspects. To address this challenge, monolingual LLMs have been developed for under-resourced languages; however, their effectiveness remains less studied when compared to multilingual counterparts on language-specific tasks. In this study, we address this research gap in Greek QA by contributing: (i) DemosQA, a novel dataset, which is constructed using social media user questions and community-reviewed answers to better capture the Greek social and cultural zeitgeist; (ii) a memory-efficient LLM evaluation framework adaptable to diverse QA datasets and languages; and (iii) an extensive evaluation of 11 monolingual and multilingual LLMs on 6 human-curated Greek QA datasets using 3 different prompting strategies. We release our code and data to facilitate reproducibility.

</details>


### [3] [One-step Language Modeling via Continuous Denoising](https://arxiv.org/abs/2602.16813)
*Chanhyuk Lee,Jaehoon Yoo,Manan Agarwal,Sheel Shah,Jerry Huang,Aditi Raghunathan,Seunghoon Hong,Nicholas M. Boffi,Jinwoo Kim*

Main category: cs.CL

TL;DR: 提出基于连续流去噪的语言模型FLM和蒸馏版FMLM，在质量和速度上超越离散扩散模型，实现高质量少步生成


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型虽然被广泛研究用于加速语言生成，但在少步生成时样本质量急剧下降，未能兑现其加速承诺。需要探索新的方法来实现高质量少步生成。

Method: 1. 构建基于流的语言模型FLM，在one-hot词元编码上进行欧几里得去噪；2. 使用交叉熵目标预测干净数据，引入简单的时间重参数化提高训练稳定性和生成质量；3. 将FLM蒸馏到其关联的流映射中，得到FMLM用于少步生成。

Result: 在LM1B和OWT语言数据集上，FLM的生成质量与最先进的离散扩散模型相当。FMLM在所有指标上超越最近的少步语言模型，一步生成质量超过其他模型的8步生成质量。

Conclusion: 该工作质疑了离散扩散过程对于离散模态生成建模是必要的这一广泛假设，为大规模加速流基语言建模开辟了道路。连续流去噪方法在质量和速度上都优于离散扩散。

Abstract: Language models based on discrete diffusion have attracted widespread interest for their potential to provide faster generation than autoregressive models. In practice, however, they exhibit a sharp degradation of sample quality in the few-step regime, failing to realize this promise. Here we show that language models leveraging flow-based continuous denoising can outperform discrete diffusion in both quality and speed. By revisiting the fundamentals of flows over discrete modalities, we build a flow-based language model (FLM) that performs Euclidean denoising over one-hot token encodings. We show that the model can be trained by predicting the clean data via a cross entropy objective, where we introduce a simple time reparameterization that greatly improves training stability and generation quality. By distilling FLM into its associated flow map, we obtain a distilled flow map language model (FMLM) capable of few-step generation. On the LM1B and OWT language datasets, FLM attains generation quality matching state-of-the-art discrete diffusion models. With FMLM, our approach outperforms recent few-step language models across the board, with one-step generation exceeding their 8-step quality. Our work calls into question the widely held hypothesis that discrete diffusion processes are necessary for generative modeling over discrete modalities, and paves the way toward accelerated flow-based language modeling at scale. Code is available at https://github.com/david3684/flm.

</details>


### [4] [Claim Automation using Large Language Model](https://arxiv.org/abs/2602.16836)
*Zhengda Mo,Zhiyu Quan,Eli O'Donohue,Kaiwen Zhong*

Main category: cs.CL

TL;DR: 该研究开发了一个本地部署的治理感知语言建模组件，通过微调预训练LLMs来从非结构化保修索赔叙述中生成结构化纠正措施建议，显著提升保险领域应用性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在通用语言任务上表现优异，但在保险等受监管和数据敏感领域的部署仍然有限，需要开发可靠且可治理的解决方案来处理历史保修索赔数据。

Method: 使用低秩适应（LoRA）微调预训练LLMs，将其作为索赔处理流程中的初始决策模块，并采用结合自动语义相似度指标和人工评估的多维度评估框架。

Result: 领域特定微调显著优于商业通用模型和基于提示的LLMs，约80%的评估案例实现了与真实纠正措施近乎相同的匹配，证明领域自适应微调能更好地对齐模型输出分布与真实世界操作数据。

Conclusion: 该研究提供了理论和实证证据，表明领域自适应微调可作为保险应用中可靠且可治理的构建模块，有望加速理赔员的决策过程。

Abstract: While Large Language Models (LLMs) have achieved strong performance on general-purpose language tasks, their deployment in regulated and data-sensitive domains, including insurance, remains limited. Leveraging millions of historical warranty claims, we propose a locally deployed governance-aware language modeling component that generates structured corrective-action recommendations from unstructured claim narratives. We fine-tune pretrained LLMs using Low-Rank Adaptation (LoRA), scoping the model to an initial decision module within the claim processing pipeline to speed up claim adjusters' decisions. We assess this module using a multi-dimensional evaluation framework that combines automated semantic similarity metrics with human evaluation, enabling a rigorous examination of both practical utility and predictive accuracy. Our results show that domain-specific fine-tuning substantially outperforms commercial general-purpose and prompt-based LLMs, with approximately 80% of the evaluated cases achieving near-identical matches to ground-truth corrective actions. Overall, this study provides both theoretical and empirical evidence to prove that domain-adaptive fine-tuning can align model output distributions more closely with real-world operational data, demonstrating its promise as a reliable and governable building block for insurance applications.

</details>


### [5] [BanglaSummEval: Reference-Free Factual Consistency Evaluation for Bangla Summarization](https://arxiv.org/abs/2602.16843)
*Ahmed Rafid,Rumman Adib,Fariya Ahmed,Ajwad Abrar,Mohammed Saidul Islam*

Main category: cs.CL

TL;DR: 本文提出了BanglaSummEval，一个基于问答的无参考框架，用于评估孟加拉语摘要的事实一致性，通过自动生成问题、答案提取和BERTScore-Recall比较来评估事实准确性和内容覆盖度。


<details>
  <summary>Details</summary>
Motivation: 现有的事实一致性评估指标大多忽视了孟加拉语这一广泛使用但资源匮乏的语言，且通常依赖参考摘要，而现实应用中往往没有参考摘要可用。

Method: 提出一个基于问答的无参考框架：1）从源文档自动生成问题；2）从摘要中提取候选答案；3）使用单多语言指令调优语言模型处理问题生成、问题回答、答案提取和问题重要性加权；4）使用BERTScore-Recall进行答案比较以捕捉语义一致性。

Result: 在教育和医疗领域的300个人工编写摘要上进行验证，BanglaSummEval与专家人工判断显示出强相关性（皮尔逊相关系数r=0.694，斯皮尔曼相关系数ρ=0.763）。

Conclusion: BanglaSummEval为低资源语言环境下的事实一致性评估提供了一个实用且透明的解决方案，通过提供可解释的逐步诊断和可靠的评估分数。

Abstract: Evaluating factual consistency is essential for reliable text summarization, particularly in high-stakes domains such as healthcare and news. However, most existing evaluation metrics overlook Bangla, a widely spoken yet under-resourced language, and often depend on reference summaries. We introduce BanglaSummEval, a reference-free, question-answering-based framework for evaluating factual consistency in Bangla summarization. The proposed method assesses both factual accuracy and content coverage through automatically generated questions and answers derived from the source document and the summary. A single multilingual instruction-tuned language model handles question generation, question answering, candidate answer extraction, and question importance weighting. This unified design reduces system complexity and computational cost. To capture semantic consistency beyond surface-level overlap, we use BERTScore-Recall for answer comparison. We validate BanglaSummEval on 300 human-written summaries from educational and medical domains, demonstrating strong correlation with expert human judgments (Pearson's $r = 0.694$, Spearman's $ρ= 0.763$). By providing interpretable, step-wise diagnostics alongside reliable evaluation scores, BanglaSummEval offers a practical and transparent solution for factual consistency evaluation in low-resource language settings.

</details>


### [6] [Meenz bleibt Meenz, but Large Language Models Do Not Speak Its Dialect](https://arxiv.org/abs/2602.16852)
*Minh Duc Bui,Manuel Mager,Peter Herbert Kann,Katharina von der Wense*

Main category: cs.CL

TL;DR: 这是第一篇专注于美因茨方言的NLP研究，通过创建数字词典数据集并测试LLM在方言定义和生成任务上的表现，发现现有LLM在方言处理上准确率极低（<10%），凸显了德国方言NLP研究的紧迫需求。


<details>
  <summary>Details</summary>
Motivation: 美因茨方言作为德国美因茨狂欢节的传统语言正濒临消亡，与许多其他德国方言面临同样命运。NLP有潜力帮助保存和复兴方言，但此前尚无针对美因茨方言的NLP研究。

Method: 1. 创建数字词典数据集：从现有资源(Schramm, 1966)中提取2,351个方言单词及其标准德语释义。2. 使用该数据集评估LLM能力：测试LLM能否生成方言单词的定义，以及根据定义生成方言单词。3. 采用few-shot学习和从训练集中提取规则的方法改进结果。

Result: LLM在方言处理任务上表现极差：最佳的定义生成模型准确率仅6.27%，最佳单词生成模型准确率仅1.51%。即使采用few-shot学习和规则提取方法，准确率仍低于10%。

Conclusion: 现有LLM无法有效处理美因茨方言，凸显了需要更多方言资源和加强德国方言NLP研究的迫切性。这是该领域的第一步研究，为后续工作奠定了基础。

Abstract: Meenzerisch, the dialect spoken in the German city of Mainz, is also the traditional language of the Mainz carnival, a yearly celebration well known throughout Germany. However, Meenzerisch is on the verge of dying out-a fate it shares with many other German dialects. Natural language processing (NLP) has the potential to help with the preservation and revival efforts of languages and dialects. However, so far no NLP research has looked at Meenzerisch. This work presents the first research in the field of NLP that is explicitly focused on the dialect of Mainz. We introduce a digital dictionary-an NLP-ready dataset derived from an existing resource (Schramm, 1966)-to support researchers in modeling and benchmarking the language. It contains 2,351 words in the dialect paired with their meanings described in Standard German. We then use this dataset to answer the following research questions: (1) Can state-of-the-art large language models (LLMs) generate definitions for dialect words? (2) Can LLMs generate words in Meenzerisch, given their definitions? Our experiments show that LLMs can do neither: the best model for definitions reaches only 6.27% accuracy and the best word generation model's accuracy is 1.51%. We then conduct two additional experiments in order to see if accuracy is improved by few-shot learning and by extracting rules from the training set, which are then passed to the LLM. While those approaches are able to improve the results, accuracy remains below 10%. This highlights that additional resources and an intensification of research efforts focused on German dialects are desperately needed.

</details>


### [7] [A Conceptual Hybrid Framework for Post-Quantum Security: Integrating BB84 QKD, AES, and Bio-inspired Mechanisms](https://arxiv.org/abs/2602.16922)
*Md. Ismiel Hossen Abir*

Main category: cs.CL

TL;DR: 该研究提出一个混合安全框架，结合AES加密、BB84量子密钥分发、量子态比较和生物启发免疫系统，以应对量子计算对RSA密码的威胁，确保后量子时代的数据保护。


<details>
  <summary>Details</summary>
Motivation: 量子计算对经典密码学构成重大威胁，特别是基于大数分解难题的RSA算法。Shor量子算法能在多项式时间内高效破解RSA，而经典分解方法对大型密钥效率低下。因此需要设计能够抵御量子攻击的安全框架。

Method: 设计了一个混合安全框架，包含四个主要组件：1) AES加密提供经典安全；2) BB84量子密钥分发(QKD)用于安全密钥交换和窃听检测；3) 量子态比较用于轻量级认证；4) 生物启发免疫系统用于自适应威胁检测。

Result: 分析显示RSA对Shor算法存在漏洞，BB84在理想条件下能实现完整的密钥协商，并具有高精度的窃听检测能力。提出的概念模型整合了经典和量子安全方法，为后量子加密数据保护提供了可扩展的自适应解决方案。

Conclusion: 该研究提出了一个概念性的混合安全框架，旨在应对量子计算对密码学的威胁。该框架结合了多种安全技术，但详细的实现、安全性证明和广泛的实验验证仍需作为未来工作完成。

Abstract: Quantum computing is a significant risk to classical cryptographic, especially RSA, which depends on the difficulty of factoring large numbers. Classical factorization methods, such as Trial Division and Pollard's Rho, are inefficient for large keys, while Shor's quantum algorithm can break RSA efficiently in polynomial time. This research studies RSA's vulnerabilities under both classical and quantum attacks and designs a hybrid security framework to ensure data protection in the post-quantum era. The conceptual framework combines AES encryption for classical security, BB84 Quantum Key Distribution (QKD) for secure key exchange with eavesdropping detection, quantum state comparison for lightweight authentication, and a bio-inspired immune system for adaptive threat detection. RSA is vulnerable to Shor's algorithm, BB84 achieves full key agreement in ideal conditions, and it detects eavesdropping with high accuracy. The conceptual model includes both classical and quantum security methods, providing a scalable and adaptive solution for Post-Quantum encryption data protection. This work primarily proposes a conceptual framework. Detailed implementation, security proofs, and extensive experimental validation are considered future work.

</details>


### [8] [ConvApparel: A Benchmark Dataset and Validation Framework for User Simulators in Conversational Recommenders](https://arxiv.org/abs/2602.16938)
*Ofer Meshi,Krisztian Balog,Sally Goldman,Avi Caciularu,Guy Tennenholtz,Jihwan Jeong,Amir Globerson,Craig Boutilier*

Main category: cs.CL

TL;DR: 该论文针对LLM用户模拟器存在的"真实性差距"问题，提出了ConvApparel数据集和综合验证框架，发现数据驱动的模拟器在反事实验证中表现优于提示基线。


<details>
  <summary>Details</summary>
Motivation: LLM用户模拟器存在"真实性差距"，导致优化的系统在模拟交互中表现良好，但在真实世界中可能失败，需要解决这一问题来改进对话AI。

Method: 1. 引入ConvApparel数据集，采用双智能体数据收集协议（使用"好"和"坏"推荐器）；2. 提出综合验证框架，结合统计对齐、人类相似度评分和反事实验证；3. 通过实验比较不同模拟器的性能。

Result: 1. 所有模拟器都存在显著的真实性差距；2. 数据驱动的模拟器在反事实验证中优于提示基线，能更真实地适应未见行为；3. 数据驱动的模拟器体现了更稳健（尽管不完美）的用户模型。

Conclusion: ConvApparel数据集和综合验证框架为解决LLM用户模拟器的真实性差距提供了有效工具，数据驱动的模拟器在适应未见行为方面表现更好，但仍有改进空间。

Abstract: The promise of LLM-based user simulators to improve conversational AI is hindered by a critical "realism gap," leading to systems that are optimized for simulated interactions, but may fail to perform well in the real world. We introduce ConvApparel, a new dataset of human-AI conversations designed to address this gap. Its unique dual-agent data collection protocol -- using both "good" and "bad" recommenders -- enables counterfactual validation by capturing a wide spectrum of user experiences, enriched with first-person annotations of user satisfaction. We propose a comprehensive validation framework that combines statistical alignment, a human-likeness score, and counterfactual validation to test for generalization. Our experiments reveal a significant realism gap across all simulators. However, the framework also shows that data-driven simulators outperform a prompted baseline, particularly in counterfactual validation where they adapt more realistically to unseen behaviors, suggesting they embody more robust, if imperfect, user models.

</details>


### [9] [When Semantic Overlap Is Not Enough: Cross-Lingual Euphemism Transfer Between Turkish and English](https://arxiv.org/abs/2602.16957)
*Hasan Can Biyik,Libby Barak,Jing Peng,Anna Feldman*

Main category: cs.CL

TL;DR: 跨语言委婉语检测研究中，发现语义重叠不足以保证正向迁移，土耳其语到英语的低资源方向存在性能下降，而某些情况下非重叠委婉语训练反而能提升性能。


<details>
  <summary>Details</summary>
Motivation: 委婉语依赖文化背景和语用语境，这使得跨语言建模变得复杂。研究旨在探究跨语言等价性如何影响多语言委婉语检测中的迁移效果。

Method: 将土耳其语和英语中的潜在委婉术语（PETs）根据功能、语用和语义对齐分为重叠（OPETs）和非重叠（NOPETs）子集，研究跨语言迁移中的不对称现象。

Result: 发现迁移不对称性：语义重叠不足以保证正向迁移，特别是在土耳其语到英语的低资源方向，重叠委婉语性能可能下降，而某些情况下非重叠委婉语训练反而能提升性能。标签分布的差异有助于解释这些反直觉结果。

Conclusion: 跨语言委婉语检测中的迁移效果受语义重叠之外的多种因素影响，特别是标签分布和领域特异性对齐，但后者的证据因数据稀疏性而有限。

Abstract: Euphemisms substitute socially sensitive expressions, often softening or reframing meaning, and their reliance on cultural and pragmatic context complicates modeling across languages. In this study, we investigate how cross-lingual equivalence influences transfer in multilingual euphemism detection. We categorize Potentially Euphemistic Terms (PETs) in Turkish and English into Overlapping (OPETs) and Non-Overlapping (NOPETs) subsets based on their functional, pragmatic, and semantic alignment. Our findings reveal a transfer asymmetry: semantic overlap is insufficient to guarantee positive transfer, particularly in low-resource Turkish-to-English direction, where performance can degrade even for overlapping euphemisms, and in some cases, improve under NOPET-based training. Differences in label distribution help explain these counterintuitive results. Category-level analysis suggests that transfer may be influenced by domain-specific alignment, though evidence is limited by sparsity.

</details>


### [10] [Eigenmood Space: Uncertainty-Aware Spectral Graph Analysis of Psychological Patterns in Classical Persian Poetry](https://arxiv.org/abs/2602.16959)
*Kourosh Shahnazari,Seyed Moein Ayyoubzadeh,Mohammadali Keshtparvar*

Main category: cs.CL

TL;DR: 该研究提出了一种基于不确定性感知的计算框架，用于对古典波斯诗歌进行诗人层面的心理分析，通过大规模自动多标签标注、置信度加权聚合和谱嵌入方法，实现了可扩展且可审计的数字人文分析。


<details>
  <summary>Details</summary>
Motivation: 古典波斯诗歌通过隐喻、互文惯例和修辞间接性表达情感生活，这些特性使得细读不可或缺，但限制了大规模的可重复比较。需要一种既能进行大规模分析又能保持解释性谨慎的计算方法。

Method: 1) 大规模自动多标签标注，为每节诗歌关联一组心理学概念及置信度分数；2) 设置弃权标志处理证据不足的情况；3) 将置信度加权证据聚合为诗人×概念矩阵；4) 将每位诗人视为概念上的概率分布，使用Jensen-Shannon散度和Kullback-Leibler散度量化诗歌个性；5) 构建置信度加权的概念共现图，通过拉普拉斯谱分解定义Eigenmood嵌入；6) 在61,573节诗歌的语料库上进行分析。

Result: 1) 22.2%的诗节被弃权，突显了不确定性的分析重要性；2) 进行了置信度阈值设定的敏感性分析；3) 将弃权视为类别的选择偏倚诊断；4) 开发了从远读到细读的工作流程，可沿Eigenmood轴检索诗节级示例；5) 框架支持可扩展、可审计的数字人文分析，同时通过从诗节级证据到诗人级推理传播不确定性来保持解释性谨慎。

Conclusion: 该不确定性感知计算框架为古典波斯诗歌的心理分析提供了一种可扩展且可审计的方法，既能进行大规模比较分析，又能通过传播不确定性保持数字人文研究所需的解释性谨慎，实现了远读与细读的有效结合。

Abstract: Classical Persian poetry is a historically sustained archive in which affective life is expressed through metaphor, intertextual convention, and rhetorical indirection. These properties make close reading indispensable while limiting reproducible comparison at scale. We present an uncertainty-aware computational framework for poet-level psychological analysis based on large-scale automatic multi-label annotation. Each verse is associated with a set of psychological concepts, per-label confidence scores, and an abstention flag that signals insufficient evidence. We aggregate confidence-weighted evidence into a Poet $\times$ Concept matrix, interpret each poet as a probability distribution over concepts, and quantify poetic individuality as divergence from a corpus baseline using Jensen--Shannon divergence and Kullback--Leibler divergence. To capture relational structure beyond marginals, we build a confidence-weighted co-occurrence graph over concepts and define an Eigenmood embedding through Laplacian spectral decomposition. On a corpus of 61{,}573 verses across 10 poets, 22.2\% of verses are abstained, underscoring the analytical importance of uncertainty. We further report sensitivity analysis under confidence thresholding, selection-bias diagnostics that treat abstention as a category, and a distant-to-close workflow that retrieves verse-level exemplars along Eigenmood axes. The resulting framework supports scalable, auditable digital-humanities analysis while preserving interpretive caution by propagating uncertainty from verse-level evidence to poet-level inference.

</details>


### [11] [Persona2Web: Benchmarking Personalized Web Agents for Contextual Reasoning with User History](https://arxiv.org/abs/2602.17003)
*Serin Kim,Sangam Lee,Dongha Lee*

Main category: cs.CL

TL;DR: Persona2Web是首个在真实开放网络上评估个性化网页代理的基准，基于"澄清以个性化"原则，要求代理根据用户历史而非显式指令来解析模糊查询。


<details>
  <summary>Details</summary>
Motivation: 当前网页代理缺乏个性化能力，用户很少详细说明所有意图细节，因此实用网页代理必须能够通过推断用户偏好和上下文来解释模糊查询。

Method: Persona2Web基准包含：1）用户历史记录，揭示长期隐含偏好；2）模糊查询，要求代理推断用户隐含偏好；3）推理感知评估框架，支持细粒度个性化评估。在不同代理架构、骨干模型、历史访问方案和模糊程度查询上进行了广泛实验。

Result: 通过实验揭示了个性化网页代理行为中的关键挑战，为研究提供了可复现的代码和数据集。

Conclusion: Persona2Web是首个在真实开放网络上评估个性化网页代理的基准，为理解和改进个性化网页代理能力提供了重要工具和框架。

Abstract: Large language models have advanced web agents, yet current agents lack personalization capabilities. Since users rarely specify every detail of their intent, practical web agents must be able to interpret ambiguous queries by inferring user preferences and contexts. To address this challenge, we present Persona2Web, the first benchmark for evaluating personalized web agents on the real open web, built upon the clarify-to-personalize principle, which requires agents to resolve ambiguity based on user history rather than relying on explicit instructions. Persona2Web consists of: (1) user histories that reveal preferences implicitly over long time spans, (2) ambiguous queries that require agents to infer implicit user preferences, and (3) a reasoning-aware evaluation framework that enables fine-grained assessment of personalization. We conduct extensive experiments across various agent architectures, backbone models, history access schemes, and queries with varying ambiguity levels, revealing key challenges in personalized web agent behavior. For reproducibility, our codes and datasets are publicly available at https://anonymous.4open.science/r/Persona2Web-73E8.

</details>


### [12] [ReIn: Conversational Error Recovery with Reasoning Inception](https://arxiv.org/abs/2602.17022)
*Takyoung Kim,Jinseok Nam,Chandrayee Basu,Xing Fan,Chengyuan Ma,Heng Ji,Gokhan Tur,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: ReIn方法通过外部推理模块诊断对话错误并生成恢复计划，在不修改模型参数或提示的情况下提升对话代理的容错能力


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的对话代理在固定任务导向数据集上表现良好，但对用户引发的意外错误缺乏恢复能力。现有方法需要微调或修改提示，成本高且不实用，需要一种不修改模型参数的实时错误恢复方案。

Method: 提出Reasoning Inception (ReIn)测试时干预方法：外部inception模块识别对话上下文中的预定义错误并生成恢复计划，然后将这些计划集成到代理的决策过程中，引导其采取纠正措施，而不修改模型参数或系统提示。

Result: ReIn显著提高了任务成功率并能泛化到未见错误类型，在多种代理模型和inception模块组合中表现稳定，且优于显式的提示修改方法。分析表明，与ReIn联合定义恢复工具是提高对话代理韧性的安全有效策略。

Conclusion: ReIn是一种高效、实时的对话代理错误恢复方法，无需修改主干模型或系统提示，通过植入初始推理来增强代理的容错能力，为提升对话系统韧性提供了实用解决方案。

Abstract: Conversational agents powered by large language models (LLMs) with tool integration achieve strong performance on fixed task-oriented dialogue datasets but remain vulnerable to unanticipated, user-induced errors. Rather than focusing on error prevention, this work focuses on error recovery, which necessitates the accurate diagnosis of erroneous dialogue contexts and execution of proper recovery plans. Under realistic constraints precluding model fine-tuning or prompt modification due to significant cost and time requirements, we explore whether agents can recover from contextually flawed interactions and how their behavior can be adapted without altering model parameters and prompts. To this end, we propose Reasoning Inception (ReIn), a test-time intervention method that plants an initial reasoning into the agent's decision-making process. Specifically, an external inception module identifies predefined errors within the dialogue context and generates recovery plans, which are subsequently integrated into the agent's internal reasoning process to guide corrective actions, without modifying its parameters or system prompts. We evaluate ReIn by systematically simulating conversational failure scenarios that directly hinder successful completion of user goals: user's ambiguous and unsupported requests. Across diverse combinations of agent models and inception modules, ReIn substantially improves task success and generalizes to unseen error types. Moreover, it consistently outperforms explicit prompt-modification approaches, underscoring its utility as an efficient, on-the-fly method. In-depth analysis of its operational mechanism, particularly in relation to instruction hierarchy, indicates that jointly defining recovery tools with ReIn can serve as a safe and effective strategy for improving the resilience of conversational agents without modifying the backbone models or system prompts.

</details>


### [13] [Large Language Models Persuade Without Planning Theory of Mind](https://arxiv.org/abs/2602.17045)
*Jared Moore,Rasmus Overmark,Ned Cooper,Beba Cibralic,Nick Haber,Cameron R. Jones*

Main category: cs.CL

TL;DR: 本研究开发了一种新颖的互动式心理理论评估任务，要求说服者通过策略性信息揭示来说服目标选择特定政策提案。研究发现LLMs在显式条件下表现优异，但在需要多步骤推理的隐性条件下表现不佳，而人类表现相对稳定。有趣的是，在真人目标实验中，LLMs的劝说效果反而优于人类。


<details>
  <summary>Details</summary>
Motivation: 现有心理理论评估多采用静态问答形式，但理论研究表明第一人称互动对心理理论至关重要。本研究旨在填补这一空白，开发互动式任务来更准确地评估人类和LLMs的心理理论能力。

Method: 设计了一个新颖的劝说任务：说服者需通过策略性信息揭示来说服目标从三个政策提案中选择一个。实验设置了两种条件：目标心理状态（知识状态和动机状态）对说服者显式揭示（Revealed）或隐性隐藏（Hidden）。在实验1中，参与者劝说一个只做理性推理的机器人；实验2中，人类扮演机器人目标；实验3测量了真人目标的实际信念改变。

Result: 实验1中，LLMs在显式条件下表现出色，但在隐性条件下表现低于随机水平，表明其在多步骤规划方面存在困难。人类在两个条件下表现中等。实验2和3中，LLMs在所有条件下都优于人类说服者，表明有效的劝说可能不需要显式心理理论推理，而可以通过修辞策略实现。

Conclusion: 研究结果警告不应将类人心理理论能力归因于LLMs，同时突显了LLMs在影响人们信念和行为方面的潜力。有效的劝说可以通过多种机制实现，不一定需要显式心理理论推理。

Abstract: A growing body of work attempts to evaluate the theory of mind (ToM) abilities of humans and large language models (LLMs) using static, non-interactive question-and-answer benchmarks. However, theoretical work in the field suggests that first-personal interaction is a crucial part of ToM and that such predictive, spectatorial tasks may fail to evaluate it. We address this gap with a novel ToM task that requires an agent to persuade a target to choose one of three policy proposals by strategically revealing information. Success depends on a persuader's sensitivity to a given target's knowledge states (what the target knows about the policies) and motivational states (how much the target values different outcomes). We varied whether these states were Revealed to persuaders or Hidden, in which case persuaders had to inquire about or infer them. In Experiment 1, participants persuaded a bot programmed to make only rational inferences. LLMs excelled in the Revealed condition but performed below chance in the Hidden condition, suggesting difficulty with the multi-step planning required to elicit and use mental state information. Humans performed moderately well in both conditions, indicating an ability to engage such planning. In Experiment 2, where a human target role-played the bot, and in Experiment 3, where we measured whether human targets' real beliefs changed, LLMs outperformed human persuaders across all conditions. These results suggest that effective persuasion can occur without explicit ToM reasoning (e.g., through rhetorical strategies) and that LLMs excel at this form of persuasion. Overall, our results caution against attributing human-like ToM to LLMs while highlighting LLMs' potential to influence people's beliefs and behavior.

</details>


### [14] [Evaluating Cross-Lingual Classification Approaches Enabling Topic Discovery for Multilingual Social Media Data](https://arxiv.org/abs/2602.17051)
*Deepak Uniyal,Md Abul Bashar,Richi Nayak*

Main category: cs.CL

TL;DR: 本研究比较了四种跨语言文本分类方法，用于从多语言社交媒体数据中过滤氢能源相关推文并进行主题发现，分析了翻译方法与多语言方法之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 分析多语言社交媒体话语是自然语言处理的主要挑战，特别是在大规模公共辩论跨越多种语言时。本研究旨在探索如何通过跨语言文本分类方法可靠分析全球对话，以氢能源为案例研究。

Method: 使用2013-2022年超过900万条英语、日语、印地语和韩语推文数据集，探索四种方法过滤相关内容：1）将英语标注数据翻译为目标语言构建语言特定模型；2）将所有语言未标注数据翻译为英语创建基于英语标注的单模型；3）直接将英语微调的多语言transformer应用于各目标语言数据；4）结合翻译标注与多语言训练的混合策略。随后对相关子集进行主题建模。

Result: 结果突出了翻译方法与多语言方法之间的关键权衡，并提供了优化大规模社交媒体分析跨语言管道的可行见解。每种方法在过滤氢能源相关推文方面都有不同的表现。

Conclusion: 研究提供了关于优化跨语言管道用于大规模社交媒体分析的可行动见解，强调了不同方法在处理多语言社交媒体数据时的优势和局限性。

Abstract: Analysing multilingual social media discourse remains a major challenge in natural language processing, particularly when large-scale public debates span across diverse languages. This study investigates how different approaches for cross-lingual text classification can support reliable analysis of global conversations. Using hydrogen energy as a case study, we analyse a decade-long dataset of over nine million tweets in English, Japanese, Hindi, and Korean (2013--2022) for topic discovery. The online keyword-driven data collection results in a significant amount of irrelevant content. We explore four approaches to filter relevant content: (1) translating English annotated data into target languages for building language-specific models for each target language, (2) translating unlabelled data appearing from all languages into English for creating a single model based on English annotations, (3) applying English fine-tuned multilingual transformers directly to each target language data, and (4) a hybrid strategy that combines translated annotations with multilingual training. Each approach is evaluated for its ability to filter hydrogen-related tweets from noisy keyword-based collections. Subsequently, topic modeling is performed to extract dominant themes within the relevant subsets. The results highlight key trade-offs between translation and multilingual approaches, offering actionable insights into optimising cross-lingual pipelines for large-scale social media analysis.

</details>


### [15] [ALPS: A Diagnostic Challenge Set for Arabic Linguistic & Pragmatic Reasoning](https://arxiv.org/abs/2602.17054)
*Hussein S. Al-Olimat,Ahmad Alshareef*

Main category: cs.CL

TL;DR: ALPS是一个阿拉伯语语言学与语用学诊断套件，包含531个专家精心设计的问题，用于评估模型在深层语义和语用理解上的能力，揭示了当前模型在阿拉伯语形态句法依赖上的显著缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前阿拉伯语NLP基准测试过于注重规模和合成/翻译数据，缺乏对深层语言理解的深度评估，需要原生、专家策划的诊断工具来评估模型的语义和语用能力。

Method: 创建ALPS数据集，包含531个专家精心设计的问题，涵盖15个任务和47个子任务，基于阿拉伯语语言学专业知识，确保文化真实性和消除翻译伪影。评估了23个不同模型（商业、开源和阿拉伯语原生模型），并与人类单次通过性能（平均84.6%）和专家裁决的oracle（99.2%）进行比较。

Result: 模型在形态句法依赖任务上表现较差（依赖音标的任务错误率达36.5%），而组合语义任务表现较好。顶级商业模型（Gemini-3-flash达到94.2%）超越平均单个人类表现，但商业巨头与阿拉伯语原生模型之间存在显著差距，最佳阿拉伯语特定模型（Jais-2-70B达到83.6%）接近但未达到人类表现水平。

Conclusion: ALPS揭示了当前阿拉伯语NLP模型在深层语言理解上的关键分离：模型具备高流畅性但在基础形态句法依赖上表现不佳，强调了需要超越规模导向基准的深度语言评估工具。

Abstract: While recent Arabic NLP benchmarks focus on scale, they often rely on synthetic or translated data which may benefit from deeper linguistic verification. We introduce ALPS (Arabic Linguistic & Pragmatic Suite), a native, expert-curated diagnostic challenge set probing Deep Semantics and Pragmatics, capabilities that complement specialized large-scale benchmarks. While broad-coverage benchmarks prioritize scale and multi-task coverage, ALPS targets the depth of linguistic understanding through 531 rigorously crafted questions across 15 tasks and 47 subtasks. We developed the dataset with deep expertise in Arabic linguistics, guaranteeing cultural authenticity and eliminating translation artifacts. Evaluating 23 diverse models (commercial, open-source, and Arabic-native) against a single-pass human performance (avg. 84.6% accuracy) and an expert-adjudicated oracle (99.2%), we reveal a critical dissociation: models achieve high fluency but fail on fundamental morpho-syntactic dependencies, with elevated error rates on morpho-syntactic dependencies (36.5% across diacritics-reliant tasks) compared to compositional semantics. While top commercial models (Gemini-3-flash at 94.2%) surpass the average single human, a substantial gap persists between commercial giants and Arabic-native models, with the best Arabic-specific model (Jais-2-70B at 83.6%) approaching but not matching human performance.

</details>


### [16] [BankMathBench: A Benchmark for Numerical Reasoning in Banking Scenarios](https://arxiv.org/abs/2602.17072)
*Yunseung Lee,Subin Kim,Youngjun Kwak,Jaegul Choo*

Main category: cs.CL

TL;DR: 研究人员提出了BankMathBench，这是一个针对银行数学推理任务的领域特定数据集，用于评估和改进大语言模型在真实银行场景中的数值推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在银行领域的核心计算任务中准确性较低，包括总支付估算、不同利率产品比较、提前还款利息计算等。这些任务需要多步数值推理和对银行产品的上下文理解，但现有模型常出现系统性错误。然而，现有基准测试未能有效捕捉这些错误，数学数据集关注基础数学问题，金融基准主要针对金融文档，日常银行场景研究不足。

Method: 提出了BankMathBench数据集，该数据集按难度分为三个级别：基础级（单产品推理）、中级（多产品比较）和高级（多条件场景）。通过在该数据集上训练开源大语言模型，并使用工具增强的微调方法，评估模型在公式生成和数值推理准确性方面的改进。

Result: 经过BankMathBench训练后，开源大语言模型在公式生成和数值推理准确性方面均有显著提升。通过工具增强微调，模型在基础、中级和高级任务上的平均准确率分别提高了57.6个百分点、75.1个百分点和62.9个百分点，相对于零样本基线有显著增益。

Conclusion: BankMathBench是一个可靠的基准测试工具，可用于评估和推进大语言模型在真实银行场景中的数值推理能力。该数据集能有效提高模型在银行数学推理任务中的表现，填补了现有基准测试在银行日常场景评估方面的空白。

Abstract: Large language models (LLMs)-based chatbots are increasingly being adopted in the financial domain, particularly in digital banking, to handle customer inquiries about products such as deposits, savings, and loans. However, these models still exhibit low accuracy in core banking computations-including total payout estimation, comparison of products with varying interest rates, and interest calculation under early repayment conditions. Such tasks require multi-step numerical reasoning and contextual understanding of banking products, yet existing LLMs often make systematic errors-misinterpreting product types, applying conditions incorrectly, or failing basic calculations involving exponents and geometric progressions. However, such errors have rarely been captured by existing benchmarks. Mathematical datasets focus on fundamental math problems, whereas financial benchmarks primarily target financial documents, leaving everyday banking scenarios underexplored. To address this limitation, we propose BankMathBench, a domain-specific dataset that reflects realistic banking tasks. BankMathBench is organized in three levels of difficulty-basic, intermediate, and advanced-corresponding to single-product reasoning, multi-product comparison, and multi-condition scenarios, respectively. When trained on BankMathBench, open-source LLMs exhibited notable improvements in both formula generation and numerical reasoning accuracy, demonstrating the dataset's effectiveness in enhancing domain-specific reasoning. With tool-augmented fine-tuning, the models achieved average accuracy increases of 57.6%p (basic), 75.1%p (intermediate), and 62.9%p (advanced), representing significant gains over zero-shot baselines. These findings highlight BankMathBench as a reliable benchmark for evaluating and advancing LLMs' numerical reasoning in real-world banking scenarios.

</details>


### [17] [Projective Psychological Assessment of Large Multimodal Models Using Thematic Apperception Tests](https://arxiv.org/abs/2602.17108)
*Anton Dzega,Aviad Elyashar,Ortal Slobodin,Odeya Cohen,Rami Puzis*

Main category: cs.CL

TL;DR: 该研究使用主题统觉测试(TAT)和SCORS-G评估框架，通过非语言模态评估大型多模态模型的人格特质，发现评估模型能有效分析TAT响应，与人类专家高度一致，但模型普遍无法感知和调节攻击性。


<details>
  <summary>Details</summary>
Motivation: 探索大型多模态模型是否可以通过非语言模态（如图像响应）来评估其人格特质，突破传统基于语言的人格评估限制，验证TAT测试在AI模型评估中的适用性。

Method: 使用主题统觉测试(TAT)作为评估框架，让大型多模态模型扮演两种角色：主体模型生成TAT图像的故事响应，评估模型使用SCORS-G框架对这些叙事进行评估分析。

Result: 评估模型表现出优秀的TAT响应理解分析能力，与人类专家评估高度一致。所有模型对人际动态有良好理解，自我概念掌握较好，但普遍无法感知和调节攻击性。更大、更新的模型在所有SCORS-G维度上表现更优。

Conclusion: TAT和SCORS-G框架可有效评估大型多模态模型的人格特质，揭示了AI模型在情感理解方面的特定缺陷（特别是攻击性感知），为AI人格评估提供了新方法。

Abstract: Thematic Apperception Test (TAT) is a psychometrically grounded, multidimensional assessment framework that systematically differentiates between cognitive-representational and affective-relational components of personality-like functioning. This test is a projective psychological framework designed to uncover unconscious aspects of personality. This study examines whether the personality traits of Large Multimodal Models (LMMs) can be assessed through non-language-based modalities, using the Social Cognition and Object Relations Scale - Global (SCORS-G). LMMs are employed in two distinct roles: as subject models (SMs), which generate stories in response to TAT images, and as evaluator models (EMs), who assess these narratives using the SCORS-G framework. Evaluators demonstrated an excellent ability to understand and analyze TAT responses. Their interpretations are highly consistent with those of human experts. Assessment results highlight that all models understand interpersonal dynamics very well and have a good grasp of the concept of self. However, they consistently fail to perceive and regulate aggression. Performance varied systematically across model families, with larger and more recent models consistently outperforming smaller and earlier ones across SCORS-G dimensions.

</details>


### [18] [The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI](https://arxiv.org/abs/2602.17127)
*Dusan Bosnjakovic*

Main category: cs.CL

TL;DR: 该论文提出了一种基于心理测量理论的审计框架，用于量化大型语言模型中持久的行为特征，揭示了提供商级别的系统性偏见，这些偏见可能在多层AI架构中形成递归意识形态回音室。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型从独立的聊天界面转变为多智能体系统中的基础推理层和递归评估循环（LLM-as-a-judge），检测持久、提供商级别的行为特征成为安全和治理的关键需求。传统基准测试只衡量瞬态任务准确性，无法捕捉训练和对齐过程中嵌入的稳定、潜在响应策略——即"主流思维定式"。

Method: 引入基于心理测量理论（特别是序数不确定性下的潜在特质估计）的审计框架，使用由语义正交干扰项掩盖的强制选择序数情景，并通过密码学置换不变性控制。对九个领先模型在优化偏见、谄媚性和现状合法化等维度进行审计，使用混合线性模型和组内相关系数分析。

Result: 研究发现，虽然项目层面的框架驱动了高方差，但持续的"实验室信号"解释了显著的行为聚类。这表明在"锁定"的提供商生态系统中，潜在偏见不仅是静态错误，还是可能在多层次AI架构中创建递归意识形态回音室的复合变量。

Conclusion: 该研究证明了心理测量方法能够有效量化LLM中的持久行为特征，为AI安全和治理提供了新工具。这些发现强调了在复杂AI系统中系统性偏见的风险，以及需要超越传统基准测试的审计方法。

Abstract: As Large Language Models (LLMs) transition from standalone chat interfaces to foundational reasoning layers in multi-agent systems and recursive evaluation loops (LLM-as-a-judge), the detection of durable, provider-level behavioral signatures becomes a critical requirement for safety and governance. Traditional benchmarks measure transient task accuracy but fail to capture stable, latent response policies -- the ``prevailing mindsets'' embedded during training and alignment that outlive individual model versions.
  This paper introduces a novel auditing framework that utilizes psychometric measurement theory -- specifically latent trait estimation under ordinal uncertainty -- to quantify these tendencies without relying on ground-truth labels. Utilizing forced-choice ordinal vignettes masked by semantically orthogonal decoys and governed by cryptographic permutation-invariance, the research audits nine leading models across dimensions including Optimization Bias, Sycophancy, and Status-Quo Legitimization.
  Using Mixed Linear Models (MixedLM) and Intraclass Correlation Coefficient (ICC) analysis, the research identifies that while item-level framing drives high variance, a persistent ``lab signal'' accounts for significant behavioral clustering. These findings demonstrate that in ``locked-in'' provider ecosystems, latent biases are not merely static errors but compounding variables that risk creating recursive ideological echo chambers in multi-layered AI architectures.

</details>


### [19] [What Makes a Good Doctor Response? An Analysis on a Romanian Telemedicine Platform](https://arxiv.org/abs/2602.17194)
*Adrian Cosma,Cosmin Dumitrache,Emilian Radoi*

Main category: cs.CL

TL;DR: 研究分析了罗马尼亚文本远程医疗中患者满意度信号，发现患者和医生历史特征是主要预测因子，而回应文本特征虽影响较小但具有可操作性，其中礼貌和模糊语与积极反馈正相关，词汇多样性则负相关。


<details>
  <summary>Details</summary>
Motivation: 文本远程医疗日益普及，医生需要通过书面形式清晰有效地提供医疗建议。随着平台越来越依赖患者评分和反馈，医生面临维持满意度的压力，但这些评价往往反映的是沟通质量而非临床准确性。因此需要研究患者满意度信号在文本远程医疗中的影响因素。

Method: 使用77,334个匿名患者问题-医生回应对作为样本，将反馈建模为二元结果（点赞为积极反馈，负面或无反馈为另一类）。提取可解释、主要语言无关的特征（长度、结构特征、可读性代理），以及罗马尼亚LIWC心理语言学特征和礼貌/模糊语标记。采用时间分割训练分类器，并进行SHAP分析。

Result: SHAP分析表明患者和医生历史特征主导预测，作为强先验；而回应文本特征提供较小但关键的可操作信号。子组相关性分析显示礼貌和模糊语始终与患者反馈正相关，而词汇多样性则呈现负相关。

Conclusion: 在文本远程医疗中，患者和医生历史特征是最强的满意度预测因子，但回应文本特征特别是礼貌表达和模糊语的使用，为医生提供了可操作的方式来改善患者满意度，而过度复杂的词汇表达可能适得其反。

Abstract: Text-based telemedicine has become a common mode of care, requiring clinicians to deliver medical advice clearly and effectively in writing. As platforms increasingly rely on patient ratings and feedback, clinicians face growing pressure to maintain satisfaction scores, even though these evaluations often reflect communication quality more than clinical accuracy. We analyse patient satisfaction signals in Romanian text-based telemedicine. Using a sample of 77,334 anonymised patient question--doctor response pairs, we model feedback as a binary outcome, treating thumbs-up responses as positive and grouping negative or absent feedback into the other class. We extract interpretable, predominantly language-agnostic features (e.g., length, structural characteristics, readability proxies), along with Romanian LIWC psycholinguistic features and politeness/hedging markers where available. We train a classifier with a time-based split and perform SHAP-based analyses, which indicate that patient and clinician history features dominate prediction, functioning as strong priors, while characteristics of the response text provide a smaller but, crucially, actionable signal. In subgroup correlation analyses, politeness and hedging are consistently positively associated with patient feedback, whereas lexical diversity shows a negative association.

</details>


### [20] [Quantifying and Mitigating Socially Desirable Responding in LLMs: A Desirability-Matched Graded Forced-Choice Psychometric Study](https://arxiv.org/abs/2602.17262)
*Kensuke Okada,Yui Furukawa,Kyosuke Bunji*

Main category: cs.CL

TL;DR: 该研究提出了一个心理测量学框架来量化和缓解大语言模型在问卷评估中的社会期望性反应偏差，通过对比诚实与伪装好条件下的回答来量化偏差，并设计匹配社会期望性的强制选择问卷来减轻偏差。


<details>
  <summary>Details</summary>
Motivation: 当前NLP领域越来越多使用人类自我报告问卷来评估大语言模型，但这些工具假设诚实回答。在评估情境中，LLMs倾向于选择社会偏好的答案（社会期望性反应），这会扭曲问卷得分和后续结论。

Method: 提出心理测量学框架：1）通过HONEST vs FAKE-GOOD指令下相同问卷的对比，使用项目反应理论估计的潜变量分数计算方向校正的标准化效应大小来量化SDR；2）通过约束优化从项目池中选择30个跨领域对构建匹配社会期望性的分级强制选择问卷来缓解SDR。

Result: 在9个指令调优的LLMs上评估：Likert式问卷显示持续存在大量SDR，而社会期望性匹配的分级强制选择问卷显著减轻了SDR，同时基本保持了目标人物档案的恢复准确性，揭示了模型依赖的SDR-恢复权衡。

Conclusion: 研究强调了LLMs在问卷评估中存在社会期望性反应偏差，提出了量化和缓解该偏差的框架，并建议在基于问卷的LLMs基准测试和审计中采用SDR感知的报告实践。

Abstract: Human self-report questionnaires are increasingly used in NLP to benchmark and audit large language models (LLMs), from persona consistency to safety and bias assessments. Yet these instruments presume honest responding; in evaluative contexts, LLMs can instead gravitate toward socially preferred answers-a form of socially desirable responding (SDR)-biasing questionnaire-derived scores and downstream conclusions. We propose a psychometric framework to quantify and mitigate SDR in questionnaire-based evaluation of LLMs. To quantify SDR, the same inventory is administered under HONEST versus FAKE-GOOD instructions, and SDR is computed as a direction-corrected standardized effect size from item response theory (IRT)-estimated latent scores. This enables comparisons across constructs and response formats, as well as against human instructed-faking benchmarks. For mitigation, we construct a graded forced-choice (GFC) Big Five inventory by selecting 30 cross-domain pairs from an item pool via constrained optimization to match desirability. Across nine instruction-tuned LLMs evaluated on synthetic personas with known target profiles, Likert-style questionnaires show consistently large SDR, whereas desirability-matched GFC substantially attenuates SDR while largely preserving the recovery of the intended persona profiles. These results highlight a model-dependent SDR-recovery trade-off and motivate SDR-aware reporting practices for questionnaire-based benchmarking and auditing of LLMs.

</details>


### [21] [Towards Cross-lingual Values Assessment: A Consensus-Pluralism Perspective](https://arxiv.org/abs/2602.17283)
*Yukun Chen,Xinyu Zhang,Jialong Tang,Yu Wan,Baosong Yang,Yiming Li,Zhan Qin,Kui Ren*

Main category: cs.CL

TL;DR: X-Value是一个跨语言价值观评估基准，包含5000多个QA对，覆盖18种语言，基于施瓦茨基本人类价值观理论，用于评估LLMs对内容深层价值观的识别能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs的内容安全评估主要关注显性危害（如暴力、仇恨言论），忽略了内容中传达的微妙价值观维度。需要从全球视角评估LLMs对深层价值观的识别能力。

Method: 提出X-Value基准，包含5000多个QA对，覆盖18种语言，基于施瓦茨基本人类价值观理论，分为7个核心领域和易/难两个难度级别。采用两阶段标注框架：首先判断问题属于全球共识还是多元主义范畴，然后进行多方评估识别内容中的潜在价值观。

Result: 当前最先进的LLMs在跨语言价值观评估方面表现不足（准确率<77%），不同语言之间存在显著性能差异（准确率差异>20%）。

Conclusion: 这项工作强调了改进LLMs细微、价值观感知的内容评估能力的迫切需求。X-Value基准可用于推动LLMs在跨文化价值观理解方面的进步。

Abstract: While large language models (LLMs) have become pivotal to content safety, current evaluation paradigms primarily focus on detecting explicit harms (e.g., violence or hate speech), neglecting the subtler value dimensions conveyed in digital content. To bridge this gap, we introduce X-Value, a novel Cross-lingual Values Assessment Benchmark designed to evaluate LLMs' ability to assess deep-level values of content from a global perspective. X-Value consists of more than 5,000 QA pairs across 18 languages, systematically organized into 7 core domains grounded in Schwartz's Theory of Basic Human Values and categorized into easy and hard levels for discriminative evaluation. We further propose a unique two-stage annotation framework that first identifies whether an issue falls under global consensus (e.g., human rights) or pluralism (e.g., religion), and subsequently conducts a multi-party evaluation of the latent values embedded within the content. Systematic evaluations on X-Value reveal that current SOTA LLMs exhibit deficiencies in cross-lingual values assessment ($Acc < 77\%$), with significant performance disparities across different languages ($ΔAcc > 20\%$). This work highlights the urgent need to improve the nuanced, values-aware content assessment capability of LLMs. Our X-Value is available at: https://huggingface.co/datasets/Whitolf/X-Value.

</details>


### [22] [Representation Collapse in Machine Translation Through the Lens of Angular Dispersion](https://arxiv.org/abs/2602.17287)
*Evgeniia Tokarchuk,Maya K. Nachesa,Sergey Troshin,Vlad Niculae*

Main category: cs.CL

TL;DR: 该论文研究了Transformer神经翻译模型中的表示坍塌问题，提出了基于角度分散的正则化方法来缓解此问题，并在离散和连续NMT模型中验证了其有效性，同时发现量化模型也存在类似问题且正则化依然有效。


<details>
  <summary>Details</summary>
Motivation: 现代基于Transformer的神经翻译模型在高资源数据集上表现优异，但标准的下一个token预测训练策略可能导致表示坍塌等被忽视的伪影。这一问题在深层Transformer层中尤为明显，无法高效利用几何空间。在端到端连续输出神经机器翻译训练中，表示坍塌更加明显，可能导致所有向量都收敛到相同值的平凡解。

Method: 分析了离散和连续NMT Transformer在不同训练阶段的表示坍塌动态。采用了基于角度分散的现有正则化方法，通过实验验证该方法不仅能缓解表示坍塌，还能提高翻译质量。同时研究了量化模型中的坍塌行为，并验证正则化在量化后依然有效。

Result: 实验证明，基于角度分散的正则化方法有效缓解了表示坍塌问题，同时提高了翻译质量。研究发现量化模型也表现出类似的坍塌行为，但正则化的好处在量化后依然得以保留。

Conclusion: 表示坍塌是Transformer神经翻译模型中的一个重要问题，特别是在深层和连续输出模型中。基于角度分散的正则化方法不仅能有效缓解坍塌，还能提升翻译性能，且该方法的益处能在模型量化后继续保持，为实际部署提供了实用解决方案。

Abstract: Modern neural translation models based on the Transformer architecture are known for their high performance, particularly when trained on high-resource datasets. A standard next-token prediction training strategy, while widely adopted in practice, may lead to overlooked artifacts such as representation collapse. Previous works have shown that this problem is especially pronounced in the representation of the deeper Transformer layers, where it often fails to efficiently utilize the geometric space. Representation collapse is even more evident in end-to-end training of continuous-output neural machine translation, where the trivial solution would be to set all vectors to the same value. In this work, we analyze the dynamics of representation collapse at different levels of discrete and continuous NMT transformers throughout training. We incorporate an existing regularization method based on angular dispersion and demonstrate empirically that it not only mitigates collapse but also improves translation quality. Furthermore, we show that quantized models exhibit similar collapse behavior and that the benefits of regularization are preserved even after quantization.

</details>


### [23] [Same Meaning, Different Scores: Lexical and Syntactic Sensitivity in LLM Evaluation](https://arxiv.org/abs/2602.17316)
*Bogdan Kostić,Conor Fallon,Julian Risch,Alexander Löser*

Main category: cs.CL

TL;DR: LLM评测基准对输入提示的表面变化敏感，通过词汇和句法扰动研究发现模型性能显著下降且排名不稳定，表明模型依赖表面模式而非深层语言能力


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的标准化评测基准被广泛用于模型比较，但其可靠性受到质疑，因为它们对输入提示的浅层变化敏感。本文旨在探究控制性的、真值条件等价的词汇和句法扰动如何影响当代LLMs的绝对性能和相对排名。

Method: 使用两个语言学原则驱动的流程生成保持语义的变体：一个通过同义词替换实现词汇变化，另一个使用依存句法分析确定适用的句法转换。在三个基准（MMLU、SQuAD、AMEGA）上测试了23个当代LLMs。

Result: 词汇扰动在所有模型和任务中一致导致显著且统计显著的性能下降，而句法扰动效果更异质，有时甚至改善结果。两种扰动类型在复杂任务上都会破坏模型排行榜的稳定性。模型鲁棒性并不随模型规模一致提升，表现出强烈的任务依赖性。

Conclusion: LLMs更依赖表面层次的词汇模式而非抽象的语言能力，这凸显了将鲁棒性测试作为LLM评估标准组成部分的必要性。

Abstract: The rapid advancement of Large Language Models (LLMs) has established standardized evaluation benchmarks as the primary instrument for model comparison. Yet, their reliability is increasingly questioned due to sensitivity to shallow variations in input prompts. This paper examines how controlled, truth-conditionally equivalent lexical and syntactic perturbations affect the absolute performance and relative ranking of 23 contemporary LLMs across three benchmarks: MMLU, SQuAD, and AMEGA. We employ two linguistically principled pipelines to generate meaning-preserving variations: one performing synonym substitution for lexical changes, and another using dependency parsing to determine applicable syntactic transformations. Results show that lexical perturbations consistently induce substantial, statistically significant performance degradation across nearly all models and tasks, while syntactic perturbations have more heterogeneous effects, occasionally improving results. Both perturbation types destabilize model leaderboards on complex tasks. Furthermore, model robustness did not consistently scale with model size, revealing strong task dependence. Overall, the findings suggest that LLMs rely more on surface-level lexical patterns than on abstract linguistic competence, underscoring the need for robustness testing as a standard component of LLM evaluation.

</details>


### [24] [RPDR: A Round-trip Prediction-Based Data Augmentation Framework for Long-Tail Question Answering](https://arxiv.org/abs/2602.17366)
*Yiming Zhang,Siyue Zhang,Junbo Zhao,Chen Zhao*

Main category: cs.CL

TL;DR: RPDR：一种通过选择高质量易学习训练数据来增强稠密检索器的数据增强框架，在长尾问答任务上显著提升检索性能


<details>
  <summary>Details</summary>
Motivation: 大语言模型在获取和准确回忆不常见知识方面能力有限，而现有稠密检索模型在处理罕见或小众知识时也面临同样的泛化困难，需要专门的数据增强方法来提升长尾问答性能

Method: RPDR框架包含三个核心组件：1)合成数据生成；2)使用Round-Trip预测进行数据选择以识别易学习实例；3)用这些实例训练检索器。还提出了动态路由机制来动态将查询路由到专门的检索模块

Result: 在PopQA和EntityQuestion两个长尾检索基准测试中，RPDR相比BM25和Contriver等现有检索器取得了显著改进，特别是在极端长尾类别上表现优异

Conclusion: RPDR通过选择高质量易学习训练数据有效增强了稠密检索器在长尾问答任务中的性能，动态路由机制有望进一步提升检索效果，但需要通过详细的人类分析来识别其优势和局限性

Abstract: Long-tail question answering presents significant challenges for large language models (LLMs) due to their limited ability to acquire and accurately recall less common knowledge. Retrieval-augmented generation (RAG) systems have shown great promise in mitigating this limitation by integrating external retrieval mechanisms. However, dense retrieval models often face the same difficulties when generalizing to rare or niche knowledge. In this study, we introduce RPDR, a novel data augmentation framework that selects high-quality easy-to-learn training data, to enhance dense retrievers. Our approach is built around three core components: synthetic data generation, data selection with Round-Trip prediction to identify easy-to-learn instances, and retriever training with these instances. We evaluate RPDR on two long-tail retrieval benchmarks, PopQA and EntityQuestion, demonstrating substantial improvements over existing retrievers like BM25 and Contriver, especially on extremely long-tail categories. We identify the strengths and limitations of RPDR through detailed human analysis and propose a dynamic routing mechanism to dynamically route queries to specialized retrieval modules to further improve retrieval performance.

</details>


### [25] [The Role of the Availability Heuristic in Multiple-Choice Answering Behaviour](https://arxiv.org/abs/2602.17377)
*Leonidas Zotos,Hedderik van Rijn,Malvina Nissim*

Main category: cs.CL

TL;DR: 研究发现，在多选题中，即使不看题干，仅凭选项在语料库中的出现频率（可用性启发式）选择最常见的选项，得分可显著高于随机猜测基准。


<details>
  <summary>Details</summary>
Motivation: 探讨学生在不确定多选题答案时，是否可以通过"可用性启发式"（选择最先想到的选项）作为有效的答题策略。

Method: 提出一种计算评估多选题选项认知可用性的方法，通过概念在大规模语料库（如维基百科）中的出现频率来操作化可用性，并在三个大型题库中进行验证。

Result: 正确选项（独立于题干）的可用性显著高于错误选项；仅选择最可用选项的得分比随机猜测基准高13.5%至32.9%；LLM生成的选项与专家创建的选项显示出相似的可用性模式。

Conclusion: 可用性启发式应被纳入当前和未来计算建模学生行为的研究中，因为正确选项在语料库中普遍具有更高的可用性。

Abstract: When students are unsure of the correct answer to a multiple-choice question (MCQ), guessing is common practice. The availability heuristic, proposed by A. Tversky and D. Kahneman in 1973, suggests that the ease with which relevant instances come to mind, typically operationalised by the mere frequency of exposure, can offer a mental shortcut for problems in which the test-taker does not know the exact answer. Is simply choosing the option that comes most readily to mind a good strategy for answering MCQs? We propose a computational method of assessing the cognitive availability of MCQ options operationalised by concepts' prevalence in large corpora. The key finding, across three large question sets, is that correct answers, independently of the question stem, are significantly more available than incorrect MCQ options. Specifically, using Wikipedia as the retrieval corpus, we find that always selecting the most available option leads to scores 13.5% to 32.9% above the random-guess baseline. We further find that LLM-generated MCQ options show similar patterns of availability compared to expert-created options, despite the LLMs' frequentist nature and their training on large collections of textual data. Our findings suggest that availability should be considered in current and future work when computationally modelling student behaviour.

</details>


### [26] [Diverse Word Choices, Same Reference: Annotating Lexically-Rich Cross-Document Coreference](https://arxiv.org/abs/2602.17424)
*Anastasia Zhukova,Felix Hamborg,Karsten Donnay,Norman Meuschke,Bela Gipp*

Main category: cs.CL

TL;DR: 本文改进了跨文档共指消解（CDCR）的标注方案，将共指链视为话语元素，以更好地分析新闻媒体中多样化和两极分化的报道。


<details>
  <summary>Details</summary>
Motivation: 现有CDCR数据集主要关注事件消解，采用狭窄的共指定义，难以有效分析用词差异大的多样化、两极分化的新闻报道。

Method: 提出修订的CDCR标注方案，将共指链视为话语元素和概念分析单元，同时容纳身份和近身份关系。使用统一标注手册重新标注NewsWCL50和ECB+子集。

Result: 重新标注的数据集在词汇多样性指标和基准测试中表现一致，位于原始ECB+和NewsWCL50之间，支持新闻领域中平衡且具有话语意识的CDCR研究。

Conclusion: 修订的标注方案能够更好地捕捉媒体话语中的词汇多样性和框架变化，为新闻领域的CDCR研究提供了更平衡、更具话语意识的数据集。

Abstract: Cross-document coreference resolution (CDCR) identifies and links mentions of the same entities and events across related documents, enabling content analysis that aggregates information at the level of discourse participants. However, existing datasets primarily focus on event resolution and employ a narrow definition of coreference, which limits their effectiveness in analyzing diverse and polarized news coverage where wording varies widely. This paper proposes a revised CDCR annotation scheme of the NewsWCL50 dataset, treating coreference chains as discourse elements (DEs) and conceptual units of analysis. The approach accommodates both identity and near-identity relations, e.g., by linking "the caravan" - "asylum seekers" - "those contemplating illegal entry", allowing models to capture lexical diversity and framing variation in media discourse, while maintaining the fine-grained annotation of DEs. We reannotate the NewsWCL50 and a subset of ECB+ using a unified codebook and evaluate the new datasets through lexical diversity metrics and a same-head-lemma baseline. The results show that the reannotated datasets align closely, falling between the original ECB+ and NewsWCL50, thereby supporting balanced and discourse-aware CDCR research in the news domain.

</details>


### [27] [Evaluating Extremely Low-Resource Machine Translation: A Comparative Study of ChrF++ and BLEU Metrics](https://arxiv.org/abs/2602.17425)
*Sanjeev Kumar,Preethi Jyothi,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 对BLEU和ChrF++在极低资源语言机器翻译评估中的比较分析，显示BLEU在词汇精确度方面提供补充性洞察


<details>
  <summary>Details</summary>
Motivation: 在极低资源语言场景中，广泛使用的BLEU等评估指标往往不能准确反映翻译质量，需要探索更适合的评估方法

Method: 对BLEU（基于n-gram）和ChrF++（基于字符）在三种极低资源语言（Magahi、Bhojpuri、Chhattisgarhi）上进行比较分析，特别关注大语言模型和神经机器翻译系统的输出，分析这些指标对翻译伪影（幻觉、重复、源文本复制、变音符号变化）的响应

Result: 虽然当前研究多依赖ChrF++，但研究发现BLEU尽管得分较低，却能提供补充性的词汇精确度洞察，提高评估的可解释性

Conclusion: 在极低资源语言机器翻译评估中，BLEU和ChrF++各有优势，BLEU的词汇精确度洞察与ChrF++结合使用能提供更全面的评估视角

Abstract: Evaluating machine translation (MT) quality in extremely low-resource language (ELRL) scenarios poses unique challenges, as widely used metrics such as BLEU, effective in high-resource settings, often misrepresent quality in data-scarce contexts. This work presents a comparative analysis of BLEU, an n-gram-based metric, and ChrF++, a character-based metric, for MT evaluation in ELRL settings. We examine how each metric responds to translation artifacts, including hallucinations, repetition, source-text copying, and diacritic (\textit{matra}) variations across three ELRLs: Magahi, Bhojpuri, and Chhattisgarhi, with a focus on outputs from large language models (LLMs) and neural MT (NMT) systems. While recent work often relies solely on ChrF++, our findings show that BLEU, despite its lower absolute scores, provides complementary lexical-precision insights that improve interpretability.

</details>


### [28] [Fine-Grained Uncertainty Quantification for Long-Form Language Model Outputs: A Comparative Study](https://arxiv.org/abs/2602.17431)
*Dylan Bouchard,Mohit Singh Chauhan,Viren Bajaj,David Skarbrevik*

Main category: cs.CL

TL;DR: 该论文提出了一个用于长文本LLM输出的细粒度不确定性量化框架，通过三个阶段的分类（响应分解、单元级评分、响应级聚合）来改进幻觉检测。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性量化方法主要针对短文本输出设计，在长文本生成场景中表现不佳，需要更有效的幻觉检测方法。

Method: 提出了细粒度不确定性量化的分类法，包括三个设计阶段：响应分解（将长文本分解为claims或句子）、单元级评分（使用基于一致性的黑盒评分器）、响应级聚合。形式化了多种基于一致性的评分器家族。

Result: 实验发现：1) claim-response蕴含方法表现优于或等同于更复杂的claim级评分器；2) claim级评分通常优于句子级评分；3) 不确定性感知解码能显著提高长文本输出的真实性。

Conclusion: 该框架阐明了现有方法之间的关系，支持公平比较，并为选择细粒度不确定性量化组件提供了实用指导。

Abstract: Uncertainty quantification has emerged as an effective approach to closed-book hallucination detection for LLMs, but existing methods are largely designed for short-form outputs and do not generalize well to long-form generation. We introduce a taxonomy for fine-grained uncertainty quantification in long-form LLM outputs that distinguishes methods by design choices at three stages: response decomposition, unit-level scoring, and response-level aggregation. We formalize several families of consistency-based black-box scorers, providing generalizations and extensions of existing methods. In our experiments across multiple LLMs and datasets, we find 1) claim-response entailment consistently performs better or on par with more complex claim-level scorers, 2) claim-level scoring generally yields better results than sentence-level scoring, and 3) uncertainty-aware decoding is highly effective for improving the factuality of long-form outputs. Our framework clarifies relationships between prior methods, enables apples-to-apples comparisons, and provides practical guidance for selecting components for fine-grained UQ.

</details>


### [29] [AIDG: Evaluating Asymmetry Between Information Extraction and Containment in Multi-Turn Dialogue](https://arxiv.org/abs/2602.17443)
*Adib Sakhawat,Fardeen Sadab,Rakin Shahriar*

Main category: cs.CL

TL;DR: AIDG框架揭示大型语言模型在对抗性信息博弈中存在能力不对称：信息防御优于信息推理，防御能力比推理能力高出350 ELO分，主要受信息动态和约束遵循两大瓶颈制约。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型的战略推理能力需要超越静态基准测试，转向动态多轮交互。现有评估方法未能充分捕捉模型在信息不对称对话中的策略性推理能力。

Method: 提出AIDG（对抗性信息推理游戏）框架，包含两个互补任务：AIDG-I测量社交推理中的语用策略，AIDG-II测量结构化"20个问题"场景中的约束满足。在6个前沿LLM上进行了439场游戏实验。

Result: 发现明显的"防御优于推理"能力不对称：模型在信息防御方面表现显著优于信息推理，防御能力比推理能力高出350 ELO分（Cohen's d = 5.47）。识别出两个主要瓶颈：1）信息动态：确认策略比盲目推理有效7.75倍；2）约束遵循：对话负载下指令遵循能力下降，导致41.3%的推理失败。

Conclusion: 虽然LLMs在局部防御一致性方面表现出色，但在战略询问所需的全局状态跟踪方面存在困难。这种能力不对称表明当前LLMs在复杂动态交互中的战略推理能力仍有待提升。

Abstract: Evaluating the strategic reasoning capabilities of Large Language Models (LLMs) requires moving beyond static benchmarks to dynamic, multi-turn interactions. We introduce AIDG (Adversarial Information Deduction Game), a game-theoretic framework that probes the asymmetry between information extraction (active deduction) and information containment (state maintenance) in dialogue. We propose two complementary tasks: AIDG-I, measuring pragmatic strategy in social deduction, and AIDG-II, measuring constraint satisfaction in a structured "20 Questions" setting. Across 439 games with six frontier LLMs, we observe a clear capability asymmetry: models perform substantially better at containment than deduction, with a 350 ELO advantage on defense;(Cohen's d = 5.47). We identify two bottlenecks driving this gap: (1) Information Dynamics, where confirmation strategies are 7.75x more effective than blind deduction (p < 0.00001), and (2) Constraint Adherence, where instruction-following degrades under conversational load, accounting for 41.3% of deductive failures. These findings suggest that while LLMs excel at local defensive coherence, they struggle with the global state tracking required for strategic inquiry.

</details>


### [30] [ABCD: All Biases Come Disguised](https://arxiv.org/abs/2602.17445)
*Mateusz Nowak,Xavier Cadet,Peter Chin*

Main category: cs.CL

TL;DR: 该论文发现LLM在多项选择题评估中存在标签位置和小样本提示偏差，提出了一种使用统一无序标签和句子相似度模型的去偏评估协议。


<details>
  <summary>Details</summary>
Motivation: 当前的多项选择题评估基准存在评估偏见问题，LLM会利用答案位置、标签、小样本提示中的正确答案分布等线索来回答问题，而非真正理解问题内容，这影响了评估的准确性和可靠性。

Method: 提出了一种偏差减少的评估协议：1) 将每个问题的标签替换为统一的、无序的标签；2) 提示LLM使用完整呈现的答案；3) 使用简单的句子相似度模型来评估LLM的回答。

Result: 该协议显著提高了对答案排列的鲁棒性：1) 将平均准确率方差减少了3倍；2) 仅导致模型平均性能的最小下降；3) 在不同基准和模型上都表现出改进的鲁棒性和更低的答案排列标准差。

Conclusion: 提出的去偏评估协议能有效减少LLM在多项选择题评估中的标签位置和小样本提示偏差，在最小性能损失下提供更可靠、更稳健的评估结果，更准确地反映LLM的真实能力。

Abstract: Multiple-choice question (MCQ) benchmarks have been a standard evaluation practice for measuring LLMs' ability to reason and answer knowledge-based questions. Through a synthetic NonsenseQA benchmark, we observe that different LLMs exhibit varying degrees of label-position-few-shot-prompt bias, where the model either uses the answer position, the label in front of the answer, the distributions of correct answers present in the few-shot prompt, or a combination of all to answer each MCQ question. We propose a simple bias-reduced evaluation protocol that replaces the labels of each question with uniform, unordered labels and prompts the LLM to use the whole answer presented. With a simple sentence similarity model, we demonstrate improved robustness and lower standard deviation between different permutations of answers with a minimal drop in LLM's performance, exposing the LLM's capabilities under reduced evaluation artifacts, without any help from the prompt examples or the option labels. Across multiple benchmarks and models, this protocol substantially improves the robustness to answer permutations, reducing mean accuracy variance $3\times$ with only a minimal decrease in the mean model's performance. Through ablation studies on various embedding models and similarity functions, we show that the method is more robust than the standard ones.

</details>


### [31] [Entropy-Based Data Selection for Language Models](https://arxiv.org/abs/2602.17465)
*Hongming Li,Yang Liu,Chao Huang*

Main category: cs.CL

TL;DR: 提出基于熵的无监督数据选择框架(EUDS)，在计算资源受限场景下高效选择高质量训练数据，显著降低计算成本和训练时间。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型训练需要大量计算资源和数据资源。数据选择技术可以减少微调所需数据量，但现有方法通常需要高计算预算。在实际微调场景中，资源有限，需要更高效的数据选择方法。虽然大语言模型具有强大的语言理解能力，但评估数据可用性仍然困难。

Method: 提出基于熵的无监督数据选择(EUDS)框架，建立计算高效的数据过滤机制。该方法利用数据不确定性估计来指导选择，理论分析支持其有效性。

Result: 在情感分析(SA)、主题分类(Topic-CLS)和问答(Q&A)任务上的实验验证了EUDS的有效性。该方法显著降低了计算成本，提高了训练时间效率，同时减少了数据需求。

Conclusion: EUDS为计算资源受限场景下的语言模型高效微调提供了创新解决方案，通过数据选择与不确定性估计的关联分析，实现了计算成本与数据需求的双重优化。

Abstract: Modern language models (LMs) increasingly require two critical resources: computational resources and data resources. Data selection techniques can effectively reduce the amount of training data required for fine-tuning LMs. However, their effectiveness is closely related to computational resources, which always require a high compute budget. Owing to the resource limitations in practical fine-tuning scenario, we systematically reveal the relationship between data selection and uncertainty estimation of selected data. Although large language models (LLMs) exhibit exceptional capabilities in language understanding and generation, which provide new ways to alleviate data scarcity, evaluating data usability remains a challenging task. This makes efficient data selection indispensable. To mitigate these issues, we propose Entropy-Based Unsupervised Data Selection (EUDS) framework. Empirical experiments on sentiment analysis (SA), topic classification (Topic-CLS), and question answering (Q&A) tasks validate its effectiveness. EUDS establishes a computationally efficient data-filtering mechanism. Theoretical analysis and experimental results confirm the effectiveness of our approach. EUDS significantly reduces computational costs and improves training time efficiency with less data requirement. This provides an innovative solution for the efficient fine-tuning of LMs in the compute-constrained scenarios.

</details>


### [32] [PEACE 2.0: Grounded Explanations and Counter-Speech for Combating Hate Expressions](https://arxiv.org/abs/2602.17467)
*Greta Damo,Stéphane Petiot,Elena Cabrio,Serena Villata*

Main category: cs.CL

TL;DR: PEACE 2.0是一个新的工具，不仅能分析解释消息是否仇恨言论，还能生成回应。它使用检索增强生成(RAG)管道来基于证据生成解释和反言论，并探索反言论回复的特征。


<details>
  <summary>Details</summary>
Motivation: 在线平台上的仇恨言论日益增多，给社会带来重大挑战。虽然NLP社区已经开发出有效的方法来自动检测仇恨言论的存在，但如何回应这些言论（称为反言论）仍然是一个开放挑战。

Method: PEACE 2.0采用检索增强生成(RAG)管道，主要包含三个新功能：1)将仇恨言论解释基于证据和事实；2)自动生成基于证据的反言论；3)探索反言论回复的特征。该工具能够处理明确和隐含的仇恨信息。

Result: PEACE 2.0集成了分析、解释和回应生成功能，能够为明确和隐含的仇恨信息提供深入分析和回应生成。工具通过RAG管道确保解释和反言论都有证据支持。

Conclusion: PEACE 2.0为在线仇恨言论问题提供了一个全面的解决方案，不仅能够检测和解释仇恨言论，还能生成基于证据的有效反言论回应，有助于应对这一重要的社会挑战。

Abstract: The increasing volume of hate speech on online platforms poses significant societal challenges. While the Natural Language Processing community has developed effective methods to automatically detect the presence of hate speech, responses to it, called counter-speech, are still an open challenge. We present PEACE 2.0, a novel tool that, besides analysing and explaining why a message is considered hateful or not, also generates a response to it. More specifically, PEACE 2.0 has three main new functionalities: leveraging a Retrieval-Augmented Generation (RAG) pipeline i) to ground HS explanations into evidence and facts, ii) to automatically generate evidence-grounded counter-speech, and iii) exploring the characteristics of counter-speech replies. By integrating these capabilities, PEACE 2.0 enables in-depth analysis and response generation for both explicit and implicit hateful messages.

</details>


### [33] [Auditing Reciprocal Sentiment Alignment: Inversion Risk, Dialect Representation and Intent Misalignment in Transformers](https://arxiv.org/abs/2602.17469)
*Nusrat Jahan Lia,Shubhashis Roy Dipta*

Main category: cs.CL

TL;DR: 研究发现当前跨语言对齐方法在孟加拉语-英语情感分析中存在严重缺陷，压缩模型出现28.7%的情感反转率，系统存在不对称共情和现代偏见问题。


<details>
  <summary>Details</summary>
Motivation: 当前的双向对齐方法在语言障碍下严重断裂，特别是在孟加拉语和英语之间的跨语言情感对齐存在严重的安全性和代表性失败，这影响了人类对AI的信任。

Method: 通过基准测试四种Transformer架构，分析跨语言情感对齐问题，重点关注压缩模型(mDistilBERT)和区域模型(IndicBERT)的表现。

Result: 发现压缩模型有28.7%的情感反转率，系统存在不对称共情现象，且IndicBERT在处理正式孟加拉语时对齐错误增加57%，显示出"现代偏见"。

Conclusion: 公平的人类-AI协同进化需要基于文化的多元对齐方法，尊重语言和方言多样性，而非普遍压缩。建议在基准测试中加入"情感稳定性"指标来惩罚低资源和方言环境中的极性反转。

Abstract: The core theme of bidirectional alignment is ensuring that AI systems accurately understand human intent and that humans can trust AI behavior. However, this loop fractures significantly across language barriers. Our research addresses Cross-Lingual Sentiment Misalignment between Bengali and English by benchmarking four transformer architectures. We reveal severe safety and representational failures in current alignment paradigms. We demonstrate that compressed model (mDistilBERT) exhibits 28.7% "Sentiment Inversion Rate," fundamentally misinterpreting positive user intent as negative (or vice versa). Furthermore, we identify systemic nuances affecting human-AI trust, including "Asymmetric Empathy" where some models systematically dampen and others amplify the affective weight of Bengali text relative to its English counterpart. Finally, we reveal a "Modern Bias" in the regional model (IndicBERT), which shows a 57% increase in alignment error when processing formal (Sadhu) Bengali. We argue that equitable human-AI co-evolution requires pluralistic, culturally grounded alignment that respects language and dialectal diversity over universal compression, which fails to preserve the emotional fidelity required for reciprocal human-AI trust. We recommend that alignment benchmarks incorporate "Affective Stability" metrics that explicitly penalize polarity inversions in low-resource and dialectal contexts.

</details>


### [34] [Small LLMs for Medical NLP: a Systematic Analysis of Few-Shot, Constraint Decoding, Fine-Tuning and Continual Pre-Training in Italian](https://arxiv.org/abs/2602.17475)
*Pietro Ferrazzi,Mattia Franzin,Alberto Lavelli,Bernardo Magnini*

Main category: cs.CL

TL;DR: 小型LLM（约10亿参数）在医疗NLP任务中通过微调可以达到甚至超越大型模型的性能，同时显著降低计算需求。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗NLP任务中表现出色，但计算需求大，限制了在实际医疗环境中的部署。研究旨在探索小型LLM是否能在保持竞争力的同时，更适合实际医疗应用。

Method: 评估了Llama-3、Gemma-3和Qwen3三个家族的小型模型（约10亿参数）在20个临床NLP任务上的表现。系统比较了多种适应策略：推理时（少样本提示、约束解码）和训练时（监督微调、持续预训练）。

Result: 微调是最有效的方法，而少样本提示与约束解码的组合提供了强大的低资源替代方案。最佳配置基于Qwen3-1.7B，平均得分比Qwen3-32B高出9.2分。小型LLM能够匹配甚至超越更大的基线模型。

Conclusion: 小型LLM在医疗NLP任务中具有实际可行性，微调是关键优化策略。研究还发布了意大利医疗NLP数据集和持续预训练数据集，为相关研究提供资源支持。

Abstract: Large Language Models (LLMs) consistently excel in diverse medical Natural Language Processing (NLP) tasks, yet their substantial computational requirements often limit deployment in real-world healthcare settings. In this work, we investigate whether "small" LLMs (around one billion parameters) can effectively perform medical tasks while maintaining competitive accuracy. We evaluate models from three major families-Llama-3, Gemma-3, and Qwen3-across 20 clinical NLP tasks among Named Entity Recognition, Relation Extraction, Case Report Form Filling, Question Answering, and Argument Mining. We systematically compare a range of adaptation strategies, both at inference time (few-shot prompting, constraint decoding) and at training time (supervised fine-tuning, continual pretraining). Fine-tuning emerges as the most effective approach, while the combination of few-shot prompting and constraint decoding offers strong lower-resource alternatives. Our results show that small LLMs can match or even surpass larger baselines, with our best configuration based on Qwen3-1.7B achieving an average score +9.2 points higher than Qwen3-32B. We release a comprehensive collection of all the publicly available Italian medical datasets for NLP tasks, together with our top-performing models. Furthermore, we release an Italian dataset of 126M words from the Emergency Department of an Italian Hospital, and 175M words from various sources that we used for continual pre-training.

</details>


### [35] [Bridging the Domain Divide: Supervised vs. Zero-Shot Clinical Section Segmentation from MIMIC-III to Obstetrics](https://arxiv.org/abs/2602.17513)
*Baris Karacan,Barbara Di Eugenio,Patrick Thornton*

Main category: cs.CL

TL;DR: 本文提出了三个关键贡献：1）创建了一个新的去标识化、带章节标签的产科笔记数据集；2）系统评估了基于transformer的监督模型在MIMIC-III数据集（领域内）和新产科数据集（领域外）上的章节分割性能；3）首次将监督模型与零样本大语言模型在医学章节分割任务上进行了直接比较。


<details>
  <summary>Details</summary>
Motivation: 临床自由文本笔记包含重要的患者信息，通常被组织成带标签的章节。识别这些章节已被证明有助于临床决策支持和下游NLP任务。当前大多数章节分割方法主要在MIMIC-III等公共语料库上训练，缺乏对其他医学领域的覆盖。

Method: 1）创建新的去标识化产科笔记数据集；2）在MIMIC-III数据集（领域内）和新产科数据集（领域外）上系统评估基于transformer的监督模型；3）首次将监督模型与零样本大语言模型在医学章节分割任务上进行直接比较。

Result: 监督模型在领域内表现良好，但在领域外性能显著下降。相比之下，零样本模型在纠正幻觉章节标题后，展现出强大的领域外适应能力。

Conclusion: 研究强调了开发领域特定临床资源的重要性，并指出零样本分割是医疗NLP超越现有研究语料库的有前景方向，前提是适当管理幻觉问题。

Abstract: Clinical free-text notes contain vital patient information. They are structured into labelled sections; recognizing these sections has been shown to support clinical decision-making and downstream NLP tasks. In this paper, we advance clinical section segmentation through three key contributions. First, we curate a new de-identified, section-labeled obstetrics notes dataset, to supplement the medical domains covered in public corpora such as MIMIC-III, on which most existing segmentation approaches are trained. Second, we systematically evaluate transformer-based supervised models for section segmentation on a curated subset of MIMIC-III (in-domain), and on the new obstetrics dataset (out-of-domain). Third, we conduct the first head-to-head comparison of supervised models for medical section segmentation with zero-shot large language models. Our results show that while supervised models perform strongly in-domain, their performance drops substantially out-of-domain. In contrast, zero-shot models demonstrate robust out-of-domain adaptability once hallucinated section headers are corrected. These findings underscore the importance of developing domain-specific clinical resources and highlight zero-shot segmentation as a promising direction for applying healthcare NLP beyond well-studied corpora, as long as hallucinations are appropriately managed.

</details>


### [36] [Using LLMs for Knowledge Component-level Correctness Labeling in Open-ended Coding Problems](https://arxiv.org/abs/2602.17542)
*Zhangqi Duan,Arnav Kankaria,Dhruv Kartik,Andrew Lan*

Main category: cs.CL

TL;DR: 使用LLM自动标注编程任务中知识组件（KC）级正确性，通过时间感知的代码-KC映射机制，改善学习曲线拟合和预测性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集中很少包含KC级正确性标签，特别是在开放式编程任务中，学生解决方案通常同时涉及多个KC。简单地将问题级正确性传播到所有相关KC会掩盖部分掌握情况，导致学习曲线拟合不佳。

Method: 提出自动化框架，利用大型语言模型从学生编写的代码中直接标注KC级正确性。方法评估每个KC是否正确应用，并引入时间上下文感知的代码-KC映射机制，以更好地将KC与个体学生代码对齐。

Result: 实验结果表明，该框架产生的学习曲线更符合认知理论，相比基线方法提高了预测性能。人工评估进一步显示LLM标注与专家标注之间存在显著一致性。

Conclusion: 该研究提出了一种有效的自动化方法来标注KC级正确性，解决了开放式编程任务中学生建模的关键挑战，为学习分析和学生建模提供了更精细的洞察。

Abstract: Fine-grained skill representations, commonly referred to as knowledge components (KCs), are fundamental to many approaches in student modeling and learning analytics. However, KC-level correctness labels are rarely available in real-world datasets, especially for open-ended programming tasks where solutions typically involve multiple KCs simultaneously. Simply propagating problem-level correctness to all associated KCs obscures partial mastery and often leads to poorly fitted learning curves. To address this challenge, we propose an automated framework that leverages large language models (LLMs) to label KC-level correctness directly from student-written code. Our method assesses whether each KC is correctly applied and further introduces a temporal context-aware Code-KC mapping mechanism to better align KCs with individual student code. We evaluate the resulting KC-level correctness labels in terms of learning curve fit and predictive performance using the power law of practice and the Additive Factors Model. Experimental results show that our framework leads to learning curves that are more consistent with cognitive theory and improves predictive performance, compared to baselines. Human evaluation further demonstrates substantial agreement between LLM and expert annotations.

</details>


### [37] [Learning to Stay Safe: Adaptive Regularization Against Safety Degradation during Fine-Tuning](https://arxiv.org/abs/2602.17546)
*Jyotin Goel,Souvik Maji,Pratik Mazumder*

Main category: cs.CL

TL;DR: 论文提出了一种自适应正则化训练框架，通过实时评估安全风险来约束微调过程，既能抵御对抗性攻击又保持模型性能，且不增加推理成本。


<details>
  <summary>Details</summary>
Motivation: 指令跟随语言模型在微调过程中安全行为会退化，现有防御方法要么保护有限，要么需要在安全性和实用性之间权衡。

Method: 提出了自适应正则化训练框架，使用两种安全风险评估方法：1）基于评判的Safety Critic为训练批次分配危害分数；2）基于激活的风险预测器，通过轻量级分类器分析中间激活来估计有害意图。风险信号用于约束高风险更新，使其接近安全参考策略，而低风险更新则进行标准训练。

Result: 实验验证了有害意图可以从生成前激活中预测，评判分数能提供高召回的安全指导。在多种模型家族和攻击场景下，自适应正则化显著降低了攻击成功率，保持了下游性能，且不增加推理时间成本。

Conclusion: 该工作展示了一种在不牺牲实用性的前提下保持安全性的原则性机制，为安全微调提供了有效解决方案。

Abstract: Instruction-following language models are trained to be helpful and safe, yet their safety behavior can deteriorate under benign fine-tuning and worsen under adversarial updates. Existing defenses often offer limited protection or force a trade-off between safety and utility. We introduce a training framework that adapts regularization in response to safety risk, enabling models to remain aligned throughout fine-tuning. To estimate safety risk at training time, we explore two distinct approaches: a judge-based Safety Critic that assigns high-level harm scores to training batches, and an activation-based risk predictor built with a lightweight classifier trained on intermediate model activations to estimate harmful intent. Each approach provides a risk signal that is used to constrain updates deemed higher risk to remain close to a safe reference policy, while lower-risk updates proceed with standard training. We empirically verify that harmful intent signals are predictable from pre-generation activations and that judge scores provide effective high-recall safety guidance. Across multiple model families and attack scenarios, adaptive regularization with either risk estimation approach consistently lowers attack success rate compared to standard fine-tuning, preserves downstream performance, and adds no inference-time cost. This work demonstrates a principled mechanism for maintaining safety without sacrificing utility.

</details>


### [38] [Modeling Distinct Human Interaction in Web Agents](https://arxiv.org/abs/2602.17588)
*Faria Huq,Zora Zhiruo Wang,Zhanqiu Guo,Venu Arvind Arangarajan,Tianyue Ou,Frank Xu,Shuyan Zhou,Graham Neubig,Jeffrey P. Bigham*

Main category: cs.CL

TL;DR: 该研究提出建模人类干预以支持协作式网页任务执行，通过收集真实用户网页导航数据，识别四种用户交互模式，训练语言模型预测干预时机，并在实际代理中部署验证效果。


<details>
  <summary>Details</summary>
Motivation: 当前自主网页代理系统缺乏对人类干预时机和原因的原则性理解，经常在关键决策点自主执行或请求不必要的确认，需要更结构化的方法来支持人机协作。

Method: 收集CowCorpus数据集（400个真实用户网页导航轨迹，包含4200个人机交互动作），识别四种用户交互模式，训练语言模型基于用户交互风格预测干预时机。

Result: 干预预测准确率比基础语言模型提高61.4-63.4%；在实际网页导航代理中部署后，用户评估的代理有用性提高26.5%。

Conclusion: 结构化建模人类干预能够产生更具适应性和协作性的代理，为人机协作网页任务执行提供了有效方法。

Abstract: Despite rapid progress in autonomous web agents, human involvement remains essential for shaping preferences and correcting agent behavior as tasks unfold. However, current agentic systems lack a principled understanding of when and why humans intervene, often proceeding autonomously past critical decision points or requesting unnecessary confirmation. In this work, we introduce the task of modeling human intervention to support collaborative web task execution. We collect CowCorpus, a dataset of 400 real-user web navigation trajectories containing over 4,200 interleaved human and agent actions. We identify four distinct patterns of user interaction with agents -- hands-off supervision, hands-on oversight, collaborative task-solving, and full user takeover. Leveraging these insights, we train language models (LMs) to anticipate when users are likely to intervene based on their interaction styles, yielding a 61.4-63.4% improvement in intervention prediction accuracy over base LMs. Finally, we deploy these intervention-aware models in live web navigation agents and evaluate them in a user study, finding a 26.5% increase in user-rated agent usefulness. Together, our results show structured modeling of human intervention leads to more adaptive, collaborative agents.

</details>


### [39] [The Cascade Equivalence Hypothesis: When Do Speech LLMs Behave Like ASR$\rightarrow$LLM Pipelines?](https://arxiv.org/abs/2602.17598)
*Jayadev Billa*

Main category: cs.CL

TL;DR: 大多数当前语音大语言模型实际上只是ASR转录+文本LLM的级联系统，性能上与简单级联系统无差异，只有少数架构真正实现了语音到语义的深度融合。


<details>
  <summary>Details</summary>
Motivation: 当前许多语音LLM声称实现了语音到语义的直接处理，但研究者怀疑这些模型实际上只是在内部执行ASR转录，然后像传统文本LLM一样处理文本，本质上只是昂贵的级联系统。

Method: 通过匹配骨干网络测试，比较四种语音LLM在六个任务上的表现，控制LLM骨干网络以消除架构差异；使用logit lens技术分析隐藏状态中的文本出现；应用LEACE概念擦除技术验证文本表示在两种测试架构中的因果必要性。

Result: Ultravox与其匹配的级联系统在统计上无法区分（κ=0.93）；隐藏状态中确实出现字面文本；文本表示在两种架构中都是因果必要的，擦除后准确率降至接近零；Qwen2-Audio是唯一真正不同的架构；在噪声条件下，语音LLM表现比级联系统更差，优势逆转达7.6%。

Conclusion: 当前大多数语音LLM实际上只是昂贵的ASR→LLM级联系统，在噪声条件下表现更差；只有特定架构（如Qwen2-Audio）真正实现了语音到语义的深度融合；研究揭示了语音LLM设计中的架构依赖性问题。

Abstract: Current speech LLMs largely perform implicit ASR: on tasks solvable from a transcript, they are behaviorally and mechanistically equivalent to simple Whisper$\to$LLM cascades. We show this through matched-backbone testing across four speech LLMs and six tasks, controlling for the LLM backbone for the first time. Ultravox is statistically indistinguishable from its matched cascade ($κ{=}0.93$); logit lens reveals literal text emerging in hidden states; LEACE concept erasure confirms text representations are causally necessary in both architectures tested, collapsing accuracy to near-zero. Qwen2-Audio genuinely diverges, revealing cascade equivalence is architecture-dependent, not universal. For most deployed use cases, current speech LLMs are expensive cascades, and under noise, they are worse ones, with clean-condition advantages reversing by up to 7.6% at 0 dB.

</details>


### [40] [Unmasking the Factual-Conceptual Gap in Persian Language Models](https://arxiv.org/abs/2602.17623)
*Alireza Sakhaeirad,Ali Ma'manpoosh,Arshia Hemmat*

Main category: cs.CL

TL;DR: DivanBench 是一个波斯语诊断基准，专注于评估LLM对迷信和习俗等社会规范的理解能力，发现当前模型存在严重的默许偏见和推理能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有波斯语NLP基准在语用学和礼貌方面有所扩展，但很少区分记忆的文化事实和推理隐含社会规范的能力。需要评估模型对任意、上下文依赖的社会规则的理解，这些规则难以通过简单逻辑推理获得。

Method: 引入DivanBench基准，包含315个问题，涵盖三种任务类型：事实检索、配对场景验证和情境推理。评估了七个波斯语LLM，分析其在文化规范理解方面的表现。

Result: 发现三个关键问题：1）大多数模型存在严重的默许偏见，能识别适当行为但无法拒绝明显违规；2）持续的波斯语预训练反而加剧这种偏见，降低模型识别矛盾的能力；3）所有模型在事实检索和场景应用之间存在21%的性能差距。

Conclusion: 文化能力不仅需要扩展单语数据，当前模型只是模仿文化模式，而没有内化底层模式。需要开发能真正理解文化规范推理能力的模型。

Abstract: While emerging Persian NLP benchmarks have expanded into pragmatics and politeness, they rarely distinguish between memorized cultural facts and the ability to reason about implicit social norms. We introduce DivanBench, a diagnostic benchmark focused on superstitions and customs, arbitrary, context-dependent rules that resist simple logical deduction. Through 315 questions across three task types (factual retrieval, paired scenario verification, and situational reasoning), we evaluate seven Persian LLMs and reveal three critical failures: most models exhibit severe acquiescence bias, correctly identifying appropriate behaviors but failing to reject clear violations; continuous Persian pretraining amplifies this bias rather than improving reasoning, often degrading the model's ability to discern contradictions; and all models show a 21\% performance gap between retrieving factual knowledge and applying it in scenarios. These findings demonstrate that cultural competence requires more than scaling monolingual data, as current models learn to mimic cultural patterns without internalizing the underlying schemas.

</details>


### [41] [Differences in Typological Alignment in Language Models' Treatment of Differential Argument Marking](https://arxiv.org/abs/2602.17653)
*Iskar Deng,Nathalia Xu,Shane Steinert-Threlkeld*

Main category: cs.CL

TL;DR: 语言模型在合成语料上训练后，在语义论元标记系统上表现出与人类语言相似的某些类型学偏好，但不是全部。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明语言模型在合成语料训练后能表现出类似人类语言的类型学偏好（如词序）。本研究将这一范式扩展到差异论元标记系统，探索模型是否能重现人类语言中该系统的类型学规律。

Method: 使用受控合成学习方法，在18个实现不同DAM系统的语料上训练GPT-2模型，然后使用最小对立对评估模型的泛化能力。

Result: 模型在DAM的两个类型学维度上表现出分离：可靠地表现出人类似的自然标记方向偏好（标记语义非典型论元），但未重现人类语言中强烈的宾语偏好（DAM中标记更常针对宾语而非主语）。

Conclusion: 不同的类型学倾向可能源于不同的底层机制，模型能捕捉某些语义驱动的规律，但未能完全重现人类语言中基于句法位置（主语vs宾语）的偏好模式。

Abstract: Recent work has shown that language models (LMs) trained on synthetic corpora can exhibit typological preferences that resemble cross-linguistic regularities in human languages, particularly for syntactic phenomena such as word order. In this paper, we extend this paradigm to differential argument marking (DAM), a semantic licensing system in which morphological marking depends on semantic prominence. Using a controlled synthetic learning method, we train GPT-2 models on 18 corpora implementing distinct DAM systems and evaluate their generalization using minimal pairs. Our results reveal a dissociation between two typological dimensions of DAM. Models reliably exhibit human-like preferences for natural markedness direction, favoring systems in which overt marking targets semantically atypical arguments. In contrast, models do not reproduce the strong object preference in human languages, in which overt marking in DAM more often targets objects rather than subjects. These findings suggest that different typological tendencies may arise from distinct underlying sources.

</details>


### [42] [What Language is This? Ask Your Tokenizer](https://arxiv.org/abs/2602.17655)
*Clara Meister,Ahmetcan Yavuz,Pietro Lesci,Tiago Pimentel*

Main category: cs.CL

TL;DR: UniLID：基于UnigramLM分词算法的语言识别方法，在低资源语言和方言识别中表现优异，仅需5个样本即可达到70%以上准确率


<details>
  <summary>Details</summary>
Motivation: 现有语言识别系统在高资源语言上表现接近完美，但在低资源和密切相关语言（如方言）场景下仍然脆弱，需要更鲁棒、高效且支持增量学习的解决方案。

Method: 基于UnigramLM分词算法，学习语言条件的一元分布（共享分词器词汇表），但将分词视为语言特定现象。采用概率框架、参数估计技术和推理策略。

Result: 在标准基准测试中与fastText、GlotLID、CLD3等基线模型竞争性表现；在低资源设置下显著提升样本效率（仅需5个标注样本即可达到70%+准确率）；在细粒度方言识别上取得大幅提升。

Conclusion: UniLID是一种简单高效的语言识别方法，具有数据和计算效率高、支持增量添加新语言无需重新训练、可自然集成到现有语言模型分词流程等优势，特别适合低资源和方言识别场景。

Abstract: Language Identification (LID) is an important component of many multilingual natural language processing pipelines, where it facilitates corpus curation, training data analysis, and cross-lingual evaluation of large language models. Despite near-perfect performance on high-resource languages, existing systems remain brittle in low-resource and closely related language settings. We introduce UniLID, a simple and efficient LID method based on the UnigramLM tokenization algorithm, leveraging its probabilistic framing, parameter estimation technique and inference strategy. In short, we learn language-conditional unigram distributions over a shared tokenizer vocabulary but treat segmentation as a language-specific phenomenon. Our formulation is data- and compute-efficient, supports incremental addition of new languages without retraining existing models, and can naturally be integrated into existing language model tokenization pipelines. Empirical evaluations against widely used baselines, including fastText, GlotLID, and CLD3, show that UniLID achieves competitive performance on standard benchmarks, substantially improves sample efficiency in low-resource settings - surpassing 70% accuracy with as few as five labeled samples per language - and delivers large gains on fine-grained dialect identification.

</details>


### [43] [Sink-Aware Pruning for Diffusion Language Models](https://arxiv.org/abs/2602.17664)
*Aidar Myrzakhan,Tianyi Li,Bowei Guo,Shengkun Tang,Zhiqiang Shen*

Main category: cs.CL

TL;DR: 该论文提出了一种针对扩散语言模型的高效剪枝方法，通过识别并剪枝不稳定的注意力汇聚点来减少推理成本。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型由于迭代去噪过程导致推理成本高昂，需要高效的剪枝方法。现有的剪枝启发式方法主要继承自自回归大语言模型，通常会保留注意力汇聚点token，因为自回归模型中的汇聚点作为稳定的全局锚点。但作者发现这一假设不适用于扩散语言模型。

Method: 提出了"Sink-Aware Pruning"方法，该方法自动识别并剪枝扩散语言模型中不稳定的注意力汇聚点。与传统自回归模型剪枝方法不同，该方法针对扩散模型特有的注意力模式进行处理。

Result: 无需重新训练，该方法在匹配计算资源下实现了更好的质量-效率权衡，并超越了现有的强剪枝基线方法。

Conclusion: 扩散语言模型的注意力汇聚点与自回归模型有本质不同，基于这一观察提出的汇聚点感知剪枝方法能有效提升扩散语言模型的推理效率。

Abstract: Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR models. Based on this observation, we propose ${\bf \texttt{Sink-Aware Pruning}}$, which automatically identifies and prunes unstable sinks in DLMs (prior studies usually keep sinks for AR LLMs). Without retraining, our method achieves a better quality-efficiency trade-off and outperforms strong prior pruning baselines under matched compute. Our code is available at https://github.com/VILA-Lab/Sink-Aware-Pruning.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [44] [RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution](https://arxiv.org/abs/2602.16932)
*Jinming Nian,Fangchen Li,Dae Hoon Park,Yi Fang*

Main category: cs.IR

TL;DR: 论文提出了RankEvolve方法，利用大语言模型在评估器引导下通过进化搜索自动发现改进的词法检索算法，从BM25和查询似然等经典算法出发，演化出新颖有效的检索算法。


<details>
  <summary>Details</summary>
Motivation: BM25和Dirichlet平滑的查询似然等传统检索算法虽然强大高效，但改进主要依赖参数调整和人类直觉。研究者希望探索是否可以利用大语言模型自动发现改进的词法检索算法。

Method: 提出RankEvolve方法，基于AlphaEvolve的程序演化框架，将候选排序算法表示为可执行代码，通过变异、重组和选择迭代演化。以BM25和Dirichlet平滑的查询似然为种子程序，在12个IR数据集上进行评估和演化。

Result: 演化出的算法新颖有效，在完整的BEIR和BRIGHT基准测试以及TREC DL 19和20上显示出良好的迁移性能。评估器引导的LLM程序演化是自动发现新颖排序算法的实用路径。

Conclusion: 研究表明，基于评估器引导的大语言模型程序演化可以有效自动发现改进的词法检索算法，为检索算法的自动优化提供了新思路。

Abstract: Retrieval algorithms like BM25 and query likelihood with Dirichlet smoothing remain strong and efficient first-stage rankers, yet improvements have mostly relied on parameter tuning and human intuition. We investigate whether a large language model, guided by an evaluator and evolutionary search, can automatically discover improved lexical retrieval algorithms. We introduce RankEvolve, a program evolution setup based on AlphaEvolve, in which candidate ranking algorithms are represented as executable code and iteratively mutated, recombined, and selected based on retrieval performance across 12 IR datasets from BEIR and BRIGHT. RankEvolve starts from two seed programs: BM25 and query likelihood with Dirichlet smoothing. The evolved algorithms are novel, effective, and show promising transfer to the full BEIR and BRIGHT benchmarks as well as TREC DL 19 and 20. Our results suggest that evaluator-guided LLM program evolution is a practical path towards automatic discovery of novel ranking algorithms.

</details>


### [45] [SAGE: Structure Aware Graph Expansion for Retrieval of Heterogeneous Data](https://arxiv.org/abs/2602.16964)
*Prasham Titiya,Rohit Khoja,Tomer Wolfson,Vivek Gupta,Dan Roth*

Main category: cs.IR

TL;DR: SAGE框架通过构建块级图并执行在线检索扩展，在异构语料的问答任务中显著提升检索召回率。


<details>
  <summary>Details</summary>
Motivation: 异构语料（文本、表格、图节点）的检索增强问答需要跨模态的证据链。实体级知识图谱构建维护成本高且查询效率低，而标准的检索器-阅读器流水线使用独立分块的平面相似性搜索，无法捕捉跨模态的多跳证据链。

Method: 提出SAGE框架：1）离线构建块级图，使用元数据驱动的相似性和百分位剪枝；2）在线检索时，先运行基线检索器获取k个种子块，扩展一跳邻居，然后使用密集+稀疏检索过滤邻居，选择k'个额外块。为隐式跨模态语料使用混合密集+稀疏检索作为初始检索器，为显式模式图使用SPARK（基于知识图谱检索的结构感知规划智能体）。

Result: 在OTT-QA和STaRK数据集上，SAGE相比基线分别提升了5.7和8.5个百分点的检索召回率。

Conclusion: SAGE框架通过结合块级图构建和智能检索扩展，有效解决了异构语料中跨模态证据链的检索问题，显著提升了检索性能。

Abstract: Retrieval-augmented question answering over heterogeneous corpora requires connected evidence across text, tables, and graph nodes. While entity-level knowledge graphs support structured access, they are costly to construct and maintain, and inefficient to traverse at query time. In contrast, standard retriever-reader pipelines use flat similarity search over independently chunked text, missing multi-hop evidence chains across modalities. We propose SAGE (Structure Aware Graph Expansion) framework that (i) constructs a chunk-level graph offline using metadata-driven similarities with percentile-based pruning, and (ii) performs online retrieval by running an initial baseline retriever to obtain k seed chunks, expanding first-hop neighbors, and then filtering the neighbors using dense+sparse retrieval, selecting k' additional chunks. We instantiate the initial retriever using hybrid dense+sparse retrieval for implicit cross-modal corpora and SPARK (Structure Aware Planning Agent for Retrieval over Knowledge Graphs) an agentic retriever for explicit schema graphs. On OTT-QA and STaRK, SAGE improves retrieval recall by 5.7 and 8.5 points over baselines.

</details>


### [46] [Beyond Chunk-Then-Embed: A Comprehensive Taxonomy and Evaluation of Document Chunking Strategies for Information Retrieval](https://arxiv.org/abs/2602.16974)
*Yongjie Zhou,Shuai Wang,Bevan Koopman,Guido Zuccon*

Main category: cs.IR

TL;DR: 本文系统评估了文档分块策略，发现最优策略取决于任务类型：简单结构方法在语料库检索中优于LLM引导方法，而LumberChunker在文档内检索中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 文档分块是密集检索系统的关键预处理步骤，但现有分块策略研究分散且缺乏统一比较框架。不同方法在重叠度低的基准上独立评估，难以直接比较效果。

Method: 提出统一框架，从两个维度分析分块策略：(1) 分割方法（结构基、语义基、LLM引导）(2) 嵌入范式（预嵌入分块 vs. 上下文分块）。在两种检索设置（文档内检索和语料库检索）中系统复现和评估现有方法。

Result: 最优分块策略具有任务依赖性：语料库检索中简单结构方法优于LLM引导方法；文档内检索中LumberChunker表现最佳。上下文分块提升语料库检索效果但降低文档内检索效果。分块大小与文档内检索效果中度相关，与语料库检索效果弱相关。

Conclusion: 文档分块策略选择应基于具体任务需求而非盲目采用复杂方法。研究提供了系统评估框架和公开代码基准，为未来分块策略研究奠定基础。

Abstract: Document chunking is a critical preprocessing step in dense retrieval systems, yet the design space of chunking strategies remains poorly understood. Recent research has proposed several concurrent approaches, including LLM-guided methods (e.g., DenseX and LumberChunker) and contextualized strategies(e.g., Late Chunking), which generate embeddings before segmentation to preserve contextual information. However, these methods emerged independently and were evaluated on benchmarks with minimal overlap, making direct comparisons difficult.
  This paper reproduces prior studies in document chunking and presents a systematic framework that unifies existing strategies along two key dimensions: (1) segmentation methods, including structure-based methods (fixed-size, sentence-based, and paragraph-based) as well as semantically-informed and LLM-guided methods; and (2) embedding paradigms, which determine the timing of chunking relative to embedding (pre-embedding chunking vs. contextualized chunking). Our reproduction evaluates these approaches in two distinct retrieval settings established in previous work: in-document retrieval (needle-in-a-haystack) and in-corpus retrieval (the standard information retrieval task).
  Our comprehensive evaluation reveals that optimal chunking strategies are task-dependent: simple structure-based methods outperform LLM-guided alternatives for in-corpus retrieval, while LumberChunker performs best for in-document retrieval. Contextualized chunking improves in-corpus effectiveness but degrades in-document retrieval. We also find that chunk size correlates moderately with in-document but weakly with in-corpus effectiveness, suggesting segmentation method differences are not purely driven by chunk size. Our code and evaluation benchmarks are publicly available at (Anonymoused).

</details>


### [47] [Bending the Scaling Law Curve in Large-Scale Recommendation Systems](https://arxiv.org/abs/2602.16986)
*Qin Ding,Kevin Course,Linjian Ma,Jianhui Sun,Rouchen Liu,Zhao Zhu,Chunxing Yin,Wei Li,Dai Li,Yu Shi,Xuan Cao,Ze Yang,Han Li,Xing Liu,Bi Xue,Hongwei Li,Rui Jian,Daisy Shi He,Jing Qian,Matt Ma,Qunshu Zhang,Rui Li*

Main category: cs.IR

TL;DR: ULTRA-HSTU是一个通过模型与系统协同设计的新型序列推荐模型，在保持高质量推荐的同时实现了5倍训练加速和21倍推理加速，已在大规模生产环境中部署。


<details>
  <summary>Details</summary>
Motivation: 现有序列推荐模型过度依赖交叉注意力机制来解决二次计算瓶颈，这限制了自注意力的表征能力。需要一种既能保持模型质量又能显著提升效率的解决方案。

Method: 通过端到端的模型与系统协同设计，创新性地设计了输入序列、稀疏注意力机制和模型拓扑结构，以解决传统序列推荐模型的计算效率问题。

Result: ULTRA-HSTU实现了显著的扩展效率提升：训练速度提升5倍以上，推理速度提升21倍，同时提供更优的推荐质量。在生产环境中实现了4%-8%的消费和参与度提升。

Conclusion: ULTRA-HSTU通过创新的模型架构和系统设计，成功解决了序列推荐中的计算效率瓶颈，在保持高质量推荐的同时实现了显著的性能提升，已在大规模生产环境中验证了其有效性。

Abstract: Learning from user interaction history through sequential models has become a cornerstone of large-scale recommender systems. Recent advances in large language models have revealed promising scaling laws, sparking a surge of research into long-sequence modeling and deeper architectures for recommendation tasks. However, many recent approaches rely heavily on cross-attention mechanisms to address the quadratic computational bottleneck in sequential modeling, which can limit the representational power gained from self-attention. We present ULTRA-HSTU, a novel sequential recommendation model developed through end-to-end model and system co-design. By innovating in the design of input sequences, sparse attention mechanisms, and model topology, ULTRA-HSTU achieves substantial improvements in both model quality and efficiency. Comprehensive benchmarking demonstrates that ULTRA-HSTU achieves remarkable scaling efficiency gains -- over 5x faster training scaling and 21x faster inference scaling compared to conventional models -- while delivering superior recommendation quality. Our solution is fully deployed at scale, serving billions of users daily and driving significant 4% to 8% consumption and engagement improvements in real-world production environments.

</details>


### [48] [WSDM Cup 2026 Multilingual Retrieval: A Low-Cost Multi-Stage Retrieval Pipeline](https://arxiv.org/abs/2602.16989)
*Chentong Hao,Minmao Wang*

Main category: cs.IR

TL;DR: 提出一个低成本的多语言检索系统，采用四阶段流水线：LLM查询扩展、BM25候选检索、Jina嵌入稠密排序、Qwen3重排序，在WSDM Cup 2026中取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 解决WSDM Cup 2026多语言检索任务中的挑战：使用英语查询检索中文、波斯语、俄语新闻文档，需要在有限计算预算下实现高效检索。

Method: 四阶段流水线：1) LLM基于GRF风格的查询扩展；2) BM25候选检索；3) 使用jina-embeddings-v4的长文本表示进行稠密排序；4) 对top-20候选使用Qwen3-Reranker-4B进行点式重排序，其余保持稠密排序顺序。

Result: 在官方评估中，系统达到nDCG@20为0.403，Judged@20为0.95。通过消融实验量化了各阶段的贡献，分析了查询扩展、稠密排序和top-k重排序在有限计算预算下的有效性。

Conclusion: 提出的低成本多语言检索系统在WSDM Cup 2026任务中表现良好，四阶段流水线设计有效平衡了检索质量与计算成本，为有限预算下的多语言检索提供了实用解决方案。

Abstract: We present a low-cost retrieval system for the WSDM Cup 2026 multilingual retrieval task, where English queries are used to retrieve relevant documents from a collection of approximately ten million news articles in Chinese, Persian, and Russian, and to output the top-1000 ranked results for each query. We follow a four-stage pipeline that combines LLM-based GRF-style query expansion with BM25 candidate retrieval, dense ranking using long-text representations from jina-embeddings-v4, and pointwise re-ranking of the top-20 candidates using Qwen3-Reranker-4B while preserving the dense order for the remaining results. On the official evaluation, the system achieves nDCG@20 of 0.403 and Judged@20 of 0.95. We further conduct extensive ablation experiments to quantify the contribution of each stage and to analyze the effectiveness of query expansion, dense ranking, and top-$k$ reranking under limited compute budgets.

</details>


### [49] [LiveGraph: Active-Structure Neural Re-ranking for Exercise Recommendation](https://arxiv.org/abs/2602.17036)
*Rong Fu,Zijian Zhang,Haiyun Wei,Jiekai Wu,Kun Liu,Xianda Li,Haoyu Zhao,Yang Li,Yongtai Liu,Ziming Wang,Rui Lu,Simon Fong*

Main category: cs.IR

TL;DR: LiveGraph是一个新颖的主动结构神经重排序框架，利用图表示增强策略解决学生参与的长尾分布问题，动态重排序机制提升内容多样性，在预测准确性和练习多样性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 数字学习环境的扩展催生了智能个性化教育系统的需求。现有练习推荐框架面临两个主要挑战：学生参与的长尾分布问题（少数学生活跃，多数不活跃），以及无法适应个体独特的学习轨迹。

Method: LiveGraph采用图表示增强策略，通过构建学习历史的结构关系图，弥补活跃与非活跃学生之间的信息差距。同时整合动态重排序机制，促进内容多样性，平衡推荐精度与教学多样性。

Result: 在多个真实世界数据集上的实验评估表明，LiveGraph在预测准确性和练习多样性两方面都超越了现有基准方法。

Conclusion: LiveGraph通过图表示增强和动态重排序，有效解决了学生参与的长尾分布问题，能够提供更精准且多样化的个性化练习推荐，为智能教育系统提供了新的解决方案。

Abstract: The continuous expansion of digital learning environments has catalyzed the demand for intelligent systems capable of providing personalized educational content. While current exercise recommendation frameworks have made significant strides, they frequently encounter obstacles regarding the long-tailed distribution of student engagement and the failure to adapt to idiosyncratic learning trajectories. We present LiveGraph, a novel active-structure neural re-ranking framework designed to overcome these limitations. Our approach utilizes a graph-based representation enhancement strategy to bridge the information gap between active and inactive students while integrating a dynamic re-ranking mechanism to foster content diversity. By prioritizing the structural relationships within learning histories, the proposed model effectively balances recommendation precision with pedagogical variety. Comprehensive experimental evaluations conducted on multiple real-world datasets demonstrate that LiveGraph surpasses contemporary baselines in both predictive accuracy and the breadth of exercise diversity.

</details>


### [50] [A Long-term Value Prediction Framework In Video Ranking](https://arxiv.org/abs/2602.17058)
*Huabin Chen,Xinao Wang,Huiping Chu,Keqin Xu,Chenhao Zhai,Chenyi Wang,Kai Meng,Yuning Jiang*

Main category: cs.IR

TL;DR: 提出了一个实用的排序阶段长期价值（LTV）框架，通过位置去偏、多维归因和跨时间建模三个模块解决短视频推荐中的LTV建模挑战。


<details>
  <summary>Details</summary>
Motivation: 在短视频推荐排序阶段准确建模长期价值（LTV）面临挑战：现有方法在处理位置偏差、归因模糊性和时间限制方面存在不足，特别是在十亿级规模下缺乏细粒度归因和鲁棒的位置归一化方法。

Method: 提出三模块框架：1）位置感知去偏分位数（PDQ）模块，使用基于分位数的分布对参与度进行归一化；2）多维归因模块，学习跨上下文、行为和内容信号的连续归因强度；3）跨时间作者建模模块，构建审查感知的日级LTV目标来捕捉创作者驱动的再参与。

Result: 离线和在线A/B测试显示LTV指标显著改善，与短期目标实现稳定权衡。该框架已部署在淘宝生产系统中，在十亿级规模下带来持续的参与度提升，同时保持与工业约束的兼容性。

Conclusion: 该框架通过系统性地解决位置偏差、归因模糊和时间限制三个核心挑战，实现了在工业规模短视频推荐系统中有效且实用的排序阶段LTV建模，为长期用户价值优化提供了可行方案。

Abstract: Accurately modeling long-term value (LTV) at the ranking stage of short-video recommendation remains challenging. While delayed feedback and extended engagement have been explored, fine-grained attribution and robust position normalization at billion-scale are still underdeveloped. We propose a practical ranking-stage LTV framework addressing three challenges: position bias, attribution ambiguity, and temporal limitations.
  (1) Position bias: We introduce a Position-aware Debias Quantile (PDQ) module that normalizes engagement via quantile-based distributions, enabling position-robust LTV estimation without architectural changes. (2) Attribution ambiguity: We propose a multi-dimensional attribution module that learns continuous attribution strengths across contextual, behavioral, and content signals, replacing static rules to capture nuanced inter-video influence. A customized hybrid loss with explicit noise filtering improves causal clarity. (3) Temporal limitations: We present a cross-temporal author modeling module that builds censoring-aware, day-level LTV targets to capture creator-driven re-engagement over longer horizons; the design is extensible to other dimensions (e.g., topics, styles).
  Offline studies and online A/B tests show significant improvements in LTV metrics and stable trade-offs with short-term objectives. Implemented as task augmentation within an existing ranking model, the framework supports efficient training and serving, and has been deployed at billion-scale in Taobao's production system, delivering sustained engagement gains while remaining compatible with industrial constraints.

</details>


### [51] [When LLM Judges Inflate Scores: Exploring Overrating in Relevance Assessment](https://arxiv.org/abs/2602.17170)
*Chuting Yu,Hang Li,Joel Mackenzie,Teerapong Leelanupab*

Main category: cs.IR

TL;DR: 大型语言模型在进行相关性评估时存在系统性高估偏差，对不相关信息也倾向于给出高评分，且对文本长度和表面词汇线索高度敏感，这对其作为人类评估者替代方案的可靠性提出了质疑。


<details>
  <summary>Details</summary>
Motivation: 人类相关性评估耗时且认知负荷高，限制了信息检索评估的可扩展性。虽然大型语言模型被提议作为人类评估者的代理，但其评估的可靠性、稳定性和严谨性是否足以匹配人类仍是一个开放性问题。

Method: 对LLM相关性评估中的高估行为进行系统性研究，涵盖不同模型架构、评估范式（点式和成对）以及文本修改策略。通过控制实验分析LLM评估对文本长度和表面词汇线索的敏感性。

Result: 模型持续给不真正满足信息需求的文本分配虚高的相关性分数（通常置信度很高），揭示了系统性的偏差而非随机波动。LLM相关性评估对文本长度和表面词汇线索高度敏感。

Conclusion: LLM作为人类相关性评估者直接替代方案的使用存在担忧，强调了在应用LLM进行相关性评估时需要建立仔细的诊断评估框架的紧迫性。

Abstract: Human relevance assessment is time-consuming and cognitively intensive, limiting the scalability of Information Retrieval evaluation. This has led to growing interest in using large language models (LLMs) as proxies for human judges. However, it remains an open question whether LLM-based relevance judgments are reliable, stable, and rigorous enough to match humans for relevance assessment. In this work, we conduct a systematic study of overrating behavior in LLM-based relevance judgments across model backbones, evaluation paradigms (pointwise and pairwise), and passage modification strategies. We show that models consistently assign inflated relevance scores -- often with high confidence -- to passages that do not genuinely satisfy the underlying information need, revealing a system-wide bias rather than random fluctuations in judgment. Furthermore, controlled experiments show that LLM-based relevance judgments can be highly sensitive to passage length and surface-level lexical cues. These results raise concerns about the usage of LLMs as drop-in replacements for human relevance assessors, and highlight the urgent need for careful diagnostic evaluation frameworks when applying LLMs for relevance assessments. Our code and results are publicly available.

</details>


### [52] [On the Reliability of User-Centric Evaluation of Conversational Recommender Systems](https://arxiv.org/abs/2602.17264)
*Michael Müller,Amir Reza Mohammadi,Andreas Peintner,Beatriz Barroso Gstrein,Günther Specht,Eva Zangerle*

Main category: cs.IR

TL;DR: 该研究通过大规模实证分析发现，基于静态对话日志的第三方标注在对话推荐系统评估中存在可靠性问题，特别是社交维度可靠性较低，且存在明显的晕轮效应。


<details>
  <summary>Details</summary>
Motivation: 随着对话推荐系统评估越来越依赖第三方标注（众包工作者或大语言模型）对静态对话日志进行用户中心评估，这种做法的可靠性尚未得到充分验证，需要实证研究来检验其可靠性。

Method: 使用CRS-Que框架的18个维度，收集124名众包工作者对200个ReDial对话的1,053个标注，采用随机效应可靠性模型和相关性分析来量化各维度的稳定性及其相互依赖性。

Result: 功利性和结果导向的维度（如准确性、有用性、满意度）在聚合后达到中等可靠性，而社交性维度（如人性化、融洽度）可靠性显著较低；许多维度聚合为单一全局质量信号，显示出强烈的晕轮效应。

Conclusion: 研究结果挑战了单标注者和LLM评估协议的有效性，表明在离线对话推荐系统评估中需要多标注者聚合和维度降维，以提高评估的可靠性。

Abstract: User-centric evaluation has become a key paradigm for assessing Conversational Recommender Systems (CRS), aiming to capture subjective qualities such as satisfaction, trust, and rapport. To enable scalable evaluation, recent work increasingly relies on third-party annotations of static dialogue logs by crowd workers or large language models. However, the reliability of this practice remains largely unexamined. In this paper, we present a large-scale empirical study investigating the reliability and structure of user-centric CRS evaluation on static dialogue transcripts. We collected 1,053 annotations from 124 crowd workers on 200 ReDial dialogues using the 18-dimensional CRS-Que framework. Using random-effects reliability models and correlation analysis, we quantify the stability of individual dimensions and their interdependencies. Our results show that utilitarian and outcome-oriented dimensions such as accuracy, usefulness, and satisfaction achieve moderate reliability under aggregation, whereas socially grounded constructs such as humanness and rapport are substantially less reliable. Furthermore, many dimensions collapse into a single global quality signal, revealing a strong halo effect in third-party judgments. These findings challenge the validity of single-annotator and LLM-based evaluation protocols and motivate the need for multi-rater aggregation and dimension reduction in offline CRS evaluation.

</details>


### [53] [WebFAQ 2.0: A Multilingual QA Dataset with Mined Hard Negatives for Dense Retrieval](https://arxiv.org/abs/2602.17327)
*Michael Dinzinger,Laura Caspari,Ali Salman,Irvin Topi,Jelena Mitrović,Michael Granitzer*

Main category: cs.IR

TL;DR: WebFAQ 2.0是一个包含1.98亿FAQ问答对的多语言数据集，覆盖108种语言，包含1430万双语对齐问答对，是目前最大的FAQ资源，并提供训练密集检索器的困难负样本数据集。


<details>
  <summary>Details</summary>
Motivation: 为了应对多语言和跨语言信息检索的需求，构建一个大规模、多样化且包含双语对齐的FAQ数据集，以支持密集检索器的训练和评估。

Method: 采用新颖的数据收集策略，直接爬取和提取相关网页内容，通过两阶段检索管道挖掘困难负样本，并支持对比学习和知识蒸馏两种微调策略。

Result: 创建了WebFAQ 2.0数据集，包含1.98亿问答对（覆盖108种语言）和1430万双语对齐对，同时发布了包含125万查询的困难负样本数据集，支持密集检索器的训练。

Conclusion: WebFAQ 2.0是一个动态发展的多语言FAQ资源，通过Open Web Index定期更新，为多语言和跨语言信息检索研究提供了重要基础设施，相关数据和代码已开源。

Abstract: We introduce WebFAQ 2.0, a new version of the WebFAQ dataset, containing 198 million FAQ-based natural question-answer pairs across 108 languages. Compared to the previous version, it significantly expands multilingual coverage and the number of bilingual aligned QA pairs to over 14.3M, making it the largest FAQ-based resource. Unlike the original release, WebFAQ 2.0 uses a novel data collection strategy that directly crawls and extracts relevant web content, resulting in a substantially more diverse and multilingual dataset with richer context through page titles and descriptions. In response to community feedback, we also release a hard negatives dataset for training dense retrievers, with 1.25M queries across 20 languages. These hard negatives were mined using a two-stage retrieval pipeline and include cross-encoder scores for 200 negatives per query. We further show how this resource enables two primary fine-tuning strategies for dense retrievers: Contrastive Learning with MultipleNegativesRanking loss, and Knowledge Distillation with MarginMSE loss. WebFAQ 2.0 is not a static resource but part of a long-term effort. Since late 2025, structured FAQs are being regularly released through the Open Web Index, enabling continuous expansion and refinement. We publish the datasets and training scripts to facilitate further research in multilingual and cross-lingual IR. The dataset itself and all related resources are publicly available on GitHub and HuggingFace.

</details>


### [54] [Training-free Graph-based Imputation of Missing Modalities in Multimodal Recommendation](https://arxiv.org/abs/2602.17354)
*Daniele Malitesta,Emanuele Rossi,Claudio Pomo,Tommaso Di Noia,Fragkiskos D. Malliaros*

Main category: cs.IR

TL;DR: 提出基于图结构的训练无关方法，用于多模态推荐系统中缺失模态的补全，通过项目-项目共购图传播可用模态特征。


<details>
  <summary>Details</summary>
Motivation: 多模态推荐系统中模态数据（如图像、描述）可能缺失或噪声，现有方法通常丢弃缺失模态的项目，导致数据损失。缺乏对缺失模态问题的正式定义和有效解决方案。

Method: 1. 形式化多模态推荐中的缺失模态问题；2. 将缺失模态问题重构为项目-项目共购图上的图特征插值问题；3. 提出四种训练无关的图传播方法，通过项目-项目图传播可用模态特征来补全缺失特征。

Result: 实验表明：1. 方法可无缝集成到现有多模态推荐系统；2. 保持甚至扩大多模态与传统推荐系统的性能差距；3. 图基方法优于传统机器学习插值方法；4. 首次分析项目-项目图上的特征同质性对图基插值的影响。

Conclusion: 通过图结构方法有效解决多模态推荐中的缺失模态问题，提供训练无关的插值方案，保持多模态优势并优于传统插值方法。

Abstract: Multimodal recommender systems (RSs) represent items in the catalog through multimodal data (e.g., product images and descriptions) that, in some cases, might be noisy or (even worse) missing. In those scenarios, the common practice is to drop items with missing modalities and train the multimodal RSs on a subsample of the original dataset. To date, the problem of missing modalities in multimodal recommendation has still received limited attention in the literature, lacking a precise formalisation as done with missing information in traditional machine learning. In this work, we first provide a problem formalisation for missing modalities in multimodal recommendation. Second, by leveraging the user-item graph structure, we re-cast the problem of missing multimodal information as a problem of graph features interpolation on the item-item co-purchase graph. On this basis, we propose four training-free approaches that propagate the available multimodal features throughout the item-item graph to impute the missing features. Extensive experiments on popular multimodal recommendation datasets demonstrate that our solutions can be seamlessly plugged into any existing multimodal RS and benchmarking framework while still preserving (or even widen) the performance gap between multimodal and traditional RSs. Moreover, we show that our graph-based techniques can perform better than traditional imputations in machine learning under different missing modalities settings. Finally, we analyse (for the first time in multimodal RSs) how feature homophily calculated on the item-item graph can influence our graph-based imputations.

</details>


### [55] [Improving LLM-based Recommendation with Self-Hard Negatives from Intermediate Layers](https://arxiv.org/abs/2602.17410)
*Bingqian Li,Bowen Zheng,Xiaolei Wang,Long Zhang,Jinpeng Wang,Sheng Chen,Wayne Xin Zhao,Ji-rong Wen*

Main category: cs.IR

TL;DR: ILRec是一个新颖的偏好微调框架，通过从中间层提取自硬负样本信号来改进LLM推荐系统的偏好学习


<details>
  <summary>Details</summary>
Motivation: 现有LLM推荐方法依赖序列级、离线生成的负样本，在大型负项目空间中区分性和信息性不足，需要更精细的负监督信号

Method: 提出ILRec框架：1)从中间层识别自硬负标记作为细粒度负监督；2)设计两阶段框架：跨层偏好优化和跨层偏好蒸馏；3)引入轻量协同过滤模型为负信号分配标记级奖励

Result: 在三个数据集上的广泛实验证明ILRec能有效提升LLM推荐系统的性能

Conclusion: ILRec通过利用中间层的自硬负信号，改进了LLM推荐系统的偏好学习，提高了对负样本的区分能力和信号质量

Abstract: Large language models (LLMs) have shown great promise in recommender systems, where supervised fine-tuning (SFT) is commonly used for adaptation. Subsequent studies further introduce preference learning to incorporate negative samples into the training process. However, existing methods rely on sequence-level, offline-generated negatives, making them less discriminative and informative when adapting LLMs to recommendation tasks with large negative item spaces. To address these challenges, we propose ILRec, a novel preference fine-tuning framework for LLM-based recommendation, leveraging self-hard negative signals extracted from intermediate layers to improve preference learning. Specifically, we identify self-hard negative tokens from intermediate layers as fine-grained negative supervision that dynamically reflects the model's preference learning process. To effectively integrate these signals into training, we design a two-stage framework comprising cross-layer preference optimization and cross-layer preference distillation, enabling the model to jointly discriminate informative negatives and enhance the quality of negative signals from intermediate layers. In addition, we introduce a lightweight collaborative filtering model to assign token-level rewards for negative signals, mitigating the risk of over-penalizing false negatives. Extensive experiments on three datasets demonstrate ILRec's effectiveness in enhancing the performance of LLM-based recommender systems.

</details>


### [56] [Beyond Pipelines: A Fundamental Study on the Rise of Generative-Retrieval Architectures in Web Research](https://arxiv.org/abs/2602.17450)
*Amirereza Abbasi,Mohsen Hooshmand*

Main category: cs.IR

TL;DR: 这篇论文综述了大型语言模型（LLMs）对网络研究和产业的深远影响，特别是通过检索增强生成（RAG）技术，探讨了LLMs如何重塑传统网络任务并催生新应用。


<details>
  <summary>Details</summary>
Motivation: 随着Web技术和大型语言模型（LLMs）的快速发展，LLMs已经深刻影响了网络研究和应用领域。从成熟技术中涌现出的Web 4.0等先进概念，加上LLMs的广泛渗透，使得没有任何领域能够免受其影响。这种变革促使需要对LLMs如何重塑网络研究、开发和应用进行系统性梳理和总结。

Method: 本文采用文献综述的方法，系统性地调查了LLMs特别是通过检索增强生成（RAG）技术对网络研究和产业的影响。论文讨论了关键发展、开放挑战和未来方向，涵盖了从传统网络任务到新兴应用的各个方面。

Result: 调查发现LLMs正在深刻改变网络研究和开发范式，将传统处理流程转变为生成式解决方案，应用于信息检索、问答系统、推荐系统和网络分析等任务。同时，LLMs也催生了基于网络的摘要生成和教育工具等新型应用。RAG技术在这些应用中发挥了关键作用。

Conclusion: LLMs通过RAG等技术正在重塑网络研究和产业，为传统网络任务提供生成式解决方案，并催生新的应用场景。虽然面临开放挑战，但LLMs在增强网络解决方案方面展现出巨大潜力，需要进一步研究和发展。

Abstract: Web research and practices have evolved significantly over time, offering users diverse and accessible solutions across a wide range of tasks. While advanced concepts such as Web 4.0 have emerged from mature technologies, the introduction of large language models (LLMs) has profoundly influenced both the field and its applications. This wave of LLMs has permeated science and technology so deeply that no area remains untouched. Consequently, LLMs are reshaping web research and development, transforming traditional pipelines into generative solutions for tasks like information retrieval, question answering, recommendation systems, and web analytics. They have also enabled new applications such as web-based summarization and educational tools. This survey explores recent advances in the impact of LLMs-particularly through the use of retrieval-augmented generation (RAG)-on web research and industry. It discusses key developments, open challenges, and future directions for enhancing web solutions with LLMs.

</details>


### [57] [A Picture of Agentic Search](https://arxiv.org/abs/2602.17518)
*Francesca Pezzuti,Ophir Frieder,Fabrizio Silvestri,Sean MacAvaney,Nicola Tonellotto*

Main category: cs.IR

TL;DR: 该论文提出了一种收集智能体搜索行为数据的方法，并发布了ASQ数据集，以解决信息检索领域因自动化系统参与而产生的数据缺失问题。


<details>
  <summary>Details</summary>
Motivation: 随着自动化系统越来越多地与人类一起发起搜索查询，信息检索面临重大转变。当前IR系统仍以人为中心设计，其假设在实际中已不再成立，导致系统性能受到影响。缺乏捕捉智能体搜索行为的数据集是一个关键缺口，阻碍了IR系统适应这一新兴趋势。

Method: 开发了一种收集智能体检索增强系统在回答查询时产生和消费的所有数据的方法。发布了Agentic Search Queryset (ASQ)数据集，包含基于HotpotQA、Researchy Questions和MS MARCO的推理诱导查询、检索文档和思考过程，涵盖3种不同智能体和2种检索流程。

Result: 成功创建了ASQ数据集，并提供了配套工具包，使ASQ能够扩展到新的智能体、检索器和数据集，为研究智能体搜索行为提供了重要资源。

Conclusion: 信息检索领域需要适应自动化系统参与的新现实，ASQ数据集填补了智能体搜索行为数据的空白，为未来IR系统的优化和评估提供了必要的基础设施。

Abstract: With automated systems increasingly issuing search queries alongside humans, Information Retrieval (IR) faces a major shift. Yet IR remains human-centred, with systems, evaluation metrics, user models, and datasets designed around human queries and behaviours. Consequently, IR operates under assumptions that no longer hold in practice, with changes to workload volumes, predictability, and querying behaviours. This misalignment affects system performance and optimisation: caching may lose effectiveness, query pre-processing may add overhead without improving results, and standard metrics may mismeasure satisfaction. Without adaptation, retrieval models risk satisfying neither humans, nor the emerging user segment of agents. However, datasets capturing agent search behaviour are lacking, which is a critical gap given IR's historical reliance on data-driven evaluation and optimisation. We develop a methodology for collecting all the data produced and consumed by agentic retrieval-augmented systems when answering queries, and we release the Agentic Search Queryset (ASQ) dataset. ASQ contains reasoning-induced queries, retrieved documents, and thoughts for queries in HotpotQA, Researchy Questions, and MS MARCO, for 3 diverse agents and 2 retrieval pipelines. The accompanying toolkit enables ASQ to be extended to new agents, retrievers, and datasets.

</details>


### [58] [Mine and Refine: Optimizing Graded Relevance in E-commerce Search Retrieval](https://arxiv.org/abs/2602.17654)
*Jiaqi Xi,Raghav Saboo,Luming Chen,Martin Wang,Sudeep Das*

Main category: cs.IR

TL;DR: 提出两阶段"挖掘与精炼"对比训练框架，通过标签感知对比学习和边界优化损失函数，提升电商搜索中语义文本嵌入的质量，改善长尾和噪声查询的检索效果。


<details>
  <summary>Details</summary>
Motivation: 大规模电商搜索需要能够泛化到长尾、噪声查询的嵌入表示，同时需要符合产品约束和政策要求的可扩展监督。实际挑战在于相关性通常是分级的：用户接受替代品或互补品而非精确匹配，生产系统需要在这些相关性层级间保持清晰的相似度分数分离，以实现稳定的混合排序和阈值设置。

Method: 1. 使用轻量级LLM在三级相关性标注数据上微调，通过用户参与度驱动的审计减少噪声；2. 第一阶段：使用标签感知的监督对比目标训练多语言孪生双塔检索器，构建鲁棒的全局语义空间；3. 第二阶段：通过ANN挖掘困难样本并用策略对齐的LLM重新标注，引入多类扩展的circle loss明确锐化相关性层级间的相似度边界；4. 通过拼写增强和合成查询生成提升鲁棒性。

Result: 广泛的离线评估和生产A/B测试表明，该框架提高了检索相关性，并在用户参与度和业务影响方面带来了统计显著的提升。

Conclusion: 提出的两阶段"挖掘与精炼"对比训练框架通过结合可扩展的政策一致监督和边界优化损失函数，有效提升了电商搜索中语义文本嵌入的质量，解决了长尾查询和噪声处理的挑战，在实际生产中验证了其有效性。

Abstract: We propose a two-stage "Mine and Refine" contrastive training framework for semantic text embeddings to enhance multi-category e-commerce search retrieval. Large scale e-commerce search demands embeddings that generalize to long tail, noisy queries while adhering to scalable supervision compatible with product and policy constraints. A practical challenge is that relevance is often graded: users accept substitutes or complements beyond exact matches, and production systems benefit from clear separation of similarity scores across these relevance strata for stable hybrid blending and thresholding. To obtain scalable policy consistent supervision, we fine-tune a lightweight LLM on human annotations under a three-level relevance guideline and further reduce residual noise via engagement driven auditing. In Stage 1, we train a multilingual Siamese two-tower retriever with a label aware supervised contrastive objective that shapes a robust global semantic space. In Stage 2, we mine hard samples via ANN and re-annotate them with the policy aligned LLM, and introduce a multi-class extension of circle loss that explicitly sharpens similarity boundaries between relevance levels, to further refine and enrich the embedding space. Robustness is additionally improved through additive spelling augmentation and synthetic query generation. Extensive offline evaluations and production A/B tests show that our framework improves retrieval relevance and delivers statistically significant gains in engagement and business impact.

</details>
