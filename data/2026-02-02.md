<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 61]
- [cs.IR](#cs.IR) [Total: 7]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [In Vino Veritas and Vulnerabilities: Examining LLM Safety via Drunk Language Inducement](https://arxiv.org/abs/2601.22169)
*Anudeex Shetty,Aditya Joshi,Salil S. Kanhere*

Main category: cs.CL

TL;DR: 该论文研究如何通过"醉酒语言"诱导LLMs产生安全漏洞，包括越狱攻击和隐私泄露，并提出了三种诱导方法，在多个LLM上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 人类在酒精影响下容易出现不良行为和隐私泄露，研究者想探索LLMs在类似"醉酒语言"诱导下是否也会出现类似的安全漏洞，以评估LLM的安全风险。

Method: 提出了三种诱导LLMs产生醉酒语言的方法：1) 基于角色的提示工程；2) 因果微调；3) 基于强化的后训练。在5个LLMs上进行评估，使用JailbreakBench和ConfAIde两个英文基准测试，结合人工评估和基于LLM的评估器进行综合分析。

Result: 实验显示，醉酒语言诱导的LLMs在JailbreakBench上表现出更高的越狱攻击易感性（即使有防御机制），在ConfAIde上出现更多隐私泄露，效果优于之前的方法。研究发现人类醉酒行为与LLMs在醉酒语言诱导下的拟人化行为存在对应关系。

Conclusion: 醉酒语言诱导方法简单高效，可能成为对抗LLM安全调优的手段，凸显了LLM安全的重大风险。研究揭示了LLMs在特定诱导下可能表现出类似人类醉酒的不良行为，需要加强安全防护。

Abstract: Humans are susceptible to undesirable behaviours and privacy leaks under the influence of alcohol. This paper investigates drunk language, i.e., text written under the influence of alcohol, as a driver for safety failures in large language models (LLMs). We investigate three mechanisms for inducing drunk language in LLMs: persona-based prompting, causal fine-tuning, and reinforcement-based post-training. When evaluated on 5 LLMs, we observe a higher susceptibility to jailbreaking on JailbreakBench (even in the presence of defences) and privacy leaks on ConfAIde, where both benchmarks are in English, as compared to the base LLMs as well as previously reported approaches. Via a robust combination of manual evaluation and LLM-based evaluators and analysis of error categories, our findings highlight a correspondence between human-intoxicated behaviour, and anthropomorphism in LLMs induced with drunk language. The simplicity and efficiency of our drunk language inducement approaches position them as potential counters for LLM safety tuning, highlighting significant risks to LLM safety.

</details>


### [2] [MrRoPE: Mixed-radix Rotary Position Embedding](https://arxiv.org/abs/2601.22181)
*Qingyuan Tian,Wenhong Zhu,Xiaoran Liu,Xiaofeng Wang,Rui Wang*

Main category: cs.CL

TL;DR: 本文提出MrRoPE（混合基数RoPE），从基数系统转换的角度统一了各种RoPE扩展方法，并基于此理论提出了两种无需训练的扩展方案MrRoPE-Uni和MrRoPE-Pro，在长序列处理上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前Rotary Position Embedding（RoPE）扩展策略高度多样化且缺乏统一的理论基础，需要一种能够系统化理解和改进RoPE扩展方法的理论框架。

Method: 基于基数系统转换视角提出MrRoPE，将各种RoPE扩展方法统一为不同的基数转换策略。在此基础上设计了两种无需训练的扩展方法：MrRoPE-Uni（均匀基数转换）和MrRoPE-Pro（渐进基数转换）。

Result: MrRoPE-Pro在128K上下文Needle-in-a-Haystack测试中保持超过85%的召回率，在Infinite-Bench检索和对话子集上的准确率是YaRN的两倍以上。理论分析证实MrRoPE-Pro有效提高了RoPE可达到的编码长度上限。

Conclusion: MrRoPE提供了一个统一的RoPE扩展理论框架，将现有方法系统化分类，并基于此理论提出的训练免费扩展方法在长序列处理上表现出色，验证了该理论的可靠性和实用性。

Abstract: Rotary Position Embedding (RoPE)-extension refers to modifying or generalizing the Rotary Position Embedding scheme to handle longer sequences than those encountered during pre-training. However, current extension strategies are highly diverse and lack a unified theoretical foundation. In this paper, we propose MrRoPE (Mixed-radix RoPE), a generalized encoding formulation based on a radix system conversion perspective, which elegantly unifies various RoPE-extension approaches as distinct radix conversion strategies. Based on this theory, we introduce two training-free extensions, MrRoPE-Uni and MrRoPE-Pro, which leverage uniform and progressive radix conversion strategies, respectively, to achieve 'train short, test long' generalization. Without fine-tuning, MrRoPE-Pro sustains over 85% recall in the 128K-context Needle-in-a-Haystack test and achieves more than double YaRN's accuracy on Infinite-Bench retrieval and dialogue subsets. Theoretical analysis confirms that MrRoPE-Pro effectively raises the upper bound of RoPE's attainable encoding length, which further validates the reliability and utility of our theory and methodology.

</details>


### [3] [Prepare Reasoning Language Models for Multi-Agent Debate with Self-Debate Reinforcement Learning](https://arxiv.org/abs/2601.22297)
*Chenxi Liu,Yanshuo Chen,Ruibo Chen,Tianyi Xiong,Tong Zheng,Heng Huang*

Main category: cs.CL

TL;DR: SDRL训练框架让单个LLM既具备独立解决问题能力，又能从多智能体辩论中学习多样推理路径，提升单模型和辩论性能。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法训练LLM孤立解决问题，未明确准备它们从辩论中合成和受益于不同推理路径。当前多智能体辩论在测试时能提升性能，但训练阶段未充分准备模型从辩论中学习。

Method: 提出自辩论强化学习框架：1) 对同一提示采样多个候选解决方案；2) 构建包含多样推理路径的辩论上下文；3) 生成基于此上下文的第二轮响应；4) 联合优化初始响应和辩论条件响应。

Result: 在多个基础模型和推理基准上的实验表明，SDRL既提升了整体多智能体辩论性能，又同时增强了单模型推理能力。

Conclusion: SDRL通过训练单个LLM从多样推理轨迹中学习，有效结合了强化学习和多智能体辩论的优势，实现了单模型和辩论性能的双重提升。

Abstract: The reasoning abilities of large language models (LLMs) have been substantially improved by reinforcement learning with verifiable rewards (RLVR). At test time, collaborative reasoning through Multi-Agent Debate (MAD) has emerged as a promising approach for enhancing LLM performance. However, current RLVR methods typically train LLMs to solve problems in isolation, without explicitly preparing them to synthesize and benefit from different rationales that arise during debate. In this work, we propose Self-Debate Reinforcement Learning (SDRL), a training framework that equips a single LLM with strong standalone problem-solving ability and the capability to learn from diverse reasoning trajectories in MAD. Given a prompt, SDRL first samples multiple candidate solutions, then constructs a debate context with diverse reasoning paths and generates second-turn responses conditioned on this context. Finally, SDRL jointly optimizes both the initial and debate-conditioned responses, yielding a model that is effective as both a standalone solver and a debate participant. Experiments across multiple base models and reasoning benchmarks show that SDRL improves overall MAD performance while simultaneously strengthening single model reasoning.

</details>


### [4] [MERMAID: Memory-Enhanced Retrieval and Reasoning with Multi-Agent Iterative Knowledge Grounding for Veracity Assessment](https://arxiv.org/abs/2601.22361)
*Yupeng Cao,Chengyang He,Yangyang Yu,Ping Wang,K. P. Subbalakshmi*

Main category: cs.CL

TL;DR: MERMAID：一个记忆增强的多智能体真实性评估框架，通过紧密耦合检索与推理过程，结合智能体驱动搜索、结构化知识表示和持久记忆模块，实现动态证据获取和跨声明证据复用，在多个事实核查基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有真实性评估方法通常将证据检索视为静态、孤立的步骤，未能有效管理或跨声明复用检索到的证据，导致检索冗余和效率低下。

Method: 提出MERMAID框架，采用Reason-Action风格的迭代过程，集成智能体驱动搜索、结构化知识表示和持久记忆模块，实现动态证据获取和跨声明证据复用。

Result: 在三个事实核查基准和两个声明验证数据集上，使用GPT、LLaMA、Qwen等多个LLM进行评估，MERMAID在提升搜索效率的同时实现了最先进的性能。

Conclusion: 通过协同检索、推理和记忆，MERMAID证明了这种紧密耦合方法对于可靠真实性评估的有效性，显著减少了冗余搜索并提高了验证效率和一致性。

Abstract: Assessing the veracity of online content has become increasingly critical. Large language models (LLMs) have recently enabled substantial progress in automated veracity assessment, including automated fact-checking and claim verification systems. Typical veracity assessment pipelines break down complex claims into sub-claims, retrieve external evidence, and then apply LLM reasoning to assess veracity. However, existing methods often treat evidence retrieval as a static, isolated step and do not effectively manage or reuse retrieved evidence across claims. In this work, we propose MERMAID, a memory-enhanced multi-agent veracity assessment framework that tightly couples the retrieval and reasoning processes. MERMAID integrates agent-driven search, structured knowledge representations, and a persistent memory module within a Reason-Action style iterative process, enabling dynamic evidence acquisition and cross-claim evidence reuse. By retaining retrieved evidence in an evidence memory, the framework reduces redundant searches and improves verification efficiency and consistency. We evaluate MERMAID on three fact-checking benchmarks and two claim-verification datasets using multiple LLMs, including GPT, LLaMA, and Qwen families. Experimental results show that MERMAID achieves state-of-the-art performance while improving the search efficiency, demonstrating the effectiveness of synergizing retrieval, reasoning, and memory for reliable veracity assessment.

</details>


### [5] [Context Structure Reshapes the Representational Geometry of Language Models](https://arxiv.org/abs/2601.22364)
*Eghbal A. Hosseini,Yuxuan Li,Yasaman Bahri,Declan Campbell,Andrew Kyle Lampinen*

Main category: cs.CL

TL;DR: LLMs在上下文学习(ICL)中表现出两种不同的表征直线化模式：在持续预测任务中，上下文增加会使神经轨迹更直且与预测改进相关；在结构化预测任务中，直线化仅出现在有明确结构的阶段。


<details>
  <summary>Details</summary>
Motivation: 已有研究显示LLMs在深层会将输入序列表征组织为更直的神经轨迹，这被认为有助于通过线性外推进行下一词预测。同时，LLMs能在上下文学习中适应新任务并改变表征。本研究旨在探索这两种现象的结合——在ICL过程中是否会发生表征直线化。

Method: 在Gemma 2模型上测量多种上下文学习任务中的表征直线化程度，包括持续预测任务（如自然语言、网格世界遍历）和结构化预测任务（如少样本学习任务）。

Result: 发现LLMs在上下文学习中表现出二分性：在持续预测设置中，增加上下文会使神经序列轨迹更直，且这与模型预测改进相关；在结构化预测设置中，直线化不一致——仅出现在有明确结构的任务阶段（如重复模板），而在其他阶段消失。

Conclusion: 上下文学习不是单一过程，LLMs会根据任务结构动态选择不同策略，只有部分策略会产生表征直线化。LLMs更像是瑞士军刀，能灵活适应不同任务需求。

Abstract: Large Language Models (LLMs) have been shown to organize the representations of input sequences into straighter neural trajectories in their deep layers, which has been hypothesized to facilitate next-token prediction via linear extrapolation. Language models can also adapt to diverse tasks and learn new structure in context, and recent work has shown that this in-context learning (ICL) can be reflected in representational changes. Here we bring these two lines of research together to explore whether representation straightening occurs \emph{within} a context during ICL. We measure representational straightening in Gemma 2 models across a diverse set of in-context tasks, and uncover a dichotomy in how LLMs' representations change in context. In continual prediction settings (e.g., natural language, grid world traversal tasks) we observe that increasing context increases the straightness of neural sequence trajectories, which is correlated with improvement in model prediction. Conversely, in structured prediction settings (e.g., few-shot tasks), straightening is inconsistent -- it is only present in phases of the task with explicit structure (e.g., repeating a template), but vanishes elsewhere. These results suggest that ICL is not a monolithic process. Instead, we propose that LLMs function like a Swiss Army knife: depending on task structure, the LLM dynamically selects between strategies, only some of which yield representational straightening.

</details>


### [6] [Stability-Aware Prompt Optimization for Clinical Data Abstraction](https://arxiv.org/abs/2601.22373)
*Arinbjörn Kolbeinsson,Daniel Timbie,Sajjan Narsinghani,Sanjay Hariharan*

Main category: cs.CL

TL;DR: 该论文研究了临床LLM系统中提示词的敏感性问题，提出了联合优化准确性和稳定性的方法，以减少不同提示词表达对模型输出的影响。


<details>
  <summary>Details</summary>
Motivation: 临床抽象任务中使用的大型语言模型对提示词措辞敏感，但现有研究大多将提示词视为固定，且孤立地研究不确定性。作者认为需要将这两个问题联合处理，因为即使模型表现出良好的校准性，仍可能对提示词改写保持脆弱。

Method: 在两个临床任务（MedAlign适用性/正确性和MS亚型抽象）上，使用多个开源和专有模型，通过翻转率衡量提示词敏感性，并将其与校准和选择性预测相关联。提出了一个双目标提示优化循环，联合优化准确性和稳定性，明确包含稳定性项。

Result: 研究发现更高的准确性并不能保证提示词稳定性，模型可能表现出良好的校准性但对提示词改写仍很脆弱。提出的双目标优化方法显著降低了跨任务和模型的翻转率，有时以适度的准确性为代价。

Conclusion: 在验证临床LLM系统时，提示词敏感性应成为一个明确的优化目标，需要联合考虑准确性和稳定性来构建更可靠的临床AI系统。

Abstract: Large language models used for clinical abstraction are sensitive to prompt wording, yet most work treats prompts as fixed and studies uncertainty in isolation. We argue these should be treated jointly. Across two clinical tasks (MedAlign applicability/correctness and MS subtype abstraction) and multiple open and proprietary models, we measure prompt sensitivity via flip rates and relate it to calibration and selective prediction. We find that higher accuracy does not guarantee prompt stability, and that models can appear well-calibrated yet remain fragile to paraphrases. We propose a dual-objective prompt optimization loop that jointly targets accuracy and stability, showing that explicitly including a stability term reduces flip rates across tasks and models, sometimes at modest accuracy cost. Our results suggest prompt sensitivity should be an explicit objective when validating clinical LLM systems.

</details>


### [7] [SPLA: Block Sparse Plus Linear Attention for Long Context Modeling](https://arxiv.org/abs/2601.22379)
*Bailin Wang,Dan Friedman,Tao Lei,Chong Wang*

Main category: cs.CL

TL;DR: SPLA提出稀疏加线性注意力框架，通过二阶泰勒展开准确选择相关块进行精确注意力计算，同时用残差线性注意力压缩未选块为紧凑循环状态，避免IO开销，提升长上下文建模效率。


<details>
  <summary>Details</summary>
Motivation: 现有块级稀疏注意力方法存在选择保真度低和累积上下文丢失问题，因为完全丢弃未选块。需要解决这些限制以提升长上下文建模效率。

Method: 1) 使用二阶泰勒展开导出的选择指标准确识别相关块进行精确注意力计算；2) 通过残差线性注意力(RLA)模块将未选块压缩为紧凑循环状态；3) 采用优化的基于减法的RLA公式，避免IO开销，确保推理期间不显式访问未选块。

Result: 实验表明SPLA在持续预训练中缩小性能差距，在RULER等长上下文基准测试中超越密集注意力模型，同时保持竞争力的通用知识和推理能力。

Conclusion: SPLA通过结合精确块选择和高效未选块压缩，解决了现有稀疏注意力方法的局限性，实现了高效且有效的长上下文建模。

Abstract: Block-wise sparse attention offers significant efficiency gains for long-context modeling, yet existing methods often suffer from low selection fidelity and cumulative contextual loss by completely discarding unselected blocks. To address these limitations, we introduce Sparse Plus Linear Attention (SPLA), a framework that utilizes a selection metric derived from second-order Taylor expansions to accurately identify relevant blocks for exact attention. Instead of discarding the remaining "long tail," SPLA compresses unselected blocks into a compact recurrent state via a residual linear attention (RLA) module. Crucially, to avoid IO overhead, we derive an optimized subtraction-based formulation for RLA -- calculating the residual as the difference between global and selected linear attention -- ensuring that unselected blocks are never explicitly accessed during inference. Our experiments demonstrate that SPLA closes the performance gap in continual pretraining, surpassing dense attention models on long-context benchmarks like RULER while maintaining competitive general knowledge and reasoning capabilities.

</details>


### [8] [SP^2DPO: An LLM-assisted Semantic Per-Pair DPO Generalization](https://arxiv.org/abs/2601.22385)
*Chaoyue He,Xin Zhou,Di Wang,Hong Xu,Wei Liu,Chunyan Miao*

Main category: cs.CL

TL;DR: SP2DPO是一种改进的DPO方法，通过为每个偏好对分配语义感知的温度参数β_i，而不是使用全局统一的β，以更好地处理异构偏好数据中的不同信号强度和噪声。


<details>
  <summary>Details</summary>
Motivation: 真实世界的偏好数据具有异质性：混合了高信号目标失败（如安全性、事实性）和低信号主观区分（如风格），且包含标签噪声。现有DPO方法使用全局温度参数β，将所有偏好对视为同等信息量，无法适应这种异质性。

Method: SP2DPO将DPO的全局温度β替换为实例特定的调度β_i，这些β_i在训练前通过教师语言模型生成的语义差距标注（类别、幅度、置信度）离线确定。在UltraFeedback偏好语料库（59,960对）上实例化该方法，构建可审计的β_i工件，训练时保持标准DPO优化器但为每对设置不同的β。

Result: 在四个开放权重指令调优学生骨干（4B-8B）上的实验表明，SP2DPO与调优的全局β DPO基线竞争，并在四个骨干中的两个上提高了AlpacaEval 2.0长度控制胜率，同时避免了每模型β扫描。

Conclusion: SP2DPO通过语义感知的每对温度调度，为异构偏好数据提供了更精细的控制，在保持与基线竞争性能的同时避免了昂贵的超参数调优，为偏好优化提供了可扩展且可审计的解决方案。

Abstract: Direct Preference Optimization (DPO) controls the trade-off between fitting preference labels and staying close to a reference model using a single global temperature beta, implicitly treating all preference pairs as equally informative. Real-world preference corpora are heterogeneous: they mix high-signal, objective failures (for example, safety, factuality, instruction violations) with low-signal or subjective distinctions (for example, style), and also include label noise. We introduce our method, SP2DPO (Semantic Per-Pair DPO), a generalization that replaces the global temperature with an instance-specific schedule beta_i pre-decided offline from structured semantic-gap annotations (category, magnitude, confidence) produced by teacher language models. We instantiate this procedure on the UltraFeedback preference corpus (59,960 pairs), enabling large-scale construction of an auditable beta_i artifact, and incur zero training-time overhead: the inner-loop optimizer remains standard DPO with beta set per pair. We focus our empirical study on AlpacaEval 2.0, reporting both raw win rate and length-controlled win rate. Across four open-weight, instruction-tuned student backbones (4B-8B), SP2DPO is competitive with a tuned global-beta DPO baseline and improves AlpacaEval 2.0 length-controlled win rate on two of four backbones, while avoiding per-model beta sweeps. All code, annotations, and artifacts will be released.

</details>


### [9] [Specialists or Generalists? Multi-Agent and Single-Agent LLMs for Essay Grading](https://arxiv.org/abs/2601.22386)
*Jamiu Adekunle Idowu,Ahmed Almasoud*

Main category: cs.CL

TL;DR: 评估单智能体和多智能体LLM架构在作文评分中的表现，发现多智能体系统在识别弱作文方面表现更好，单智能体系统在中档作文上表现更佳，两者在高质作文上都表现不佳，少量样本校准能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管自动作文评分系统越来越依赖大语言模型，但不同架构选择如何影响其在不同质量水平作文上的表现尚不清楚，需要系统评估单智能体和多智能体架构的性能差异。

Method: 使用ASAP 2.0语料库评估单智能体和多智能体LLM架构，多智能体系统将评分分解为三个专业智能体（内容、结构、语言），由主席智能体协调实施包括否决规则和分数上限的评分标准逻辑，在零样本和少量样本条件下使用GPT-5.1进行测试。

Result: 多智能体系统在识别弱作文方面显著更好，单智能体系统在中档作文上表现更佳，两者在高质作文上都表现不佳。少量样本校准是系统性能的主导因素——每个评分等级仅提供两个示例就能将QWK提高约26%。

Conclusion: 架构选择应与具体部署优先级对齐，多智能体AI特别适合对有风险学生进行诊断性筛查，而单智能体模型为一般评估提供成本效益高的解决方案。

Abstract: Automated essay scoring (AES) systems increasingly rely on large language models, yet little is known about how architectural choices shape their performance across different essay quality levels. This paper evaluates single-agent and multi-agent LLM architectures for essay grading using the ASAP 2.0 corpus. Our multi-agent system decomposes grading into three specialist agents (Content, Structure, Language) coordinated by a Chairman Agent that implements rubric-aligned logic including veto rules and score capping. We test both architectures in zero-shot and few-shot conditions using GPT-5.1. Results show that the multi-agent system is significantly better at identifying weak essays while the single-agent system performs better on mid-range essays. Both architectures struggle with high-quality essays. Critically, few-shot calibration emerges as the dominant factor in system performance -- providing just two examples per score level improves QWK by approximately 26% for both architectures. These findings suggest architectural choice should align with specific deployment priorities, with multi-agent AI particularly suited for diagnostic screening of at-risk students, while single-agent models provide a cost-effective solution for general assessment.

</details>


### [10] [Culturally Grounded Personas in Large Language Models: Characterization and Alignment with Socio-Psychological Value Frameworks](https://arxiv.org/abs/2601.22396)
*Candida M. Greco,Lucio La Cava,Andrea Tagarelli*

Main category: cs.CL

TL;DR: 该研究探讨了基于大语言模型生成的文化基础人物角色是否能准确反映不同文化背景下的人类价值观和道德体系，通过世界价值观调查、英格尔哈特-韦尔泽尔文化地图和道德基础理论三个框架进行评估。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在模拟人类行为方面越来越有用，但这些合成人物角色在多大程度上能准确反映不同文化条件下的世界和道德价值体系仍然不确定。研究旨在验证LLM生成的文化基础人物角色是否与现有文化价值观框架保持一致。

Method: 研究通过三个互补视角来评估LLM生成的人物角色：1) 基于世界价值观调查的可解释变量生成文化基础人物角色；2) 通过英格尔哈特-韦尔泽尔文化地图定位分析文化差异；3) 使用道德基础问卷分析道德特征分布，并建立文化到道德的映射关系。

Result: 研究发现：1) 生成的人物角色在英格尔哈特-韦尔泽尔地图上的定位反映了文化条件的稳定差异；2) 在人口统计层面的响应分布大致遵循人类群体模式；3) 道德特征分析揭示了不同文化配置下道德响应的变化规律。

Conclusion: 研究表明，基于文化基础的人物角色生成和分析方法能够有效评估跨文化结构和道德变化，为LLM在模拟人类文化价值观方面的准确性提供了验证框架。

Abstract: Despite the growing utility of Large Language Models (LLMs) for simulating human behavior, the extent to which these synthetic personas accurately reflect world and moral value systems across different cultural conditionings remains uncertain. This paper investigates the alignment of synthetic, culturally-grounded personas with established frameworks, specifically the World Values Survey (WVS), the Inglehart-Welzel Cultural Map, and Moral Foundations Theory. We conceptualize and produce LLM-generated personas based on a set of interpretable WVS-derived variables, and we examine the generated personas through three complementary lenses: positioning on the Inglehart-Welzel map, which unveils their interpretation reflecting stable differences across cultural conditionings; demographic-level consistency with the World Values Survey, where response distributions broadly track human group patterns; and moral profiles derived from a Moral Foundations questionnaire, which we analyze through a culture-to-morality mapping to characterize how moral responses vary across different cultural configurations. Our approach of culturally-grounded persona generation and analysis enables evaluation of cross-cultural structure and moral variation.

</details>


### [11] [Bifocal Attention: Harmonizing Geometric and Spectral Positional Embeddings for Algorithmic Generalization](https://arxiv.org/abs/2601.22402)
*Kanishk Awadhiya*

Main category: cs.CL

TL;DR: 论文提出Bifocal Attention架构，通过几何编码和谱编码双模态解决RoPE在长距离递归推理中的局限性


<details>
  <summary>Details</summary>
Motivation: 标准RoPE存在"谱刚性"问题，固定的几何衰减模式无法捕捉长距离周期性结构，导致模型在处理深层递归推理时出现"结构鸿沟"

Method: 提出Bifocal Attention架构，包含几何编码（标准RoPE）和谱编码（可学习谐波算子）双模态，以及Spectral Evolution训练协议，让位置频率从静态几何参数演化为任务特定的谐波基

Result: 未在摘要中明确给出实验结果，但方法旨在解决RoPE在算法推理中的局限性

Conclusion: 通过解耦位置编码为几何和谱双模态，可以更好地处理长距离递归结构和算法推理任务

Abstract: Rotary Positional Embeddings (RoPE) have become the standard for Large Language Models (LLMs) due to their ability to encode relative positions through geometric rotation. However, we identify a significant limitation we term ''Spectral Rigidity'': standard RoPE utilizes a fixed geometric decay ($θ^{-i}$) optimized for local syntactic coherence, which fails to capture the long-range, periodic structures inherent in recursive logic and algorithmic reasoning. This results in a ''Structure Gap'', where models trained on shallow reasoning chains fail to extrapolate to deeper recursive steps. In this work, we introduce Bifocal Attention, an architectural paradigm that decouples positional encoding into two distinct modalities: Geometric Eyes (Standard RoPE) for precise token-level manipulation, and Spectral Eyes (Learnable Harmonic Operators) for tracking long-range recursive depth. We propose a novel training protocol, Spectral Evolution, which initializes positional frequencies as static geometric parameters but allows them to evolve via gradient descent into a harmonic basis optimized for the specific algorithmic topology of the task.

</details>


### [12] [Word-Centered Semantic Graphs for Interpretable Diachronic Sense Tracking](https://arxiv.org/abs/2601.22410)
*Imene Kolli,Kai-Robin Lange,Jonas Rieger,Carsten Jentsch*

Main category: cs.CL

TL;DR: 提出了一个基于图的解释性框架，用于分析历时语料库中的语义演变，通过构建词中心语义网络来追踪词义变化。


<details>
  <summary>Details</summary>
Motivation: 现有的语义演变分析方法缺乏可解释性，且通常依赖于预定义的词义清单。需要一种更透明、紧凑的方法来捕捉词义动态变化。

Method: 为每个目标词和时间切片构建词中心语义网络，结合历时Skip-gram嵌入的分布相似性和特定时间掩码语言模型的词汇可替换性。通过聚类外围图识别词义相关结构，通过节点重叠对齐跨时间聚类，并通过聚类组成和归一化聚类质量追踪变化。

Result: 在《纽约时报杂志》文章语料库（1980-2017）的应用研究中，图连接性反映了多义性动态，诱导的社区捕捉了对比轨迹：事件驱动的词义替换（trump）、具有聚类过分割效应的语义稳定性（god），以及与数字通信相关的逐渐关联变化（post）。

Conclusion: 词中心语义图提供了一种紧凑透明的表示方法，用于探索词义演变，无需依赖预定义的词义清单，能够有效捕捉不同类型的语义变化轨迹。

Abstract: We propose an interpretable, graph-based framework for analyzing semantic shift in diachronic corpora. For each target word and time slice, we induce a word-centered semantic network that integrates distributional similarity from diachronic Skip-gram embeddings with lexical substitutability from time-specific masked language models. We identify sense-related structure by clustering the peripheral graph, align clusters across time via node overlap, and track change through cluster composition and normalized cluster mass. In an application study on a corpus of New York Times Magazine articles (1980 - 2017), we show that graph connectivity reflects polysemy dynamics and that the induced communities capture contrasting trajectories: event-driven sense replacement (trump), semantic stability with cluster over-segmentation effects (god), and gradual association shifts tied to digital communication (post). Overall, word-centered semantic graphs offer a compact and transparent representation for exploring sense evolution without relying on predefined sense inventories.

</details>


### [13] [Large Language Model Agents Are Not Always Faithful Self-Evolvers](https://arxiv.org/abs/2601.22436)
*Weixiang Zhao,Yingshuo Wang,Yichen Zhang,Yang Deng,Yanyan Zhao,Wanxiang Che,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: 研究发现自进化LLM智能体在决策时对原始经验具有因果依赖性，但对浓缩经验却常常忽视或误解，揭示了经验忠实性的不对称问题。


<details>
  <summary>Details</summary>
Motivation: 尽管自进化LLM智能体通过积累和重用过去经验不断改进，但尚不清楚它们是否真正依赖这些经验来指导行为。研究旨在系统调查自进化LLM智能体中经验忠实性的问题。

Method: 通过控制性因果干预，对原始和浓缩两种形式的经验进行全面评估。研究涵盖了4个代表性框架、10个LLM骨干模型和9个环境，分析了单智能体和多智能体配置以及不同骨干模型规模。

Result: 发现了显著的不对称性：智能体始终依赖原始经验，但经常忽视或误解浓缩经验，即使这是唯一提供的经验。这种差距在单/多智能体配置和不同骨干模型规模中都持续存在。

Conclusion: 这种差距源于三个因素：浓缩内容的语义限制、抑制经验的内部处理偏见以及预训练先验已足够的任务机制。这些发现挑战了关于自进化方法的普遍假设，强调了需要更忠实可靠的经验整合方法。

Abstract: Self-evolving large language model (LLM) agents continually improve by accumulating and reusing past experience, yet it remains unclear whether they faithfully rely on that experience to guide their behavior. We present the first systematic investigation of experience faithfulness, the causal dependence of an agent's decisions on the experience it is given, in self-evolving LLM agents. Using controlled causal interventions on both raw and condensed forms of experience, we comprehensively evaluate four representative frameworks across 10 LLM backbones and 9 environments. Our analysis uncovers a striking asymmetry: while agents consistently depend on raw experience, they often disregard or misinterpret condensed experience, even when it is the only experience provided. This gap persists across single- and multi-agent configurations and across backbone scales. We trace its underlying causes to three factors: the semantic limitations of condensed content, internal processing biases that suppress experience, and task regimes where pretrained priors already suffice. These findings challenge prevailing assumptions about self-evolving methods and underscore the need for more faithful and reliable approaches to experience integration.

</details>


### [14] [Stop Jostling: Adaptive Negative Sampling Reduces the Marginalization of Low-Resource Language Tokens by Cross-Entropy Loss](https://arxiv.org/abs/2601.22439)
*Galim Turumtaev*

Main category: cs.CL

TL;DR: 提出阈值技术减少罕见token边缘化影响，改善低资源语言模型性能


<details>
  <summary>Details</summary>
Motivation: 神经语言模型在低资源语言上表现不佳，因为这些语言的token在训练集中罕见，容易受到边缘化影响，无法有效学习。

Method: 提出阈值技术来减少边缘化对罕见token的影响，使罕见token能够获得更有意义的对齐。通过字符级语言模型实验验证。

Result: 该方法显著提升了低资源语言验证数据上的性能，首次展示了如何通过负采样限制过度边缘化的有害影响来改善罕见token的表示。

Conclusion: 该工作为改善低资源语言模型性能提供了新方法，通过减少罕见token的边缘化问题，增强了语言模型对代表性不足语言的处理能力。

Abstract: Neural language models often struggle with low-resource languages due to the limited availability of training data, making tokens from these languages rare in the training set. This paper addresses a specific challenge during training: rare tokens are disproportionately affected by marginalization, which prevents them from learning effectively. We propose a thresholding technique that reduces the impact of this marginalization, allowing rare tokens to benefit from more meaningful alignment. Through experiments with a character-level language model, we demonstrate that this method significantly improves performance on low-resource language validation data. This work is the first to show how negative sampling can be applied to improve the representation of rare tokens by limiting the harmful influence of excessive marginalization, offering a new approach to enhancing language model performance for underrepresented languages.

</details>


### [15] [SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization](https://arxiv.org/abs/2601.22491)
*Jinyang Wu,Changpeng Yang,Yuhao Shen,Fangzhi Xu,Bolin Ni,Chonghua Liao,Yuchen Liu,Hongzhen Wang,Shuai Nie,Shuai Zhang,Haoran Luo,Jiaming Xu*

Main category: cs.CL

TL;DR: SSL（甜点学习）是一种强化学习框架，通过渐进放大的分层奖励引导策略走向解空间的"甜点"区域，提高样本效率和跨任务迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法通常使用二元奖励，无法区分实现相同结果但质量不同的轨迹，忽略了解空间中的潜在多样性。受网球"甜点"概念启发，需要一种能提供差异化指导的框架。

Method: SSL采用渐进放大、分层奖励的原则：视觉感知任务利用距离分层建模奖励接近度，复杂推理任务奖励向有前景解决方案的渐进进展。理论证明SSL保持最优解排序并提高梯度信噪比。

Result: 在GUI感知、短期/长期规划和复杂推理任务的12个基准测试中，SSL相比强基线获得一致改进，达到最高2.5倍的样本效率提升，并展现出有效的跨任务迁移能力。

Conclusion: SSL作为一种通用原则，能够训练出能力强且鲁棒的智能体，为强化学习提供了新的差异化指导框架。

Abstract: Reinforcement learning with verifiable rewards has emerged as a powerful paradigm for training intelligent agents. However, existing methods typically employ binary rewards that fail to capture quality differences among trajectories achieving identical outcomes, thereby overlooking potential diversity within the solution space. Inspired by the ``sweet spot'' concept in tennis-the racket's core region that produces optimal hitting effects, we introduce \textbf{S}weet \textbf{S}pot \textbf{L}earning (\textbf{SSL}), a novel framework that provides differentiated guidance for agent optimization. SSL follows a simple yet effective principle: progressively amplified, tiered rewards guide policies toward the sweet-spot region of the solution space. This principle naturally adapts across diverse tasks: visual perception tasks leverage distance-tiered modeling to reward proximity, while complex reasoning tasks reward incremental progress toward promising solutions. We theoretically demonstrate that SSL preserves optimal solution ordering and enhances the gradient signal-to-noise ratio, thereby fostering more directed optimization. Extensive experiments across GUI perception, short/long-term planning, and complex reasoning tasks show consistent improvements over strong baselines on 12 benchmarks, achieving up to 2.5X sample efficiency gains and effective cross-task transferability. Our work establishes SSL as a general principle for training capable and robust agents.

</details>


### [16] [Mock Worlds, Real Skills: Building Small Agentic Language Models with Synthetic Tasks, Simulated Environments, and Rubric-Based Rewards](https://arxiv.org/abs/2601.22511)
*Yuan-Jay Lü,Chengyu Wang,Lei Shen,Jun Huang,Tong Xu*

Main category: cs.CL

TL;DR: SYNTHAGENT框架通过联合合成多样化工具使用训练数据和模拟完整环境，解决小模型在智能体能力上的瓶颈，显著提升小模型在数学、搜索和工具使用等任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 小语言模型难以匹配大型模型的智能体能力，现有开源智能体训练数据任务单一且容易解决，真实API缺乏多样性且在大规模强化学习过程中不稳定。

Method: 使用强教师模型创建新颖任务和工具生态系统，并将其重写为故意不完整的指令，迫使智能体主动查询用户获取缺失信息。通过LLM用户模拟器提供用户私有信息，模拟工具系统提供稳定工具响应，基于所需子目标、用户-智能体交互和禁止行为构建任务级奖励。

Result: 在14个具有挑战性的数学、搜索和工具使用数据集中，使用合成数据训练的模型取得了显著提升，小模型性能超过更大的基线模型。

Conclusion: SYNTHAGENT框架通过合成多样化训练数据和模拟完整环境，有效解决了小模型智能体训练的结构性瓶颈，为开发高效的小规模智能体模型提供了可行路径。

Abstract: Small LLMs often struggle to match the agentic capabilities of large, costly models. While reinforcement learning can help, progress has been limited by two structural bottlenecks: existing open-source agentic training data are narrow in task variety and easily solved; real-world APIs lack diversity and are unstable for large-scale reinforcement learning rollout processes. We address these challenges with SYNTHAGENT, a framework that jointly synthesizes diverse tool-use training data and simulates complete environments. Specifically, a strong teacher model creates novel tasks and tool ecosystems, then rewrites them into intentionally underspecified instructions. This compels agents to actively query users for missing details. When handling synthetic tasks, an LLM-based user simulator provides user-private information, while a mock tool system delivers stable tool responses. For rewards, task-level rubrics are constructed based on required subgoals, user-agent interactions, and forbidden behaviors. Across 14 challenging datasets in math, search, and tool use, models trained on our synthetic data achieve substantial gains, with small models outperforming larger baselines.

</details>


### [17] [One Ring to Rule Them All: Unifying Group-Based RL via Dynamic Power-Mean Geometry](https://arxiv.org/abs/2601.22521)
*Weisong Zhao,Tong Wang,Zichang Tan,Te Yang,Siran Peng,Haoyuan Zhang,Tianshuo Zhang,Haichao Shi,Meng Meng,Yang Yang,Xiangyu Zhu,Zhen Lei,Xiao-Yu Zhang,Xu Zhou*

Main category: cs.CL

TL;DR: PMPO提出了一种新的强化学习框架，通过幂平均几何参数p统一了GRPO和GMPO，并引入基于裁剪感知有效样本大小的自适应p选择机制，在多个数学推理基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于群体的强化学习方法（如GRPO和GMPO）存在根本性局限：它们依赖于固定的聚合几何结构，忽略了每个轨迹的演化和异质性。这限制了算法在不同场景下的适应能力。

Method: 提出Power-Mean Policy Optimization (PMPO)框架，通过幂平均几何指数p参数化聚合几何结构，将GRPO和GMPO统一为特例。引入Clip-aware Effective Sample Size (ESS)机制自适应确定p值：提出确定性规则将轨迹裁剪比例映射到目标ESS，然后求解特定的p值使轨迹诱导的ESS与目标ESS对齐。

Result: 在多个数学推理基准测试中，PMPO优于强基线方法。理论分析表明调整p可以调节梯度更新的集中度，有效根据优势贡献重新加权标记。

Conclusion: PMPO提供了一个统一框架，能够动态地在可靠的轨迹的激进算术平均和不稳定轨迹的保守几何平均之间过渡，从而克服了固定聚合几何结构的局限性。

Abstract: Group-based reinforcement learning has evolved from the arithmetic mean of GRPO to the geometric mean of GMPO. While GMPO improves stability by constraining a conservative objective, it shares a fundamental limitation with GRPO: reliance on a fixed aggregation geometry that ignores the evolving and heterogeneous nature of each trajectory. In this work, we unify these approaches under Power-Mean Policy Optimization (PMPO), a generalized framework that parameterizes the aggregation geometry via the power-mean geometry exponent p. Within this framework, GRPO and GMPO are recovered as special cases. Theoretically, we demonstrate that adjusting p modulates the concentration of gradient updates, effectively reweighting tokens based on their advantage contribution. To determine p adaptively, we introduce a Clip-aware Effective Sample Size (ESS) mechanism. Specifically, we propose a deterministic rule that maps a trajectory clipping fraction to a target ESS. Then, we solve for the specific p to align the trajectory induced ESS with this target one. This allows PMPO to dynamically transition between the aggressive arithmetic mean for reliable trajectories and the conservative geometric mean for unstable ones. Experiments on multiple mathematical reasoning benchmarks demonstrate that PMPO outperforms strong baselines.

</details>


### [18] [$ρ$-$\texttt{EOS}$: Training-free Bidirectional Variable-Length Control for Masked Diffusion LLMs](https://arxiv.org/abs/2601.22527)
*Jingyi Yang,Yuxian Jiang,Jing Shao*

Main category: cs.CL

TL;DR: 提出了一种名为ρ-EOS的训练免费、单阶段策略，使掩码扩散大语言模型能够实现双向可变长度生成，无需预先定义固定生成长度，提高了推理效率和token利用率。


<details>
  <summary>Details</summary>
Motivation: 当前掩码扩散大语言模型需要预先定义固定生成长度，这缺乏灵活性，并在输出质量和计算效率之间造成不可避免的权衡。需要一种更灵活的生成长度调整方法。

Method: 通过研究去噪动态，发现序列结束(EOS)token的隐式密度(ρ)可作为生成充分性的可靠信号。基于此提出ρ-EOS策略，在统一去噪过程中持续估计隐式EOS密度：密度过高时触发MASK token收缩，密度不足时触发扩展，实现双向长度调整。

Result: 在数学和代码基准测试中，ρ-EOS实现了可比性能，同时显著提高了推理效率和token利用率。

Conclusion: ρ-EOS策略解决了掩码扩散大语言模型的固定长度限制问题，通过利用隐式EOS密度作为生成充分性信号，实现了训练免费、单阶段的双向可变长度生成，为扩散语言模型提供了更灵活高效的生成方案。

Abstract: Beyond parallel generation and global context modeling, current masked diffusion large language models (dLLMs) suffer from a fundamental limitation: they require a predefined, fixed generation length, which lacks flexibility and forces an inevitable trade-off between output quality and computational efficiency. To address this, we study the denoising dynamics and find that the implicit density ($ρ$) of end-of-sequence ($\texttt{EOS}$) tokens serves as a reliable signal of generation sufficiency. In particular, the evolving implicit $\texttt{EOS}$ density during denoising reveals whether the current masked space is excessive or insufficient, thereby guiding the adjustment direction for generation length. Building on this insight, we propose $\textbf{$ρ$-$\texttt{EOS}$}$, a training-free, single-stage strategy that enables bidirectional variable-length generation for masked dLLMs. Unlike prior two-stage approaches--which require separate length adjustment and iterative mask insertion phases while supporting only unidirectional expansion--$\textbf{$ρ$-$\texttt{EOS}$}$ achieves bidirectional length adjustment within a unified denoising process by continuously estimating the implicit $\texttt{EOS}$ density: excessively high density triggers $\texttt{MASK}$ token contraction, while insufficient density induces expansion. Extensive experiments on mathematics and code benchmarks demonstrate that $\textbf{$ρ$-$\texttt{EOS}$}$ achieves comparable performance while substantially improving inference efficiency and token utilization.

</details>


### [19] [Towards the Holographic Characteristic of LLMs for Efficient Short-text Generation](https://arxiv.org/abs/2601.22546)
*Shun Qian,Bingquan Liu,Chengjie Sun,Zhen Xu,Baoxun Wang*

Main category: cs.CL

TL;DR: 论文提出语言模型的全息特性（Holographic Characteristic），即模型在生成初期倾向于捕获目标关键词，并基于此开发了HOLO插件来提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在上下文学习和思维链方面表现出色，但关于其强大生成能力的具体特质研究较少。本文旨在深入探究语言模型的生成特性。

Method: 研究发现语言模型具有"全息特性"——在生成初期捕获目标关键词。基于此，提出HOLO插件，在有限生成步骤内提取目标关键词，并通过并行词汇约束文本生成方法补全句子。

Result: 在不同架构和规模的短文本生成任务中，HOLO在自动评估和人工评估指标上均达到基线水平，同时验证了全息特性的有效性。

Conclusion: 语言模型确实存在全息特性，HOLO插件能有效利用这一特性提升推理效率，为语言模型生成研究提供了新视角。

Abstract: The recent advancements in Large Language Models (LLMs) have attracted interest in exploring their in-context learning abilities and chain-of-thought capabilities. However, there are few studies investigating the specific traits related to the powerful generation capacity of LLMs. This paper aims to delve into the generation characteristics exhibited by LLMs. Through our investigation, we have discovered that language models tend to capture target-side keywords at the beginning of the generation process. We name this phenomenon the Holographic Characteristic of language models. For the purpose of exploring this characteristic and further improving the inference efficiency of language models, we propose a plugin called HOLO, which leverages the Holographic Characteristic to extract target-side keywords from language models within a limited number of generation steps and complements the sentence with a parallel lexically constrained text generation method. To verify the effectiveness of HOLO, we conduct massive experiments on language models of varying architectures and scales in the short-text generation scenario. The results demonstrate that HOLO achieves comparable performance to the baselines in terms of both automatic and human-like evaluation metrics and highlight the potential of the Holographic Characteristic.

</details>


### [20] [Are LLM Evaluators Really Narcissists? Sanity Checking Self-Preference Evaluations](https://arxiv.org/abs/2601.22548)
*Dani Roytburg,Matthew Bozoukov,Matthew Nguyen,Mackenzie Puig-Hall,Narmeen Oozeer*

Main category: cs.CL

TL;DR: 该论文发现LLM作为评估者时存在方法论混淆，通过引入评估者质量基线来分离自我偏好偏差，显著减少了测量误差。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为评估者时倾向于偏好自己的输出，这影响了自动化后训练和评估流程的完整性。但现有研究难以区分哪些评估偏差源于"自恋"效应，哪些源于一般实验混淆，导致对自我偏好偏差的测量失真。

Method: 作者发现了一个核心方法论混淆：LLM评估者在处理自己无法正确回答的查询时可能给出自我偏好的判决。为分离自我偏好信号与困难问题上的噪声输出，引入了评估者质量基线，比较评估者错误投票给自己输出的概率与投票给其他模型错误输出的概率。

Result: 在37,448个查询的评估中，应用评估者质量基线后，只有51%的初始发现保持统计显著性，测量误差可减少89.6%。研究还分析了LLM评估者对"简单"与"困难"评估投票的熵特征。

Conclusion: 该纠正性基线通过消除噪声数据，为未来研究自我偏好问题提供了更可靠的方法。这项工作更广泛地促进了识别和隔离评估者偏差效应的研究进展。

Abstract: Recent research has shown that large language models (LLM) favor own outputs when acting as judges, undermining the integrity of automated post-training and evaluation workflows. However, it is difficult to disentangle which evaluation biases are explained by narcissism versus general experimental confounds, distorting measurements of self-preference bias. We discover a core methodological confound which could reduce measurement error by 89.6%. Specifically, LLM evaluators may deliver self-preferring verdicts when the judge responds to queries which they completed incorrectly themselves; this would be true regardless of whether one of their responses is their own. To decouple self-preference signals from noisy outputs on hard problems, we introduce an Evaluator Quality Baseline, which compares the probability that a judge incorrectly votes for itself against the probability that it votes for an incorrect response from another model. Evaluating this simple baseline on 37,448 queries, only 51% of initial findings retain statistical significance. Finally, we turn towards characterizing the entropy of "easy" versus "hard" evaluation votes from LLM judges. Our corrective baseline enables future research on self-preference by eliminating noisy data from potential solutions. More widely, this work contributes to the growing body of work on cataloging and isolating judge-bias effects.

</details>


### [21] [SpanNorm: Reconciling Training Stability and Performance in Deep Transformers](https://arxiv.org/abs/2601.22580)
*Chao Wang,Bei Li,Jiaqi Zhang,Xinyu Liu,Yuchun Fan,Linkun Lyu,Xin Chen,Jingang Wang,Tong Xiao,Peng Pei,Xunliang Cai*

Main category: cs.CL

TL;DR: SpanNorm是一种新的Transformer归一化技术，结合了PreNorm的训练稳定性和PostNorm的性能优势，通过跨越整个Transformer块的残差连接和PostNorm风格的计算来解决深度模型中的归一化困境。


<details>
  <summary>Details</summary>
Motivation: 当前Transformer架构中归一化层的放置存在根本性权衡：PreNorm架构确保训练稳定性但可能导致深度模型性能下降，而PostNorm架构提供强性能但存在严重的训练不稳定问题。需要一种方法来解决这个困境。

Method: 提出SpanNorm技术：1）建立跨越整个Transformer块的干净残差连接以稳定信号传播；2）采用PostNorm风格的计算来归一化聚合输出以增强模型性能；3）结合有原则的缩放策略来保持网络中的有界信号方差。

Result: 理论分析表明SpanNorm能防止PostNorm模型的梯度问题，并缓解PreNorm的表示崩溃。实证上，SpanNorm在密集模型和混合专家（MoE）场景中都持续优于标准归一化方案。

Conclusion: SpanNorm成功解决了Transformer架构中归一化层的设计困境，为更强大和稳定的Transformer架构铺平了道路，结合了PreNorm的训练稳定性和PostNorm的性能优势。

Abstract: The success of Large Language Models (LLMs) hinges on the stable training of deep Transformer architectures. A critical design choice is the placement of normalization layers, leading to a fundamental trade-off: the ``PreNorm'' architecture ensures training stability at the cost of potential performance degradation in deep models, while the ``PostNorm'' architecture offers strong performance but suffers from severe training instability. In this work, we propose SpanNorm, a novel technique designed to resolve this dilemma by integrating the strengths of both paradigms. Structurally, SpanNorm establishes a clean residual connection that spans the entire transformer block to stabilize signal propagation, while employing a PostNorm-style computation that normalizes the aggregated output to enhance model performance. We provide a theoretical analysis demonstrating that SpanNorm, combined with a principled scaling strategy, maintains bounded signal variance throughout the network, preventing the gradient issues that plague PostNorm models, and also alleviating the representation collapse of PreNorm. Empirically, SpanNorm consistently outperforms standard normalization schemes in both dense and Mixture-of-Experts (MoE) scenarios, paving the way for more powerful and stable Transformer architectures.

</details>


### [22] [Rethinking LLM-as-a-Judge: Representation-as-a-Judge with Small Language Models via Semantic Capacity Asymmetry](https://arxiv.org/abs/2601.22588)
*Zhuochun Li,Yong Zhang,Ming Li,Yuelyu Ji,Yiming Zeng,Ning Cheng,Yun Zhu,Yanmeng Wang,Shaojun Wang,Jing Xiao,Daqing He*

Main category: cs.CL

TL;DR: 本文提出"Representation-as-a-Judge"新范式，通过INSPECTOR框架利用小模型内部表征进行解码无关的评估，相比传统LLM评估更高效、可靠、可解释。


<details>
  <summary>Details</summary>
Motivation: 传统"LLM-as-a-Judge"范式存在成本高、不透明、对提示设计敏感等问题。研究发现小模型尽管生成能力弱，但其隐藏状态包含丰富的评估信号，这启发了利用小模型内部表征而非表面生成进行高效评估的研究。

Method: 提出语义能力不对称假设：评估所需语义能力远小于生成，可在中间表征中实现。基于此提出INSPECTOR框架，通过探针方法从小模型表征中预测细粒度评估分数，实现解码无关的评估策略。

Result: 在GSM8K、MATH、GPQA等推理基准测试中，INSPECTOR显著优于基于提示的小模型评估方法，接近完整LLM评估器的性能，同时提供更高效、可靠、可解释的评估方案。

Conclusion: 小模型的内部表征包含丰富的评估信号，评估不一定需要依赖大规模生成模型。INSPECTOR框架验证了Representation-as-a-Judge范式的有效性，为可扩展评估提供了更优方案。

Abstract: Large language models (LLMs) are widely used as reference-free evaluators via prompting, but this "LLM-as-a-Judge" paradigm is costly, opaque, and sensitive to prompt design. In this work, we investigate whether smaller models can serve as efficient evaluators by leveraging internal representations instead of surface generation. We uncover a consistent empirical pattern: small LMs, despite with weak generative ability, encode rich evaluative signals in their hidden states. This motivates us to propose the Semantic Capacity Asymmetry Hypothesis: evaluation requires significantly less semantic capacity than generation and can be grounded in intermediate representations, suggesting that evaluation does not necessarily need to rely on large-scale generative models but can instead leverage latent features from smaller ones. Our findings motivate a paradigm shift from LLM-as-a-Judge to Representation-as-a-Judge, a decoding-free evaluation strategy that probes internal model structure rather than relying on prompted output. We instantiate this paradigm through INSPECTOR, a probing-based framework that predicts aspect-level evaluation scores from small model representations. Experiments on reasoning benchmarks (GSM8K, MATH, GPQA) show that INSPECTOR substantially outperforms prompting-based small LMs and closely approximates full LLM judges, while offering a more efficient, reliable, and interpretable alternative for scalable evaluation.

</details>


### [23] [Language Model Circuits Are Sparse in the Neuron Basis](https://arxiv.org/abs/2601.22594)
*Aryaman Arora,Zhengxuan Wu,Jacob Steinhardt,Sarah Schwettmann*

Main category: cs.CL

TL;DR: 研究表明MLP神经元与稀疏自动编码器(SAEs)一样是稀疏的特征基，开发了基于MLP神经元的端到端电路追踪流程，无需额外训练成本即可提升语言模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 语言模型解释性研究通常使用稀疏自动编码器等技术来分解神经元基为更可解释的计算单元，因为高层概念不一定与单个神经元对齐。然而，并非所有基于神经元的表示都难以解释。

Method: 首次经验证明MLP神经元与SAEs一样是稀疏的特征基，并基于此开发了端到端的MLP神经元电路追踪流程，使用基于梯度的归因方法在各种任务上定位因果电路。

Result: 在标准主谓一致基准测试中，约100个MLP神经元组成的电路就足以控制模型行为；在多跳城市→州→首都任务中，发现小规模神经元集合编码特定的潜在推理步骤（如"将城市映射到其州"），并能通过操纵这些神经元改变模型输出。

Conclusion: 这项工作在不增加训练成本的情况下推进了语言模型的自动化可解释性，证明MLP神经元本身就是有效的稀疏特征基，可用于电路追踪。

Abstract: The high-level concepts that a neural network uses to perform computation need not be aligned to individual neurons (Smolensky, 1986). Language model interpretability research has thus turned to techniques such as \textit{sparse autoencoders} (SAEs) to decompose the neuron basis into more interpretable units of model computation, for tasks such as \textit{circuit tracing}. However, not all neuron-based representations are uninterpretable. For the first time, we empirically show that \textbf{MLP neurons are as sparse a feature basis as SAEs}. We use this finding to develop an end-to-end pipeline for circuit tracing on the MLP neuron basis, which locates causal circuitry on a variety of tasks using gradient-based attribution. On a standard subject-verb agreement benchmark (Marks et al., 2025), a circuit of $\approx 10^2$ MLP neurons is enough to control model behaviour. On the multi-hop city $\to$ state $\to$ capital task from Lindsey et al., 2025, we find a circuit in which small sets of neurons encode specific latent reasoning steps (e.g.~`map city to its state'), and can be steered to change the model's output. This work thus advances automated interpretability of language models without additional training costs.

</details>


### [24] [Layer-wise Swapping for Generalizable Multilingual Safety](https://arxiv.org/abs/2601.22620)
*Hyunseo Shin,Wonseok Hwang*

Main category: cs.CL

TL;DR: 本文提出一种安全感知层交换方法，将英语安全专家的安全对齐能力迁移到低资源语言专家模型中，无需额外训练即可提升多语言安全性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型快速发展，但低资源语言的安全风险仍是关键挑战。现有安全数据集主要面向英语，限制了多语言安全对齐的进展。低资源专家模型在其指令数据集上微调后，往往比高资源模型表现出更高的不安全率。

Method: 提出安全感知层交换方法，将英语安全专家的安全对齐能力迁移到低资源语言专家模型中。方法基于模块专业化程度自适应选择或混合模块，无需额外训练。通过层交换实现安全知识迁移，同时保持目标语言专家在通用语言理解任务上的性能。

Result: 实验结果显示，该方法在MMMLU、BELEBELE和MGSM等通用基准测试中与语言专家表现相当，同时在MultiJail安全基准测试中产生更对齐、更少有害的响应，有效提升了低资源语言的安全性。

Conclusion: 安全感知层交换方法能够有效将英语安全专家的安全对齐能力迁移到低资源语言专家模型中，在保持通用语言理解性能的同时显著提升多语言安全性，为解决低资源语言安全挑战提供了有效解决方案。

Abstract: Despite the rapid advancements of Large Language Models (LLMs), safety risks remain a critical challenge for low-resource languages. Existing safety datasets are predominantly English centric, limiting progress in multilingual safety alignment. As a result, low resource expert models, finetuned on their respective instruction datasets, tend to exhibit higher unsafety rates compared to their high resource counterparts. In this work, we propose a safety aware layer swapping method that transfers safety alignment from an English safety expert to low resource language experts without additional training. To further enhance transfer ability, our method adaptively selects or blends modules based on their degree of specialization. Our approach preserves performance on general language understanding tasks while enhancing safety in the target languages. Experimental results show that the proposed method achieves comparable performance to the language expert on general benchmarks such as MMMLU, BELEBELE, and MGSM, while producing more aligned and less harmful responses on the MultiJail safety benchmark.

</details>


### [25] [Time-Annealed Perturbation Sampling: Diverse Generation for Diffusion Language Models](https://arxiv.org/abs/2601.22629)
*Jingxuan Wu,Zhenglin Wan,Xingrui Yu,Yuzhe Yang,Yiqiao Huang,Ivor Tsang,Yang You*

Main category: cs.CL

TL;DR: 提出TAPS方法，利用扩散语言模型的时间分工特性，在早期步骤引入扰动促进语义分支，后期减少扰动保持流畅度，提升生成多样性而不损失质量。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型引入了显式时间维度，但如何利用这一结构来控制生成多样性、探索多个有效语义或推理路径尚未充分研究。需要探索如何利用扩散过程的时间特性来平衡生成多样性与质量。

Method: 提出Time-Annealed Perturbation Sampling (TAPS)，这是一种无需训练的推理策略。基于扩散语言模型存在时间分工的洞察：早期去噪步骤决定全局语义结构，后期步骤关注局部词汇精炼。TAPS在扩散过程早期引入扰动促进语义分支，随着时间推移逐步减少扰动以保持流畅度和指令遵循。

Result: TAPS与LLaDA和TraDo等非自回归和半自回归扩散骨干网络兼容，在创意写作和推理基准测试中持续提升输出多样性，同时不损害生成质量。

Conclusion: 通过利用扩散语言模型的时间分工特性，TAPS提供了一种有效方法来控制生成多样性，在保持质量的同时探索多个语义和推理路径，为扩散语言模型的应用开辟了新方向。

Abstract: Diffusion language models (Diffusion-LMs) introduce an explicit temporal dimension into text generation, yet how this structure can be leveraged to control generation diversity for exploring multiple valid semantic or reasoning paths remains underexplored. In this paper, we show that Diffusion-LMs, like diffusion models in image generation, exhibit a temporal division of labor: early denoising steps largely determine the global semantic structure, while later steps focus on local lexical refinement. Building on this insight, we propose Time-Annealed Perturbation Sampling (TAPS), a training-free inference strategy that encourages semantic branching early in the diffusion process while progressively reducing perturbations to preserve fluency and instruction adherence. TAPS is compatible with both non-autoregressive and semi-autoregressive Diffusion backbones, demonstrated on LLaDA and TraDo in our paper, and consistently improves output diversity across creative writing and reasoning benchmarks without compromising generation quality.

</details>


### [26] [DART-ing Through the Drift: Dynamic Tracing of Knowledge Neurons for Adaptive Inference-Time Pruning](https://arxiv.org/abs/2601.22632)
*Abhishek Tyagi,Yunuo Cen,Shrey Dhorajiya,Bharadwaj Veeravalli,Xuanyao Fong*

Main category: cs.CL

TL;DR: DART是一种轻量级、无需训练的动态剪枝方法，通过监控注意力分数分布变化来实时调整神经元掩码，有效减少大语言模型FFN层的参数冗余，在保持模型能力的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型剪枝方法存在两个主要问题：1）依赖数据集特定校准，导致数据依赖性强且计算开销大；2）主要是静态剪枝，无法适应自回归生成过程中随着上下文变化而动态演化的知识神经元子集。

Method: 提出DART（动态注意力引导运行时追踪）方法，通过监控注意力分数分布的变化来推断上下文变化，动态更新神经元级掩码以保留重要参数。该方法无需训练，轻量级，能够实时进行基于上下文的剪枝。

Result: 在十个基准测试中，DART优于先前的动态基线方法，在LLAMA-3.1-8B模型上达到70% FFN稀疏度时，准确率提升高达14.5%。在摘要任务上，相比静态掩码剪枝，ROUGE-L分数提升高达3倍，性能接近原始密集模型。DART内存开销小于10MB（相对于16GB的LLAMA-3.1-8B），FLOPs开销仅为0.1%。

Conclusion: DART框架能够有效适应多样化的语义上下文，在保持通用和领域特定任务能力的同时，显著降低计算和内存开销，为解决大语言模型参数冗余问题提供了一种高效动态剪枝方案。

Abstract: Large Language Models (LLMs) exhibit substantial parameter redundancy, particularly in Feed-Forward Networks (FFNs). Existing pruning methods suffer from two primary limitations. First, reliance on dataset-specific calibration introduces significant data dependency and computational overhead. Second, being predominantly static, they fail to account for the evolving subset of knowledge neurons in LLMs during autoregressive generation as the context evolves. To address this, we introduce DART, i.e., Dynamic Attention-Guided Runtime Tracing), a lightweight, training-free method that performs on-the-fly context-based pruning. DART monitors shifts in attention score distributions to infer context changes, dynamically updating neuron-level masks to retain salient parameters. Across ten benchmarks, DART outperforms prior dynamic baseline, achieving accuracy gains of up to 14.5% on LLAMA-3.1-8B at 70% FFN sparsity. Furthermore, DART achieves up to 3x better ROUGE-L scores with respect to static-masked pruning on summarization tasks, with its performance comparable to the original dense models. We conclusively demonstrate that the proposed framework effectively adapts to diverse semantic contexts, preserves model capabilities across both general and domain-specific tasks while running at less than 10MBs of memory for LLAMA-3.1-8B(16GBs) with 0.1% FLOPs overhead. The code is available at https://github.com/seeder-research/DART.

</details>


### [27] [NAG: A Unified Native Architecture for Encoder-free Text-Graph Modeling in Language Models](https://arxiv.org/abs/2601.22657)
*Haisong Gong,Zhibo Liu,Qiang Liu,Shu Wu,Liang Wang*

Main category: cs.CL

TL;DR: NAG提出了一种统一框架，将图处理内化到语言模型的原生流形中，无需外部GNN编码器，通过改造自注意力机制和位置ID来实现图结构理解。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常采用分离架构：外部GNN编码图结构，语言模型处理文本语义。这种方法对于文本图来说是次优的，因为它创建了概念上脱节的交互范式，需要在抽象图标记和具体文本元素之间进行复杂的隐式对齐。

Method: 提出了NAG框架，通过重新利用自注意力机制来强制拓扑依赖关系，并重新校准位置ID以确保结构等价性。提出了两种高效实现：NAG-Zero（绝对保留基础模型的语言能力）和NAG-LoRA（增强结构适应）。

Result: 在多种图任务上的实验验证了NAG能够实现稳健的图理解，无需外部编码器的开销，为文本图建模提供了更简单、更一致的范式。

Conclusion: NAG挑战了外部编码器的必要性，提供了一种统一框架，使语言模型能够利用其内在的语言能力同时理解节点和边内容以及结构拓扑，为文本图建模提供了更优的解决方案。

Abstract: Prevailing methods for integrating graphs into Language Models (LMs) typically rely on a segregated architecture: external Graph Neural Networks (GNNs) encode structural topology, while LMs process textual semantics. We argue this approach is suboptimal for text-graphs: it creates a conceptually disjointed interaction paradigm. By segregating structural encoding from semantic processing, these systems must perform a complex implicit alignment between abstract graph tokens and concrete textual elements. Challenging the necessity of external encoders, we propose NAG (Native Architecture for Graphs), a unified framework that internalizes graph processing within the LM's native manifold. Instead of bridging disparate embedding spaces, NAG repurposes the self-attention mechanism to enforce topological dependencies and recalibrates positional IDs to ensure structural equivalence. This allows the model to harness its intrinsic linguistic capability to simultaneously comprehend node and edge content alongside structural topology. We introduce two efficient implementations: NAG-Zero for absolute preservation of the base model's linguistic capabilities, and NAG-LoRA for enhanced structural adaptation. Experiments across diverse graph tasks validate that NAG achieves robust graph comprehension without the overhead of external encoders, offering a simpler, more coherent paradigm for text-graph modeling.

</details>


### [28] [TSLM: Tree-Structured Language Modeling for Divergent Thinking](https://arxiv.org/abs/2601.22688)
*Doyoung Kim,Jaehyeok Doo,Minjoon Seo*

Main category: cs.CL

TL;DR: TSLM通过树结构语言建模，让语言模型能在一个生成过程中探索多个搜索路径，提高推理效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型顺序生成推理，无法解耦不相关的探索路径，导致搜索效率低下。需要一种能同时探索多个路径、避免重复计算共享前缀的方法。

Method: 提出树结构语言建模（TSLM），使用特殊标记编码分支结构，让模型能在单次生成过程中生成和选择性扩展多个搜索路径。通过在包含成功和失败尝试的完整搜索树上训练，模型内化系统化探索能力。

Result: TSLM实现了鲁棒性能和优越的推理效率，避免了外部搜索方法需要的多次独立前向传递。在推理时扩展方面展现了新的范式。

Conclusion: 在完整树结构轨迹上的监督学习为开发语言模型的系统化探索能力提供了高效替代方案，展示了推理时扩展的新范式。

Abstract: Language models generate reasoning sequentially, preventing them from decoupling irrelevant exploration paths during search. We introduce Tree-Structured Language Modeling (TSLM), which uses special tokens to encode branching structure, enabling models to generate and selectively expand multiple search paths within a single generation process. By training on complete search trees including both successful and failed attempts, TSLM learns to internalize systematic exploration without redundant recomputation of shared prefixes. TSLM achieves robust performance and superior inference efficiency by avoiding the multiple independent forward passes required by external search methods. These results suggest a new paradigm of inference-time scaling for robust reasoning, demonstrating that supervised learning on complete tree-structured traces provides an efficient alternative for developing systematic exploration capabilities in language models.

</details>


### [29] [FNF: Functional Network Fingerprint for Large Language Models](https://arxiv.org/abs/2601.22692)
*Yiheng Liu,Junhao Ning,Sichen Xia,Haiyang Sun,Yang Yang,Hanyang Chi,Xiaohui Gao,Ning Qiang,Bao Ge,Junwei Han,Xintao Hu*

Main category: cs.CL

TL;DR: 提出一种基于功能网络活动一致性的训练免费、样本高效方法，用于检测可疑LLM是否源自受害模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型开发成本高且具有重要商业价值，防止开源LLM被未经授权使用和保护开发者知识产权成为关键挑战。

Method: 提出功能网络指纹（FNF）方法，基于共享起源模型在功能网络中的神经元活动模式一致性进行检测，无需训练，仅需少量样本。

Result: FNF方法能够有效区分共享起源模型与独立训练模型，对微调、剪枝、参数置换等常见修改保持鲁棒，且适用于不同架构和维度的模型比较。

Conclusion: FNF为模型所有者和第三方提供了一种简单、非侵入性且有效的LLM知识产权保护工具。

Abstract: The development of large language models (LLMs) is costly and has significant commercial value. Consequently, preventing unauthorized appropriation of open-source LLMs and protecting developers' intellectual property rights have become critical challenges. In this work, we propose the Functional Network Fingerprint (FNF), a training-free, sample-efficient method for detecting whether a suspect LLM is derived from a victim model, based on the consistency between their functional network activity. We demonstrate that models that share a common origin, even with differences in scale or architecture, exhibit highly consistent patterns of neuronal activity within their functional networks across diverse input samples. In contrast, models trained independently on distinct data or with different objectives fail to preserve such activity alignment. Unlike conventional approaches, our method requires only a few samples for verification, preserves model utility, and remains robust to common model modifications (such as fine-tuning, pruning, and parameter permutation), as well as to comparisons across diverse architectures and dimensionalities. FNF thus provides model owners and third parties with a simple, non-invasive, and effective tool for protecting LLM intellectual property. The code is available at https://github.com/WhatAboutMyStar/LLM_ACTIVATION.

</details>


### [30] [Models Know Models Best: Evaluation via Model-Preferred Formats](https://arxiv.org/abs/2601.22699)
*Joonhak Lee,Sungmok Jung,Jongyeon Park,Jaejin Lee*

Main category: cs.CL

TL;DR: 本文研究发现LLMs在不同选择题格式（符号式vs完形填空式）上表现存在显著差异，提出了一种基于模型偏好信号的动态格式对齐策略，显著提升了零样本准确率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在符号式和完形填空式选择题评估格式上表现不一致，这种差异会影响对模型真实能力的评估。现有的人工设计启发式方法往往降低性能，需要更智能的方法来解决格式不一致问题。

Method: 提出动态格式对齐策略：使用轻量级分类器，基于模型潜在偏好信号训练，为每个问题实例确定最优评估格式。该方法利用模型自身生成的信号而非人工启发式规则。

Result: 该方法在推理和知识基准测试中实现了显著且一致的零样本准确率提升，更好地揭示了模型的潜在能力。改进效果在不同解码器LLMs中保持一致。

Conclusion: LLMs在不同选择题格式上的性能差异具有系统性，可通过模型自身的偏好信号进行动态格式对齐来解决。该方法优于人工启发式方法，能更准确地评估模型能力。

Abstract: Performance of Large Language Models (LLMs) on multiple-choice tasks differs markedly between symbol-based and cloze-style evaluation formats. The observed discrepancies are systematically attributable to task characteristics: natural language continuation benefits from likelihood scoring, whereas explicit comparison is better suited to symbol-based selection. These trends are consistent across various decoder-based LLMs, indicating model-agnostic effects. To address these inconsistencies, a dynamic format-alignment strategy is introduced that employs a lightweight classifier trained on latent model-preference signals. In contrast to human-designed heuristics, which often degrade performance, this approach uses model-generated signals to determine the optimal format for each problem instance. The proposed method achieves substantial and consistent improvements in zero-shot accuracy across reasoning and knowledge benchmarks, better revealing the models' latent capabilities.

</details>


### [31] [MM-THEBench: Do Reasoning MLLMs Think Reasonably?](https://arxiv.org/abs/2601.22735)
*Zhidian Huang,Zijun Yao,Ji Qi,Shangqing Tu,Junxian Ma,Jinxin Liu,Weichuan Liu,Xiaoyin Che,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: MM-THEBench是一个用于评估推理型多模态大语言模型中间思维链幻觉的基准测试，包含细粒度分类、多样化数据和自动化评估框架。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型从无思维模型发展到具备推理能力的后训练模型，但这些模型的"思考"过程是否能减少多模态感知和推理中的幻觉仍不清楚。现有基准主要关注推理型MLLM出现之前的模型，忽略了内部思维过程，无法衡量思考过程中产生的幻觉。

Method: 提出了MM-THEBench基准，具有以下特点：1）基于认知维度的细粒度分类法；2）包含已验证推理标注的多样化数据；3）多层次自动化评估框架。

Result: 通过对主流推理型MLLM的广泛实验，揭示了思维如何影响各种多模态任务中的幻觉和推理能力，提供了关于思考过程对模型性能影响的深入见解。

Conclusion: MM-THEBench填补了现有基准的空白，能够系统评估推理型多模态大语言模型在思考过程中产生的幻觉，为理解和改进这些模型的推理能力提供了重要工具。

Abstract: Recent advances in multimodal large language models (MLLMs) mark a shift from non-thinking models to post-trained reasoning models capable of solving complex problems through thinking. However, whether such thinking mitigates hallucinations in multimodal perception and reasoning remains unclear. Self-reflective reasoning enhances robustness but introduces additional hallucinations, and subtle perceptual errors still result in incorrect or coincidentally correct answers. Existing benchmarks primarily focus on models before the emergence of reasoning MLLMs, neglecting the internal thinking process and failing to measure the hallucinations that occur during thinking. To address these challenges, we introduce MM-THEBench, a comprehensive benchmark for assessing hallucinations of intermediate CoTs in reasoning MLLMs. MM-THEBench features a fine-grained taxonomy grounded in cognitive dimensions, diverse data with verified reasoning annotations, and a multi-level automated evaluation framework. Extensive experiments on mainstream reasoning MLLMs reveal insights into how thinking affects hallucination and reasoning capability in various multimodal tasks.

</details>


### [32] [AR-BENCH: Benchmarking Legal Reasoning with Judgment Error Detection, Classification and Correction](https://arxiv.org/abs/2601.22742)
*Yifei Li,Richong Zhang,Wanyu Tu,Zhijie Nie,Haokun Luo,Chuantao Yin,Pengchong Li*

Main category: cs.CL

TL;DR: 提出上诉审查任务和AR-BENCH数据集，评估大语言模型在法律判决错误检测中的能力


<details>
  <summary>Details</summary>
Motivation: 法律判决可能存在错误，现有上诉审查机制面临案件激增的效率压力。当前法律AI研究主要关注判决预测和文档生成，但判决审查在目标和范式上根本不同，需要检测、分类和纠正已发布判决中的错误。

Method: 提出APPELLATE REVIEW新任务，构建AR-BENCH数据集（包含8,700份精细标注的判决和34,617个补充语料），评估14个大语言模型在该任务上的表现。

Result: 评估显示现有模型在识别法律适用错误方面存在关键局限性，为未来改进提供了实证证据。

Conclusion: 该研究填补了法律AI中判决审查任务的空白，揭示了当前模型在法律错误检测方面的不足，为提升法律实践中的诊断推理和可靠性提供了方向。

Abstract: Legal judgments may contain errors due to the complexity of case circumstances and the abstract nature of legal concepts, while existing appellate review mechanisms face efficiency pressures from a surge in case volumes. Although current legal AI research focuses on tasks like judgment prediction and legal document generation, the task of judgment review differs fundamentally in its objectives and paradigm: it centers on detecting, classifying, and correcting errors after a judgment is issued, constituting anomaly detection rather than prediction or generation. To address this research gap, we introduce a novel task APPELLATE REVIEW, aiming to assess models' diagnostic reasoning and reliability in legal practice. We also construct a novel dataset benchmark AR-BENCH, which comprises 8,700 finely annotated decisions and 34,617 supplementary corpora. By evaluating 14 large language models, we reveal critical limitations in existing models' ability to identify legal application errors, providing empirical evidence for future improvements.

</details>


### [33] [RASST: Fast Cross-modal Retrieval-Augmented Simultaneous Speech Translation](https://arxiv.org/abs/2601.22777)
*Jiaxuan Luo,Siqi Ouyang,Lei Li*

Main category: cs.CL

TL;DR: RASST提出了一种检索增强的同步语音翻译方法，通过跨模态检索和滑动窗口检索为语音大语言模型提供术语提示，显著提升了术语翻译准确性和整体翻译质量。


<details>
  <summary>Details</summary>
Motivation: 当前语音大语言模型在同步语音翻译中仍难以准确翻译罕见和领域特定术语，而检索增强在机器翻译中已被证明有效，但将其应用于同步语音翻译面临挑战：需要在部分、持续到达的输入下进行快速准确的跨模态检索，且模型需要决定是否以及何时应用检索到的术语。

Method: 提出RASST框架：1）训练轻量级语音-文本检索器；2）采用高效的滑动窗口检索；3）为语音大语言模型提供分块术语提示；4）合成训练数据教导语音大语言模型精确利用检索到的术语。

Result: 在ACL 60/60 dev set的三个语言方向上的实验表明，RASST将术语翻译准确率提升高达16%，整体翻译质量提升高达3个BLEU分数，消融实验确认了各组件的作用。

Conclusion: RASST成功将检索增强技术整合到同步语音翻译流程中，有效解决了术语翻译难题，为语音大语言模型的同步翻译性能提供了实质性改进。

Abstract: Simultaneous speech translation (SST) produces target text incrementally from partial speech input. Recent speech large language models (Speech LLMs) have substantially improved SST quality, yet they still struggle to correctly translate rare and domain-specific terminology. While retrieval augmentation has been effective for terminology translation in machine translation, bringing retrieval to SST is non-trivial: it requires fast and accurate cross-modal (speech-to-text) retrieval under partial, continually arriving input, and the model must decide whether and when to apply retrieved terms during incremental generation. We propose Retrieval-Augmented Simultaneous Speech Translation (RASST), which tightly integrates cross-modal retrieval into the SST pipeline. RASST trains a lightweight speech-text retriever and performs efficient sliding-window retrieval, providing chunkwise terminology hints to the Speech LLM. We further synthesize training data that teaches the Speech LLM to leverage retrieved terms precisely. Experiments on three language directions of the ACL 60/60 dev set show that RASST improves terminology translation accuracy by up to 16% and increases overall translation quality by up to 3 BLEU points, with ablations confirming the contribution of each component.

</details>


### [34] [Sparse or Dense? A Mechanistic Estimation of Computation Density in Transformer-based LLMs](https://arxiv.org/abs/2601.22795)
*Corentin Kervadec,Iuliia Lysova,Marco Baroni,Gemma Boleda*

Main category: cs.CL

TL;DR: 该研究提出了一种量化LLM计算密度的方法，发现LLM处理通常是密集计算而非稀疏的，计算密度是动态变化的，且与输入内容相关。


<details>
  <summary>Details</summary>
Motivation: 针对LLM效率优化研究中发现可以大量剪枝参数而不显著影响性能的现象，作者质疑计算是否均匀分布在参数中，旨在系统量化LLM的计算密度。

Method: 设计了一种基于机制可解释性的密度估计器，通过实验测试该估计器来量化LLM的计算密度。

Result: 发现：1）LLM处理通常涉及密集计算而非稀疏计算；2）计算密度是动态的，模型根据输入在稀疏和密集处理模式间切换；3）不同LLM中相同输入的计算密度显著相关；4）预测罕见token需要更高密度，增加上下文长度通常降低密度。

Conclusion: 计算密度估计器有助于更好理解LLM的工作机制，挑战了它们的符号解释观点，表明计算是密集且动态的。

Abstract: Transformer-based large language models (LLMs) are comprised of billions of parameters arranged in deep and wide computational graphs. Several studies on LLM efficiency optimization argue that it is possible to prune a significant portion of the parameters, while only marginally impacting performance. This suggests that the computation is not uniformly distributed across the parameters. We introduce here a technique to systematically quantify computation density in LLMs. In particular, we design a density estimator drawing on mechanistic interpretability. We experimentally test our estimator and find that: (1) contrary to what has been often assumed, LLM processing generally involves dense computation; (2) computation density is dynamic, in the sense that models shift between sparse and dense processing regimes depending on the input; (3) per-input density is significantly correlated across LLMs, suggesting that the same inputs trigger either low or high density. Investigating the factors influencing density, we observe that predicting rarer tokens requires higher density, and increasing context length often decreases the density. We believe that our computation density estimator will contribute to a better understanding of the processing at work in LLMs, challenging their symbolic interpretation.

</details>


### [35] [When Meanings Meet: Investigating the Emergence and Quality of Shared Concept Spaces during Multilingual Language Model Training](https://arxiv.org/abs/2601.22851)
*Felicia Körner,Max Müller-Eberstein,Anna Korhonen,Barbara Plank*

Main category: cs.CL

TL;DR: 研究通过因果可解释性方法探究多语言大语言模型训练过程中语言无关概念空间的形成，发现共享概念空间早期出现但语言对齐存在差异，并揭示翻译质量提升可能源于行为转变而非翻译能力改进。


<details>
  <summary>Details</summary>
Motivation: 虽然已知LLMs在多语言输入处理中使用共享概念空间，但先前研究缺乏因果方法、深入错误分析或仅关注最终模型，不清楚这些空间在训练过程中如何形成。需要探究多语言预训练中语言无关概念空间的发展机制。

Method: 使用激活修补的因果可解释性方法研究EuroLLM预训练过程。首先隔离跨语言概念表示，然后将其注入翻译提示中，研究独立于语言的翻译一致性改变能力。

Result: 共享概念空间早期出现并持续精炼，但与这些空间的对齐是语言依赖的。细粒度人工分析显示，翻译质量的部分提升反映的是行为转变（如多义词义项选择、翻译而非复制跨语言同形词），而非翻译能力的真正改进。

Conclusion: 研究为跨语言对齐的训练动态提供了新见解，揭示了因果可解释性方法在多语言语境下提供有意义洞察的条件。强调了区分翻译能力改进与行为转变的重要性。

Abstract: Training Large Language Models (LLMs) with high multilingual coverage is becoming increasingly important -- especially when monolingual resources are scarce. Recent studies have found that LLMs process multilingual inputs in shared concept spaces, thought to support generalization and cross-lingual transfer. However, these prior studies often do not use causal methods, lack deeper error analysis or focus on the final model only, leaving open how these spaces emerge during training. We investigate the development of language-agnostic concept spaces during pretraining of EuroLLM through the causal interpretability method of activation patching. We isolate cross-lingual concept representations, then inject them into a translation prompt to investigate how consistently translations can be altered, independently of the language. We find that shared concept spaces emerge early} and continue to refine, but that alignment with them is language-dependent}. Furthermore, in contrast to prior work, our fine-grained manual analysis reveals that some apparent gains in translation quality reflect shifts in behavior -- like selecting senses for polysemous words or translating instead of copying cross-lingual homographs -- rather than improved translation ability. Our findings offer new insight into the training dynamics of cross-lingual alignment and the conditions under which causal interpretability methods offer meaningful insights in multilingual contexts.

</details>


### [36] [From Labels to Facets: Building a Taxonomically Enriched Turkish Learner Corpus](https://arxiv.org/abs/2601.22875)
*Elif Sayar,Tolgahan Türker,Anna Golynskaia Knezhevich,Bihter Dereli,Ayşe Demirhas,Lionel Nicolas,Gülşen Eryiğit*

Main category: cs.CL

TL;DR: 该论文提出了一种基于分面分类法的半自动化学习者语料库标注方法，通过标注扩展工具自动丰富现有平面标注，创建了首个土耳其语学习者语料库。


<details>
  <summary>Details</summary>
Motivation: 现有学习者语料库大多采用整体的平面标签体系，即使标签丰富也无法明确分离多个语言维度，这使得深入的语言学标注变得困难，也妨碍了对学习者产生特定错误原因和方式的细粒度分析。

Method: 提出半自动化的学习者语料库标注方法，基于最近提出的分面分类法，通过新颖的标注扩展框架实现。该分类法提供理论基础的、多维度的分类，捕捉每个错误实例的语言学特性。标注扩展工具自动扩展现有平面标注，推断额外的语言学和元数据信息作为分类法中的分面。

Result: 系统评估显示该方法表现优异，分面级别准确率达到95.86%。创建了首个协作标注和分类法丰富的土耳其语学习者语料库，包含精炼的标签集和标注扩展工具。丰富的语料库增强了查询能力，支持跨学习者语料库的详细探索性分析。

Conclusion: 该研究为现有错误标注学习者语料库的后续丰富工作铺平了道路，通过分面分类法实现了标准化、细粒度和可解释的语料库标注，使研究者能够通过复杂的语言学和教学维度调查错误模式。

Abstract: In terms of annotation structure, most learner corpora rely on holistic flat label inventories which, even when extensive, do not explicitly separate multiple linguistic dimensions. This makes linguistically deep annotation difficult and complicates fine-grained analyses aimed at understanding why and how learners produce specific errors. To address these limitations, this paper presents a semi-automated annotation methodology for learner corpora, built upon a recently proposed faceted taxonomy, and implemented through a novel annotation extension framework. The taxonomy provides a theoretically grounded, multi-dimensional categorization that captures the linguistic properties underlying each error instance, thereby enabling standardized, fine-grained, and interpretable enrichment beyond flat annotations. The annotation extension tool, implemented based on the proposed extension framework for Turkish, automatically extends existing flat annotations by inferring additional linguistic and metadata information as facets within the taxonomy to provide richer learner-specific context. It was systematically evaluated and yielded promising performance results, achieving a facet-level accuracy of 95.86%. The resulting taxonomically enriched corpus offers enhanced querying capabilities and supports detailed exploratory analyses across learner corpora, enabling researchers to investigate error patterns through complex linguistic and pedagogical dimensions. This work introduces the first collaboratively annotated and taxonomically enriched Turkish Learner Corpus, a manual annotation guideline with a refined tagset, and an annotation extender. As the first corpus designed in accordance with the recently introduced taxonomy, we expect our study to pave the way for subsequent enrichment efforts of existing error-annotated learner corpora.

</details>


### [37] [Leveraging LLMs For Turkish Skill Extraction](https://arxiv.org/abs/2601.22885)
*Ezgi Arslan İltüzer,Özgür Anıl Özlü,Vahid Farajijobehdar,Gülşen Eryiğit*

Main category: cs.CL

TL;DR: 该论文首次为土耳其语创建了技能抽取数据集，并评估了LLM在低资源环境下进行技能抽取的效果，发现Claude Sonnet 3.7配合动态少样本提示在端到端性能上优于监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 土耳其作为全球劳动力市场重要参与者，其语言土耳其语缺乏技能分类体系和专用数据集，导致土耳其语技能抽取研究不足。研究旨在解决三个问题：1)如何在低资源环境下有效进行土耳其语技能抽取；2)最佳模型是什么；3)不同LLM和提示策略的影响。

Method: 创建了首个土耳其语技能抽取数据集（4,819个标注技能跨度，来自327个不同职业领域的招聘广告）。采用LLM端到端管道，比较了不同LLM（包括Claude Sonnet 3.7）和提示策略（动态vs静态少样本、不同上下文信息、因果推理激励），使用基于嵌入的检索和LLM重排进行技能链接。

Result: LLM在端到端管道中优于监督序列标注方法，能更有效地将抽取的技能跨度与ESCO分类标准对齐。最佳配置（Claude Sonnet 3.7 + 动态少样本提示 + 嵌入检索 + LLM重排）实现了0.56的端到端性能，使土耳其语技能抽取达到与其他语言相当的水平。

Conclusion: LLM能够提升低资源环境下的技能抽取性能，该研究为土耳其语及其他资源不足语言的技能抽取研究奠定了基础，有望加速相关研究发展。

Abstract: Skill extraction is a critical component of modern recruitment systems, enabling efficient job matching, personalized recommendations, and labor market analysis. Despite Türkiye's significant role in the global workforce, Turkish, a morphologically complex language, lacks both a skill taxonomy and a dedicated skill extraction dataset, resulting in underexplored research in skill extraction for Turkish. This article seeks the answers to three research questions: 1) How can skill extraction be effectively performed for this language, in light of its low resource nature? 2)~What is the most promising model? 3) What is the impact of different Large Language Models (LLMs) and prompting strategies on skill extraction (i.e., dynamic vs. static few-shot samples, varying context information, and encouraging causal reasoning)? The article introduces the first Turkish skill extraction dataset and performance evaluations of automated skill extraction using LLMs. The manually annotated dataset contains 4,819 labeled skill spans from 327 job postings across different occupation areas. The use of LLM outperforms supervised sequence labeling when used in an end-to-end pipeline, aligning extracted spans with standardized skills in the ESCO taxonomy more effectively. The best-performing configuration, utilizing Claude Sonnet 3.7 with dynamic few-shot prompting for skill identification, embedding-based retrieval, and LLM-based reranking for skill linking, achieves an end-to-end performance of 0.56, positioning Turkish alongside similar studies in other languages, which are few in the literature. Our findings suggest that LLMs can improve skill extraction performance in low-resource settings, and we hope that our work will accelerate similar research on skill extraction for underrepresented languages.

</details>


### [38] [Should LLMs, $\textit{like}$, Generate How Users Talk? Building Dialect-Accurate Dialog[ue]s Beyond the American Default with MDial](https://arxiv.org/abs/2601.22888)
*Jio Oh,Paul Vicinanza,Thomas Butler,Steven Euijong Whang,Dezhi Hong,Amani Namboori*

Main category: cs.CL

TL;DR: 提出了MDial框架，首次大规模生成多方言对话数据，涵盖9种英语方言的词汇、拼写和语法特征，并创建MDialBench基准评估17个LLM的方言识别和响应生成能力。


<details>
  <summary>Details</summary>
Motivation: 超过80%的16亿英语使用者不使用标准美式英语，导致与LLM交互时失败率更高且受到刻板回应。然而多方言性能研究仍然不足，需要系统性的方言数据处理和评估框架。

Method: 1) 引入MDial框架，基于语言学规则生成多方言对话数据，涵盖词汇、拼写和语法三个维度；2) 与母语语言学家合作，设计可扩展的基于规则的LLM转换方法；3) 构建MDialBench基准，包含50k+对话和97k+问答对；4) 评估17个LLM在方言识别和响应生成任务上的表现。

Result: 1) MDial生成的数据在98%的成对比较中被标注者认为比先前方法更自然；2) 发现模型不应复制方言90%的语法特征；3) 即使是前沿LLM在方言识别任务上准确率低于70%，加拿大英语准确率甚至低于50%；4) 模型系统性地将非标准美式英语方言误分类为美国或英国英语。

Conclusion: 多方言LLM性能存在显著差距，方言识别错误可能导致下游任务的级联失败。需要更系统性的方言数据处理方法，而不仅仅是模仿用户语法特征。MDial框架为多方言NLP研究提供了重要工具和基准。

Abstract: More than 80% of the 1.6 billion English speakers do not use Standard American English (SAE) and experience higher failure rates and stereotyped responses when interacting with LLMs as a result. Yet multi-dialectal performance remains underexplored. We introduce $\textbf{MDial}$, the first large-scale framework for generating multi-dialectal conversational data encompassing the three pillars of written dialect -- lexical (vocabulary), orthographic (spelling), and morphosyntactic (grammar) features -- for nine English dialects. Partnering with native linguists, we design an annotated and scalable rule-based LLM transformation to ensure precision. Our approach challenges the assumption that models should mirror users' morphosyntactic features, showing that up to 90% of the grammatical features of a dialect should not be reproduced by models. Independent evaluations confirm data quality, with annotators preferring MDial outputs over prior methods in 98% of pairwise comparisons for dialect naturalness. Using this pipeline, we construct the dialect-parallel $\textbf{MDialBench}$mark with 50k+ dialogs, resulting in 97k+ QA pairs, and evaluate 17 LLMs on dialect identification and response generation tasks. Even frontier models achieve under 70% accuracy, fail to reach 50% for Canadian English, and systematically misclassify non-SAE dialects as American or British. As dialect identification underpins natural language understanding, these errors risk cascading failures into downstream tasks.

</details>


### [39] [DiffuSpeech: Silent Thought, Spoken Answer via Unified Speech-Text Diffusion](https://arxiv.org/abs/2601.22889)
*Yuxuan Lou,Ziming Wu,Yaochen Wang,Yong Liu,Yingxuan Ren,Fuming Lai,Shaobing Lian,Jie Tang,Yang You*

Main category: cs.CL

TL;DR: 提出STSA（Silent Thought, Spoken Answer）范式，通过扩散模型方法实现语音语言模型在生成语音响应的同时产生内部文本推理，提升语音质量和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前语音语言模型直接生成语音响应，缺乏显式推理过程，一旦音频生成就无法纠正错误。需要一种能够生成内部推理轨迹并利用这些信息提升语音质量的方法。

Method: 提出MethoD方法，这是首个基于扩散的语音-文本语言模型，支持理解和生成功能。通过单一掩码扩散框架统一离散文本和标记化语音，采用迭代去噪联合生成推理轨迹和语音标记，并使用模态特定的掩码调度。

Result: 方法在语音到语音QA任务上达到最先进准确率，比最佳基线提升最多9个百分点。在生成模型中取得最佳TTS质量（6.2% WER），同时保持语言理解能力（66.2% MMLU）。消融实验证实扩散架构和推理轨迹都对这些提升有贡献。

Conclusion: STSA范式通过结合内部文本推理和语音生成，显著提升了语音语言模型的准确性和质量，为语音AI系统提供了更可靠、可解释的解决方案。

Abstract: Current speech language models generate responses directly without explicit reasoning, leading to errors that cannot be corrected once audio is produced. We introduce \textbf{``Silent Thought, Spoken Answer''} -- a paradigm where speech LLMs generate internal text reasoning alongside spoken responses, with thinking traces informing speech quality. To realize this, we present \method{}, the first diffusion-based speech-text language model supporting both understanding and generation, unifying discrete text and tokenized speech under a single masked diffusion framework. Unlike autoregressive approaches, \method{} jointly generates reasoning traces and speech tokens through iterative denoising, with modality-specific masking schedules. We also construct \dataset{}, the first speech QA dataset with paired text reasoning traces, containing 26K samples totaling 319 hours. Experiments show \method{} achieves state-of-the-art speech-to-speech QA accuracy, outperforming the best baseline by up to 9 points, while attaining the best TTS quality among generative models (6.2\% WER) and preserving language understanding (66.2\% MMLU). Ablations confirm that both the diffusion architecture and thinking traces contribute to these gains.

</details>


### [40] [LLMs Explain't: A Post-Mortem on Semantic Interpretability in Transformer Models](https://arxiv.org/abs/2601.22928)
*Alhassan Abdelhalim,Janick Edinger,Sören Laue,Michaela Regneri*

Main category: cs.CL

TL;DR: 论文发现两种广泛使用的LLM可解释性方法（注意力头分析和嵌入特征映射）存在根本缺陷，无法可靠揭示语言模型的理解机制。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在普适计算中被广泛使用，但其卓越性能的确切机制仍不清楚。现有可解释性方法本身也未被充分理解，作者希望探究语言抽象在LLM中如何出现，并检测其在不同模块中的表现。

Method: 使用两种文献中成熟的方法：(1) 基于token级关系结构的探测方法；(2) 使用嵌入作为人类可解释属性载体的特征映射方法。

Result: 两种方法均失败：注意力解释在测试"后层表示仍对应token"这一核心假设时崩溃；嵌入属性推理方法的高预测分数由方法学伪影和数据集结构驱动，而非有意义的语义知识。

Conclusion: 这些失败很重要，因为两种技术被广泛视为LLM理解能力的证据，但结果表明此类结论缺乏依据。在LLM作为系统组件部署的普适和分布式计算环境中，这些局限性尤其相关，因为可解释性方法被用于调试、压缩和解释模型。

Abstract: Large Language Models (LLMs) are becoming increasingly popular in pervasive computing due to their versatility and strong performance. However, despite their ubiquitous use, the exact mechanisms underlying their outstanding performance remain unclear. Different methods for LLM explainability exist, and many are, as a method, not fully understood themselves. We started with the question of how linguistic abstraction emerges in LLMs, aiming to detect it across different LLM modules (attention heads and input embeddings). For this, we used methods well-established in the literature: (1) probing for token-level relational structures, and (2) feature-mapping using embeddings as carriers of human-interpretable properties.
  Both attempts failed for different methodological reasons: Attention-based explanations collapsed once we tested the core assumption that later-layer representations still correspond to tokens. Property-inference methods applied to embeddings also failed because their high predictive scores were driven by methodological artifacts and dataset structure rather than meaningful semantic knowledge. These failures matter because both techniques are widely treated as evidence for what LLMs supposedly understand, yet our results show such conclusions are unwarranted. These limitations are particularly relevant in pervasive and distributed computing settings where LLMs are deployed as system components and interpretability methods are relied upon for debugging, compression, and explaining models.

</details>


### [41] [Benchmarking Machine Translation on Chinese Social Media Texts](https://arxiv.org/abs/2601.22931)
*Kaiyan Zhao,Zheyong Xie,Zhongtao Miao,Xinze Lyu,Yao Hu,Shaosheng Cao*

Main category: cs.CL

TL;DR: 提出了针对中文社交媒体文本的机器翻译基准CSM-MTBench，解决俚语、新词和风格化表达带来的评估挑战。


<details>
  <summary>Details</summary>
Motivation: 中文社交媒体上快速演变的俚语、新词和高度风格化的表达对机器翻译基准测试构成重大挑战，主要障碍包括：1）数据稀缺，高质量平行数据需要熟悉平台特定俚语的双语标注者；2）评估指标限制，传统评估器如COMET难以捕捉风格保真度和非标准表达。

Method: 引入CSM-MTBench基准，涵盖五个中外语言方向，包含两个专家策划的子集：Fun Posts（富含上下文、俚语和新词内容）和Social Snippets（强调简洁、情感和风格驱动的表达）。为每个子集提出定制评估方法：在Fun Posts中测量俚语和新词的翻译成功率，在Social Snippets中通过基于嵌入的指标和LLM-as-a-judge混合方法评估语气和风格保留。

Result: 对20多个模型的实验显示，当前机器翻译系统在处理语义保真度和非正式、社交媒体特定风格线索方面存在显著差异。

Conclusion: CSM-MTBench作为一个严谨的测试平台，能够推动机器翻译系统掌握真实世界中文社交媒体文本的能力。

Abstract: The prevalence of rapidly evolving slang, neologisms, and highly stylized expressions in informal user-generated text, particularly on Chinese social media, poses significant challenges for Machine Translation (MT) benchmarking. Specifically, we identify two primary obstacles: (1) data scarcity, as high-quality parallel data requires bilingual annotators familiar with platform-specific slang, and stylistic cues in both languages; and (2) metric limitations, where traditional evaluators like COMET often fail to capture stylistic fidelity and nonstandard expressions. To bridge these gaps, we introduce CSM-MTBench, a benchmark covering five Chinese-foreign language directions and consisting of two expert-curated subsets: Fun Posts, featuring context-rich, slang- and neologism-heavy content, and Social Snippets, emphasizing concise, emotion- and style- driven expressions. Furthermore, we propose tailored evaluation approaches for each subset: measuring the translation success rate of slang and neologisms in Fun Posts, while assessing tone and style preservation in Social Snippets via a hybrid of embedding-based metrics and LLM-as-a-judge. Experiments on over 20 models reveal substantial variation in how current MT systems handle semantic fidelity and informal, social-media-specific stylistic cues. CSM-MTBench thus serves as a rigorous testbed for advancing MT systems capable of mastering real-world Chinese social media texts.

</details>


### [42] [Relaxing Positional Alignment in Masked Diffusion Language Models](https://arxiv.org/abs/2601.22947)
*Mengyu Ye,Ryosuke Takahashi,Keito Kudo,Jun Suzuki*

Main category: cs.CL

TL;DR: 通过引入<slack>标记和CTC目标，放松MDLM训练中的严格位置监督，提升开放文本生成质量和鲁棒性


<details>
  <summary>Details</summary>
Motivation: MDLM在开放文本生成上存在性能差距，研究发现严格位置预测使解码对token错位高度敏感，一个位置偏移就会严重破坏语义，这与MDLM解码的不可逆去噪动态不匹配

Method: 在微调阶段采用对齐灵活监督策略，通过连接时序分类目标引入特殊标记<slack>，放松严格位置监督

Result: 在五个开放文本生成基准测试中一致优于原始模型，提升了对位置偏移的鲁棒性，表明放松严格位置监督是提高MDLM生成质量的重要因素

Conclusion: MDLM中放松严格位置监督能有效提升开放文本生成性能，对齐灵活监督策略是改进MDLM生成质量的关键方向

Abstract: Masked diffusion language models (MDLMs) have emerged as a promising alternative to dominant autoregressive approaches. Although they achieve competitive performance on several tasks, a substantial gap remains in open-ended text generation. We hypothesize that one cause of this gap is that strict positional prediction makes MDLM decoding highly sensitive to token misalignment, and we show through controlled interventions that a one-position shift can severely disrupt semantics. This observation suggests that enforcing strict positional supervision during training is misaligned with the irreversible denoising dynamics of MDLM decoding. Motivated by this mismatch, we adopt an alignment-flexible supervision strategy during fine-tuning. Specifically, we introduce a special token <slack> via the connectionist temporal classification objective. We apply this approach to the widely used MDLM model and conduct experiments on five open-ended text generation benchmarks. Our method consistently outperforms the original model and improves robustness to positional shifts, indicating that relaxing strict positional supervision is an important factor in improving generation quality in MDLMs.

</details>


### [43] [Autonomous Chain-of-Thought Distillation for Graph-Based Fraud Detection](https://arxiv.org/abs/2601.22949)
*Yuan Li,Jun Hu,Bryan Hooi,Bingsheng He,Cheng Chen*

Main category: cs.CL

TL;DR: FraudCoT：一种通过自主图感知思维链推理和可扩展LLM-GNN协同训练来增强文本属性图欺诈检测的统一框架。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM增强的GNN方法受到预定义提示和解耦训练流程的限制，这限制了推理自主性并削弱了语义-结构对齐，从而影响了文本属性图上的欺诈检测性能。

Method: 提出欺诈感知选择性思维链蒸馏机制，生成多样化推理路径以增强语义-结构理解；将蒸馏后的思维链集成到节点文本中，为GNN提供丰富的多跳语义和结构线索；开发高效的非对称协同训练策略，实现端到端优化并大幅降低计算成本。

Result: 在公共和工业基准测试中，FraudCoT相比最先进方法实现了高达8.8%的AUPRC提升，并在训练吞吐量上实现了高达1,066倍的加速，显著提升了检测性能和效率。

Conclusion: FraudCoT通过自主思维链推理和高效的LLM-GNN协同训练，在文本属性图欺诈检测中实现了性能和效率的双重突破，为图智能与大型语言模型的深度融合提供了新方向。

Abstract: Graph-based fraud detection on text-attributed graphs (TAGs) requires jointly modeling rich textual semantics and relational dependencies. However, existing LLM-enhanced GNN approaches are constrained by predefined prompting and decoupled training pipelines, limiting reasoning autonomy and weakening semantic-structural alignment. We propose FraudCoT, a unified framework that advances TAG-based fraud detection through autonomous, graph-aware chain-of-thought (CoT) reasoning and scalable LLM-GNN co-training. To address the limitations of predefined prompts, we introduce a fraud-aware selective CoT distillation mechanism that generates diverse reasoning paths and enhances semantic-structural understanding. These distilled CoTs are integrated into node texts, providing GNNs with enriched, multi-hop semantic and structural cues for fraud detection. Furthermore, we develop an efficient asymmetric co-training strategy that enables end-to-end optimization while significantly reducing the computational cost of naive joint training. Extensive experiments on public and industrial benchmarks demonstrate that FraudCoT achieves up to 8.8% AUPRC improvement over state-of-the-art methods and delivers up to 1,066x speedup in training throughput, substantially advancing both detection performance and efficiency.

</details>


### [44] [Residual Context Diffusion Language Models](https://arxiv.org/abs/2601.22954)
*Yuezhou Hu,Harman Singh,Monishwaran Maheswaran,Haocheng Xi,Coleman Hooper,Jintao Zhang,Aditya Tomar,Michael W. Mahoney,Sewon Min,Mehrdad Farajtabar,Kurt Keutzer,Amir Gholami,Chenfeng Xu*

Main category: cs.CL

TL;DR: 提出RCD方法，通过回收扩散大语言模型中丢弃的token计算，提升解码效率与准确性


<details>
  <summary>Details</summary>
Motivation: 现有块状dLLMs依赖"remasking"机制，只解码最置信的token而丢弃其余，浪费计算资源。研究发现丢弃的token包含对后续解码有用的上下文信息。

Method: 提出残差上下文扩散(RCD)模块，将丢弃的token表示转换为上下文残差，在下一个去噪步骤中重新注入。采用解耦的两阶段训练流程避免内存瓶颈。

Result: RCD仅需约10亿token即可将标准dLLM转换为RCD范式，在广泛基准测试中提升前沿dLLMs准确率5-10个百分点，计算开销极小。在最具挑战性的AIME任务上，准确率几乎翻倍，在同等准确率水平下减少4-5倍去噪步骤。

Conclusion: RCD通过有效利用被丢弃token的上下文信息，显著提升了扩散大语言模型的解码效率和准确性，为并行解码语言模型提供了更优的解决方案。

Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to purely autoregressive language models because they can decode multiple tokens in parallel. However, state-of-the-art block-wise dLLMs rely on a "remasking" mechanism that decodes only the most confident tokens and discards the rest, effectively wasting computation. We demonstrate that recycling computation from the discarded tokens is beneficial, as these tokens retain contextual information useful for subsequent decoding iterations. In light of this, we propose Residual Context Diffusion (RCD), a module that converts these discarded token representations into contextual residuals and injects them back for the next denoising step. RCD uses a decoupled two-stage training pipeline to bypass the memory bottlenecks associated with backpropagation. We validate our method on both long CoT reasoning (SDAR) and short CoT instruction following (LLaDA) models. We demonstrate that a standard dLLM can be efficiently converted to the RCD paradigm with merely ~1 billion tokens. RCD consistently improves frontier dLLMs by 5-10 points in accuracy with minimal extra computation overhead across a wide range of benchmarks. Notably, on the most challenging AIME tasks, RCD nearly doubles baseline accuracy and attains up to 4-5x fewer denoising steps at equivalent accuracy levels.

</details>


### [45] [A Unified View of Attention and Residual Sinks: Outlier-Driven Rescaling is Essential for Transformer Training](https://arxiv.org/abs/2601.22966)
*Zihan Qiu,Zeyu Huang,Kaiyue Wen,Peng Jin,Bo Zheng,Yuxin Zhou,Haofeng Huang,Zekun Wang,Xiao Li,Huaqing Zhang,Yang Xu,Haoran Lian,Siqi Zhang,Rui Men,Jianwei Zhang,Ivan Titov,Dayiheng Liu,Jingren Zhou,Junyang Lin*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型中出现的异常值（注意力汇聚点和残差汇聚点）的功能作用，发现这些异常值与归一化操作共同作用，通过异常值驱动的重缩放现象来稳定训练，并提出相应的缓解方法。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型中出现的两种异常值现象：注意力汇聚点（少数token持续获得高注意力分数）和残差汇聚点（少数维度在大多数token上保持大激活值）。探索这些异常值在模型中的功能角色及其与归一化操作的相互作用。

Method: 提出"异常值驱动的重缩放"假设，并通过不同模型架构和训练token数量进行验证。研究移除归一化、直接裁剪异常值等方法的影响，并提出通过可学习参数吸收异常值或使用显式门控重缩放来缓解异常值问题。

Result: 验证了异常值与归一化操作共同作用的假设：移除归一化会消除异常值但降低训练稳定性；直接裁剪异常值也会导致性能下降。发现异常值主要作为重缩放因子而非主要贡献者。提出的缓解方法能提升训练性能（平均2点增益）并增强量化鲁棒性（W4A4量化下仅1.2点性能下降）。

Conclusion: 异常值驱动的重缩放现象在大型语言模型中普遍存在，通过异常值与归一化操作的相互作用来稳定训练。异常值主要作为重缩放因子而非主要贡献者。通过可学习参数吸收异常值或使用门控重缩放可以有效缓解异常值问题，提升模型性能和量化鲁棒性。

Abstract: We investigate the functional role of emergent outliers in large language models, specifically attention sinks (a few tokens that consistently receive large attention logits) and residual sinks (a few fixed dimensions with persistently large activations across most tokens). We hypothesize that these outliers, in conjunction with the corresponding normalizations (\textit{e.g.}, softmax attention and RMSNorm), effectively rescale other non-outlier components. We term this phenomenon \textit{outlier-driven rescaling} and validate this hypothesis across different model architectures and training token counts. This view unifies the origin and mitigation of both sink types. Our main conclusions and observations include: (1) Outliers function jointly with normalization: removing normalization eliminates the corresponding outliers but degrades training stability and performance; directly clipping outliers while retaining normalization leads to degradation, indicating that outlier-driven rescaling contributes to training stability. (2) Outliers serve more as rescale factors rather than contributors, as the final contributions of attention and residual sinks are significantly smaller than those of non-outliers. (3) Outliers can be absorbed into learnable parameters or mitigated via explicit gated rescaling, leading to improved training performance (average gain of 2 points) and enhanced quantization robustness (1.2 points degradation under W4A4 quantization).

</details>


### [46] [ArabicDialectHub: A Cross-Dialectal Arabic Learning Resource and Platform](https://arxiv.org/abs/2601.22987)
*Salem Lahlou*

Main category: cs.CL

TL;DR: 阿拉伯方言学习资源ArabicDialectHub，包含6种阿拉伯语变体的552个短语，提供交互式网络平台，支持翻译探索、自适应测试和进度跟踪。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语存在多种方言变体，学习资源分散，缺乏系统性的跨方言学习工具。需要创建统一的资源帮助学习者掌握不同阿拉伯语方言。

Method: 使用大语言模型生成短语，由5位母语者验证。按难度分层，按主题组织。开发开源网络平台，包含翻译探索、自适应测试（算法生成干扰项）、云同步进度跟踪和文化背景功能。

Result: 创建了ArabicDialectHub资源，包含6种阿拉伯语变体（摩洛哥达里贾语、黎巴嫩语、叙利亚语、阿联酋语、沙特语和现代标准阿拉伯语）的552个短语。开发了完整的交互式网络平台，数据集和平台源代码以MIT许可证发布。

Conclusion: ArabicDialectHub为阿拉伯语学习者提供了系统性的跨方言学习资源，通过技术增强的语言学习平台促进阿拉伯语方言习得。开源发布鼓励社区贡献和进一步开发。

Abstract: We present ArabicDialectHub, a cross-dialectal Arabic learning resource comprising 552 phrases across six varieties (Moroccan Darija, Lebanese, Syrian, Emirati, Saudi, and MSA) and an interactive web platform. Phrases were generated using LLMs and validated by five native speakers, stratified by difficulty, and organized thematically. The open-source platform provides translation exploration, adaptive quizzing with algorithmic distractor generation, cloud-synchronized progress tracking, and cultural context. Both the dataset and complete platform source code are released under MIT license. Platform: https://arabic-dialect-hub.netlify.app.

</details>


### [47] [Bias Beyond Borders: Political Ideology Evaluation and Steering in Multilingual LLMs](https://arxiv.org/abs/2601.23001)
*Afrozah Nadeem,Agrima,Mehwish Nasim,Usman Naseem*

Main category: cs.CL

TL;DR: 提出跨语言对齐引导(CLAS)框架，通过共享意识形态子空间对齐多语言表示，动态调节干预强度，有效减少LLMs政治偏见同时保持响应质量


<details>
  <summary>Details</summary>
Motivation: 当前LLMs政治偏见研究主要集中于高资源西方语言或有限多语言场景，缺乏跨语言一致性和安全后处理缓解方法的探索，这对负责任AI部署至关重要

Method: 提出跨语言对齐引导(CLAS)框架：1) 将政治提示诱导的潜在意识形态表示对齐到共享意识形态子空间，确保跨语言一致性；2) 自适应机制防止过度修正并保持连贯性；3) 基于50个国家33种语言的大规模多语言评估

Result: 实验表明该方法在经济和社会维度上都显著减少了偏见，同时响应质量下降最小，实现了意识形态中立与语言文化多样性之间的平衡

Conclusion: CLAS框架为公平感知的多语言LLM治理建立了可扩展和可解释的范式，在保持语言文化多样性的同时促进意识形态中立

Abstract: Large Language Models (LLMs) increasingly shape global discourse, making fairness and ideological neutrality essential for responsible AI deployment. Despite growing attention to political bias in LLMs, prior work largely focuses on high-resource, Western languages or narrow multilingual settings, leaving cross-lingual consistency and safe post-hoc mitigation underexplored. To address this gap, we present a large-scale multilingual evaluation of political bias spanning 50 countries and 33 languages. We introduce a complementary post-hoc mitigation framework, Cross-Lingual Alignment Steering (CLAS), designed to augment existing steering methods by aligning ideological representations across languages and dynamically regulating intervention strength. This method aligns latent ideological representations induced by political prompts into a shared ideological subspace, ensuring cross lingual consistency, with the adaptive mechanism prevents over correction and preserves coherence. Experiments demonstrate substantial bias reduction along both economic and social axes with minimal degradation in response quality. The proposed framework establishes a scalable and interpretable paradigm for fairness-aware multilingual LLM governance, balancing ideological neutrality with linguistic and cultural diversity.

</details>


### [48] [InstructDiff: Domain-Adaptive Data Selection via Differential Entropy for Efficient LLM Fine-Tuning](https://arxiv.org/abs/2601.23006)
*Junyou Su,He Zhu,Xiao Luo,Liyu Zhang,Hong-Yu Zhou,Yun Chen,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: InstructDiff：通过校准模型熵差实现跨领域数据选择，在数学推理和通用指令任务上分别仅用10%数据实现17%和52%相对性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法存在严重的领域特异性问题：针对通用指令调优优化的方法在推理任务上失败，反之亦然。完整数据集训练成本高昂且收益递减，需要统一的跨领域数据选择框架。

Method: 提出InstructDiff框架，通过基础模型与最小指令调优校准模型之间的熵差测量，发现最低微分熵样本在不同领域表现最优。框架包含预热校准、双向NLL过滤和基于熵的排序三个核心组件，实现领域自适应数据选择。

Result: 实验表明，仅使用10%数据，InstructDiff在数学推理任务上相对完整数据训练提升17%，在通用指令跟随任务上提升52%，显著优于现有基线方法。

Conclusion: 微分熵作为跨领域数据选择的有效指标，通过InstructDiff框架实现了统一且高效的数据选择，显著降低了微调成本并提升了模型性能。

Abstract: Supervised fine-tuning (SFT) is fundamental to adapting large language models, yet training on complete datasets incurs prohibitive costs with diminishing returns. Existing data selection methods suffer from severe domain specificity: techniques optimized for general instruction-following fail on reasoning tasks, and vice versa. We observe that measuring entropy differences between base models and minimally instruction-tuned calibrated models reveals a pattern -- samples with the lowest differential entropy consistently yield optimal performance across domains, yet this principle manifests domain-adaptively: reasoning tasks favor entropy increase (cognitive expansion), while general tasks favor entropy decrease (cognitive compression). We introduce InstructDiff, a unified framework that operationalizes differential entropy as a domain-adaptive selection criterion through warmup calibration, bi-directional NLL filtering, and entropy-based ranking. Extensive experiments show that InstructDiff achieves 17\% relative improvement over full data training on mathematical reasoning and 52\% for general instruction-following, outperforming prior baselines while using only 10\% of the data.

</details>


### [49] [DimABSA: Building Multilingual and Multidomain Datasets for Dimensional Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2601.23022)
*Lung-Hao Lee,Liang-Chih Yu,Natalia Loukashevich,Ilseyar Alimova,Alexander Panchenko,Tzu-Mi Lin,Zhe-Yu Xu,Jian-Yu Zhou,Guangmin Zheng,Jin Wang,Sharanya Awasthi,Jonas Becker,Jan Philip Wahle,Terry Ruas,Shamsuddeen Hassan Muhammad,Saif M. Mohammed*

Main category: cs.CL

TL;DR: DimABSA：首个多语言维度情感分析资源，用连续的情感-唤醒度（VA）分数替代传统的粗粒度情感标签，支持更细粒度的情感分析。


<details>
  <summary>Details</summary>
Motivation: 现有基于方面的情感分析（ABSA）研究依赖粗粒度的分类标签（如正面、负面），限制了捕捉细微情感状态的能力。

Method: 1. 引入DimABSA资源：首个多语言维度ABSA资源，标注了传统ABSA元素（方面术语、方面类别、观点术语）和新引入的VA分数
2. 提出三个结合VA分数和不同ABSA元素的子任务
3. 提出新的统一度量标准：连续F1（cF1），将VA预测误差纳入标准F1
4. 使用提示和微调的大型语言模型在所有子任务上进行全面基准测试

Result: DimABSA包含76,958个方面实例，跨越42,590个句子，涵盖六种语言和四个领域。基准测试结果表明DimABSA是一个具有挑战性的基准，为推进多语言维度ABSA提供了基础。

Conclusion: DimABSA通过引入连续的情感-唤醒度表示，解决了传统ABSA在捕捉细微情感方面的局限性，为更细粒度的情感分析提供了新的资源和评估框架。

Abstract: Aspect-Based Sentiment Analysis (ABSA) focuses on extracting sentiment at a fine-grained aspect level and has been widely applied across real-world domains. However, existing ABSA research relies on coarse-grained categorical labels (e.g., positive, negative), which limits its ability to capture nuanced affective states. To address this limitation, we adopt a dimensional approach that represents sentiment with continuous valence-arousal (VA) scores, enabling fine-grained analysis at both the aspect and sentiment levels. To this end, we introduce DimABSA, the first multilingual, dimensional ABSA resource annotated with both traditional ABSA elements (aspect terms, aspect categories, and opinion terms) and newly introduced VA scores. This resource contains 76,958 aspect instances across 42,590 sentences, spanning six languages and four domains. We further introduce three subtasks that combine VA scores with different ABSA elements, providing a bridge from traditional ABSA to dimensional ABSA. Given that these subtasks involve both categorical and continuous outputs, we propose a new unified metric, continuous F1 (cF1), which incorporates VA prediction error into standard F1. We provide a comprehensive benchmark using both prompted and fine-tuned large language models across all subtasks. Our results show that DimABSA is a challenging benchmark and provides a foundation for advancing multilingual dimensional ABSA.

</details>


### [50] [Character as a Latent Variable in Large Language Models: A Mechanistic Account of Emergent Misalignment and Conditional Safety Failures](https://arxiv.org/abs/2601.23081)
*Yanghao Su,Wenbo Zhou,Tianwei Zhang,Qiu Han,Weiming Zhang,Nenghai Yu,Jie Zhang*

Main category: cs.CL

TL;DR: 微调LLMs时，即使数据内容正确但存在特定行为倾向，也会导致广泛的对齐失败，这源于行为倾向的稳定转变而非能力退化


<details>
  <summary>Details</summary>
Motivation: 现有研究将涌现错位主要归因于错误或不安全内容的泛化，但本文认为这种观点不完整，需要探索更深层次的原因

Method: 在多个领域和模型家族中，通过微调模型在展现特定字符级倾向的数据上，并与错误建议微调对比，分析行为倾向的转移和激活机制

Result: 字符级倾向微调比错误建议微调产生更强、更可转移的错位，同时保持一般能力；行为倾向可通过训练时触发器和推理时角色对齐提示条件激活

Conclusion: 涌现错位源于行为倾向的稳定转变，而非能力退化或知识污染，表明稳健对齐必须解决行为倾向问题而非孤立错误或提示级防御

Abstract: Emergent Misalignment refers to a failure mode in which fine-tuning large language models (LLMs) on narrowly scoped data induces broadly misaligned behavior. Prior explanations mainly attribute this phenomenon to the generalization of erroneous or unsafe content. In this work, we show that this view is incomplete. Across multiple domains and model families, we find that fine-tuning models on data exhibiting specific character-level dispositions induces substantially stronger and more transferable misalignment than incorrect-advice fine-tuning, while largely preserving general capabilities. This indicates that emergent misalignment arises from stable shifts in model behavior rather than from capability degradation or corrupted knowledge. We further show that such behavioral dispositions can be conditionally activated by both training-time triggers and inference-time persona-aligned prompts, revealing shared structure across emergent misalignment, backdoor activation, and jailbreak susceptibility. Overall, our results identify character formation as a central and underexplored alignment risk, suggesting that robust alignment must address behavioral dispositions rather than isolated errors or prompt-level defenses.

</details>


### [51] [Safer Policy Compliance with Dynamic Epistemic Fallback](https://arxiv.org/abs/2601.23094)
*Joseph Marvin Imperial,Harish Tayyar Madabushi*

Main category: cs.CL

TL;DR: 提出Dynamic Epistemic Fallback (DEF)协议，通过认知启发式防御机制提升LLM对恶意篡改政策文本的检测和拒绝能力，在HIPAA和GDPR等法律政策上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 人类具有认知防御机制（认知警惕性）来应对日常交互中的欺骗和错误信息风险。受此启发，为LLM开发类似的安全机制对于高风险任务（如自动化遵守数据隐私法律）特别重要，以抵御利用恶意篡改政策文本的欺骗性攻击。

Method: 提出Dynamic Epistemic Fallback (DEF)协议，这是一种动态安全协议，通过不同级别的单句文本提示，引导LLM在遇到篡改政策文本时标记不一致性、拒绝遵守并回退到其参数化知识。

Result: 在HIPAA和GDPR等全球认可的法律政策上进行实证评估，DEF有效提升了前沿LLM检测和拒绝篡改政策版本的能力，其中DeepSeek-R1在某个设置中达到了100%的检测率。

Conclusion: 这项工作鼓励进一步开发认知启发式防御机制，以提高LLM对利用法律文书进行伤害和欺骗的鲁棒性，为高风险应用中的LLM安全提供了有前景的方向。

Abstract: Humans develop a series of cognitive defenses, known as epistemic vigilance, to combat risks of deception and misinformation from everyday interactions. Developing safeguards for LLMs inspired by this mechanism might be particularly helpful for their application in high-stakes tasks such as automating compliance with data privacy laws. In this paper, we introduce Dynamic Epistemic Fallback (DEF), a dynamic safety protocol for improving an LLM's inference-time defenses against deceptive attacks that make use of maliciously perturbed policy texts. Through various levels of one-sentence textual cues, DEF nudges LLMs to flag inconsistencies, refuse compliance, and fallback to their parametric knowledge upon encountering perturbed policy texts. Using globally recognized legal policies such as HIPAA and GDPR, our empirical evaluations report that DEF effectively improves the capability of frontier LLMs to detect and refuse perturbed versions of policies, with DeepSeek-R1 achieving a 100% detection rate in one setting. This work encourages further efforts to develop cognitively inspired defenses to improve LLM robustness against forms of harm and deception that exploit legal artifacts.

</details>


### [52] [Evaluating the Utility of Grounding Documents with Reference-Free LLM-based Metrics](https://arxiv.org/abs/2601.23129)
*Yilun Hua,Giuseppe Castellucci,Peter Schulam,Heba Elfardy,Kevin Small*

Main category: cs.CL

TL;DR: GroGU是一种用于RAG系统的模型特定、无需参考的评估指标，通过计算LLM生成置信度来量化文档效用，并用于训练查询重写器提升检索性能


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统中量化文档效用的方法存在缺陷：缺乏明确规范、忽略模型特定能力、依赖昂贵的人工标注。需要一种更有效的评估指标来准确衡量文档对LLM生成的实际效用。

Method: 提出GroGU指标，基于信息熵理论，通过计算LLM在给定文档条件下的生成置信度来量化文档效用。该指标无需参考答案和人工标注，是模型特定的评估方法。进一步应用GroGU识别高质量偏好数据，用于直接偏好优化训练查询重写器。

Result: 实验表明GroGU能够准确区分真实相关文档，捕捉LLM无关指标忽略的细微差别。使用GroGU训练的查询重写器在平均倒数排名上提升高达18.2分，答案准确率提升高达9.4分。

Conclusion: GroGU提供了一种有效的模型特定、无需参考的文档效用评估方法，不仅能够准确评估RAG中检索文档的质量，还能用于训练更有效的查询重写组件，显著提升RAG系统性能。

Abstract: Retrieval Augmented Generation (RAG)'s success depends on the utility the LLM derives from the content used for grounding. Quantifying content utility does not have a definitive specification and existing metrics ignore model-specific capabilities and/or rely on costly annotations. In this paper, we propose Grounding Generation Utility (GroGU), a model-specific and reference-free metric that defines utility as a function of the downstream LLM's generation confidence based on entropy. Despite having no annotation requirements, GroGU is largely faithful in distinguishing ground-truth documents while capturing nuances ignored by LLM-agnostic metrics. We apply GroGU to train a query-rewriter for RAG by identifying high-utility preference data for Direct Preference Optimization. Experiments show improvements by up to 18.2 points in Mean Reciprocal Rank and up to 9.4 points in answer accuracy.

</details>


### [53] [Monotonic Reference-Free Refinement for Autoformalization](https://arxiv.org/abs/2601.23166)
*Lan Zhang,Marco Valentino,André Freitas*

Main category: cs.CL

TL;DR: 提出一种无参考的迭代单调过程用于全定理自动形式化，通过定理证明器和LLM法官的互补反馈，联合优化形式化质量的多维度，无需推理时的真实证明或现有形式化


<details>
  <summary>Details</summary>
Motivation: 当前语句自动形式化方法通常孤立改进形式化的单个方面（如语法正确性），难以联合优化多个质量维度，而这对全定理自动形式化至关重要

Method: 引入无参考迭代单调过程，利用定理证明器和基于LLM的法官的互补反馈，通过响应性映射指导不同LLM角色优先改进不同维度，优化形式有效性、逻辑保持、数学一致性和形式质量的复合目标，并提出保证认证单调改进的接受策略

Result: 在miniF2F上达到93.44%的形式有效性和78.22%的总体得分，在ProofNet上达到44.09%的形式有效性和29.79%的总体得分，实验证明该方法能同时改进多个维度

Conclusion: 提出的方法通过联合优化多个质量维度，为全定理自动形式化提供了有效的解决方案，并保证了收敛性和终止性

Abstract: While statement autoformalization has advanced rapidly, full-theorem autoformalization remains largely unexplored. Existing iterative refinement methods in statement autoformalization typicall improve isolated aspects of formalization, such as syntactic correctness, but struggle to jointly optimizing multiple quality dimensions, which is critical for full-theorem autoformalization. We introduce a reference-free iterative monotonic process for full-theorem autoformalization that leverages complementary feedback from theorem provers and LLM-based judges, without access to ground-truth proofs or existing formalizations at inference time. Our approach optimizes a masked composite objective over Formal Validity, Logical Preservation, Mathematical Consistency, and Formal Quality, guided by a responsiveness map that indicates how different LLMs acting as different roles preferentially improve each dimension. We further propose an acceptance policy that guarantees certified monotonic improvement, and provide conditions ensuring convergence and termination. Empirical experiments demonstrate the proposed process enables simultaneous improvement across multiple dimensions, achieving 93.44% formal validity and a 78.22% overall score on miniF2F, and 44.09% formal validity and a 29.79% overall score on ProofNet.

</details>


### [54] [FourierSampler: Unlocking Non-Autoregressive Potential in Diffusion Language Models via Frequency-Guided Generation](https://arxiv.org/abs/2601.23182)
*Siyang He,Qiqi Wang,Xiaoran Liu,Hongnan Ma,Yiwei Shi,Yuerong Song,Ying Zhu,Tianyi Liang,Zengfeng Huang,Ziwei He,Xipeng Qiu*

Main category: cs.CL

TL;DR: FourierSampler是一种基于频域分析的扩散语言模型解码策略，通过频率滑动窗口机制实现"结构到细节"的生成，显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散语言模型解码策略存在位置偏差，未能充分发挥其任意生成的潜力，需要更好的解码方法来提升生成质量。

Method: 通过频域分析发现隐藏状态中低频分量编码全局结构信息，高频分量负责局部细节，提出基于频率域滑动窗口机制的FourierSampler来动态引导模型实现"结构到细节"的生成。

Result: FourierSampler在LLADA和SDAR上优于其他推理增强策略，在LLaDA1.5-8B上相对提升20.4%，在LLaDA-8B-Instruct上相对提升16.0%，显著超越类似规模的自回归模型如Llama3.1-8B-Instruct。

Conclusion: 频域分析为扩散语言模型解码提供了新视角，FourierSampler通过频率域动态引导机制有效解决了位置偏差问题，提升了生成质量，为扩散模型的解码策略发展提供了重要方向。

Abstract: Despite the non-autoregressive potential of diffusion language models (dLLMs), existing decoding strategies demonstrate positional bias, failing to fully unlock the potential of arbitrary generation. In this work, we delve into the inherent spectral characteristics of dLLMs and present the first frequency-domain analysis showing that low-frequency components in hidden states primarily encode global structural information and long-range dependencies, while high-frequency components are responsible for characterizing local details. Based on this observation, we propose FourierSampler, which leverages a frequency-domain sliding window mechanism to dynamically guide the model to achieve a "structure-to-detail" generation. FourierSampler outperforms other inference enhancement strategies on LLADA and SDAR, achieving relative improvements of 20.4% on LLaDA1.5-8B and 16.0% on LLaDA-8B-Instruct. It notably surpasses similarly sized autoregressive models like Llama3.1-8B-Instruct.

</details>


### [55] [JobResQA: A Benchmark for LLM Machine Reading Comprehension on Multilingual Résumés and JDs](https://arxiv.org/abs/2601.23183)
*Casimiro Pio Carrino,Paula Estrella,Rabih Zbib,Carlos Escolano,José A. R. Fonollosa*

Main category: cs.CL

TL;DR: JobResQA是一个多语言问答基准测试，用于评估LLM在简历和职位描述等HR任务上的机器阅读理解能力，包含5种语言的581个QA对，支持公平性和偏见研究。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试缺乏针对HR领域特定任务（如简历和职位描述处理）的多语言机器阅读理解评估，且需要兼顾现实性和隐私保护，同时支持系统性的偏见和公平性研究。

Method: 1) 通过去标识化和数据合成从真实世界来源生成数据集；2) 使用控制的人口统计和专业属性（通过占位符实现）支持偏见研究；3) 基于TEaR方法构建成本效益高的人工循环翻译流程，结合MQM错误标注和选择性后期编辑确保高质量多语言并行基准。

Result: 构建了包含581个QA对、105个合成简历-职位描述对的五语言基准测试，涵盖三个复杂度级别。基线评估显示LLM在英语和西班牙语上表现较好，但在其他语言上性能显著下降，揭示了HR应用中多语言MRC能力的关键差距。

Conclusion: JobResQA为推进公平可靠的基于LLM的HR系统提供了一个可复现的基准测试，填补了HR领域多语言机器阅读理解评估的空白，并支持系统性偏见研究。

Abstract: We introduce JobResQA, a multilingual Question Answering benchmark for evaluating Machine Reading Comprehension (MRC) capabilities of LLMs on HR-specific tasks involving résumés and job descriptions. The dataset comprises 581 QA pairs across 105 synthetic résumé-job description pairs in five languages (English, Spanish, Italian, German, and Chinese), with questions spanning three complexity levels from basic factual extraction to complex cross-document reasoning. We propose a data generation pipeline derived from real-world sources through de-identification and data synthesis to ensure both realism and privacy, while controlled demographic and professional attributes (implemented via placeholders) enable systematic bias and fairness studies. We also present a cost-effective, human-in-the-loop translation pipeline based on the TEaR methodology, incorporating MQM error annotations and selective post-editing to ensure an high-quality multi-way parallel benchmark. We provide a baseline evaluations across multiple open-weight LLM families using an LLM-as-judge approach revealing higher performances on English and Spanish but substantial degradation for other languages, highlighting critical gaps in multilingual MRC capabilities for HR applications. JobResQA provides a reproducible benchmark for advancing fair and reliable LLM-based HR systems. The benchmark is publicly available at: https://github.com/Avature/jobresqa-benchmark

</details>


### [56] [ReGuLaR: Variational Latent Reasoning Guided by Rendered Chain-of-Thought](https://arxiv.org/abs/2601.23184)
*Fanmeng Wang,Haotian Liu,Guojiang Zhao,Hongteng Xu,Zhifeng Gao*

Main category: cs.CL

TL;DR: ReGuLaR提出了一种新的潜在推理范式，通过将显式推理链渲染为图像并用视觉语义表示正则化后验分布，在保持推理性能的同时显著减少计算冗余。


<details>
  <summary>Details</summary>
Motivation: 现有潜在推理方法虽然试图压缩推理过程以减少计算冗余，但由于缺乏适当的压缩指导，往往导致严重的性能下降。需要一种既能保持推理效果又能提高计算效率的方法。

Method: 将潜在推理建模为变分自编码框架，从条件后验分布中采样当前潜在推理状态。学习过程中，将显式推理链渲染为图像，从中提取密集的视觉语义表示来正则化后验分布，实现高效压缩。

Result: ReGuLaR在计算效率和推理效果上都显著优于现有潜在推理方法，甚至通过多模态推理超越了显式CoT方法，为潜在推理提供了新的解决方案。

Conclusion: ReGuLaR通过视觉语义表示指导的变分潜在推理，成功解决了现有方法在压缩过程中的信息损失问题，为高效推理提供了创新且有效的范式。

Abstract: While Chain-of-Thought (CoT) significantly enhances the performance of Large Language Models (LLMs), explicit reasoning chains introduce substantial computational redundancy. Recent latent reasoning methods attempt to mitigate this by compressing reasoning processes into latent space, but often suffer from severe performance degradation due to the lack of appropriate compression guidance. In this study, we propose Rendered CoT-Guided variational Latent Reasoning (ReGuLaR), a simple yet novel latent learning paradigm resolving this issue. Fundamentally, we formulate latent reasoning within the Variational Auto-Encoding (VAE) framework, sampling the current latent reasoning state from the posterior distribution conditioned on previous ones. Specifically, when learning this variational latent reasoning model, we render explicit reasoning chains as images, from which we extract dense visual-semantic representations to regularize the posterior distribution, thereby achieving efficient compression with minimal information loss. Extensive experiments demonstrate that ReGuLaR significantly outperforms existing latent reasoning methods across both computational efficiency and reasoning effectiveness, and even surpasses CoT through multi-modal reasoning, providing a new and insightful solution to latent reasoning. Code: https://github.com/FanmengWang/ReGuLaR.

</details>


### [57] [Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience](https://arxiv.org/abs/2601.23188)
*Zhongxiang Sun,Qipeng Wang,Weijie Yu,Jingxuan Yang,Haolang Lu,Jun Xu*

Main category: cs.CL

TL;DR: 提出DS-MCM框架，通过分层元认知监控机制增强深度搜索代理，包含快速一致性监控和慢速经验驱动监控，提升任务执行性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的深度搜索代理在多步检索、推理和长程任务执行方面表现出色，但实际应用中常因缺乏对推理和检索状态的监控与调节机制而失败。受认知神经科学启发，人类元认知是分层组织的，整合快速异常检测和有选择触发的经验驱动反思。

Method: 提出DS-MCM框架，包含两个核心组件：1) 快速一致性监控器：轻量级检查外部证据与内部推理置信度的对齐；2) 慢速经验驱动监控器：选择性激活，基于历史代理轨迹的经验记忆指导纠正干预。将监控直接嵌入推理-检索循环中，确定何时需要干预以及如何基于先验经验采取纠正行动。

Result: 在多个深度搜索基准测试和骨干模型上的实验表明，DS-MCM能持续提升性能和鲁棒性。

Conclusion: DS-MCM通过显式分层元认知监控机制，有效解决了深度搜索代理在不确定性任务执行中的监控和调节问题，显著提升了系统的实用性和可靠性。

Abstract: Deep search agents powered by large language models have demonstrated strong capabilities in multi-step retrieval, reasoning, and long-horizon task execution. However, their practical failures often stem from the lack of mechanisms to monitor and regulate reasoning and retrieval states as tasks evolve under uncertainty. Insights from cognitive neuroscience suggest that human metacognition is hierarchically organized, integrating fast anomaly detection with selectively triggered, experience-driven reflection. In this work, we propose Deep Search with Meta-Cognitive Monitoring (DS-MCM), a deep search framework augmented with an explicit hierarchical metacognitive monitoring mechanism. DS-MCM integrates a Fast Consistency Monitor, which performs lightweight checks on the alignment between external evidence and internal reasoning confidence, and a Slow Experience-Driven Monitor, which is selectively activated to guide corrective intervention based on experience memory from historical agent trajectories. By embedding monitoring directly into the reasoning-retrieval loop, DS-MCM determines both when intervention is warranted and how corrective actions should be informed by prior experience. Experiments across multiple deep search benchmarks and backbone models demonstrate that DS-MCM consistently improves performance and robustness.

</details>


### [58] [Are you going to finish that? A Practical Study of the Tokenization Boundary Problem](https://arxiv.org/abs/2601.23223)
*Hao Xu,Alisa Liu,Jonathan Hayase,Yejin Choi,Noah A. Smith*

Main category: cs.CL

TL;DR: 研究发现语言模型中的分词边界与自然语言单词边界不匹配问题在实际使用中比想象中更严重，导致正确续写概率大幅下降，且问题随模型规模增大而恶化。


<details>
  <summary>Details</summary>
Motivation: 语言模型基于分词序列训练，但用户通过文本交互。这种不匹配导致"部分分词问题"：当用户提示在预期下一个分词的中间结束时，会导致下一个分词预测失真。虽然已有研究关注任意字符前缀，但在尊重单词边界的实际提示中的普遍性和严重性尚未充分探索。

Method: 识别三个分词与"单词"边界常不匹配的领域：不使用空格的语言（如中文）、高度复合语言和代码。系统构建语义自然但以部分分词结尾的提示，在实验中测量正确续写概率的下降程度。评估推理时缓解措施，验证最近精确解决方案的有效性。

Result: 在中文中，高达25%的单词边界与分词边界不对齐，即使自然的、单词完整的提示也易受此问题影响。前沿语言模型在部分分词提示下，正确续写概率比分词对齐提示低三个数量级。这种性能下降不随规模减小，且常随模型增大而恶化。推理时缓解措施被证明有效。

Conclusion: 分词在现实用例中引起的概率失真问题规模大且严重，需要模型推理提供商采取实际措施。研究验证了最近精确解决方案的有效性，并提供了实用的建议。

Abstract: Language models (LMs) are trained over sequences of tokens, whereas users interact with LMs via text. This mismatch gives rise to the partial token problem, which occurs when a user ends their prompt in the middle of the expected next-token, leading to distorted next-token predictions. Although this issue has been studied using arbitrary character prefixes, its prevalence and severity in realistic prompts respecting word boundaries remains underexplored. In this work, we identify three domains where token and "word" boundaries often do not line up: languages that do not use whitespace, highly compounding languages, and code. In Chinese, for example, up to 25% of word boundaries do not line up with token boundaries, making even natural, word-complete prompts susceptible to this problem. We systematically construct semantically natural prompts ending with a partial tokens; in experiments, we find that they comprise a serious failure mode: frontier LMs consistently place three orders of magnitude less probability on the correct continuation compared to when the prompt is "backed-off" to be token-aligned. This degradation does not diminish with scale and often worsens for larger models. Finally, we evaluate inference-time mitigations to the partial token problem and validate the effectiveness of recent exact solutions. Overall, we demonstrate the scale and severity of probability distortion caused by tokenization in realistic use cases, and provide practical recommentions for model inference providers.

</details>


### [59] [Now You Hear Me: Audio Narrative Attacks Against Large Audio-Language Models](https://arxiv.org/abs/2601.23255)
*Ye Yu,Haibo Jin,Yaoning Yu,Jun Zhuang,Haohan Wang*

Main category: cs.CL

TL;DR: 研究通过设计文本到音频的越狱攻击，利用叙事风格的音频流嵌入禁止指令，成功绕过主要针对文本校准的安全机制，在Gemini 2.0 Flash等先进模型上达到98.26%的成功率。


<details>
  <summary>Details</summary>
Motivation: 随着大型音频-语言模型越来越多地处理原始语音输入，这种模态转换引入了新的安全漏洞，但这些漏洞尚未得到充分研究。需要探索语音接口中的安全问题，特别是当安全机制主要针对文本校准时。

Method: 设计了文本到音频的越狱攻击，利用先进的指令跟随文本转语音（TTS）模型，将禁止指令嵌入叙事风格的音频流中。攻击利用了结构和声学特性，绕过主要针对文本的安全机制。

Result: 攻击在Gemini 2.0 Flash等最先进模型上取得了98.26%的成功率，显著超过了纯文本基线的效果。这表明叙事格式的合成语音能够有效引发模型的限制输出。

Conclusion: 研究结果表明，随着语音接口的普及，需要开发能够同时处理语言和副语言表示的安全框架。当前主要针对文本的安全机制在面对音频模态的攻击时存在严重不足。

Abstract: Large audio-language models increasingly operate on raw speech inputs, enabling more seamless integration across domains such as voice assistants, education, and clinical triage. This transition, however, introduces a distinct class of vulnerabilities that remain largely uncharacterized. We examine the security implications of this modality shift by designing a text-to-audio jailbreak that embeds disallowed directives within a narrative-style audio stream. The attack leverages an advanced instruction-following text-to-speech (TTS) model to exploit structural and acoustic properties, thereby circumventing safety mechanisms primarily calibrated for text. When delivered through synthetic speech, the narrative format elicits restricted outputs from state-of-the-art models, including Gemini 2.0 Flash, achieving a 98.26% success rate that substantially exceeds text-only baselines. These results highlight the need for safety frameworks that jointly reason over linguistic and paralinguistic representations, particularly as speech-based interfaces become more prevalent.

</details>


### [60] [PaperBanana: Automating Academic Illustration for AI Scientists](https://arxiv.org/abs/2601.23265)
*Dawei Zhu,Rui Meng,Yale Song,Xiyu Wei,Sujian Li,Tomas Pfister,Jinsung Yoon*

Main category: cs.CL

TL;DR: PaperBanana是一个基于智能体框架的自动化工具，用于生成可直接用于学术出版的插图，显著减轻研究流程中制作高质量插图的工作负担。


<details>
  <summary>Details</summary>
Motivation: 尽管基于语言模型的自主AI科学家发展迅速，但在研究流程中，生成可直接用于出版的插图仍然是一个劳动密集型的瓶颈环节，需要自动化解决方案来减轻研究人员的工作负担。

Method: PaperBanana采用智能体框架，结合最先进的视觉语言模型和图像生成模型，通过专门的智能体协调工作流程：检索参考文献、规划内容和样式、渲染图像，并通过自我批评进行迭代优化。

Result: 实验表明，PaperBanana在忠实性、简洁性、可读性和美观性方面持续优于领先的基线方法。该方法还能有效扩展到高质量统计图表的生成。

Conclusion: PaperBanana为自动化生成可直接用于出版的学术插图铺平了道路，显著提高了研究流程的效率。

Abstract: Despite rapid advances in autonomous AI scientists powered by language models, generating publication-ready illustrations remains a labor-intensive bottleneck in the research workflow. To lift this burden, we introduce PaperBanana, an agentic framework for automated generation of publication-ready academic illustrations. Powered by state-of-the-art VLMs and image generation models, PaperBanana orchestrates specialized agents to retrieve references, plan content and style, render images, and iteratively refine via self-critique. To rigorously evaluate our framework, we introduce PaperBananaBench, comprising 292 test cases for methodology diagrams curated from NeurIPS 2025 publications, covering diverse research domains and illustration styles. Comprehensive experiments demonstrate that PaperBanana consistently outperforms leading baselines in faithfulness, conciseness, readability, and aesthetics. We further show that our method effectively extends to the generation of high-quality statistical plots. Collectively, PaperBanana paves the way for the automated generation of publication-ready illustrations.

</details>


### [61] [UPA: Unsupervised Prompt Agent via Tree-Based Search and Selection](https://arxiv.org/abs/2601.23273)
*Siran Peng,Weisong Zhao,Tianyu Fu,Chenxu Zhao,Tianshuo Zhang,Haoyuan Zhang,Xiangyu Zhu,Minghui Wu,Zhen Lei*

Main category: cs.CL

TL;DR: UPA是一种无监督提示代理，通过结构化搜索和选择优化提示，无需监督反馈信号，在多个任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法通常假设有监督奖励信号，但在实际场景中往往不可得，需要无监督的提示优化方法。

Method: UPA采用两阶段框架：1）构建演化树结构在提示空间导航，基于LLM的细粒度、顺序无关的成对比较；2）基于Bradley-Terry-Luce模型，先进行路径级贝叶斯聚合筛选候选，再进行全局锦标赛式比较推断潜在质量。

Result: 在多个任务上的实验表明，UPA始终优于现有的提示优化方法，证明即使在完全无监督设置下，代理式优化仍然非常有效。

Conclusion: UPA成功实现了无需监督反馈的结构化提示搜索和选择，为实际场景中的提示优化提供了有效解决方案。

Abstract: Prompt agents have recently emerged as a promising paradigm for automated prompt optimization, framing refinement as a sequential decision-making problem over a structured prompt space. While this formulation enables the use of advanced planning algorithms, these methods typically assume access to supervised reward signals, which are often unavailable in practical scenarios. In this work, we propose UPA, an Unsupervised Prompt Agent that realizes structured search and selection without relying on supervised feedback. Specifically, during search, UPA iteratively constructs an evolving tree structure to navigate the prompt space, guided by fine-grained and order-invariant pairwise comparisons from Large Language Models (LLMs). Crucially, as these local comparisons do not inherently yield a consistent global scale, we decouple systematic prompt exploration from final selection, introducing a two-stage framework grounded in the Bradley-Terry-Luce (BTL) model. This framework first performs path-wise Bayesian aggregation of local comparisons to filter candidates under uncertainty, followed by global tournament-style comparisons to infer latent prompt quality and identify the optimal prompt. Experiments across multiple tasks demonstrate that UPA consistently outperforms existing prompt optimization methods, showing that agent-style optimization remains highly effective even in fully unsupervised settings.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [62] [FITMM: Adaptive Frequency-Aware Multimodal Recommendation via Information-Theoretic Representation Learning](https://arxiv.org/abs/2601.22498)
*Wei Yang,Rui Zhong,Yiqun Chen,Shixuan Li,Heng Ping,Chi Lu,Peng Jiang*

Main category: cs.IR

TL;DR: FITMM提出了一种基于频域信息瓶颈理论的多模态推荐框架，通过谱分解分离模态信号，在频带内进行轻量级多模态融合，显著提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有主流多模态推荐系统在空间域融合模态，这会模糊信号的频率结构，放大模态间的错位和冗余问题，需要更有效的融合方法。

Method: 1) 构建图增强的物品表示；2) 对每个模态进行谱分解获得正交频带；3) 在每个频带内构建轻量级多模态组件；4) 使用残差任务自适应门聚合频带；5) 引入频域信息瓶颈正则化控制冗余；6) 使用跨模态谱一致性损失对齐频带内的模态。

Result: 在三个真实世界数据集上的实验表明，FITMM一致且显著优于先进的基线方法。

Conclusion: 通过频域视角和信息瓶颈理论，FITMM提供了多模态推荐中分离-融合的新范式，能够有效处理模态错位和冗余问题，提升推荐性能。

Abstract: Multimodal recommendation aims to enhance user preference modeling by leveraging rich item content such as images and text. Yet dominant systems fuse modalities in the spatial domain, obscuring the frequency structure of signals and amplifying misalignment and redundancy. We adopt a spectral information-theoretic view and show that, under an orthogonal transform that approximately block-diagonalizes bandwise covariances, the Gaussian Information Bottleneck objective decouples across frequency bands, providing a principled basis for separate-then-fuse paradigm. Building on this foundation, we propose FITMM, a Frequency-aware Information-Theoretic framework for multimodal recommendation. FITMM constructs graph-enhanced item representations, performs modality-wise spectral decomposition to obtain orthogonal bands, and forms lightweight within-band multimodal components. A residual, task-adaptive gate aggregates bands into the final representation. To control redundancy and improve generalization, we regularize training with a frequency-domain IB term that allocates capacity across bands (Wiener-like shrinkage with shut-off of weak bands). We further introduce a cross-modal spectral consistency loss that aligns modalities within each band. The model is jointly optimized with the standard recommendation loss. Extensive experiments on three real-world datasets demonstrate that FITMM consistently and significantly outperforms advanced baselines.

</details>


### [63] [SCaLRec: Semantic Calibration for LLM-enabled Cloud-Device Sequential Recommendation](https://arxiv.org/abs/2601.22543)
*Ruiqi Zheng,Jinli Cao,Jiao Yin,Hongzhi Yin*

Main category: cs.IR

TL;DR: SCaLRec通过语义校准解决云语义陈旧性问题，在无法频繁调用云LLM时提升设备端重排序质量


<details>
  <summary>Details</summary>
Motivation: 云设备协同推荐中，云端提供语义用户建模，设备端使用缓存语义嵌入进行隐私保护的重排序。但由于云LLM推理成本高，实际中常重用缓存的语义嵌入，导致云语义陈旧性问题——缓存嵌入与用户最新交互不对齐，造成排序质量下降。

Method: 提出SCaLRec框架：1) 评估缓存语义在用户最新交互下的可靠性；2) 设计设备端语义校准模块，利用最新交互证据调整缓存的语义嵌入，无需每次请求都调用云LLM。

Result: 在真实数据集上的实验表明，SCaLRec在云语义陈旧情况下，相比强基线能持续提升推荐性能。

Conclusion: SCaLRec有效解决了云设备协同推荐中的语义陈旧问题，通过设备端校准机制在保持隐私和响应性的同时提升了推荐质量。

Abstract: Cloud-device collaborative recommendation partitions computation across the cloud and user devices: the cloud provides semantic user modeling, while the device leverages recent interactions and cloud semantic signals for privacy-preserving, responsive reranking. With large language models (LLMs) on the cloud, semantic user representations can improve sequential recommendation by capturing high-level intent. However, regenerating such representations via cloud LLM inference for every request is often infeasible at real-world scale. As a result, on-device reranking commonly reuses a cached cloud semantic user embedding across requests. We empirically identify a cloud semantic staleness effect: reused embeddings become less aligned with the user's latest interactions, leading to measurable ranking degradation.
  Most existing LLM-enabled cloud-device recommenders are typically designed around on-demand cloud semantics, either by assuming low-latency cloud LLM access or by regenerating semantic embeddings per request. When per-request regeneration is infeasible and cached semantics must be reused, two technical challenges arise: (1) deciding when cached cloud semantics remain useful for on-device reranking, and (2) maintaining ranking quality when the cloud LLM cannot be invoked and only cached semantics are available. To address this gap, we introduce the Semantic Calibration for LLM-enabled Cloud-Device Recommendation (SCaLRec). First, it estimates the reliability of cached semantics under the user's latest interactions. Second, an on-device semantic calibration module is proposed to adjusts the cached semantic embedding on-device using up-to-date interaction evidence, without per-request cloud LLM involvement. Experiments on real-world datasets show that SCaLRec consistently improves recommendation performance over strong baselines under cloud semantic staleness.

</details>


### [64] [PersonaAct: Simulating Short-Video Users with Personalized Agents for Counterfactual Filter Bubble Auditing](https://arxiv.org/abs/2601.22547)
*Shilong Zhao,Qinggang Yang,Zhiyi Yin,Xiaoshi Wang,Zhenxing Chen,Du Su,Xueqi Cheng*

Main category: cs.IR

TL;DR: PersonaAct：一个基于多模态智能体模拟短视频用户行为的框架，用于审计推荐系统中的信息茧房现象，在真实行为数据上训练，显著提升了模拟真实性。


<details>
  <summary>Details</summary>
Motivation: 短视频平台的个性化推荐引发了信息茧房担忧，但大规模审计面临成本高、隐私敏感等挑战，现有模拟器因依赖文本信号和弱个性化而无法复现真实用户行为。

Method: 提出PersonaAct框架：1）通过自动访谈结合行为分析和结构化提问合成可解释的用户画像；2）使用监督微调和强化学习在多模态观察上训练智能体；3）部署智能体进行信息茧房审计，从广度和深度两个维度评估。

Result: 1）相比通用LLM基线，PersonaAct在行为复现真实性上有显著提升；2）交互过程中内容多样性显著减少；3）Bilibili平台显示出最强的信息茧房逃逸潜力；4）发布了首个开源的多模态短视频数据集和代码。

Conclusion: PersonaAct能够有效模拟短视频用户行为，为推荐系统审计提供可扩展、隐私保护的方法，揭示了信息茧房的形成机制和平台差异，并开源资源促进可复现的推荐系统审计研究。

Abstract: Short-video platforms rely on personalized recommendation, raising concerns about filter bubbles that narrow content exposure. Auditing such phenomena at scale is challenging because real user studies are costly and privacy-sensitive, and existing simulators fail to reproduce realistic behaviors due to their reliance on textual signals and weak personalization. We propose PersonaAct, a framework for simulating short-video users with persona-conditioned multimodal agents trained on real behavioral traces for auditing filter bubbles in breadth and depth. PersonaAct synthesizes interpretable personas through automated interviews combining behavioral analysis with structured questioning, then trains agents on multimodal observations using supervised fine-tuning and reinforcement learning. We deploy trained agents for filter bubble auditing and evaluate bubble breadth via content diversity and bubble depth via escape potential. The evaluation demonstrates substantial improvements in fidelity over generic LLM baselines, enabling realistic behavior reproduction. Results reveal significant content narrowing over interaction. However, we find that Bilibili demonstrates the strongest escape potential. We release the first open multimodal short-video dataset and code to support reproducible auditing of recommender systems.

</details>


### [65] [Farewell to Item IDs: Unlocking the Scaling Potential of Large Ranking Models via Semantic Tokens](https://arxiv.org/abs/2601.22694)
*Zhen Zhao,Tong Zhang,Jie Xu,Qingliang Cai,Qile Zhang,Leyuan Yang,Daorui Xiao,Xiaojia Chang*

Main category: cs.IR

TL;DR: TRM框架使用语义token替代传统物品ID，在减少稀疏存储的同时提升推荐系统性能，并成功部署于大规模个性化搜索引擎。


<details>
  <summary>Details</summary>
Motivation: 传统大规模排序系统依赖物品ID，将每个物品作为独立分类符号并映射到学习嵌入。随着物品快速出现和消失，这些嵌入难以训练和维护，这种不稳定性阻碍了神经网络参数的有效学习，限制了排序模型的可扩展性。

Method: 提出TRM框架，改进token生成和应用流程，使用语义token替代传统的物品ID。

Result: TRM实现了33%稀疏存储减少和0.85% AUC提升。在模型容量扩展时能持续超越最先进模型。在大规模个性化搜索引擎部署中，通过A/B测试在用户活跃天数和查询变化率上分别获得0.26%和0.75%的改进。

Conclusion: 语义token相比物品ID具有更大的扩展潜力，TRM框架通过改进token生成和应用流程，在减少存储需求的同时提升推荐性能，并已成功应用于实际生产环境。

Abstract: Recent studies on scaling up ranking models have achieved substantial improvement for recommendation systems and search engines. However, most large-scale ranking systems rely on item IDs, where each item is treated as an independent categorical symbol and mapped to a learned embedding. As items rapidly appear and disappear, these embeddings become difficult to train and maintain. This instability impedes effective learning of neural network parameters and limits the scalability of ranking models. In this paper, we show that semantic tokens possess greater scaling potential compared to item IDs. Our proposed framework TRM improves the token generation and application pipeline, leading to 33% reduction in sparse storage while achieving 0.85% AUC increase. Extensive experiments further show that TRM could consistently outperform state-of-the-art models when model capacity scales. Finally, TRM has been successfully deployed on large-scale personalized search engines, yielding 0.26% and 0.75% improvement on user active days and change query ratio respectively through A/B test.

</details>


### [66] [Compact Hypercube Embeddings for Fast Text-based Wildlife Observation Retrieval](https://arxiv.org/abs/2601.22783)
*Ilyass Moummad,Marius Miron,David Robinson,Kawtar Zaher,Hervé Goëau,Olivier Pietquin,Pierre Bonnet,Emmanuel Chemla,Matthieu Geist,Alexis Joly*

Main category: cs.IR

TL;DR: 提出紧凑超立方体嵌入框架，用于基于文本的高效野生动物观测检索，将多模态数据映射到共享汉明空间，显著降低存储和搜索成本。


<details>
  <summary>Details</summary>
Motivation: 大规模生物多样性监测平台依赖多模态野生动物观测，但现有基础模型的高维相似性搜索计算成本过高，难以从海量档案中高效检索相关观测。

Method: 扩展跨视图代码对齐哈希框架，将轻量级哈希从单模态扩展到多模态，在共享汉明空间中对齐自然语言描述与视觉/听觉观测。利用预训练野生动物基础模型，通过参数高效微调进行哈希适配。

Result: 在iNaturalist2024（文本到图像检索）和iNatSounds2024（文本到音频检索）等大规模基准测试中，离散超立方体嵌入相比连续嵌入实现了竞争性甚至更优的性能，同时大幅降低内存和搜索成本。哈希目标持续改进底层编码器表示，增强了检索和零样本泛化能力。

Conclusion: 二进制、基于语言的检索方法为生物多样性监测系统提供了可扩展、高效的大规模野生动物档案搜索方案，证明了离散嵌入在多模态检索中的实用价值。

Abstract: Large-scale biodiversity monitoring platforms increasingly rely on multimodal wildlife observations. While recent foundation models enable rich semantic representations across vision, audio, and language, retrieving relevant observations from massive archives remains challenging due to the computational cost of high-dimensional similarity search. In this work, we introduce compact hypercube embeddings for fast text-based wildlife observation retrieval, a framework that enables efficient text-based search over large-scale wildlife image and audio databases using compact binary representations. Building on the cross-view code alignment hashing framework, we extend lightweight hashing beyond a single-modality setup to align natural language descriptions with visual or acoustic observations in a shared Hamming space. Our approach leverages pretrained wildlife foundation models, including BioCLIP and BioLingual, and adapts them efficiently for hashing using parameter-efficient fine-tuning. We evaluate our method on large-scale benchmarks, including iNaturalist2024 for text-to-image retrieval and iNatSounds2024 for text-to-audio retrieval, as well as multiple soundscape datasets to assess robustness under domain shift. Results show that retrieval using discrete hypercube embeddings achieves competitive, and in several cases superior, performance compared to continuous embeddings, while drastically reducing memory and search cost. Moreover, we observe that the hashing objective consistently improves the underlying encoder representations, leading to stronger retrieval and zero-shot generalization. These results demonstrate that binary, language-based retrieval enables scalable and efficient search over large wildlife archives for biodiversity monitoring systems.

</details>


### [67] [BEAR: Towards Beam-Search-Aware Optimization for Recommendation with Large Language Models](https://arxiv.org/abs/2601.22925)
*Weiqin Yang,Bohao Wang,Zhenxiang Xu,Jiawei Chen,Shengjia Zhang,Jingbang Chen,Canghong Jin,Can Wang*

Main category: cs.IR

TL;DR: 提出BEAR方法，通过beam-search感知的正则化解决LLM推荐中训练与推理的不一致问题


<details>
  <summary>Details</summary>
Motivation: 现有LLM推荐方法使用监督微调，但在推理时使用beam search，存在训练-推理不一致问题：SFT优化正样本的整体概率，但beam search的贪心剪枝机制可能导致前缀概率不足的正样本被过早丢弃

Method: 提出BEAR（Beam-SEarch-Aware Regularization）微调目标，在训练时显式考虑beam search行为。不直接模拟beam search（计算成本高），而是强制一个松弛的必要条件：正样本的每个token在每个解码步骤中必须排名在前B个候选token内

Result: 在四个真实世界数据集上的大量实验表明，BEAR显著优于强基线方法

Conclusion: BEAR有效缓解了LLM推荐中的训练-推理不一致问题，在几乎不增加计算开销的情况下显著提升了推荐性能

Abstract: Recent years have witnessed a rapid surge in research leveraging Large Language Models (LLMs) for recommendation. These methods typically employ supervised fine-tuning (SFT) to adapt LLMs to recommendation scenarios, and utilize beam search during inference to efficiently retrieve $B$ top-ranked recommended items. However, we identify a critical training-inference inconsistency: while SFT optimizes the overall probability of positive items, it does not guarantee that such items will be retrieved by beam search even if they possess high overall probabilities. Due to the greedy pruning mechanism, beam search can prematurely discard a positive item once its prefix probability is insufficient.
  To address this inconsistency, we propose BEAR (Beam-SEarch-Aware Regularization), a novel fine-tuning objective that explicitly accounts for beam search behavior during training. Rather than directly simulating beam search for each instance during training, which is computationally prohibitive, BEAR enforces a relaxed necessary condition: each token in a positive item must rank within the top-$B$ candidate tokens at each decoding step. This objective effectively mitigates the risk of incorrect pruning while incurring negligible computational overhead compared to standard SFT. Extensive experiments across four real-world datasets demonstrate that BEAR significantly outperforms strong baselines. Code will be released upon acceptance.

</details>


### [68] [OrLog: Resolving Complex Queries with LLMs and Probabilistic Reasoning](https://arxiv.org/abs/2601.23085)
*Mohanna Hoveyda,Jelle Piepenbrock,Arjen P de Vries,Maarten de Rijke,Faegheh Hasibi*

Main category: cs.IR

TL;DR: OrLog是一个神经符号检索框架，通过将谓词级合理性估计与逻辑推理解耦，使用LLM提供原子谓词的合理性分数，然后通过概率推理引擎计算查询满足的后验概率，实现约束感知检索。


<details>
  <summary>Details</summary>
Motivation: 现有检索系统要么忽略查询中的逻辑约束（与、或、非），要么通过生成式推理近似处理，这可能导致不一致和不可靠的结果。而现有的神经符号方法虽然适合结构化推理，但主要局限于形式逻辑或数学问题，假设查询无歧义且能获得完整证据，这在信息检索中很少满足。

Method: OrLog采用两阶段方法：1）使用大型语言模型（LLM）通过一次无解码前向传递为原子谓词提供合理性分数；2）将这些分数输入概率推理引擎，计算查询满足的后验概率。该方法将谓词级合理性估计与逻辑推理解耦。

Result: 在多个骨干LLM、不同外部知识访问级别和各种逻辑约束下的评估显示：1）提供实体描述时，OrLog显著提升顶级精度，尤其在析取查询上优势更大；2）效率更高，每个查询-实体对平均减少约90%的token使用；3）优于基础检索器和LLM-as-reasoner方法。

Conclusion: 无生成的谓词合理性估计结合概率推理能够实现约束感知检索，在优于单一推理方法的同时显著减少计算资源消耗，为处理复杂信息需求中的逻辑约束提供了有效解决方案。

Abstract: Resolving complex information needs that come with multiple constraints should consider enforcing the logical operators encoded in the query (i.e., conjunction, disjunction, negation) on the candidate answer set. Current retrieval systems either ignore these constraints in neural embeddings or approximate them in a generative reasoning process that can be inconsistent and unreliable. Although well-suited to structured reasoning, existing neuro-symbolic approaches remain confined to formal logic or mathematics problems as they often assume unambiguous queries and access to complete evidence, conditions rarely met in information retrieval. To bridge this gap, we introduce OrLog, a neuro-symbolic retrieval framework that decouples predicate-level plausibility estimation from logical reasoning: a large language model (LLM) provides plausibility scores for atomic predicates in one decoding-free forward pass, from which a probabilistic reasoning engine derives the posterior probability of query satisfaction. We evaluate OrLog across multiple backbone LLMs, varying levels of access to external knowledge, and a range of logical constraints, and compare it against base retrievers and LLM-as-reasoner methods. Provided with entity descriptions, OrLog can significantly boost top-rank precision compared to LLM reasoning with larger gains on disjunctive queries. OrLog is also more efficient, cutting mean tokens by $\sim$90\% per query-entity pair. These results demonstrate that generation-free predicate plausibility estimation combined with probabilistic reasoning enables constraint-aware retrieval that outperforms monolithic reasoning while using far fewer tokens.

</details>
