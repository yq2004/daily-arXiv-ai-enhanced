<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Auxiliary Metrics Help Decoding Skill Neurons in the Wild](https://arxiv.org/abs/2511.21610)
*Yixiu Zhao,Xiaozhi Wang,Zijun Yao,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 提出了一种简单轻量的方法来识别大语言模型中编码特定技能的神经元，通过将神经元激活与外部标签和模型置信度等辅助指标关联，在文本生成和推理任务中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在各种任务中表现出色，但其内部机制仍然不透明，需要开发方法来理解模型如何编码特定技能。

Method: 基于先前通过软提示训练识别技能神经元的工作，扩展到复杂多技能场景，将神经元激活与外部标签和模型置信度等辅助指标相关联，无需手动标记聚合。

Result: 在开放文本生成和自然语言推理任务上验证了该方法，不仅能检测驱动已知技能的神经元，还发现了BigBench算术推理中以前未识别的捷径。

Conclusion: 该方法提供了一种简单有效的途径来理解大语言模型的内部工作机制，有助于揭示模型技能编码和推理策略。

Abstract: Large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, yet their internal mechanisms remain largely opaque. In this paper, we introduce a simple, lightweight, and broadly applicable method with a focus on isolating neurons that encode specific skills. Building upon prior work that identified "skill neurons" via soft prompt training on classification tasks, our approach extends the analysis to complex scenarios involving multiple skills. We correlate neuron activations with auxiliary metrics -- such as external labels and the model's own confidence score -- thereby uncovering interpretable and task-specific behaviors without the need for manual token aggregation. We empirically validate our method on tasks spanning open-ended text generation and natural language inference, demonstrating its ability to detect neurons that not only drive known skills but also reveal previously unidentified shortcuts in arithmetic reasoning on BigBench.

</details>


### [2] [Beyond URLs: Metadata Diversity and Position for Efficient LLM Pretraining](https://arxiv.org/abs/2511.21613)
*Dongyang Fan,Diba Hashemi,Sai Praneeth Karimireddy,Martin Jaggi*

Main category: cs.CL

TL;DR: 研究发现除URL外，其他元数据类型（如文档质量指标）也能加速LLM预训练，有效元数据的共同特征是细粒度信息编码。提出了元数据追加和可学习元令牌两种方法，通过分析潜在表征揭示了元数据如何塑造学习过程。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅发现URL对加速LLM预训练有效，但其他元数据类型是否具有更大潜力尚不明确。本研究旨在探索更广泛的元数据类型及其对预训练效率的影响。

Method: 1) 调查多种元数据类型在预训练中的效果；2) 引入元数据追加方法，将预测适当元数据作为辅助任务；3) 使用掩码损失训练可学习元令牌；4) 通过探针分析潜在表征。

Result: 发现细粒度的文档质量指标等元数据也能加速预训练，有效元数据的共同特征是细粒度信息编码。元数据追加和可学习元令牌方法都能提高训练效率，后者通过诱导质量感知的潜在结构恢复部分加速效果。

Conclusion: 研究为整合元数据以提升LLM预训练效率和效果提供了实用指南，证明了多种元数据类型的价值，并揭示了元数据通过塑造潜在表征来影响学习过程的机制。

Abstract: Incorporating metadata in Large Language Models (LLMs) pretraining has recently emerged as a promising approach to accelerate training. However prior work highlighted only one useful signal-URLs, leaving open the question of whether other forms of metadata could yield greater benefits. In this study, we investigate a wider range of metadata types and find other types of metadata, such as fine-grained indicators of document quality that can also accelerate pretraining when prepended. We identify a common feature among effective metadata: they encode information at a finer granularity. We further introduce metadata appending as a means of improving training efficiency, where predicting an appropriate metadata as auxiliary task can help speed up pretraining. In addition, learnable meta-tokens trained with masked loss can recover part of the speedup by inducing quality-aware latent structure. Using probing, we analyze latent representations to understand how metadata shapes learning. Together, these results yield practical guidelines for integrating metadata to improve both the efficiency and effectiveness of LLM pretraining.

</details>


### [3] [The author is dead, but what if they never lived? A reception experiment on Czech AI- and human-authored poetry](https://arxiv.org/abs/2511.21629)
*Anna Marklová,Ondřej Vinš,Martina Vokáčová,Jiří Milička*

Main category: cs.CL

TL;DR: 捷克语AI生成诗歌与人类诗歌难以区分，参与者识别准确率仅45.8%，且审美评价存在作者偏见——当认为诗歌是AI生成时评分更低，尽管AI诗歌实际评分与人类诗歌相当或更高。


<details>
  <summary>Details</summary>
Motivation: 大多数关于AI生成诗歌的研究集中在英语上，而本研究旨在探讨捷克语（一种形态复杂、训练数据较少的斯拉夫语言）中AI生成诗歌的感知和审美评价。

Method: 让捷克母语者识别和审美评价AI生成与人类创作的诗歌，使用逻辑回归模型分析识别准确率与审美评价的关系。

Result: 参与者识别准确率接近随机水平（45.8%），表明AI诗歌难以与人类诗歌区分。审美评价显示强烈的作者偏见——当认为诗歌是AI生成时评分更低，尽管AI诗歌实际评分与人类诗歌相当或更高。逻辑回归显示诗歌越受欢迎，准确识别作者的可能性越低。

Conclusion: 即使在形态复杂、训练数据较少的捷克语中，AI也能生成令人信服的诗歌。读者的作者信念与诗歌审美评价密切相关，存在明显的作者偏见。

Abstract: Large language models are increasingly capable of producing creative texts, yet most studies on AI-generated poetry focus on English -- a language that dominates training data. In this paper, we examine the perception of AI- and human-written Czech poetry. We ask if Czech native speakers are able to identify it and how they aesthetically judge it. Participants performed at chance level when guessing authorship (45.8\% correct on average), indicating that Czech AI-generated poems were largely indistinguishable from human-written ones. Aesthetic evaluations revealed a strong authorship bias: when participants believed a poem was AI-generated, they rated it as less favorably, even though AI poems were in fact rated equally or more favorably than human ones on average. The logistic regression model uncovered that the more the people liked a poem, the less probable was that they accurately assign the authorship. Familiarity with poetry or literary background had no effect on recognition accuracy. Our findings show that AI can convincingly produce poetry even in a morphologically complex, low-resource (with respect of the training data of AI models) Slavic language such as Czech. The results suggest that readers' beliefs about authorship and the aesthetic evaluation of the poem are interconnected.

</details>


### [4] [Matrix: Peer-to-Peer Multi-Agent Synthetic Data Generation Framework](https://arxiv.org/abs/2511.21686)
*Dong Wang,Yang Li,Ansong Ni,Ching-Feng Yeh,Youssef Emad,Xinjie Lei,Liam Robbins,Karthik Padthe,Hu Xu,Xian Li,Asli Celikyilmaz,Ramya Raghavendra,Lifei Huang,Carole-Jean Wu,Shang-Wen Li*

Main category: cs.CL

TL;DR: Matrix是一个去中心化的多智能体合成数据生成框架，通过分布式队列传递序列化消息来替代中央编排器，在相同硬件资源下实现2-15倍的数据生成吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体合成框架依赖中央编排器导致可扩展性瓶颈，或针对特定领域硬编码限制了灵活性。

Method: 基于Ray构建的去中心化框架，将控制和数据流表示为通过分布式队列传递的序列化消息，轻量级智能体独立处理任务，计算密集型操作由分布式服务处理。

Result: 在多种合成场景（多智能体协作对话、基于网络的推理数据提取、客户服务环境中的工具使用轨迹生成）中，Matrix在相同硬件资源下实现2-15倍的数据生成吞吐量提升，且不损害输出质量。

Conclusion: Matrix提供了一个模块化、可配置的去中心化框架，能够轻松适应广泛的数据生成工作流，显著提升了多智能体合成数据生成的效率和可扩展性。

Abstract: Synthetic data has become increasingly important for training large language models, especially when real data is scarce, expensive, or privacy-sensitive. Many such generation tasks require coordinated multi-agent workflows, where specialized agents collaborate to produce data that is higher quality, more diverse, and structurally richer. However, existing frameworks for multi-agent synthesis often depend on a centralized orchestrator, creating scalability bottlenecks, or are hardcoded for specific domains, limiting flexibility. We present \textbf{Matrix}, a decentralized framework that represents both control and data flow as serialized messages passed through distributed queues. This peer-to-peer design eliminates the central orchestrator. Each task progresses independently through lightweight agents, while compute-intensive operations, such as LLM inference or containerized environments, are handled by distributed services. Built on Ray, Matrix scales to tens of thousands of concurrent agentic workflows and provides a modular, configurable design that enables easy adaptation to a wide range of data generation workflows. We evaluate Matrix across diverse synthesis scenarios, such as multi-agent collaborative dialogue, web-based reasoning data extraction, and tool-use trajectory generation in customer service environments. In all cases, Matrix achieves $2$--$15\times$ higher data generation throughput under identical hardware resources, without compromising output quality.

</details>


### [5] [ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration](https://arxiv.org/abs/2511.21689)
*Hongjin Su,Shizhe Diao,Ximing Lu,Mingjie Liu,Jiacheng Xu,Xin Dong,Yonggan Fu,Peter Belcak,Hanrong Ye,Hongxu Yin,Yi Dong,Evelina Bakhturina,Tao Yu,Yejin Choi,Jan Kautz,Pavlo Molchanov*

Main category: cs.CL

TL;DR: ToolOrchestra训练的小型编排器Orchestrator（8B参数）在解决复杂任务时，以更低的成本超越GPT-5等大型模型，实现了性能与效率的最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然功能强大，但在解决像Humanity's Last Exam这样的深度复杂问题时，仍面临概念挑战和计算成本高昂的问题。需要更高效的方法来协调智能工具。

Method: 提出ToolOrchestra方法，使用强化学习训练小型编排器，奖励机制考虑结果质量、效率和用户偏好，协调多种智能工具。

Result: Orchestrator在HLE上获得37.1%的分数，超越GPT-5（35.1%）且效率提高2.5倍；在tau2-Bench和FRAMES上大幅超越GPT-5，仅使用约30%的成本。

Conclusion: 通过轻量级编排模型组合多样化工具，比现有方法更高效有效，为实用和可扩展的工具增强推理系统铺平了道路。

Abstract: Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.

</details>


### [6] [Revisiting Generalization Across Difficulty Levels: It's Not So Easy](https://arxiv.org/abs/2511.21692)
*Yeganeh Kordi,Nihal V. Nayak,Max Zuo,Ilana Nguyen,Stephen H. Bach*

Main category: cs.CL

TL;DR: LLMs在跨难度泛化方面存在局限，训练使用简单或困难数据都无法在所有难度范围内实现一致改进，表明训练和评估数据需要包含不同难度级别。


<details>
  <summary>Details</summary>
Motivation: 现有研究对于训练使用简单还是困难数据效果更好存在争议，且不清楚这些改进是否体现在简单还是困难的测试数据上。

Method: 使用数千个不同LLM的输出和项目反应理论(IRT)对六个数据集中的示例进行难度排序，仅基于LLM能力确定难度评级，排除人类主观判断。

Result: 跨难度泛化通常有限，训练使用简单或困难数据都无法在所有难度范围内实现一致改进。

Conclusion: LLM训练和评估数据需要包含不同难度级别，在难度方面走捷径是有风险的。

Abstract: We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation. Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address this question by conducting a systematic evaluation of LLMs' generalization across models, datasets, and fine-grained groups of example difficulty. We rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory (IRT), a well-established difficulty metric in educational testing. Unlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs, excluding human opinions of difficulty. With a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties. These results show the importance of having a range of difficulties in both training and evaluation data for LLMs, and that taking shortcuts with respect to difficulty is risky.

</details>
