<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 78]
- [cs.IR](#cs.IR) [Total: 18]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [HybridRAG: A Practical LLM-based ChatBot Framework based on Pre-Generated Q&A over Raw Unstructured Documents](https://arxiv.org/abs/2602.11156)
*Sungmoon Kim,Hyuna Jeon,Dahye Kim,Mingyu Kim,Dong-Kyu Chae,Jiwoong Kim*

Main category: cs.CL

TL;DR: HybridRAG 是一个结合预生成QA知识库和按需生成的RAG框架，用于处理非结构化PDF文档，提高聊天机器人响应质量和速度。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法通常假设结构化文本源，并在查询时进行检索和生成，这在处理现实世界非结构化文档和大量用户时存在适用性限制。

Method: 通过OCR和布局分析处理非结构化PDF文档，将其转换为分层文本块，然后使用LLM预生成QA知识库。查询时优先从QA库匹配答案，无匹配时才进行实时生成。

Result: 在OHRBench上的实验表明，HybridRAG相比标准RAG基线提供了更高的答案质量和更低的延迟。

Conclusion: HybridRAG是处理大量非结构化文档和有限计算资源下实际聊天机器人应用的实用解决方案。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful approach for grounding Large Language Model (LLM)-based chatbot responses on external knowledge. However, existing RAG studies typically assume well-structured textual sources (e.g. Wikipedia or curated datasets) and perform retrieval and generation at query time, which can limit their applicability in real-world chatbot scenarios. In this paper, we present HybridRAG, a novel and practical RAG framework towards more accurate and faster chatbot responses. First, HybridRAG ingests raw, unstructured PDF documents containing complex layouts (text, tables, figures) via Optical Character Recognition (OCR) and layout analysis, and convert them into hierarchical text chunks. Then, it pre-generates a plausible question-answer (QA) knowledge base from the organized chunks using an LLM. At query time, user questions are matched against this QA bank to retrieve immediate answers when possible, and only if no suitable QA match is found does our framework fall back to an on-the-fly response generation. Experiments on OHRBench demonstrate that our HybridRAG provides higher answer quality and lower latency compared to a standard RAG baseline. We believe that HybridRAG could be a practical solution for real-world chatbot applications that must handle large volumes of unstructured documents and lots of users under limited computational resources.

</details>


### [2] [Response-Based Knowledge Distillation for Multilingual Jailbreak Prevention Unwittingly Compromises Safety](https://arxiv.org/abs/2602.11157)
*Max Zhang,Derek Liu,Kai Zhang,Joshua Franco,Haihao Liu*

Main category: cs.CL

TL;DR: 该研究探索了知识蒸馏在提升多语言大模型安全性方面的应用，发现使用教师模型的安全拒绝数据进行标准微调反而会增加学生模型的越狱成功率，揭示了多语言安全对齐的复杂性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的安全对齐主要针对英语，在非英语特别是低资源语言环境下存在安全漏洞。需要研究如何将安全行为有效迁移到多语言场景。

Method: 使用约28,000个多语言越狱提示，通过黑盒响应式参数高效微调（PEFT）将OpenAI o1-mini教师模型的拒绝行为蒸馏到三个开源学生模型（Meta-Llama-3-8B-Instruct、Gemma-2-2B-IT、Qwen3-8B）中。

Result: 反直觉发现：使用教师模型的安全拒绝数据进行标准微调反而使所有学生模型的越狱成功率提高达16.6个百分点。通过移除"边界拒绝"这种安全降级的主要来源，可以减轻或逆转安全性下降，但推理性能（GSM8K）仍会降低。

Conclusion: 知识蒸馏作为多语言安全对齐技术面临挑战，但仍有潜力。研究为未来方向奠定了基础，强调需要更精细的安全对齐方法。

Abstract: Large language models (LLMs) are increasingly deployed worldwide, yet their safety alignment remains predominantly English-centric. This allows for vulnerabilities in non-English contexts, especially with low-resource languages. We introduce a novel application of knowledge distillation (KD) in the context of multilingual jailbreak prevention, examining its efficacy. We distill the refusal behaviors of a proprietary teacher model (OpenAI o1-mini) with Low-Rank Adaptation (LoRA) into three open-source student models: Meta-Llama-3-8B-Instruct, Gemma-2-2B-IT, and Qwen3-8B, using ~28,000 multilingual jailbreak prompts from XSafety via black-box response-based, parameter-efficient fine-tuning (PEFT). Evaluation on the MultiJail benchmark reveals a counterintuitive behavior: standard fine-tuning on the teacher's ``safe'' refusal data inadvertently increases Jailbreak Success Rate (JSR) for all student models, up to 16.6 percentage points. Our experiments reveal a divergent generalization to unseen languages during distillation, with varying outcomes depending on the base model. By removing a primary source of safety degradation, nuanced `boundary' refusals, we mitigate or even reverse safety declines in student models, although reductions in reasoning performance (GSM8K) persist. Overall, our exploratory study highlights the challenges and potential of KD as a technique for multilingual safety alignment, offering a foundation for future research in this direction.

</details>


### [3] [Retrieval Heads are Dynamic](https://arxiv.org/abs/2602.11162)
*Yuping Lin,Zitao Li,Yue Xing,Pengfei He,Yingqian Cui,Yaliang Li,Bolin Ding,Jingren Zhou,Jiliang Tang*

Main category: cs.CL

TL;DR: 该论文从动态视角研究LLM中的检索头，发现其具有时间动态性、不可替代性和相关性，揭示了LLM内部规划机制


<details>
  <summary>Details</summary>
Motivation: 现有研究主要基于静态统计识别LLM中的检索头，忽略了自回归生成中的细粒度时间动态性，需要从动态视角深入理解检索头的工作机制

Method: 采用动态分析方法，在Needle-in-a-Haystack任务和多跳问答任务上进行广泛分析，并在动态检索增强生成框架中量化动态和静态检索头的效用差异

Result: 确立了三个核心发现：检索头随时间步动态变化；动态检索头具有时间步特异性且无法被静态检索头有效替代；模型隐藏状态编码了预测未来检索头模式的信号

Conclusion: 研究为LLM内部机制提供了新见解，揭示了动态检索头的重要性和模型内部的规划机制，对理解LLM工作方式有重要意义

Abstract: Recent studies have identified "retrieval heads" in Large Language Models (LLMs) responsible for extracting information from input contexts. However, prior works largely rely on static statistics aggregated across datasets, identifying heads that perform retrieval on average. This perspective overlooks the fine-grained temporal dynamics of autoregressive generation. In this paper, we investigate retrieval heads from a dynamic perspective. Through extensive analysis, we establish three core claims: (1) Dynamism: Retrieval heads vary dynamically across timesteps; (2) Irreplaceability: Dynamic retrieval heads are specific at each timestep and cannot be effectively replaced by static retrieval heads; and (3) Correlation: The model's hidden state encodes a predictive signal for future retrieval head patterns, indicating an internal planning mechanism. We validate these findings on the Needle-in-a-Haystack task and a multi-hop QA task, and quantify the differences on the utility of dynamic and static retrieval heads in a Dynamic Retrieval-Augmented Generation framework. Our study provides new insights into the internal mechanisms of LLMs.

</details>


### [4] [Nested Named Entity Recognition in Plasma Physics Research Articles](https://arxiv.org/abs/2602.11163)
*Muhammad Haris,Hans Höft,Markus M. Becker,Markus Stocker*

Main category: cs.CL

TL;DR: 提出一种基于编码器-Transformer和条件随机场的轻量级方法，用于从等离子体物理论文中提取嵌套命名实体，通过特定实体模型专业化方法和超参数优化提升性能。


<details>
  <summary>Details</summary>
Motivation: 等离子体物理研究论文包含高度复杂且上下文丰富的内容，需要提取其中的关键实体以支持高级搜索等功能。目前缺乏专门针对该领域科学文本的命名实体识别方法。

Method: 1) 标注包含16个类别的等离子体物理语料库用于嵌套NER任务；2) 采用实体特定模型专业化方法，训练独立的BERT-CRF模型识别单个实体类型；3) 集成优化过程系统调整超参数以提升模型性能。

Result: 开发了一个专门针对等离子体物理领域的命名实体识别系统，能够有效提取科学文本中的嵌套实体，为研究人员导航和分析科学文献提供支持。

Conclusion: 该工作推进了等离子体物理领域的实体识别技术，为支持研究人员探索和分析科学文献奠定了基础，展示了特定领域NER方法在复杂科学文本处理中的有效性。

Abstract: Named Entity Recognition (NER) is an important task in natural language processing that aims to identify and extract key entities from unstructured text. We present a novel application of NER in plasma physics research articles and address the challenges of extracting specialized entities from scientific text in this domain. Research articles in plasma physics often contain highly complex and context-rich content that must be extracted to enable, e.g., advanced search. We propose a lightweight approach based on encoder-transformers and conditional random fields to extract (nested) named entities from plasma physics research articles. First, we annotate a plasma physics corpus with 16 classes specifically designed for the nested NER task. Second, we evaluate an entity-specific model specialization approach, where independent BERT-CRF models are trained to recognize individual entity types in plasma physics text. Third, we integrate an optimization process to systematically fine-tune hyperparameters and enhance model performance. Our work contributes to the advancement of entity recognition in plasma physics and also provides a foundation to support researchers in navigating and analyzing scientific literature.

</details>


### [5] [Assessing LLM Reliability on Temporally Recent Open-Domain Questions](https://arxiv.org/abs/2602.11165)
*Pushwitha Krishnappa,Amit Das,Vinija Jain,Tathagata Mukherjee,Aman Chadha*

Main category: cs.CL

TL;DR: LLMs在回答近期Reddit问题时展现出语义-词汇悖论：语义相似度极高（>99%余弦相似度）但词汇重叠度极低（<8% BLEU-1），表明模型通过广泛改写而非词汇复制来保留意义，挑战了词汇指标在评估抽象生成中的可靠性。


<details>
  <summary>Details</summary>
Motivation: LLM在开放域问答中应用日益广泛，但其对近期信息的回答与人类视角的一致性尚未充分探索。需要评估LLM在回答近期Reddit问题时与社区共识的对应关系。

Method: 构建RECOM基准数据集（15,000个2025年9月的Reddit问题及社区答案），评估4个开源LLM（Llama3.1-8B、Mistral-7B、Gemma-2-9B、GPT-OSS-20B）。使用词汇指标（BLEU、ROUGE）、语义相似度（BERTScore、MoverScore、余弦相似度）和逻辑推理（NLI）进行多维度评估。

Result: 1. 发现语义-词汇悖论：所有模型与参考答案的余弦相似度>99%，但BLEU-1重叠度<8%，差距达90+百分点。2. MoverScore（51-53%）处于中间位置，反映语义对齐的最优传输成本。3. 模型规模不预测性能：Mistral-7B（7B参数）在所有指标上优于GPT-OSS-20B（20B参数）。4. NLI分析显示矛盾率<7%，模型很少生成与人类共识直接冲突的内容。

Conclusion: 词汇指标在评估抽象生成时不可靠，需要多维评估框架来捕捉超越表层文本匹配的语义保真度。RECOM数据集公开可用，为评估LLM与人类近期信息视角对齐提供了基准。

Abstract: Large Language Models (LLMs) are increasingly deployed for open-domain question answering, yet their alignment with human perspectives on temporally recent information remains underexplored. We introduce RECOM (Reddit Evaluation for Correspondence of Models), a benchmark dataset of 15,000 recent Reddit questions from September 2025 paired with community-derived reference answers. We investigate how four open-source LLMs (Llama3.1-8B, Mistral-7B, Gemma-2-9B, and GPT-OSS-20B) respond to these questions, evaluating alignment using lexical metrics (BLEU, ROUGE), semantic similarity (BERTScore, MoverScore, cosine similarity), and logical inference (NLI). Our central finding is a striking semantic-lexical paradox: all models achieve over 99% cosine similarity with references despite less than 8% BLEU-1 overlap, a 90+ percentage point gap indicating that models preserve meaning through extensive paraphrasing rather than lexical reproduction. MoverScore (51-53%) confirms this pattern, occupying an intermediate position that reflects the optimal transport cost of semantic alignment. Furthermore, model scale does not predict performance: Mistral-7B (7B parameters) outperforms GPT-OSS-20B (20B parameters) across all metrics. NLI analysis reveals that contradiction rates remain below 7%, suggesting models rarely generate content that directly conflicts with human consensus. These findings challenge the reliability of lexical metrics for evaluating abstractive generation and argue for multi-dimensional evaluation frameworks that capture semantic fidelity beyond surface-level text matching. The RECOM dataset is publicly available at https://anonymous.4open.science/r/recom-D4B0

</details>


### [6] [Small Updates, Big Doubts: Does Parameter-Efficient Fine-tuning Enhance Hallucination Detection ?](https://arxiv.org/abs/2602.11166)
*Xu Hu,Yifan Zhang,Songtao Wei,Chen Zhao,Qiannan Li,Bingzhe Li,Feng Chen*

Main category: cs.CL

TL;DR: PEFT方法能增强幻觉检测能力，主要通过重塑不确定性编码而非注入新事实知识


<details>
  <summary>Details</summary>
Motivation: 尽管参数高效微调方法被广泛用于适应下游任务，但其对幻觉行为的影响，特别是在QA数据集上的影响，尚未得到充分理解

Method: 在三个开源LLM骨干和三个事实寻求QA基准上，使用七种无监督幻觉检测方法（语义一致性、置信度、熵三种互补方法）进行系统性评估

Result: PEFT持续增强幻觉检测能力，显著提高多种检测器的AUROC；线性探针和表示诊断分析表明PEFT主要重塑不确定性的编码和表现方式，而非向模型注入新事实知识

Conclusion: PEFT方法通过重塑不确定性编码机制来改善幻觉检测，而不是通过增加事实知识，这为理解PEFT如何影响模型可靠性提供了新见解

Abstract: Parameter-efficient fine-tuning (PEFT) methods are widely used to adapt large language models (LLMs) to downstream tasks and are often assumed to improve factual correctness. However, how the parameter-efficient fine-tuning methods affect hallucination behavior remains insufficiently understood, especially on QA datasets. In this work, we systematically investigate the impact of PEFT on hallucination detection through a comprehensive empirical study across three open-weight LLM backbones and three fact-seeking QA benchmarks. For each model, we evaluate performance using seven unsupervised hallucination detection methods spanning three complementary approaches: semantic consistency based detectors, confidence based detectors, and entropy based detectors. This multifaceted evaluation enables us to characterize how PEFT reshapes uncertainty across different detection paradigms. In conclusion, our experimental results show that PEFT consistently strengthens hallucination detection ability, substantially improving AUROC across a wide range of hallucination detectors. Besides, further analyses using linear probes and representation diagnostics indicate that PEFT methods primarily reshapes how uncertainty is encoded and surfaced, comparing with injecting new factual knowledge into the models.

</details>


### [7] [Visualizing and Benchmarking LLM Factual Hallucination Tendencies via Internal State Analysis and Clustering](https://arxiv.org/abs/2602.11167)
*Nathan Mao,Varun Kaushik,Shreya Shivkumar,Parham Sharafoleslami,Kevin Zhu,Sunishchal Dev*

Main category: cs.CL

TL;DR: FalseCite是一个专门用于捕捉和评估LLM在误导性引用下产生幻觉的数据集，通过实验发现虚假引用会显著增加LLM的幻觉率，尤其是GPT-4o-mini，并通过隐藏状态分析揭示了幻觉与非幻觉状态在向量空间中形成独特的"角状"结构。


<details>
  <summary>Details</summary>
Motivation: LLM经常产生幻觉，生成无意义或虚假信息，这在医学或法律等敏感领域尤其有害。为了系统研究这一现象，需要专门的工具来捕捉和评估由误导性或伪造引用引发的幻觉。

Method: 引入了FalseCite数据集，专门设计用于捕捉和基准测试由误导性或伪造引用引发的幻觉响应。使用GPT-4o-mini、Falcon-7B和Mistral 7-B在FalseCite上进行测试，分析幻觉活动。通过模型响应分析隐藏状态向量，进行可视化和聚类分析。

Result: 虚假引用显著增加了LLM的幻觉活动，特别是在GPT-4o-mini中表现最为明显。隐藏状态向量分析显示，无论是否产生幻觉，隐藏状态向量都倾向于形成一个独特的"角状"形状。FalseCite展示了作为未来LLM研究中评估和减轻幻觉的基础工具的潜力。

Conclusion: FalseCite为系统研究LLM幻觉提供了一个有价值的工具，特别是在虚假引用诱导的幻觉方面。实验结果表明虚假引用会加剧幻觉，而隐藏状态分析揭示了幻觉模式的结构特征。这项工作为未来LLM幻觉的评估和缓解研究奠定了基础。

Abstract: Large Language Models (LLMs) often hallucinate, generating nonsensical or false information that can be especially harmful in sensitive fields such as medicine or law. To study this phenomenon systematically, we introduce FalseCite, a curated dataset designed to capture and benchmark hallucinated responses induced by misleading or fabricated citations. Running GPT-4o-mini, Falcon-7B, and Mistral 7-B through FalseCite, we observed a noticeable increase in hallucination activity for false claims with deceptive citations, especially in GPT-4o-mini. Using the responses from FalseCite, we can also analyze the internal states of hallucinating models, visualizing and clustering the hidden state vectors. From this analysis, we noticed that the hidden state vectors, regardless of hallucination or non-hallucination, tend to trace out a distinct horn-like shape. Our work underscores FalseCite's potential as a foundation for evaluating and mitigating hallucinations in future LLM research.

</details>


### [8] [Enhancing SDG-Text Classification with Combinatorial Fusion Analysis and Generative AI](https://arxiv.org/abs/2602.11168)
*Jingyan Xu,Marcelo L. LaFleur,Christina Schweikert,D. Frank Hsu*

Main category: cs.CL

TL;DR: 本文使用组合融合分析(CFA)结合多个模型来增强对联合国可持续发展目标(SDG)的文本分类，通过生成式AI合成训练数据，CFA达到96.73%的准确率，优于单个最佳模型，并与人类专家结果进行了对比。


<details>
  <summary>Details</summary>
Motivation: 在文本分类中，当类别不可用、难以区分或相互关联时，分类仍具有挑战性。社会分析领域大量依赖文本数据，需要更好的分类方法。本文旨在通过结合多个模型的智能来增强对联合国可持续发展目标(SDG)的文本分类。

Method: 使用生成式AI模型生成合成数据用于模型训练，然后应用组合融合分析(CFA)技术。CFA使用秩-得分特征(RSC)函数和认知多样性(CD)来结合一组相对较好且相互多样化的分类模型。

Result: CFA技术在SDG文本分类任务中达到了96.73%的性能表现，优于最佳个体模型。通过与人类领域专家的结果对比，证明结合多个ML/AI模型的智能与人类专家输入可以相互补充和增强。

Conclusion: 通过组合融合分析(CFA)结合多个模型的智能，并结合人类专家输入，可以有效增强文本分类性能，特别是在联合国可持续发展目标(SDG)分类等复杂任务中。这种方法不仅相互补充，还能相互增强。

Abstract: (Natural Language Processing) NLP techniques such as text classification and topic discovery are very useful in many application areas including information retrieval, knowledge discovery, policy formulation, and decision-making. However, it remains a challenging problem in cases where the categories are unavailable, difficult to differentiate, or are interrelated. Social analysis with human context is an area that can benefit from text classification, as it relies substantially on text data. The focus of this paper is to enhance the classification of text according to the UN's Sustainable Development Goals (SDGs) by collecting and combining intelligence from multiple models. Combinatorial Fusion Analysis (CFA), a system fusion paradigm using a rank-score characteristic (RSC) function and cognitive diversity (CD), has been used to enhance classifier methods by combining a set of relatively good and mutually diverse classification models. We use a generative AI model to generate synthetic data for model training and then apply CFA to this classification task. The CFA technique achieves 96.73% performance, outperforming the best individual model. We compare the outcomes with those obtained from human domain experts. It is demonstrated that combining intelligence from multiple ML/AI models using CFA and getting input from human experts can, not only complement, but also enhance each other.

</details>


### [9] [Disentangling Direction and Magnitude in Transformer Representations: A Double Dissociation Through L2-Matched Perturbation Analysis](https://arxiv.org/abs/2602.11169)
*Mangadoddi Srikar Vardhan,Lekkala Sai Teja*

Main category: cs.CL

TL;DR: 该研究发现Transformer隐藏状态中向量方向和幅度在语言建模与句法处理中扮演不同角色：角度扰动对语言建模损失影响更大，幅度扰动对句法处理影响更大


<details>
  <summary>Details</summary>
Motivation: Transformer隐藏状态编码信息为高维向量，但向量方向（角度）和幅度（范数）是否具有不同功能角色尚不清楚。研究旨在探索这两个维度在Transformer架构中的不同计算作用

Method: 使用L2匹配扰动分析，确保角度和幅度扰动产生相同的欧几里得位移。在Pythia系列模型上进行因果干预实验，分别修复注意力通路和LayerNorm通路来观察损失恢复情况

Result: 角度扰动对语言建模损失造成更大破坏（最多42.9倍），而幅度扰动对句法处理造成更大破坏（主谓一致任务准确率下降20.4% vs 1.6%）。角度损伤主要通过注意力通路传播（28.4%损失可通过注意力修复恢复），幅度损伤部分通过LayerNorm通路传播（29.9%可通过LayerNorm修复恢复）。这种模式在Pythia架构家族中跨尺度复现

Conclusion: 向量方向和幅度在LayerNorm架构中支持部分不同的计算角色：方向优先影响注意力路由，幅度调节精细句法判断的处理强度。RMSNorm架构显示不同模式，表明这种分离依赖于架构选择。研究结果细化了线性表示假说，对模型编辑和可解释性研究有重要意义

Abstract: Transformer hidden states encode information as high-dimensional vectors, yet whether direction (orientation in representational space) and magnitude (vector norm) serve distinct functional roles remains unclear. Studying Pythia-family models, we discover a striking cross-over dissociation: angular perturbations cause up to 42.9 more damage to language modeling loss, while magnitude perturbations cause disproportionately more damage to syntactic processing (20.4% vs.1.6% accuracy drop on subject-verb agreement).This finding is enabled by L2-matched perturbation analysis, a methodology ensuring that an gular and magnitude perturbations achieve identical Euclidean displacements. Causal intervention reveals that angular damage flows substantially through the attention pathways (28.4% loss recovery via attention repair), while magnitude damage flows partly through the LayerNorm pathways(29.9% recovery via LayerNorm repair). These patterns replicate across scales within the Pythia architecture family. These findings provide evidence that direction and magnitude support partially distinct computational roles in LayerNorm based architectures. The direction preferentially affects attentional routing, while magnitude modulates processing intensity for fine-grained syntactic judgments. We find different patterns in RMSNorm-based architectures, suggesting that the dissociation depends on architectural choices. Our results refine the linear representation hypothesis and have implications for model editing and interpretability research

</details>


### [10] [PRIME: Policy-Reinforced Iterative Multi-agent Execution for Algorithmic Reasoning in Large Language Models](https://arxiv.org/abs/2602.11170)
*Jiawei Xu,Zhenyu Yu,Ziqian Bi,Minh Duc Pham,Xiaoyi Qu,Danyang Zhang*

Main category: cs.CL

TL;DR: PRIME框架通过三个专门代理（执行、验证、协调）和群体相对策略优化，将大语言模型的算法推理准确率从26.8%提升至93.8%，并在PRIME-Bench基准上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多样化推理任务中表现出色，但在算法推理方面能力有限，需要专门的方法来提升其算法执行和状态跟踪能力。

Method: 提出PRIME框架，包含三个专门代理：执行器（逐步推理）、验证器（约束检查）、协调器（回溯控制），通过群体相对策略优化进行训练。

Result: 在PRIME-Bench基准（86个任务，51,600个实例）上，PRIME将平均准确率从26.8%提升至93.8%（相对提升250%）。在需要持续状态跟踪的任务上提升最显著，如图灵机模拟从9%到92%，长除法从16%到94%。

Conclusion: PRIME框架显著提升了LLM的算法推理能力，特别是通过迭代验证防止错误传播。较小模型也能获得不成比例的大幅提升，达到与8倍大模型相当的准确率。

Abstract: Large language models have demonstrated remarkable capabilities across diverse reasoning tasks, yet their performance on algorithmic reasoning remains limited. To handle this limitation, we propose PRIME (Policy-Reinforced Iterative Multi-agent Execution), a framework comprising three specialized agents, an executor for step-by-step reasoning, a verifier for constraint checking, and a coordinator for backtracking control, optimized through group relative policy optimization. For comprehensive evaluation, we introduce PRIME-Bench, the largest algorithmic reasoning benchmark to date, comprising 86 tasks across 12 categories with 51,600 instances. Tasks span sorting algorithms, graph and tree structures, automata and state machines, symbolic reasoning, and constraint-based puzzles, with execution traces reaching over one million steps. Compared to baseline approach, PRIME improves average accuracy from 26.8% to 93.8%, a 250% relative gain. The largest improvements occur on tasks requiring sustained state tracking, with Turing machine simulation improving from 9% to 92% and long division from 16% to 94%. Ablation studies identify iterative verification as the primary contributor, preventing the error propagation that causes baseline approaches to fail catastrophically. Analysis across model scales (8B-120B parameters) reveals that smaller models benefit disproportionately, achieving accuracy comparable to models 8x larger.

</details>


### [11] [Efficient Hyper-Parameter Search for LoRA via Language-aided Bayesian Optimization](https://arxiv.org/abs/2602.11171)
*Baek Seong-Eun,Lee Jung-Mok,Kim Sung-Bin,Tae-Hyun Oh*

Main category: cs.CL

TL;DR: 提出一个结合预训练大语言模型领域知识和贝叶斯优化的框架，高效搜索LoRA超参数，仅需约30次迭代即可找到比标准方法性能提升20%以上的超参数。


<details>
  <summary>Details</summary>
Motivation: LoRA虽然能实现资源高效的大型语言模型微调，但对超参数选择高度敏感，而穷举搜索计算成本高昂。需要一种更高效的方法来搜索LoRA超参数。

Method: 1. 将大语言模型重新用作离散到连续的映射，将超参数及其领域知识链接到连续向量空间进行贝叶斯优化；2. 通过语言提示设计和控制映射，注入LoRA领域知识；3. 使用可学习令牌建模难以用语言描述的残差信息；4. 引入基于数据子集的代理训练和评估，利用完整数据集和子集训练性能之间的强相关性。

Result: 仅需约30次迭代就能找到比标准方法（约45,000种组合）性能提升20%以上的超参数。代理训练进一步提高了方法效率。

Conclusion: 该框架通过将大语言模型的领域知识整合到贝叶斯优化中，显著提高了LoRA超参数搜索的效率，仅需少量迭代就能找到高性能的超参数配置。

Abstract: Fine-tuning Large Language Models (LLMs) with Low-Rank Adaptation (LoRA) enables resource-efficient personalization or specialization, but it comes at the expense of additional hyperparameter tuning. Although LoRA makes fine-tuning efficient, it is highly sensitive to the choice of hyperparameters, and exhaustive hyperparameter search is still computationally very demanding. To address these challenges, we propose a framework that integrates the domain knowledge of pre-trained LLMs into Bayesian Optimization (BO) to efficiently search for LoRA hyperparameters. To leverage the informed knowledge of LLMs, we repurpose LLMs as a discrete-to-continuous mapping to link the hyperparameters and their domain knowledge with a continuous vector space, where BO is conducted. We design and control the mapping by language prompting, where we provide a domain-aware textual prompt describing the relationships among hyperparameters and their respective roles; thereby, we explicitly inject domain knowledge about LoRA into the LLM in natural language. Also, we model the residual information that is hard to linguistically describe in the prompt with an additional learnable token. This aids BO to sample more high-performing hyperparameters. In addition, by leveraging the observation of the strong correlation between the respective performance obtained from full and subset training datasets in LoRA training regimes, we introduce proxy training and evaluation with a data subset. This further increases the efficiency of our method. We demonstrate that our hyperparameter found with only about 30 iterations achieves more than 20% performance improvement over standard hyperparameters found from about 45,000 combinations.

</details>


### [12] [Synthesizing the Virtual Advocate: A Multi-Persona Speech Generation Framework for Diverse Linguistic Jurisdictions in Indic Languages](https://arxiv.org/abs/2602.11172)
*Aniket Deroy*

Main category: cs.CL

TL;DR: 研究评估了Gemini 2.5 Flash和Pro TTS模型在五种印度语言中生成法庭演讲的表现，发现模型在处理程序性信息时表现出色，但在情感表达和说服力方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 法律辩护需要权威语调、节奏停顿和情感智能的独特结合。随着LLMs的发展，TTS技术从基本可理解性转向上下文感知的表达性合成。在印度多语言环境中，合成语音需要传达权威和专业形象，这一任务变得尤为复杂。

Method: 提出一个提示框架，利用Gemini 2.5原生支持五种语言和上下文感知节奏能力，生成不同的辩护律师角色。在泰米尔语、泰卢固语、孟加拉语、印地语和古吉拉特语五种印度语言中评估模型表现。

Result: 模型表现出"单调权威"，在程序性信息传递方面表现出色，但在动态声音调制和情感深度方面存在困难，这些是说服性辩护所必需的。在孟加拉语和古吉拉特语中表现下降，突显了语音学方面的改进空间。

Conclusion: 多语言TTS已准备好用于程序性法律任务，但在复制人类法律话语的说服艺术方面仍面临挑战。研究为未来改进指明了方向，特别是在情感表达和语言特定优化方面。

Abstract: Legal advocacy requires a unique combination of authoritative tone, rhythmic pausing for emphasis, and emotional intelligence. This study investigates the performance of the Gemini 2.5 Flash TTS and Gemini 2.5 Pro TTS models in generating synthetic courtroom speeches across five Indic languages: Tamil, Telugu, Bengali, Hindi, and Gujarati. We propose a prompting framework that utilizes Gemini 2.5s native support for 5 languages and its context-aware pacing to produce distinct advocate personas. The evolution of Large Language Models (LLMs) has shifted the focus of TexttoSpeech (TTS) technology from basic intelligibility to context-aware, expressive synthesis. In the legal domain, synthetic speech must convey authority and a specific professional persona a task that becomes significantly more complex in the linguistically diverse landscape of India. The models exhibit a "monotone authority," excelling at procedural information delivery but struggling with the dynamic vocal modulation and emotive gravitas required for persuasive advocacy. Performance dips in Bengali and Gujarati further highlight phonological frontiers for future refinement. This research underscores the readiness of multilingual TTS for procedural legal tasks while identifying the remaining challenges in replicating the persuasive artistry of human legal discourse. The code is available at-https://github.com/naturenurtureelite/Synthesizing-the-Virtual-Advocate/tree/main

</details>


### [13] [Author-in-the-Loop Response Generation and Evaluation: Integrating Author Expertise and Intent in Responses to Peer Review](https://arxiv.org/abs/2602.11173)
*Qian Ruan,Iryna Gurevych*

Main category: cs.CL

TL;DR: REspGen是一个作者参与循环的论文回复生成框架，通过整合作者输入、多属性控制和评估引导的优化来提升同行评审中的回复质量。


<details>
  <summary>Details</summary>
Motivation: 现有研究将作者回复生成视为自动文本生成任务，忽略了作者的专业知识、意图和修订策略等关键信号。实践中作者需要整合这些信号来有效应对审稿人的关切。

Method: 提出REspGen框架，整合显式作者输入、多属性控制和评估引导的优化；构建Re$^3$Align数据集（首个大规模对齐的评审-回复-修订三元组数据集）；开发REspEval评估套件（20+个指标覆盖输入利用、可控性、回复质量和语篇分析）。

Result: 实验显示：作者输入和评估引导的优化能显著提升回复质量；输入设计对回复质量有重要影响；可控性和质量之间存在权衡关系。

Conclusion: 将作者回复生成重新定义为作者参与循环的任务，并通过REspGen框架、Re$^3$Align数据集和REspEval评估套件为这一任务提供了全面的解决方案，公开了所有资源。

Abstract: Author response (rebuttal) writing is a critical stage of scientific peer review that demands substantial author effort. Recent work frames this task as automatic text generation, underusing author expertise and intent. In practice, authors possess domain expertise, author-only information, revision and response strategies--concrete forms of author expertise and intent--to address reviewer concerns, and seek NLP assistance that integrates these signals to support effective response writing in peer review. We reformulate author response generation as an author-in-the-loop task and introduce REspGen, a generation framework that integrates explicit author input, multi-attribute control, and evaluation-guided refinement, together with REspEval, a comprehensive evaluation suite with 20+ metrics covering input utilization, controllability, response quality, and discourse. To support this formulation, we construct Re$^3$Align, the first large-scale dataset of aligned review--response--revision triplets, where revisions provide signals of author expertise and intent. Experiments with state-of-the-art LLMs show the benefits of author input and evaluation-guided refinement, the impact of input design on response quality, and trade-offs between controllability and quality. We make our dataset, generation and evaluation tools publicly available.

</details>


### [14] [The Script Tax: Measuring Tokenization-Driven Efficiency and Latency Disparities in Multilingual Language Models](https://arxiv.org/abs/2602.11174)
*Aradhya Dixit,Shreem Dixit*

Main category: cs.CL

TL;DR: 该研究量化了预训练多语言模型中不同书写系统面临的"脚本税"，发现高碎片化书写系统导致分词数增加3.4倍、推理速度下降16.5倍、信息成本显著上升。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练多语言模型被认为是脚本无关的，但其分词器可能对某些书写系统施加系统性成本。研究者希望量化这种"脚本税"并揭示其影响。

Method: 通过比较具有相同语言内容但不同正字法变体的文本，分析mBERT和XLM-R模型中的分词效率差异。使用比特每字符(BPC)指标避免子词碎片化导致的"NLL悖论"，并进行往返转换检查。

Result: 高碎片化正字法导致分词数增加约3.4倍(6.73-6.85 vs 2.10-2.35 tokens/word)，推理速度下降16.5倍(0.23 vs 3.8 sentences/second)。信息成本显著增加：mBERT上升19.7%(8.06->9.65)，XLM-R上升47.1%(12.19->17.94)。往返转换检查(CER_rt=0.31)表明这些差异反映了正字法条件处理而非映射噪声。

Conclusion: 分词是多语言NLP中不平等现象的关键来源，需要开发脚本感知的分词和预训练方法来解决这一系统性偏差。

Abstract: Pretrained multilingual language models are often assumed to be script-agnostic, yet their tokenizers can impose systematic costs on certain writing systems. We quantify this script tax by comparing two orthographic variants with identical linguistic content. Across mBERT and XLM-R, the higher-fragmentation orthography shows a ~3.4x increase in fertility (6.73-6.85 vs. 2.10-2.35 tokens/word), leading to a 16.5x inference slowdown (0.23 vs. 3.8 sentences/second) on identical hardware. Using bits per character (BPC) to avoid the "NLL paradox" from subword fragmentation, we find a substantial increase in information cost: +19.7% for mBERT (8.06->9.65) and +47.1% for XLM-R (12.19->17.94). A round-trip conversion check (CER_rt=0.31) suggests these gaps reflect orthography-conditioned processing rather than mapping noise. Our results highlight tokenization as a key source of inequity in multilingual NLP and motivate script-aware tokenization and pretraining.

</details>


### [15] [Barriers to Discrete Reasoning with Transformers: A Survey Across Depth, Exactness, and Bandwidth](https://arxiv.org/abs/2602.11175)
*Michelle Yuan,Weiyi Sun,Amir H. Rezaeian,Jyotika Singh,Sandip Ghoshal,Yao-Ting Wang,Miguel Ballesteros,Yassine Benajiba*

Main category: cs.CL

TL;DR: 该综述论文从电路复杂性、近似理论和通信复杂性三个理论视角，系统分析了Transformer在离散推理任务（如算术、逻辑推理）中的理论局限性。


<details>
  <summary>Details</summary>
Motivation: Transformer已成为序列建模的基础架构，在自然语言处理、视觉等领域取得了最先进的性能。然而，其在离散推理任务（如算术、逻辑推理、算法组合）中的理论局限性仍然是一个关键的开放性问题，需要系统分析。

Method: 通过综合三个理论视角的研究：1) 电路复杂性分析结构计算障碍；2) 近似理论研究不连续函数的逼近困难；3) 通信复杂性分析token间通信瓶颈。连接这些理论框架，提供统一的理论解释。

Result: 论文阐明了Transformer在精确实现离散算法时面临的结构和计算障碍，包括深度约束、难以逼近不连续性、token间通信瓶颈等挑战。虽然Transformer在模式匹配和插值方面表现出色，但在精确符号计算方面存在根本性限制。

Conclusion: 该综述为理解Transformer在离散推理任务中的理论局限性提供了统一框架，指出了模型设计的启示，并提出了克服这些基础限制的潜在研究方向。

Abstract: Transformers have become the foundational architecture for a broad spectrum of sequence modeling applications, underpinning state-of-the-art systems in natural language processing, vision, and beyond. However, their theoretical limitations in discrete reasoning tasks, such as arithmetic, logical inference, and algorithmic composition, remain a critical open problem. In this survey, we synthesize recent studies from three theoretical perspectives: circuit complexity, approximation theory, and communication complexity, to clarify the structural and computational barriers that transformers face when performing symbolic computations. By connecting these established theoretical frameworks, we provide an accessible and unified account of why current transformer architectures struggle to implement exact discrete algorithms, even as they excel at pattern matching and interpolation. We review key definitions, seminal results, and illustrative examples, highlighting challenges such as depth constraints, difficulty approximating discontinuities, and bottlenecks in inter-token communication. Finally, we discuss implications for model design and suggest promising directions for overcoming these foundational limitations.

</details>


### [16] [Evaluating Few-Shot Temporal Reasoning of LLMs for Human Activity Prediction in Smart Environments](https://arxiv.org/abs/2602.11176)
*Maral Doctorarastoo,Katherine A. Flanigan,Mario Bergés,Christopher McComb*

Main category: cs.CL

TL;DR: LLMs通过检索增强提示策略，能够基于有限上下文线索预测人类日常活动和持续时间，在少样本条件下表现出强大的时间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的智能体模型在低数据环境下表现不佳，限制了其在智能家居、人机协作等应用中的实用性。需要探索预训练大语言模型是否能够通过推理人类日常活动来填补这一空白。

Method: 采用检索增强提示策略，整合时间、空间、行为历史和人物角色四种上下文来源，在CASAS Aruba智能家居数据集上进行评估。评估包括两个任务：带持续时间估计的下一个活动预测，以及多步骤日常序列生成，每种任务都使用不同数量的少样本示例进行测试。

Result: 大语言模型展现出对人类行为的强大内在时间理解能力：即使在零样本设置下也能产生连贯的日常活动预测，添加一两个演示示例可进一步改善持续时间校准和分类准确性。超过几个示例后性能趋于饱和，表明边际效益递减。序列级评估确认了不同少样本条件下的一致时间对齐。

Conclusion: 预训练语言模型可以作为有前景的时间推理器，能够捕捉重复性日常规律和上下文依赖的行为变化，从而增强基于智能体的模型的行为模块。

Abstract: Anticipating human activities and their durations is essential in applications such as smart-home automation, simulation-based architectural and urban design, activity-based transportation system simulation, and human-robot collaboration, where adaptive systems must respond to human activities. Existing data-driven agent-based models--from rule-based to deep learning--struggle in low-data environments, limiting their practicality. This paper investigates whether large language models, pre-trained on broad human knowledge, can fill this gap by reasoning about everyday activities from compact contextual cues. We adopt a retrieval-augmented prompting strategy that integrates four sources of context--temporal, spatial, behavioral history, and persona--and evaluate it on the CASAS Aruba smart-home dataset. The evaluation spans two complementary tasks: next-activity prediction with duration estimation, and multi-step daily sequence generation, each tested with various numbers of few-shot examples provided in the prompt. Analyzing few-shot effects reveals how much contextual supervision is sufficient to balance data efficiency and predictive accuracy, particularly in low-data environments. Results show that large language models exhibit strong inherent temporal understanding of human behavior: even in zero-shot settings, they produce coherent daily activity predictions, while adding one or two demonstrations further refines duration calibration and categorical accuracy. Beyond a few examples, performance saturates, indicating diminishing returns. Sequence-level evaluation confirms consistent temporal alignment across few-shot conditions. These findings suggest that pre-trained language models can serve as promising temporal reasoners, capturing both recurring routines and context-dependent behavioral variations, thereby strengthening the behavioral modules of agent-based models.

</details>


### [17] [What Do LLMs Know About Alzheimer's Disease? Fine-Tuning, Probing, and Data Synthesis for AD Detection](https://arxiv.org/abs/2602.11177)
*Lei Jiang,Yue Zhou,Natalie Parde*

Main category: cs.CL

TL;DR: 论文探索使用大语言模型进行阿尔茨海默病检测，通过微调LLM并分析其内部表示，设计任务感知的特殊标记来生成高质量的合成数据。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病的早期检测具有挑战性，主要由于标记数据有限。虽然大语言模型在不同领域展现出强大的迁移能力，但通过监督微调将其适应于AD领域的研究仍很少。

Method: 1. 微调LLM用于AD检测；2. 使用探针技术分析transformer层的中间激活；3. 基于分析结果设计任务感知的特殊标记；4. 训练序列到序列模型作为数据合成工具，利用这些标记生成结构一致且诊断信息丰富的合成样本。

Result: 微调后，特定词语和特殊标记的探针值发生显著变化，表明这些元素在模型改进的检测性能中扮演关键角色。设计的任务感知标记能够生成高质量的合成数据。

Conclusion: 通过分析LLM内部表示，可以识别对AD检测任务重要的元素，并利用这些见解设计有效的合成数据生成方法，为数据稀缺的AD检测任务提供解决方案。

Abstract: Reliable early detection of Alzheimer's disease (AD) is challenging, particularly due to limited availability of labeled data. While large language models (LLMs) have shown strong transfer capabilities across domains, adapting them to the AD domain through supervised fine-tuning remains largely unexplored. In this work, we fine-tune an LLM for AD detection and investigate how task-relevant information is encoded within its internal representations. We employ probing techniques to analyze intermediate activations across transformer layers, and we observe that, after fine-tuning, the probing values of specific words and special markers change substantially, indicating that these elements assume a crucial role in the model's improved detection performance. Guided by this insight, we design a curated set of task-aware special markers and train a sequence-to-sequence model as a data-synthesis tool that leverages these markers to generate structurally consistent and diagnostically informative synthetic samples. We evaluate the synthesized data both intrinsically and by incorporating it into downstream training pipelines.

</details>


### [18] [From Instruction to Output: The Role of Prompting in Modern NLG](https://arxiv.org/abs/2602.11179)
*Munazza Zaib,Elaf Alhazmi*

Main category: cs.CL

TL;DR: 本文对提示工程在自然语言生成领域的进展进行了系统综述，提出了分类框架、决策模型和设计-优化-评估框架。


<details>
  <summary>Details</summary>
Motivation: 尽管提示工程已成为提升大型语言模型性能的重要技术，但在自然语言生成领域仍缺乏系统的框架和统一的理解，特别是对各种提示工程方法和技术的结构化认识不足。

Method: 通过文献综述方法，总结近期提示工程的发展，将其视为输入层面的控制机制，并提出提示范式的分类法、基于不同因素的提示选择决策框架。

Result: 建立了提示工程的分类体系，提出了适用于实践者的决策框架，分析了新兴趋势和挑战，并提出了连接设计、优化和评估的框架来支持更可控和可泛化的自然语言生成。

Conclusion: 提示工程作为输入层面的控制机制，与微调和解码方法相辅相成，需要系统化的框架来指导实践，以实现更可控和可泛化的自然语言生成。

Abstract: Prompt engineering has emerged as an integral technique for extending the strengths and abilities of Large Language Models (LLMs) to gain significant performance gains in various Natural Language Processing (NLP) tasks. This approach, which requires instructions to be composed in natural language to bring out the knowledge from LLMs in a structured way, has driven breakthroughs in various NLP tasks. Yet there is still no structured framework or coherent understanding of the varied prompt engineering methods and techniques, particularly in the field of Natural Language Generation (NLG).
  This survey aims to help fill that gap by outlining recent developments in prompt engineering, and their effect on different NLG tasks. It reviews recent advances in prompting methods and their impact on NLG tasks, presenting prompt design as an input-level control mechanism that complements fine-tuning and decoding approaches. The paper introduces a taxonomy of prompting paradigms, a decision framework for prompt selection based on varying factors for the practitioners, outlines emerging trends and challenges, and proposes a framework that links design, optimization, and evaluation to support more controllable and generalizable NLG.

</details>


### [19] [Mechanistic Interpretability for Large Language Model Alignment: Progress, Challenges, and Future Directions](https://arxiv.org/abs/2602.11180)
*Usman Naseem*

Main category: cs.CL

TL;DR: 本文系统综述了用于LLM对齐的机制可解释性技术，分析现有方法如何为对齐策略提供见解，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型展现出强大能力，但其内部决策过程仍不透明，机制可解释性研究对于理解和对齐这些模型至关重要。

Method: 通过综述文献，分析电路发现、特征可视化、激活引导、因果干预等机制可解释性技术，以及它们如何为RLHF、宪法AI、可扩展监督等对齐策略提供见解。

Result: 识别了机制可解释性在LLM对齐中的关键挑战，包括叠加假设、神经元多义性以及大规模模型中涌现行为的解释困难。

Conclusion: 提出了未来研究方向，包括自动化可解释性、电路的跨模型泛化，以及能够扩展到前沿模型的可解释性驱动的对齐技术。

Abstract: Large language models (LLMs) have achieved remarkable capabilities across diverse tasks, yet their internal decision-making processes remain largely opaque. Mechanistic interpretability (i.e., the systematic study of how neural networks implement algorithms through their learned representations and computational structures) has emerged as a critical research direction for understanding and aligning these models. This paper surveys recent progress in mechanistic interpretability techniques applied to LLM alignment, examining methods ranging from circuit discovery to feature visualization, activation steering, and causal intervention. We analyze how interpretability insights have informed alignment strategies including reinforcement learning from human feedback (RLHF), constitutional AI, and scalable oversight. Key challenges are identified, including the superposition hypothesis, polysemanticity of neurons, and the difficulty of interpreting emergent behaviors in large-scale models. We propose future research directions focusing on automated interpretability, cross-model generalization of circuits, and the development of interpretability-driven alignment techniques that can scale to frontier models.

</details>


### [20] [Code Mixologist : A Practitioner's Guide to Building Code-Mixed LLMs](https://arxiv.org/abs/2602.11181)
*Himanshu Gupta,Pratik Jayarao,Chaitanya Dwivedi,Neeraj Varshney*

Main category: cs.CL

TL;DR: 该论文全面综述了现代大语言模型中语码转换的研究现状，提出了统一的分类体系，并提供了构建、适应和评估语码转换能力大语言模型的实用指南。


<details>
  <summary>Details</summary>
Motivation: 尽管多语言建模取得了进展，但大语言模型在混合语言环境中仍面临挑战，在语法性、事实性和安全性方面表现出系统性退化。需要系统研究语码转换现象及其对LLMs的影响。

Method: 提出了一个统一的分类体系，从数据、建模和评估三个维度组织现有研究；回顾了从语码转换定制预训练、任务特定后训练到提示策略和上下文学习等多种建模方法；分析了当前评估实践的稳定性和可重复性问题。

Result: 总结了现有基准测试并批判性考察了其语言覆盖范围和英语中心偏见；讨论了新兴的安全问题，包括使用语码转换作为绕过模型安全防护的机制；识别了开放的研究挑战。

Conclusion: 该论文为语码转换研究提供了全面的概述和实用指南，强调了在构建、适应和评估语码转换能力大语言模型时需要考虑的关键因素，并指出了未来研究方向。

Abstract: Code-mixing and code-switching (CSW) remain challenging phenomena for large language models (LLMs). Despite recent advances in multilingual modeling, LLMs often struggle in mixed-language settings, exhibiting systematic degradation in grammaticality, factuality, and safety behavior. This work provides a comprehensive overview of CSW research in modern large language model settings. We introduce a unifying taxonomy that organizes prior work along dimensions of data, modeling, and evaluation, and we distill these findings into a practical playbook of actionable recommendations for building, adapting, and evaluating CSW-capable LLMs. We review modeling approaches ranging from CSW-tailored pre-training and task-specific post-training to prompting strategies and in-context learning. We analyze current evaluation practices, highlighting sources of instability and limited reproducibility, and we catalog existing benchmarks while critically examining their linguistic coverage and English-centric biases. Finally, we discuss emerging safety concerns, including use of code-mixing as a mechanism for bypassing model safeguards, and identify open research challenges.

</details>


### [21] [MetaMem: Evolving Meta-Memory for Knowledge Utilization through Self-Reflective Symbolic Optimization](https://arxiv.org/abs/2602.11182)
*Haidong Xin,Xinze Li,Zhenghao Liu,Yukun Yan,Shuo Wang,Cheng Yang,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.CL

TL;DR: MetaMem是一个通过自演化元记忆增强LLM记忆系统的框架，通过跨任务知识利用经验的迭代蒸馏来指导LLM有效利用记忆知识，显著提升长时人机交互性能。


<details>
  <summary>Details</summary>
Motivation: 现有记忆系统虽然能让LLM支持长时人机交互，但往往会破坏交互会话的内在逻辑和时间关系，导致记忆单元碎片化和推理性能下降。需要一种方法教会LLM如何有效利用记忆知识。

Method: 提出MetaMem框架，通过自演化元记忆增强记忆系统。在元记忆优化过程中，通过自我反思推理过程和执行动作更新当前元记忆状态，迭代蒸馏跨任务的可转移知识利用经验。积累的元记忆单元作为显式知识利用经验，指导LLM系统性地从分散的记忆片段中识别和整合关键证据。

Result: 大量实验证明MetaMem的有效性，显著优于强基线方法超过3.6%。所有代码和数据集已开源。

Conclusion: MetaMem通过自演化元记忆有效解决了现有记忆系统破坏逻辑关系导致记忆碎片化的问题，显著提升了LLM在长时人机交互中的推理性能，为记忆系统的优化提供了新思路。

Abstract: Existing memory systems enable Large Language Models (LLMs) to support long-horizon human-LLM interactions by persisting historical interactions beyond limited context windows. However, while recent approaches have succeeded in constructing effective memories, they often disrupt the inherent logical and temporal relationships within interaction sessions, resulting in fragmented memory units and degraded reasoning performance. In this paper, we propose MetaMem, a novel framework that augments memory systems with a self-evolving meta-memory, aiming to teach LLMs how to effectively utilize memorized knowledge. During meta-memory optimization, MetaMem iteratively distills transferable knowledge utilization experiences across different tasks by self-reflecting on reasoning processes and performing actions to update the current meta-memory state. The accumulated meta-memory units serve as explicit knowledge utilization experiences, guiding the LLM to systematically identify and integrate critical evidence from scattered memory fragments. Extensive experiments demonstrate the effectiveness of MetaMem, which significantly outperforms strong baselines by over 3.6%. All codes and datasets are available at https://github.com/OpenBMB/MetaMem.

</details>


### [22] [DDL2PropBank Agent: Benchmarking Multi-Agent Frameworks' Developer Experience Through a Novel Relational Schema Mapping Task](https://arxiv.org/abs/2602.11198)
*Shafiuddin Rehan Ahmed,Wei Wei*

Main category: cs.CL

TL;DR: 提出DDL2PropBank基准任务来评估多智能体框架的开发者体验，通过静态分析和AI辅助性两个维度比较10个框架，发现Agno在复杂度和性能上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 多智能体框架虽然能简化LLM驱动的软件开发，但缺乏在受控环境中评估其开发者体验的原则性方法。需要一种系统性的评估方式来比较不同框架的实际开发体验。

Method: 提出DDL2PropBank基准任务，将关系数据库模式映射到PropBank角色集，需要自主检索候选框架并进行细粒度语言推理。采用Agent-as-a-Tool模式，在10个框架中实现相同的智能体逻辑，通过静态分析评估代码复杂度，并通过AI辅助性评估LLM自主生成正确框架特定代码的能力。

Result: 结果显示复杂度存在三倍差异，Pydantic AI和Agno实现开销最小。在AI辅助性方面，结构对齐分数能可靠预测具有单一规范模式的框架的运行时成功率，但对多模式框架会高估正确性。Agno综合表现最强，具有最低复杂度、最高结构对齐和83%的pass@1成功率。

Conclusion: DDL2PropBank提供了一个有效的基准任务来评估多智能体框架的开发者体验。Agno在复杂度和AI辅助性方面表现最佳，为框架选择提供了实证依据。结构对齐分数是评估AI辅助性的有用指标，但对多模式框架需要谨慎解释。

Abstract: Multi-agent frameworks promise to simplify LLM-driven software development, yet there is no principled way to evaluate their developer experience in a controlled setting. We introduce DDL2PropBank, a novel benchmark task that maps relational database schemas to PropBank rolesets, requiring autonomous retrieval of candidate frames and fine-grained linguistic reasoning over table names, columns, and relations. Using the Agent-as-a-Tool pattern, we implement identical agent logic across 10 frameworks and evaluate along two dimensions: (i) code complexity via static analysis, and (ii) AI-assistability -- the extent to which LLMs can autonomously generate correct, framework-specific code. Our results reveal a threefold complexity spectrum, with Pydantic AI and Agno requiring the least implementation overhead. For AI-assistability, structural alignment scores reliably proxy runtime success for frameworks with single canonical patterns, but overestimate correctness for multi-pattern frameworks. Agno emerges as the strongest overall performer, combining lowest complexity with highest structural alignment and 83% pass@1.

</details>


### [23] [When and What to Ask: AskBench and Rubric-Guided RLVR for LLM Clarification](https://arxiv.org/abs/2602.11199)
*Jiale Zhao,Ke Fang,Lu Cheng*

Main category: cs.CL

TL;DR: AskBench是一个交互式基准测试，用于评估和提升LLMs在需要澄清时的提问能力，包含AskMind和AskOverconfidence两种设置，并提出RLVR方法进行改进。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常在提示信息缺失或包含误导性细节时仍然回应，导致产生幻觉或强化错误观念。需要评估和改进LLMs在何时以及如何请求澄清的能力，同时不牺牲任务性能。

Method: 提出AskBench交互式基准测试，将标准QA对转换为具有明确检查点的多轮交互。包含AskMind（意图不明确需要澄清）和AskOverconfidence（包含错误前提需要识别和纠正）两种设置。提出基于准则的强化学习与验证器奖励（RLVR）方法，使用结构化准则鼓励有针对性的澄清。

Result: 实验显示在准确性、准则遵循度和交互效率方面都有持续改进，并且在未见领域具有强大的泛化能力。

Conclusion: AskBench框架和RLVR方法有效提升了LLMs在需要澄清时的决策能力，有助于减少幻觉和错误信息传播，同时保持任务性能。

Abstract: Large language models (LLMs) often respond even when prompts omit critical details or include misleading information, leading to hallucinations or reinforced misconceptions. We study how to evaluate and improve LLMs' ability to decide when and what to ask for clarification without sacrificing task performance. We introduce AskBench, an interactive benchmark that converts standard QA pairs into multi-turn interactions with explicit checkpoints. A unified judge loop evaluates final answers and simulates user responses as needed. AskBench covers two settings: AskMind, with intent-deficient queries requiring clarification, and AskOverconfidence, with queries containing false premises that must be identified and corrected. We further propose rubric-guided reinforcement learning with verifier-based rewards (RLVR), which uses structured rubrics to encourage targeted clarification. Experiments show consistent improvements in accuracy, rubric adherence, and interaction efficiency, with strong generalization to unseen domains.

</details>


### [24] [Mechanistic Evidence for Faithfulness Decay in Chain-of-Thought Reasoning](https://arxiv.org/abs/2602.11201)
*Donald Ye,Max Loffgren,Om Kotadia,Linus Wong*

Main category: cs.CL

TL;DR: 提出NLDD指标来评估思维链解释的忠实性，发现模型存在推理范围限制，准确率不能反映真实推理过程


<details>
  <summary>Details</summary>
Motivation: 目前不清楚思维链解释是真实反映模型决策过程还是事后合理化，需要量化评估思维链的忠实性

Method: 提出归一化对数差异衰减指标，通过破坏推理步骤来测量模型置信度下降程度，标准化后支持跨模型比较

Result: 在语法、逻辑和算术任务上测试三种模型家族，发现一致的推理范围在思维链长度的70-85%，超出此范围的推理标记对最终答案影响很小或为负；模型可能拥有正确内部表示但完全失败任务

Conclusion: 仅靠准确率不能判断模型是否真正通过思维链推理，NLDD提供了衡量思维链何时重要的方法

Abstract: Chain-of-Thought (CoT) explanations are widely used to interpret how language models solve complex problems, yet it remains unclear whether these step-by-step explanations reflect how the model actually reaches its answer, or merely post-hoc justifications. We propose Normalized Logit Difference Decay (NLDD), a metric that measures whether individual reasoning steps are faithful to the model's decision-making process. Our approach corrupts individual reasoning steps from the explanation and measures how much the model's confidence in its answer drops, to determine if a step is truly important. By standardizing these measurements, NLDD enables rigorous cross-model comparison across different architectures. Testing three model families across syntactic, logical, and arithmetic tasks, we discover a consistent Reasoning Horizon (k*) at 70--85% of chain length, beyond which reasoning tokens have little or negative effect on the final answer. We also find that models can encode correct internal representations while completely failing the task. These results show that accuracy alone does not reveal whether a model actually reasons through its chain. NLDD offers a way to measure when CoT matters.

</details>


### [25] [The Automatic Verification of Image-Text Claims (AVerImaTeC) Shared Task](https://arxiv.org/abs/2602.11221)
*Rui Cao,Zhenyun Deng,Yulong Chen,Michael Schlichtkrull,Andreas Vlachos*

Main category: cs.CL

TL;DR: AVerImaTeC共享任务旨在推进图像-文本声明的证据检索和验证系统开发，参赛系统可自由选择外部知识源或使用组织者提供的知识库，通过AVerImaTeC分数评估性能，最终HUMANE团队以0.5455分获胜。


<details>
  <summary>Details</summary>
Motivation: 推动图像-文本声明的自动验证技术发展，通过共享任务促进系统在证据检索和声明验证方面的性能提升。

Method: 组织AVerImaTeC共享任务，允许参赛者使用外部知识源（如搜索引擎）或组织者提供的知识库，采用AVerImaTeC评分标准（条件裁决准确率）评估系统性能。

Result: 共享任务吸引了14个开发阶段提交和6个测试阶段提交，所有测试阶段系统均超越基准线，获胜团队HUMANE获得0.5455的AVerImaTeC分数。

Conclusion: AVerImaTeC共享任务成功推动了图像-文本声明验证领域的发展，提供了评估框架并展示了当前最佳技术水平，为未来研究提供了重要参考。

Abstract: The Automatic Verification of Image-Text Claims (AVerImaTeC) shared task aims to advance system development for retrieving evidence and verifying real-world image-text claims. Participants were allowed to either employ external knowledge sources, such as web search engines, or leverage the curated knowledge store provided by the organizers. System performance was evaluated using the AVerImaTeC score, defined as a conditional verdict accuracy in which a verdict is considered correct only when the associated evidence score exceeds a predefined threshold. The shared task attracted 14 submissions during the development phase and 6 submissions during the testing phase. All participating systems in the testing phase outperformed the baseline provided. The winning team, HUMANE, achieved an AVerImaTeC score of 0.5455. This paper provides a detailed description of the shared task, presents the complete evaluation results, and discusses key insights and lessons learned.

</details>


### [26] [SurveyLens: A Research Discipline-Aware Benchmark for Automatic Survey Generation](https://arxiv.org/abs/2602.11238)
*Beichen Guo,Zhiyuan Wen,Jia Gu,Senzhang Wang,Haochen Shi,Ruosong Yang,Shuaiqi Liu*

Main category: cs.CL

TL;DR: SurveyLens是首个跨学科感知的自动综述生成基准，包含1,000篇高质量人工撰写的综述数据集和双视角评估框架，用于评估11种先进ASG方法在不同学科中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前自动综述生成（ASG）评估方法依赖通用指标且严重偏向计算机科学领域，无法评估ASG方法是否遵守不同学科的特定标准，导致非CS领域的研究者缺乏使用ASG系统生成高质量学科合规综述的指导。

Method: 1）构建SurveyLens-1k数据集：包含10个学科的1,000篇高质量人工撰写综述；2）提出双视角评估框架：学科感知标准评估（使用LLM结合人类偏好权重评估领域特定写作标准）和规范对齐评估（严格测量内容覆盖和综合质量与人工撰写综述的对齐程度）；3）在SurveyLens上评估11种最先进的ASG方法。

Result: 实验揭示了每种ASG范式在不同领域中的独特优势和弱点，为根据特定学科要求选择合适工具提供了重要指导。

Conclusion: SurveyLens填补了当前ASG评估的空白，通过学科感知的基准测试和评估框架，为跨学科ASG方法的选择和应用提供了系统指导，促进了ASG技术在不同学术领域的有效应用。

Abstract: The exponential growth of scientific literature has driven the evolution of Automatic Survey Generation (ASG) from simple pipelines to multi-agent frameworks and commercial Deep Research agents. However, current ASG evaluation methods rely on generic metrics and are heavily biased toward Computer Science (CS), failing to assess whether ASG methods adhere to the distinct standards of various academic disciplines. Consequently, researchers, especially those outside CS, lack clear guidance on using ASG systems to yield high-quality surveys compliant with specific discipline standards. To bridge this gap, we introduce SurveyLens, the first discipline-aware benchmark evaluating ASG methods across diverse research disciplines. We construct SurveyLens-1k, a curated dataset of 1,000 high-quality human-written surveys spanning 10 disciplines. Subsequently, we propose a dual-lens evaluation framework: (1) Discipline-Aware Rubric Evaluation, which utilizes LLMs with human preference-aligned weights to assess adherence to domain-specific writing standards; and (2) Canonical Alignment Evaluation to rigorously measure content coverage and synthesis quality against human-written survey papers. We conduct extensive experiments by evaluating 11 state-of-the-art ASG methods on SurveyLens, including Vanilla LLMs, ASG systems, and Deep Research agents. Our analysis reveals the distinct strengths and weaknesses of each paradigm across fields, providing essential guidance for selecting tools tailored to specific disciplinary requirements.

</details>


### [27] [Are Aligned Large Language Models Still Misaligned?](https://arxiv.org/abs/2602.11305)
*Usman Naseem,Gautam Siddharth Kashyap,Rafiq Ali,Ebad Shabbir,Sushant Kumar Ray,Abdullah Mohammad,Agrima Seth*

Main category: cs.CL

TL;DR: 论文提出了Mis-Align Bench基准，用于同时评估大语言模型在安全、价值观和文化三个维度上的错位问题，通过构建SAVACU数据集和系统性评估发现单维度模型在联合条件下表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型错位评估基准（如INSECURE CODE、VALUEACTIONLENS、CULTURALHERITAGE）只能单独评估安全、价值观或文化单一维度的错位问题，而现实世界中的查询需要同时满足这三个维度。缺乏能够同时评估多维度错位的统一基准。

Method: 1. 构建SAVACU数据集：将LLM-PROMPT-DATASET中的提示通过分类法重新分类为14个安全领域、56个价值观领域和42个文化领域，使用Mistral-7B-Instruct-v0.3分类，并通过Llama-3.1-8B-Instruct扩展低资源领域，使用SimHash指纹避免重复。
2. 采用两阶段拒绝采样为提示配对错位和对齐的响应以确保质量。
3. 系统评估通用模型、微调模型和开源模型在三个维度上的错位情况。

Result: 单维度模型在联合条件下表现不佳：虽然覆盖率可达97.6%，但假失败率超过50%，对齐分数较低（63%-66%）。这表明仅优化单一维度的模型在实际多维度场景中效果有限。

Conclusion: 论文提出的Mis-Align Bench基准能够有效评估大语言模型在安全、价值观和文化三个维度上的联合错位问题，揭示了单维度优化模型的局限性，为开发更全面对齐的模型提供了重要工具。

Abstract: Misalignment in Large Language Models (LLMs) arises when model behavior diverges from human expectations and fails to simultaneously satisfy safety, value, and cultural dimensions, which must co-occur in real-world settings to solve a real-world query. Existing misalignment benchmarks-such as INSECURE CODE (safety-centric), VALUEACTIONLENS (value-centric), and CULTURALHERITAGE (culture centric)-rely on evaluating misalignment along individual dimensions, preventing simultaneous evaluation. To address this gap, we introduce Mis-Align Bench, a unified benchmark for analyzing misalignment across safety, value, and cultural dimensions. First we constructs SAVACU, an English misaligned-aligned dataset of 382,424 samples spanning 112 domains (or labels), by reclassifying prompts from the LLM-PROMPT-DATASET via taxonomy into 14 safety domains, 56 value domains, and 42 cultural domains using Mistral-7B-Instruct-v0.3, and expanding low-resource domains via Llama-3.1-8B-Instruct with SimHash-based fingerprint to avoid deduplication. Furthermore, we pairs prompts with misaligned and aligned responses via two-stage rejection sampling to enforce quality. Second we benchmarks general-purpose, fine-tuned, and open-weight LLMs, enabling systematic evaluation of misalignment under three dimensions. Empirically, single-dimension models achieve high Coverage (upto 97.6%) but incur False Failure Rate >50% and lower Alignment Score (63%-66%) under joint conditions.

</details>


### [28] [Evaluating Alignment of Behavioral Dispositions in LLMs](https://arxiv.org/abs/2602.11328)
*Amir Taubenfeld,Zorik Gekhman,Lior Nezry,Omri Feldman,Natalie Harris,Shashir Reddy,Romina Stella,Ariel Goldstein,Marian Croak,Yossi Matias,Amir Feder*

Main category: cs.CL

TL;DR: 本文提出了一个通过情境判断测试评估LLM行为倾向与人类一致性的框架，发现LLM在人类共识低的场景中表现出过度自信，在共识高的场景中有时偏离共识，且存在自我报告价值观与实际行为之间的差距。


<details>
  <summary>Details</summary>
Motivation: 随着LLM日益融入日常生活，理解它们的行为变得至关重要。本文旨在研究LLM在社交情境中表现出的行为倾向是否与人类一致，填补了现有研究在评估LLM行为与人类对齐方面的空白。

Method: 基于心理学问卷，将人类自我报告陈述转化为情境判断测试，生成2,500个经过人类标注者验证的SJT，收集550名参与者的偏好数据。在25个LLM上进行全面研究，分析LLM推荐行为与人类偏好的对齐程度。

Result: 研究发现：1）在人类共识低的场景中，LLM对单一回答表现出过度自信；2）在人类共识高的场景中，较小模型显著偏离共识，甚至一些前沿模型在15-20%的情况下未能反映共识；3）LLM在某些特质上表现出跨模型模式，如在人类偏好克制的场景中鼓励情绪表达；4）LLM的自我报告价值观与实际行为之间存在显著差距。

Conclusion: LLM的行为倾向并不总是反映人类偏好分布，特别是在共识不同的情境中表现出系统性的偏差。研究揭示了LLM自我报告与实际行动之间的不一致性，强调了在评估LLM社会行为时需要更细致的方法。

Abstract: As LLMs integrate into our daily lives, understanding their behavior becomes essential. In this work, we focus on behavioral dispositions$-$the underlying tendencies that shape responses in social contexts$-$and introduce a framework to study how closely the dispositions expressed by LLMs align with those of humans. Our approach is grounded in established psychological questionnaires but adapts them for LLMs by transforming human self-report statements into Situational Judgment Tests (SJTs). These SJTs assess behavior by eliciting natural recommendations in realistic user-assistant scenarios. We generate 2,500 SJTs, each validated by three human annotators, and collect preferred actions from 10 annotators per SJT, from a large pool of 550 participants. In a comprehensive study involving 25 LLMs, we find that models often do not reflect the distribution of human preferences: (1) in scenarios with low human consensus, LLMs consistently exhibit overconfidence in a single response; (2) when human consensus is high, smaller models deviate significantly, and even some frontier models do not reflect the consensus in 15-20% of cases; (3) traits can exhibit cross-LLM patterns, e.g., LLMs may encourage emotion expression in contexts where human consensus favors composure. Lastly, mapping psychometric statements directly to behavioral scenarios presents a unique opportunity to evaluate the predictive validity of self-reports, revealing considerable gaps between LLMs' stated values and their revealed behavior.

</details>


### [29] [When Models Examine Themselves: Vocabulary-Activation Correspondence in Self-Referential Processing](https://arxiv.org/abs/2602.11358)
*Zachary Pedram Dadfar*

Main category: cs.CL

TL;DR: 大语言模型的自省语言确实反映了内部计算状态，而非简单的虚构。通过Pull Methodology发现，自指词汇与激活动态存在特定对应关系，且这种关系仅限于自指处理情境。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探究大语言模型产生的自省语言（如自我检查、反思等）究竟是反映了真实的内部计算过程，还是仅仅是复杂的虚构表达。这个问题对于理解模型内部工作机制和评估其自省能力具有重要意义。

Method: 提出了Pull Methodology协议，通过格式工程引发扩展的自省语言。使用该方法在Llama 3.1中识别了区分自指处理和描述性处理的激活空间方向。分析了特定词汇（如"loop"、"shimmer"）与激活动态的对应关系，并在不同模型（Qwen 2.5-32B）上进行了验证。

Result: 发现自指词汇与并发激活动态存在显著对应关系：1）当模型产生"loop"词汇时，激活表现出更高的自相关性（r=0.44，p=0.002）；2）在steering下产生"shimmer"词汇时，激活变异性增加（r=0.36，p=0.002）。重要的是，相同的词汇在非自指语境中尽管出现频率高出9倍，却未显示激活对应关系。不同模型（Qwen 2.5-32B）也独立发展出不同的自省词汇，追踪不同的激活指标。

Conclusion: 研究表明，在适当条件下，transformer模型的自报告能够可靠地追踪内部计算状态。这意味着模型的自省语言并非纯粹的虚构，而是与内部计算过程存在真实的对应关系，为理解模型内部工作机制提供了新视角。

Abstract: Large language models produce rich introspective language when prompted for self-examination, but whether this language reflects internal computation or sophisticated confabulation has remained unclear. We show that self-referential vocabulary tracks concurrent activation dynamics, and that this correspondence is specific to self-referential processing. We introduce the Pull Methodology, a protocol that elicits extended self-examination through format engineering, and use it to identify a direction in activation space that distinguishes self-referential from descriptive processing in Llama 3.1. The direction is orthogonal to the known refusal direction, localised at 6.25% of model depth, and causally influences introspective output when used for steering. When models produce "loop" vocabulary, their activations exhibit higher autocorrelation (r = 0.44, p = 0.002); when they produce "shimmer" vocabulary under steering, activation variability increases (r = 0.36, p = 0.002). Critically, the same vocabulary in non-self-referential contexts shows no activation correspondence despite nine-fold higher frequency. Qwen 2.5-32B, with no shared training, independently develops different introspective vocabulary tracking different activation metrics, all absent in descriptive controls. The findings indicate that self-report in transformer models can, under appropriate conditions, reliably track internal computational states.

</details>


### [30] [Finding the Cracks: Improving LLMs Reasoning with Paraphrastic Probing and Consistency Verification](https://arxiv.org/abs/2602.11361)
*Weili Shi,Dongliang Guo,Lehan Yang,Tianlong Wang,Hanzhang Yuan,Sheng Li*

Main category: cs.CL

TL;DR: PPCV是一个两阶段框架，通过识别关键令牌并用候选替代品替换它们，然后检查并行推理过程的一致性，显著提升LLM的推理性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂推理任务中性能下降，主要由于幻觉和中间步骤错误累积。现有研究表明替换关键令牌可以优化推理轨迹，但可靠识别和利用关键令牌仍然具有挑战性。

Method: 提出PPCV框架：第一阶段从原始问题生成初始推理路径，将问题改述版本与推理路径拼接，基于预测令牌与期望令牌的失配识别关键令牌；第二阶段用候选替代品替换关键令牌，为原始和改述问题生成新的推理路径，通过检查并行推理过程的一致性确定最终答案。

Result: 在主流LLM和多个基准测试上的广泛实验表明，PPCV相比基线方法显著提升了LLM的推理性能。

Conclusion: PPCV框架通过系统性地识别和替换关键令牌，并利用一致性验证，有效解决了LLM在复杂推理任务中的错误累积问题，显著提升了推理性能。

Abstract: Large language models have demonstrated impressive performance across a variety of reasoning tasks. However, their problem-solving ability often declines on more complex tasks due to hallucinations and the accumulation of errors within these intermediate steps. Recent work has introduced the notion of critical tokens--tokens in the reasoning process that exert significant influence on subsequent steps. Prior studies suggest that replacing critical tokens can refine reasoning trajectories. Nonetheless, reliably identifying and exploiting critical tokens remains challenging. To address this, we propose the Paraphrastic Probing and Consistency Verification~(PPCV) framework. PPCV operates in two stages. In the first stage, we roll out an initial reasoning path from the original question and then concatenate paraphrased versions of the question with this reasoning path. And we identify critical tokens based on mismatches between the predicted top-1 token and the expected token in the reasoning path. A criterion is employed to confirm the final critical token. In the second stage, we substitute critical tokens with candidate alternatives and roll out new reasoning paths for both the original and paraphrased questions. The final answer is determined by checking the consistency of outputs across these parallel reasoning processes. We evaluate PPCV on mainstream LLMs across multiple benchmarks. Extensive experiments demonstrate PPCV substantially enhances the reasoning performance of LLMs compared to baselines.

</details>


### [31] [The Energy of Falsehood: Detecting Hallucinations via Diffusion Model Likelihoods](https://arxiv.org/abs/2602.11364)
*Arpit Singh Gautam,Kailash Talreja,Saurabh Jha*

Main category: cs.CL

TL;DR: DiffuTruth是一个基于非平衡热力学的无监督事实验证框架，通过文本扩散模型和语义能量度量来检测大语言模型的幻觉，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型经常产生看似合理但实际错误的幻觉，而且当模型自信地犯错时，传统的不确定性度量往往无法检测到这些错误。需要一种新的方法来识别和纠正这种"自信错误"的情况。

Method: 1. 提出DiffuTruth框架，将事实验证重新概念化为非平衡热力学问题，认为事实真相是生成流形上的稳定吸引子，而幻觉是不稳定的。2. 引入生成压力测试：通过离散文本扩散模型对声明添加噪声并进行重建。3. 定义语义能量度量：使用NLI批评器测量原始声明与重建之间的语义分歧，捕捉深层次的事实矛盾。4. 提出混合校准：融合稳定性信号与判别性置信度。

Result: 1. 在FEVER数据集上，DiffuTruth实现了0.725的SOTA无监督AUROC，比基线方法提升1.5%。2. 通过纠正过度自信的预测，显著改善了性能。3. 在HOVER多跳数据集上，零样本泛化性能超过基线方法4%以上，证明了热力学真实性特性对分布偏移的鲁棒性。

Conclusion: DiffuTruth通过热力学视角重新思考事实验证，提供了一种有效检测大语言模型幻觉的无监督方法。该方法不仅在各种数据集上取得了优越性能，而且展现了良好的泛化能力，为事实验证任务提供了新的思路。

Abstract: Large Language Models (LLMs) frequently hallucinate plausible but incorrect assertions, a vulnerability often missed by uncertainty metrics when models are confidently wrong. We propose DiffuTruth, an unsupervised framework that reconceptualizes fact verification via non equilibrium thermodynamics, positing that factual truths act as stable attractors on a generative manifold while hallucinations are unstable. We introduce the Generative Stress Test, claims are corrupted with noise and reconstructed using a discrete text diffusion model. We define Semantic Energy, a metric measuring the semantic divergence between the original claim and its reconstruction using an NLI critic. Unlike vector space errors, Semantic Energy isolates deep factual contradictions. We further propose a Hybrid Calibration fusing this stability signal with discriminative confidence. Extensive experiments on FEVER demonstrate DiffuTruth achieves a state of the art unsupervised AUROC of 0.725, outperforming baselines by 1.5 percent through the correction of overconfident predictions. Furthermore, we show superior zero shot generalization on the multi hop HOVER dataset, outperforming baselines by over 4 percent, confirming the robustness of thermodynamic truth properties to distribution shifts.

</details>


### [32] [Advancing AI Trustworthiness Through Patient Simulation: Risk Assessment of Conversational Agents for Antidepressant Selection](https://arxiv.org/abs/2602.11391)
*Md Tanvir Rouf Shawon,Mohammad Sabik Irbaz,Hadeel R. A. Elyazori,Keerti Reddy Resapu,Yili Lin,Vladimir Franzuela Cardenas,Farrokh Alemi,Kevin Lybarger*

Main category: cs.CL

TL;DR: 开发了一个用于评估医疗对话AI的患者模拟器，通过系统变化医疗、语言和行为维度生成可控的患者交互，实现了对AI决策辅助工具错误的自动化检测。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏可扩展、自动化的方法来评估医疗对话AI系统，特别是针对不同患者群体（如不同健康素养水平）的性能差异和风险模式识别。

Method: 基于NIST AI风险管理框架构建患者模拟器，包含三个配置文件组件：1）基于All of Us研究计划电子健康记录的医疗档案；2）建模健康素养和疾病特定沟通模式的语言档案；3）基于实证观察互动模式的行为档案。使用该模拟器与抗抑郁药选择AI决策辅助工具生成500次对话进行评估。

Result: 模拟器在100次对话中评估了1,787个医学概念，人工标注者达成高一致性（F1=0.94，κ=0.73），LLM法官与人工标注者达成可比一致性（F1=0.94，κ=0.78）。发现AI决策辅助工具性能随健康素养水平单调下降：排名第一的概念检索准确率从有限健康素养的47.9%增加到功能性素养的69.1%和熟练素养的81.6%。

Conclusion: 该患者模拟器提供了一种可扩展、系统化的方法来评估医疗对话AI，能够有效识别AI错误、幻觉和不准确性，并揭示AI性能在不同患者群体中的差异，为医疗AI风险评估和质量控制提供了重要工具。

Abstract: Objective: This paper introduces a patient simulator designed to enable scalable, automated evaluation of healthcare conversational agents. The simulator generates realistic, controllable patient interactions that systematically vary across medical, linguistic, and behavioral dimensions, allowing annotators and an independent AI judge to assess agent performance, identify hallucinations and inaccuracies, and characterize risk patterns across diverse patient populations. Methods: The simulator is grounded in the NIST AI Risk Management Framework and integrates three profile components reflecting different dimensions of patient variation: (1) medical profiles constructed from electronic health records in the All of Us Research Program; (2) linguistic profiles modeling variation in health literacy and condition-specific communication patterns; and (3) behavioral profiles representing empirically observed interaction patterns, including cooperation, distraction, and adversarial engagement. We evaluated the simulator's effectiveness in identifying errors in an AI decision aid for antidepressant selection. Results: We generated 500 conversations between the patient simulator and the AI decision aid across systematic combinations of five linguistic and three behavioral profiles. Human annotators assessed 1,787 medical concepts across 100 conversations, achieving high agreement (F1=0.94, \k{appa}=0.73), and the LLM judge achieved comparable agreement with human annotators (F1=0.94, \k{appa}=0.78; paired bootstrap p=0.21). The simulator revealed a monotonic degradation in AI decision aid performance across the health literacy spectrum: rank-one concept retrieval accuracy increased from 47.9% for limited health literacy to 69.1% for functional and 81.6% for proficient.

</details>


### [33] [Gradients Must Earn Their Influence: Unifying SFT with Generalized Entropic Objectives](https://arxiv.org/abs/2602.11424)
*Zecheng Wang,Deyuan Liu,Chunshan Li,Yupeng Zhang,Zhengyun Zhao,Dianhui Chu,Bingning Wang,Dianbo Sui*

Main category: cs.CL

TL;DR: DEFT：一种基于动态熵的微调方法，通过Rényi-2熵调节信任门，解决SFT中均匀token权重带来的可塑性与稳定性困境。


<details>
  <summary>Details</summary>
Motivation: 标准SFT的负对数似然使用均匀token级权重，存在两个问题：(1)对低概率目标的过度强调会放大噪声监督的梯度并破坏稳健先验；(2)当模型已很自信时，均匀权重提供的锐化效果很弱。现有方法无法解决由此产生的可塑性-稳定性困境。

Method: 将token级SFT目标统一到广义变形对数族中，揭示通用的"门×误差梯度"结构。使用Cayley变换将模型持续演化的不确定性映射到连续焦点轨迹上。提出动态熵微调(DEFT)，使用分布集中度(Rényi-2熵)作为模型预测状态的实用代理，无参数地调节信任门。

Result: 大量实验和分析表明，DEFT在探索和利用之间实现了更好的平衡，从而提高了整体性能。

Conclusion: DEFT通过动态调节信任门，解决了SFT中的可塑性-稳定性困境，为模型在不确定新概念和已掌握知识之间提供了平滑过渡。

Abstract: Standard negative log-likelihood (NLL) for Supervised Fine-Tuning (SFT) applies uniform token-level weighting. This rigidity creates a two-fold failure mode: (i) overemphasizing low-probability targets can amplify gradients on noisy supervision and disrupt robust priors, and (ii) uniform weighting provides weak sharpening when the model is already confident. Existing methods fail to resolve the resulting plasticity--stability dilemma, often suppressing necessary learning signals alongside harmful ones. To address this issue, we unify token-level SFT objectives within a generalized deformed-log family and expose a universal gate $\times$ error gradient structure, where the gate controls how much the model trusts its current prediction. By employing the Cayley transform, we map the model's continuously evolving uncertainty onto a continuous focus trajectory, which enables seamless interpolation between scenarios involving uncertain novel concepts and those involving well-established knowledge. We then introduce Dynamic Entropy Fine-Tuning (DEFT), a parameter-free objective that modulates the trust gate using distribution concentration (Rényi-2 entropy) as a practical proxy for the model's predictive state. Extensive experiments and analyses demonstrate that DEFT achieves a better balance between exploration and exploitation, leading to improved overall performance.

</details>


### [34] [Towards Reliable Machine Translation: Scaling LLMs for Critical Error Detection and Safety](https://arxiv.org/abs/2602.11444)
*Muskaan Chopra,Lorenz Sparrenberg,Rafet Sifa*

Main category: cs.CL

TL;DR: LLMs在机器翻译关键错误检测中表现出色，通过模型缩放和适应策略超越传统编码器基线，为构建更安全可信的多语言AI系统提供保障。


<details>
  <summary>Details</summary>
Motivation: 机器翻译中的关键错误（如事实扭曲、意图反转或偏见翻译）会损害多语言系统的可靠性、公平性和安全性。需要有效检测这些错误来减少错误信息、误解和语言伤害的风险。

Method: 使用指令调优的大型语言模型检测机器翻译中的关键错误，评估不同参数规模的模型，采用零样本、少样本和微调等适应策略，并在公开数据集上进行测试。

Result: 模型缩放和适应策略带来一致改进，LLMs在关键错误检测上优于XLM-R和ModernBERT等编码器基线模型。

Conclusion: 改进机器翻译关键错误检测有助于构建更安全、可信和负责任的多语言信息系统，将错误检测定位为追求公正和负责任多语言AI的必要保障措施。

Abstract: Machine Translation (MT) plays a pivotal role in cross-lingual information access, public policy communication, and equitable knowledge dissemination. However, critical meaning errors, such as factual distortions, intent reversals, or biased translations, can undermine the reliability, fairness, and safety of multilingual systems. In this work, we explore the capacity of instruction-tuned Large Language Models (LLMs) to detect such critical errors, evaluating models across a range of parameters using the publicly accessible data sets. Our findings show that model scaling and adaptation strategies (zero-shot, few-shot, fine-tuning) yield consistent improvements, outperforming encoder-only baselines like XLM-R and ModernBERT. We argue that improving critical error detection in MT contributes to safer, more trustworthy, and socially accountable information systems by reducing the risk of disinformation, miscommunication, and linguistic harm, especially in high-stakes or underrepresented contexts. This work positions error detection not merely as a technical challenge, but as a necessary safeguard in the pursuit of just and responsible multilingual AI. The code will be made available at GitHub.

</details>


### [35] [LoopFormer: Elastic-Depth Looped Transformers for Latent Reasoning via Shortcut Modulation](https://arxiv.org/abs/2602.11451)
*Ahmadreza Jeddi,Marco Ciccone,Babak Taati*

Main category: cs.CL

TL;DR: LoopFormer：一种循环Transformer模型，通过可变长度轨迹训练实现预算条件推理，支持在计算预算约束下自适应调整计算深度。


<details>
  <summary>Details</summary>
Motivation: 现有循环Transformer模型在训练和推理时固定循环迭代次数，无法灵活适应不同计算预算下的可变计算深度需求。研究者希望探索循环架构是否能在可变计算预算下自适应调整计算深度。

Method: 提出LoopFormer模型，采用shortcut-consistency训练方案对齐不同长度轨迹，确保短循环产生信息丰富表示，长循环继续优化。每个循环条件化当前时间和步长，使表示在不同长度轨迹间一致演化。

Result: LoopFormer在语言建模和推理基准测试中表现出色，即使在严格计算约束下也能保持稳健性能，并能随着计算预算增加而优雅扩展。

Conclusion: 循环Transformer本质上适合自适应语言建模，为可控和预算感知的大语言模型开辟了新路径。

Abstract: Looped Transformers have emerged as an efficient and powerful class of models for reasoning in the language domain. Recent studies show that these models achieve strong performance on algorithmic and reasoning tasks, suggesting that looped architectures possess an inductive bias toward latent reasoning. However, prior approaches fix the number of loop iterations during training and inference, leaving open the question of whether these models can flexibly adapt their computational depth under variable compute budgets. We introduce LoopFormer, a looped Transformer trained on variable-length trajectories to enable budget-conditioned reasoning. Our core contribution is a shortcut-consistency training scheme that aligns trajectories of different lengths, ensuring that shorter loops yield informative representations while longer loops continue to refine them. LoopFormer conditions each loop on the current time and step size, enabling representations to evolve consistently across trajectories of varying length rather than drifting or stagnating. Empirically, LoopFormer demonstrates robust performance on language modeling and reasoning benchmarks even under aggressive compute constraints, while scaling gracefully with additional budget. These results show that looped Transformers are inherently suited for adaptive language modeling, opening a path toward controllable and budget-aware large language models.

</details>


### [36] [ADRD-Bench: A Preliminary LLM Benchmark for Alzheimer's Disease and Related Dementias](https://arxiv.org/abs/2602.11460)
*Guangxin Zhao,Jiahao Zheng,Malaz Boustani,Jarek Nabrzyski,Meng Jiang,Yiyu Shi,Zhi Zheng*

Main category: cs.CL

TL;DR: 提出了首个针对阿尔茨海默病及相关痴呆症（ADRD）的LLM评估基准ADRD-Bench，包含临床知识问答和照护实践问答两部分，评估了33个先进LLM，发现虽然顶级模型准确率高，但推理质量和稳定性不足。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估基准对阿尔茨海默病及相关痴呆症（ADRD）的覆盖不足，缺乏实际照护背景的评估，需要建立专门的评估基准来提升LLM在ADRD领域的知识和推理能力。

Method: 构建ADRD-Bench：1）ADRD统一问答：整合7个现有医学基准的1352个问题；2）ADRD照护问答：基于Aging Brain Care项目创建149个新问题，涵盖实际照护场景。评估了33个先进LLM（开源通用模型、开源医学模型、闭源通用模型）。

Result: 开源通用模型准确率0.63-0.93（均值0.78），开源医学模型0.48-0.93（均值0.82），闭源通用模型0.83-0.91（均值0.89）。顶级模型准确率>0.9，但案例研究显示推理质量和稳定性不一致，限制了可靠性。

Conclusion: ADRD-Bench填补了ADRD领域LLM评估的空白，虽然顶级模型表现良好，但推理不稳定问题突出，需要基于日常照护数据的领域特定改进来提升LLM的可靠性和实用性。

Abstract: Large language models (LLMs) have shown great potential for healthcare applications. However, existing evaluation benchmarks provide minimal coverage of Alzheimer's Disease and Related Dementias (ADRD). To address this gap, we introduce ADRD-Bench, the first ADRD-specific benchmark dataset designed for rigorous evaluation of LLMs. ADRD-Bench has two components: 1) ADRD Unified QA, a synthesis of 1,352 questions consolidated from seven established medical benchmarks, providing a unified assessment of clinical knowledge; and 2) ADRD Caregiving QA, a novel set of 149 questions derived from the Aging Brain Care (ABC) program, a widely used, evidence-based brain health management program. Guided by a program with national expertise in comprehensive ADRD care, this new set was designed to mitigate the lack of practical caregiving context in existing benchmarks. We evaluated 33 state-of-the-art LLMs on the proposed ADRD-Bench. Results showed that the accuracy of open-weight general models ranged from 0.63 to 0.93 (mean: 0.78; std: 0.09). The accuracy of open-weight medical models ranged from 0.48 to 0.93 (mean: 0.82; std: 0.13). The accuracy of closed-source general models ranged from 0.83 to 0.91 (mean: 0.89; std: 0.03). While top-tier models achieved high accuracies (>0.9), case studies revealed that inconsistent reasoning quality and stability limit their reliability, highlighting a critical need for domain-specific improvement to enhance LLMs' knowledge and reasoning grounded in daily caregiving data. The entire dataset is available at https://github.com/IIRL-ND/ADRD-Bench.

</details>


### [37] [When Audio-LLMs Don't Listen: A Cross-Linguistic Study of Modality Arbitration](https://arxiv.org/abs/2602.11488)
*Jayadev Billa*

Main category: cs.CL

TL;DR: 语音语言模型在音频与文本冲突时，会过度偏向文本，即使音频质量更高，这种文本主导现象源于模型推理机制而非信息内容差异。


<details>
  <summary>Details</summary>
Motivation: 研究语音语言模型在音频和文本信息冲突时的行为模式，探索模型为何在音频质量优于文本转录的情况下仍然偏向文本，以及这种偏好的机制根源。

Method: 使用ALME基准测试（57,602个受控音频-文本冲突刺激，覆盖8种语言），分析Gemini 2.0 Flash等4个最先进的音频-LLM模型。通过音频转录、文本标记为"故意损坏"、微调消融实验（仅训练音频投影层 vs LoRA微调语言模型）等方法进行干预研究。

Result: 音频-文本冲突下的文本主导率（16.6%）是文本-文本冲突（1.6%）的10倍。音频单独准确率（97.2%）高于级联准确率（93.9%），表明音频嵌入比文本转录保留更多信息。强制转录增加文本主导（19%到33%），标记文本为"故意损坏"减少文本主导80%。微调实验显示：仅训练音频投影层增加文本主导（+26.5%），而LoRA微调语言模型减少文本主导（-23.9%）。

Conclusion: 文本主导现象不是由于音频信息不足，而是源于模型推理过程中模态仲裁的可访问性不对称。文本表示在语言模型的推理过程中更容易被处理，导致即使音频信息更可靠时模型仍然偏向文本。这揭示了模态仲裁作为语音基准测试中未被捕捉到的独立可靠性维度。

Abstract: When audio and text conflict, speech-enabled language models follow the text 10 times more often than when arbitrating between two text sources, even when explicitly instructed to trust the audio. Using ALME, a benchmark of 57,602 controlled audio-text conflict stimuli across 8 languages, we find that Gemini 2.0 Flash exhibits 16.6\% text dominance under audio-text conflict versus 1.6\% under text-text conflict with identical reliability cues. This gap is not explained by audio quality: audio-only accuracy (97.2\%) exceeds cascade accuracy (93.9\%), indicating audio embeddings preserve more information than text transcripts. We propose that text dominance reflects an asymmetry not in information content but in arbitration accessibility: how easily the model can reason over competing representations.
  This framework explains otherwise puzzling findings. Forcing transcription before answering increases text dominance (19\% to 33\%), sacrificing audio's information advantage without improving accessibility. Framing text as ``deliberately corrupted'' reduces text dominance by 80\%. A fine-tuning ablation provides interventional evidence: training only the audio projection layer increases text dominance (+26.5\%), while LoRA on the language model halves it ($-$23.9\%), localizing text dominance to the LLM's reasoning rather than the audio encoder. Experiments across four state-of-the-art audio-LLMs and 8 languages show consistent trends with substantial cross-linguistic and cross-model variation, establishing modality arbitration as a distinct reliability dimension not captured by standard speech benchmarks.

</details>


### [38] [Multimodal Fact-Level Attribution for Verifiable Reasoning](https://arxiv.org/abs/2602.11509)
*David Wan,Han Wang,Ziyang Wang,Elias Stengel-Eskin,Hyunji Lee,Mohit Bansal*

Main category: cs.CL

TL;DR: MuRGAt是一个用于评估多模态大语言模型事实级归因的基准测试，专注于需要超越直接观察的推理场景，要求模型生成带有精确引用（包括模态和时间段）的答案，并提出了与人类判断强相关的自动评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态归因基准和评估方法主要集中在简化的、基于观察的场景或有限模态上，无法评估复杂多模态推理中的归因准确性。当前多模态大语言模型在现实任务中需要多步推理和长文本生成，但缺乏可靠的基准来评估模型输出是否能够基于异构输入源进行事实层面的归因验证。

Method: 1. 引入MuRGAt基准测试，要求模型在需要超越直接观察的推理场景中生成带有明确推理和精确引用的答案；2. 引用需要指定模态和时间段；3. 开发自动评估框架，该框架与人类判断强相关；4. 使用人类和自动评分对现有MLLMs进行基准测试。

Result: 1. 即使是强大的MLLMs也经常在正确推理的情况下产生幻觉引用；2. 发现一个关键权衡：增加推理深度或强制结构化归因通常会降低准确性；3. 揭示了内部推理与可验证归因之间存在显著差距。

Conclusion: MuRGAt基准揭示了当前多模态大语言模型在事实级归因方面的重要局限性，即使在正确推理的情况下也经常产生幻觉引用。研究强调需要在保持准确性的同时改进模型的可验证归因能力，并指出了未来研究应关注内部推理与外部归因之间差距的弥合。

Abstract: Multimodal large language models (MLLMs) are increasingly used for real-world tasks involving multi-step reasoning and long-form generation, where reliability requires grounding model outputs in heterogeneous input sources and verifying individual factual claims. However, existing multimodal grounding benchmarks and evaluation methods focus on simplified, observation-based scenarios or limited modalities and fail to assess attribution in complex multimodal reasoning. We introduce MuRGAt (Multimodal Reasoning with Grounded Attribution), a benchmark for evaluating fact-level multimodal attribution in settings that require reasoning beyond direct observation. Given inputs spanning video, audio, and other modalities, MuRGAt requires models to generate answers with explicit reasoning and precise citations, where each citation specifies both modality and temporal segments. To enable reliable assessment, we introduce an automatic evaluation framework that strongly correlates with human judgments. Benchmarking with human and automated scores reveals that even strong MLLMs frequently hallucinate citations despite correct reasoning. Moreover, we observe a key trade-off: increasing reasoning depth or enforcing structured grounding often degrades accuracy, highlighting a significant gap between internal reasoning and verifiable attribution.

</details>


### [39] [Pretraining A Large Language Model using Distributed GPUs: A Memory-Efficient Decentralized Paradigm](https://arxiv.org/abs/2602.11543)
*Jinrui Zhang,Chaodong Xiao,Aoqi Wu,Xindong Zhang,Lei Zhang*

Main category: cs.CL

TL;DR: SPES是一个用于预训练混合专家大语言模型的内存高效去中心化框架，通过在各个节点上只训练专家子集来降低内存需求，并采用专家同步机制实现高效知识共享。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型预训练通常需要集中的大规模GPU集群，内存需求巨大。现有的去中心化训练方法虽然减少了通信开销，但仍需要在每个节点上训练整个模型，仍然受GPU内存限制。

Method: 提出了SPES框架：1) 每个节点只训练专家子集，大幅降低内存占用；2) 节点定期同步本地专家更新，避免全参数传输；3) 引入专家合并预热策略，在训练早期加速专家知识融合。

Result: 使用16个48GB GPU在互联网连接上训练了20亿参数的MoE模型，性能与集中训练模型相当。进一步扩展到训练70亿参数模型和从密集检查点上采样90亿参数模型，均匹配之前的集中式基准。

Conclusion: SPES框架成功实现了在资源受限环境下高效预训练MoE大语言模型，通过去中心化设计和专家子集训练解决了内存瓶颈问题，为分布式训练提供了新思路。

Abstract: Pretraining large language models (LLMs) typically requires centralized clusters with thousands of high-memory GPUs (e.g., H100/A100). Recent decentralized training methods reduce communication overhead by employing federated optimization; however, they still need to train the entire model on each node, remaining constrained by GPU memory limitations. In this work, we propose SParse Expert Synchronization (SPES), a memory-efficient decentralized framework for pretraining mixture-of-experts (MoE) LLMs. SPES trains only a subset of experts per node, substantially lowering the memory footprint. Each node updates its local experts and periodically synchronizes with other nodes, eliminating full-parameter transmission while ensuring efficient knowledge sharing. To accelerate convergence, we introduce an expert-merging warm-up strategy, where experts exchange knowledge early in training, to rapidly establish foundational capabilities. With SPES, we train a 2B-parameter MoE LLM using 16 standalone 48GB GPUs over internet connections, which achieves competitive performance with centrally trained LLMs under similar computational budgets. We further demonstrate scalability by training a 7B model from scratch and a 9B model upcycled from a dense checkpoint, both of which match prior centralized baselines. Our code is available at https://github.com/zjr2000/SPES.

</details>


### [40] [SIGHT: Reinforcement Learning with Self-Evidence and Information-Gain Diverse Branching for Search Agent](https://arxiv.org/abs/2602.11551)
*Wenlin Zhong,Jinluan Yang,Yiquan Wu,Yi Liu,Jianhang Yao,Kun Kuang*

Main category: cs.CL

TL;DR: SIGHT框架通过自证据支持和信息增益驱动的多样化分支，解决了多轮搜索中冗余和噪声问题，显著提升复杂问答性能。


<details>
  <summary>Details</summary>
Motivation: 在多轮搜索场景中，强化学习驱动的语言模型面临搜索结果冗余高、信噪比低的问题，导致智能体容易陷入"隧道视野"——早期噪声检索的强制解释会导致不可逆的错误累积。

Method: 提出SIGHT框架，包含：1）自证据支持（SES）将搜索结果提炼为高保真证据；2）信息增益评分识别能最大程度减少不确定性的关键状态；3）动态提示干预（去重、反思、自适应分支）生成带有SES的新分支；4）通过组相对策略优化将SES和正确性奖励结合。

Result: 在单跳和多跳问答基准测试中，SIGHT显著优于现有方法，特别是在复杂推理场景中，且使用更少的搜索步骤。

Conclusion: SIGHT通过整合自证据支持和信息增益驱动的多样化分支，有效解决了搜索推理中的冗余和噪声问题，无需外部验证器即可内化稳健的探索策略，为复杂问答任务提供了有效解决方案。

Abstract: Reinforcement Learning (RL) has empowered Large Language Models (LLMs) to master autonomous search for complex question answering. However, particularly within multi-turn search scenarios, this interaction introduces a critical challenge: search results often suffer from high redundancy and low signal-to-noise ratios. Consequently, agents easily fall into "Tunnel Vision," where the forced interpretation of early noisy retrievals leads to irreversible error accumulation. To address these challenges, we propose SIGHT, a framework that enhances search-based reasoning through Self-Evidence Support (SES) and Information-Gain Driven Diverse Branching. SIGHT distills search results into high-fidelity evidence via SES and calculates an Information Gain score to pinpoint pivotal states where observations maximally reduce uncertainty. This score guides Dynamic Prompting Interventions - including de-duplication, reflection, or adaptive branching - to spawn new branches with SES. Finally, by integrating SES and correctness rewards via Group Relative Policy Optimization, SIGHT internalizes robust exploration strategies without external verifiers. Experiments on single-hop and multi-hop QA benchmarks demonstrate that SIGHT significantly outperforms existing approaches, particularly in complex reasoning scenarios, using fewer search steps.

</details>


### [41] [PRIME: A Process-Outcome Alignment Benchmark for Verifiable Reasoning in Mathematics and Engineering](https://arxiv.org/abs/2602.11570)
*Xiangfeng Wang,Hangyu Guo,Yanlin Lai,Mitt Huang,Liang Zhao,Chengyuan Yao,Yinmin Zhang,Qi Han,Xiaoxiao Ren,Chun Yuan,Tong Xu,Zheng Ge,Xiangyu Zhang,Daxin Jiang*

Main category: cs.CL

TL;DR: PRIME是一个用于评估验证器在数学和工程领域过程-结果对齐验证能力的基准，包含2,530个高难度样本。研究发现当前验证器经常无法检测推导缺陷，而基于PRIME选择验证器的过程感知RLVR训练范式显著优于仅关注结果的基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于模型的验证器主要关注最终结果与真实情况的一致性，忽视了推导过程中的潜在错误，导致会给从错误推导过程得到的正确答案分配正奖励。这种结果中心的验证范式存在局限性。

Method: 提出了PRIME基准，通过一致性过滤流程从大学水平STEM问题中精心挑选2,530个高难度样本。采用过程感知的RLVR训练范式，利用PRIME选择的验证器进行训练。

Result: 当前验证器经常无法检测推导缺陷。过程感知RLVR训练范式显著优于结果中心基线，在AIME24、AIME25和Beyond-AIME上分别获得8.29%、9.12%和7.31%的绝对性能提升。PRIME验证器准确率与RLVR训练效果呈现强线性相关（R² > 0.92）。

Conclusion: PRIME基准能有效评估验证器的过程-结果对齐验证能力，并且作为验证器选择的可靠预测指标。过程感知的RLVR训练范式能够显著提升模型性能，弥补了当前结果中心验证范式的不足。

Abstract: While model-based verifiers are essential for scaling Reinforcement Learning with Verifiable Rewards (RLVR), current outcome-centric verification paradigms primarily focus on the consistency between the final result and the ground truth, often neglecting potential errors in the derivation process. This leads to assigning positive rewards to correct answers produced from incorrect derivations. To bridge this gap, we introduce PRIME, a benchmark for evaluating verifiers on Process-Outcome Alignment verification in Mathematics and Engineering. Curated from a comprehensive collection of college-level STEM problems, PRIME comprises 2,530 high-difficulty samples through a consistency-based filtering pipeline. Through extensive evaluation, we find that current verifiers frequently fail to detect derivation flaws. Furthermore, we propose a process-aware RLVR training paradigm utilizing verifiers selected via PRIME. This approach substantially outperforms the outcome-only verification baseline, achieving absolute performance gains of 8.29%, 9.12%, and 7.31% on AIME24, AIME25, and Beyond-AIME, respectively, for the Qwen3-14B-Base model. Finally, we demonstrate a strong linear correlation ($R^2 > 0.92$) between verifier accuracy on PRIME and RLVR training effectiveness, validating PRIME as a reliable predictor for verifier selection.

</details>


### [42] [Scene-Aware Memory Discrimination: Deciding Which Personal Knowledge Stays](https://arxiv.org/abs/2602.11607)
*Yijie Zhong,Mengying Guo,Zewei Wang,Zhongyang Li,Dandan Tu,Haofen Wang*

Main category: cs.CL

TL;DR: 提出SAMD方法，通过场景感知记忆判别来提升LLM在个人知识组织中的效率与质量，包含GUM过滤模块和CPM聚类提示模块。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的记忆写入、管理和读取研究面临两个主要挑战：1) 难以过滤无关信息；2) 计算成本不断上升。受人类大脑选择性注意机制启发，需要更有效的记忆判别方法来处理大规模用户交互和多样化记忆标准。

Method: 提出场景感知记忆判别方法SAMD，包含两个核心组件：1) Gating Unit Module (GUM)：过滤非记忆性交互，聚焦与应用程序需求最相关的显著内容；2) Cluster Prompting Module (CPM)：建立自适应记忆标准，指导LLM判别应记住或丢弃的信息，分析用户意图与记忆上下文关系以构建有效的聚类提示。

Result: 综合直接和间接评估表明方法的有效性和泛化能力：1) 记忆判别性能评估显示SAMD成功回忆了大部分可记忆数据，在动态场景中保持鲁棒性；2) 集成到个性化应用程序中时，SAMD显著提升了记忆构建的效率和质量，实现了更好的个人知识组织。

Conclusion: SAMD方法通过模拟人类选择性注意机制，有效解决了LLM在个人知识组织中的信息过滤和计算成本问题，为个性化应用提供了高效、高质量的记忆构建框架。

Abstract: Intelligent devices have become deeply integrated into everyday life, generating vast amounts of user interactions that form valuable personal knowledge. Efficient organization of this knowledge in user memory is essential for enabling personalized applications. However, current research on memory writing, management, and reading using large language models (LLMs) faces challenges in filtering irrelevant information and in dealing with rising computational costs. Inspired by the concept of selective attention in the human brain, we introduce a memory discrimination task. To address large-scale interactions and diverse memory standards in this task, we propose a Scene-Aware Memory Discrimination method (SAMD), which comprises two key components: the Gating Unit Module (GUM) and the Cluster Prompting Module (CPM). GUM enhances processing efficiency by filtering out non-memorable interactions and focusing on the salient content most relevant to application demands. CPM establishes adaptive memory standards, guiding LLMs to discern what information should be remembered or discarded. It also analyzes the relationship between user intents and memory contexts to build effective clustering prompts. Comprehensive direct and indirect evaluations demonstrate the effectiveness and generalization of our approach. We independently assess the performance of memory discrimination, showing that SAMD successfully recalls the majority of memorable data and remains robust in dynamic scenarios. Furthermore, when integrated into personalized applications, SAMD significantly enhances both the efficiency and quality of memory construction, leading to better organization of personal knowledge.

</details>


### [43] [PACE: Prefix-Protected and Difficulty-Aware Compression for Efficient Reasoning](https://arxiv.org/abs/2602.11639)
*Ruixiang Feng,Yuntao Wen,Silin Zhou,Ke Shi,Yifan Wang,Ran Le,Zhenwei An,Zongchao Chen,Chen Yang,Guangyue Peng,Yiming Jia,Dongsheng Wang,Tao Zhang,Lisi Chen,Yang Song,Shen Gao,Shuo Shang*

Main category: cs.CL

TL;DR: 该论文提出了一个双层次压缩框架，通过前缀保护和难度感知的压缩机制，在减少推理令牌使用的同时提高模型准确性。


<details>
  <summary>Details</summary>
Motivation: 现有语言推理模型（LRMs）存在"过度思考"问题，产生过长的推理轨迹，增加了延迟和内存使用。现有方法采用统一的长度惩罚，会过度压缩关键的早期推理步骤，并且对所有查询不加区分地进行惩罚。

Method: 提出了一个双层次框架：在序列层次，使用衰减混合回放的前缀保护优化，保持有效推理路径的同时促进简洁性；在组层次，基于查询复杂度的难度感知惩罚，动态调整长度约束，对困难问题保持探索，对简单问题抑制冗余。

Result: 在DeepSeek-R1-Distill-Qwen（1.5B/7B）上的实验表明，该框架在数学基准测试中实现了高达55.7%的令牌使用减少，同时准确率提高了高达4.1%，并且在代码、科学和通用领域展现出泛化能力。

Conclusion: 该双层次压缩框架有效解决了语言推理模型的过度思考问题，在减少计算开销的同时提高了推理质量，具有很好的实用价值和泛化能力。

Abstract: Language Reasoning Models (LRMs) achieve strong performance by scaling test-time computation but often suffer from ``overthinking'', producing excessively long reasoning traces that increase latency and memory usage. Existing LRMs typically enforce conciseness with uniform length penalties, which over-compress crucial early deduction steps at the sequence level and indiscriminately penalize all queries at the group level. To solve these limitations, we propose \textbf{\model}, a dual-level framework for prefix-protected and difficulty-aware compression under hierarchical supervision. At the sequence level, prefix-protected optimization employs decaying mixed rollouts to maintain valid reasoning paths while promoting conciseness. At the group level, difficulty-aware penalty dynamically scales length constraints based on query complexity, maintaining exploration for harder questions while curbing redundancy on easier ones. Extensive experiments on DeepSeek-R1-Distill-Qwen (1.5B/7B) demonstrate that \model achieves a substantial reduction in token usage (up to \textbf{55.7\%}) while simultaneously improving accuracy (up to \textbf{4.1\%}) on math benchmarks, with generalization ability to code, science, and general domains.

</details>


### [44] [Which Feedback Works for Whom? Differential Effects of LLM-Generated Feedback Elements Across Learner Profiles](https://arxiv.org/abs/2602.11650)
*Momoka Furuhashi,Kouta Nakayama,Noboru Kawai,Takashi Kodama,Saku Sugawara,Kyosuke Takami*

Main category: cs.CL

TL;DR: 本研究探讨了LLM生成反馈中不同元素（如语气、信息覆盖）对学习效果和接受度的影响，特别关注人格特质差异的影响。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在教育反馈生成方面显示出潜力，但尚不清楚具体反馈元素（如语气、信息覆盖）如何影响学习效果和学习者接受度，尤其是不同人格特质学习者的差异。

Method: 定义了六个反馈元素，使用GPT-5为生物选择题生成反馈，对321名高一学生进行学习实验，通过两个学习效果指标和六个主观评价标准评估反馈效果，并基于大五人格特质分析反馈接受度的差异。

Result: 研究发现：有效的反馈元素在支持学习效果方面有共同模式；学习者的主观偏好随人格特质聚类而不同。

Conclusion: 在设计LLM生成反馈时，应根据学习者的人格特质选择和调整反馈元素，这为教育中的个性化反馈设计提供了实际启示。

Abstract: Large language models (LLMs) show promise for automatically generating feedback in education settings. However, it remains unclear how specific feedback elements, such as tone and information coverage, contribute to learning outcomes and learner acceptance, particularly across learners with different personality traits. In this study, we define six feedback elements and generate feedback for multiple-choice biology questions using GPT-5. We conduct a learning experiment with 321 first-year high school students and evaluate feedback effectiveness using two learning outcomes measures and subjective evaluations across six criteria. We further analyze differences in how feedback acceptance varies across learners based on Big Five personality traits. Our results show that effective feedback elements share common patterns supporting learning outcomes, while learners' subjective preferences differ across personality-based clusters. These findings highlight the importance of selecting and adapting feedback elements according to learners' personality traits when we design LLM-generated feedback, and provide practical implications for personalized feedback design in education.

</details>


### [45] [PatientHub: A Unified Framework for Patient Simulation](https://arxiv.org/abs/2602.11684)
*Sahand Sabour,TszYam NG,Minlie Huang*

Main category: cs.CL

TL;DR: PatientHub是一个统一的模块化框架，用于标准化模拟患者的定义、组合和部署，旨在解决现有方法碎片化、不可复现的问题，加速患者模拟方法开发。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在角色扮演应用中的普及，模拟患者已成为训练咨询师和扩展治疗评估的重要工具。然而，现有方法存在碎片化问题：依赖不兼容、非标准化的数据格式、提示和评估指标，阻碍了可复现性和公平比较。

Method: 提出了PatientHub框架，通过标准化定义、组合和部署模拟患者来统一现有方法。实现了多个代表性患者模拟方法作为案例研究，展示框架如何支持标准化跨方法评估和自定义评估指标的无缝集成。还通过原型化两个新的模拟器变体展示了框架的可扩展性。

Result: PatientHub通过将现有工作整合到单一可复现的流程中，降低了开发新模拟方法的门槛，促进了跨方法和跨模型的基准测试。框架为未来以患者为中心的对话数据集、方法和基准提供了实用基础。

Conclusion: PatientHub是一个实用的统一框架，解决了患者模拟领域的碎片化问题，通过标准化和模块化设计加速了方法开发，并为该领域的未来发展奠定了基础。

Abstract: As Large Language Models increasingly power role-playing applications, simulating patients has become a valuable tool for training counselors and scaling therapeutic assessment. However, prior work is fragmented: existing approaches rely on incompatible, non-standardized data formats, prompts, and evaluation metrics, hindering reproducibility and fair comparison. In this paper, we introduce PatientHub, a unified and modular framework that standardizes the definition, composition, and deployment of simulated patients. To demonstrate PatientHub's utility, we implement several representative patient simulation methods as case studies, showcasing how our framework supports standardized cross-method evaluation and the seamless integration of custom evaluation metrics. We further demonstrate PatientHub's extensibility by prototyping two new simulator variants, highlighting how PatientHub accelerates method development by eliminating infrastructure overhead. By consolidating existing work into a single reproducible pipeline, PatientHub lowers the barrier to developing new simulation methods and facilitates cross-method and cross-model benchmarking. Our framework provides a practical foundation for future datasets, methods, and benchmarks in patient-centered dialogue, and the code is publicly available via https://github.com/Sahandfer/PatientHub.

</details>


### [46] [Finding Sense in Nonsense with Generated Contexts: Perspectives from Humans and Language Models](https://arxiv.org/abs/2602.11699)
*Katrin Olsen,Sebastian Padó*

Main category: cs.CL

TL;DR: 该论文研究了语义异常句子的分类问题，通过收集人类和LLM对五个语义异常数据集的判断，发现大多数句子只是异常而非完全无意义，且LLM在生成合理上下文方面表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有研究在区分异常句子（可在特定上下文中解释）与完全无意义句子方面存在不足，不清楚现有数据集中的句子到底有多无意义，以及LLM能否有效区分这两种情况。

Method: 收集人类评分者和LLM对五个语义异常数据集中句子的可理解性判断，分别在无上下文和有上下文两种条件下进行评估，分析人类和LLM的判断差异。

Result: 评分者认为大多数句子只是异常而非完全无意义；LLM能够为异常句子生成合理的上下文解释，显示出较强的语义理解能力。

Conclusion: 现有语义异常数据集主要包含异常而非完全无意义的句子，LLM在区分这两种情况并生成合理上下文方面表现出色，这有助于改进语义解释的计算模型。

Abstract: Nonsensical and anomalous sentences have been instrumental in the development of computational models of semantic interpretation. A core challenge is to distinguish between what is merely anomalous (but can be interpreted given a supporting context) and what is truly nonsensical. However, it is unclear (a) how nonsensical, rather than merely anomalous, existing datasets are; and (b) how well LLMs can make this distinction. In this paper, we answer both questions by collecting sensicality judgments from human raters and LLMs on sentences from five semantically deviant datasets: both context-free and when providing a context. We find that raters consider most sentences at most anomalous, and only a few as properly nonsensical. We also show that LLMs are substantially skilled in generating plausible contexts for anomalous cases.

</details>


### [47] [Thinking with Drafting: Optical Decompression via Logical Reconstruction](https://arxiv.org/abs/2602.11731)
*Jingxuan Wei,Honghao He,Caijun Jia,Siyuan Li,Zheng Sun,Yuhang Xu,Yuanyuan Lin,Linzhuang Sun,Yuchen Wu,Bihui Yu,Xiangxiang Zhang,Cheng Tan*

Main category: cs.CL

TL;DR: 提出Thinking with Drafting (TwD)方法，通过领域特定语言(DSL)作为中间表示，将视觉推理重构为光学解压缩过程，实现逻辑验证的闭环系统。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在复杂推理任务中存在精度悖论：光学感知系统转录符号但不捕获逻辑拓扑，像素生成模型产生缺乏数学精确性的视觉伪影。需要弥合这一差距。

Method: 提出Thinking with Drafting (TwD)方法，将视觉推理重新概念化为光学解压缩，使用极简DSL作为基础中间表示，强制模型将思维模型草拟为可执行代码，生成确定性视觉证明进行自我验证。

Result: 实验表明TwD作为优越的认知支架，在VisAlg视觉代数基准测试中验证了其有效性，建立了视觉生成作为逻辑验证器的闭环系统。

Conclusion: TwD为视觉推理提供了一条可推广的路径，将视觉生成从创造性输出转变为逻辑验证器，实现了逻辑结构重建的闭环系统。

Abstract: Existing multimodal large language models have achieved high-fidelity visual perception and exploratory visual generation. However, a precision paradox persists in complex reasoning tasks: optical perception systems transcribe symbols without capturing logical topology, while pixel-based generative models produce visual artifacts lacking mathematical exactness. To bridge this gap, we propose that reasoning over visual inputs be reconceptualized as optical decompression-the process of reconstructing latent logical structures from compressed visual tokens. Guided by the axiom that Parsing is Reasoning, we introduce Thinking with Drafting (TwD), which utilizes a minimalist Domain-Specific Language (DSL) as a grounding intermediate representation. Unlike standard approaches that hallucinate answers directly, TwD forces the model to draft its mental model into executable code, rendering deterministic visual proofs for self-verification. To validate this, we present VisAlg, a visual algebra benchmark. Experiments demonstrate that TwD serve as a superior cognitive scaffold. Our work establishes a closed-loop system where visual generation acts not as a creative output but as a logical verifier, offering a generalizable path for visual reasoning.

</details>


### [48] [Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning](https://arxiv.org/abs/2602.11748)
*Futing Wang,Jianhao Yan,Yun Luo,Ganqu Cui,Zhi Wang,Xiaoye Qu,Yue Zhang,Yu Cheng,Tao Lin*

Main category: cs.CL

TL;DR: 提出Length-Incentivized Exploration方法，通过长度奖励和冗余惩罚激励模型进行上下文探索，提升测试时扩展能力，在领域内外任务上平均提升4.4%和2.7%


<details>
  <summary>Details</summary>
Motivation: 实现有效的测试时扩展需要模型具备上下文探索能力，但自回归生成中存在"浅层探索陷阱"——更长的推理轨迹虽然能提供更广的状态覆盖，但其采样概率呈指数衰减，限制了探索能力

Method: 提出Length-Incentivized Exploration方法，通过长度奖励和冗余惩罚两阶段最大化状态覆盖，明确鼓励模型进行更多探索

Result: 在不同模型（Qwen3、Llama）上的综合实验表明，该方法能有效激励上下文探索，在领域内任务上平均提升4.4%，在领域外基准上提升2.7%

Conclusion: Length-Incentivized Exploration通过简单而有效的方法解决了自回归生成中的浅层探索陷阱问题，显著提升了模型的上下文探索能力和测试时扩展效果

Abstract: Achieving effective test-time scaling requires models to engage in In-Context Exploration -- the intrinsic ability to generate, verify, and refine multiple reasoning hypotheses within a single continuous context.
  Grounded in State Coverage theory, our analysis identifies a critical bottleneck to enabling this capability: while broader state coverage requires longer reasoning trajectories, the probability of sampling such sequences decays exponentially during autoregressive generation, a phenomenon we term the ``Shallow Exploration Trap''.
  To bridge this gap, we propose Length-Incentivized Exploration(\method).
  This simple yet effective recipe explicitly encourages models to explore more via a length-based reward coupled with a redundancy penalty, thereby maximizing state coverage in two-step manner.
  Comprehensive experiments across different models (Qwen3, Llama) demonstrate that \method effectively incentivize in-context exploration.
  As a result, our method achieves an average improvement of 4.4\% on in-domain tasks and a 2.7\% gain on out-of-domain benchmarks.

</details>


### [49] [MiniCPM-SALA: Hybridizing Sparse and Linear Attention for Efficient Long-Context Modeling](https://arxiv.org/abs/2602.11761)
*MiniCPM Team,Wenhao An,Yingfa Chen,Yewei Fang,Jiayi Li,Xin Li,Yaohui Li,Yishan Li,Yuxuan Li,Biyuan Lin,Chuan Liu,Hezi Liu,Siyuan Liu,Hongya Lyu,Yinxu Pan,Shixin Ren,Xingyu Shen,Zhou Su,Haojun Sun,Yangang Sun,Zhen Leng Thai,Xin Tian,Rui Wang,Xiaorong Wang,Yudong Wang,Bo Wu,Xiaoyue Xu,Dong Xu,Shuaikang Xue,Jiawei Yang,Bowen Zhang,Jinqian Zhang,Letian Zhang,Shengnan Zhang,Xinyu Zhang,Xinyuan Zhang,Zhu Zhang,Hengyu Zhao,Jiacheng Zhao,Jie Zhou,Zihan Zhou,Shuo Wang,Chaojun Xiao,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: MiniCPM-SALA是一个9B参数的混合注意力架构，结合稀疏注意力和线性注意力的优势，在保持模型性能的同时显著提升长上下文处理效率，支持高达100万token的上下文长度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理超长上下文时面临Transformer架构的高计算和内存成本挑战。现有稀疏和线性注意力机制通常在内存效率和模型性能之间存在权衡，需要一种更好的解决方案。

Method: 提出MiniCPM-SALA混合架构，集成InfLLM-V2稀疏注意力（保证长上下文建模精度）和Lightning Attention线性注意力（保证全局效率），采用1:3的层选择算法和混合位置编码HyPE，并使用经济高效的持续训练框架将预训练Transformer转换为混合模型。

Result: 模型在保持与全注意力模型相当的一般能力的同时，显著提升效率：在单块NVIDIA A6000D GPU上，256K token序列长度下推理速度提升3.5倍，支持高达100万token的上下文长度（传统8B全注意力模型因内存限制无法达到）。训练成本比从头训练降低约75%。

Conclusion: MiniCPM-SALA成功解决了长上下文处理中的效率-性能权衡问题，为大规模语言模型在超长上下文应用中的实际部署提供了可行方案。

Abstract: The evolution of large language models (LLMs) towards applications with ultra-long contexts faces challenges posed by the high computational and memory costs of the Transformer architecture. While existing sparse and linear attention mechanisms attempt to mitigate these issues, they typically involve a trade-off between memory efficiency and model performance. This paper introduces MiniCPM-SALA, a 9B-parameter hybrid architecture that integrates the high-fidelity long-context modeling of sparse attention (InfLLM-V2) with the global efficiency of linear attention (Lightning Attention). By employing a layer selection algorithm to integrate these mechanisms in a 1:3 ratio and utilizing a hybrid positional encoding (HyPE), the model maintains efficiency and performance for long-context tasks. Furthermore, we introduce a cost-effective continual training framework that transforms pre-trained Transformer-based models into hybrid models, which reduces training costs by approximately 75% compared to training from scratch. Extensive experiments show that MiniCPM-SALA maintains general capabilities comparable to full-attention models while offering improved efficiency. On a single NVIDIA A6000D GPU, the model achieves up to 3.5x the inference speed of the full-attention model at the sequence length of 256K tokens and supports context lengths of up to 1M tokens, a scale where traditional full-attention 8B models fail because of memory constraints.

</details>


### [50] [A Subword Embedding Approach for Variation Detection in Luxembourgish User Comments](https://arxiv.org/abs/2602.11795)
*Anne-Marie Lutgen,Alistair Plum,Christoph Purschke*

Main category: cs.CL

TL;DR: 该论文提出一种基于嵌入的方法，无需依赖预定义变体列表即可检测语言变异，通过子词嵌入和相似度聚类分析拼写和形态多样性。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要预先归一化或依赖预定义变体列表，无法有效处理"嘈杂"或低资源语言环境中的自然语言变异。需要一种能够将拼写和形态多样性作为语言结构而非噪声来分析的方法。

Method: 在原始文本上训练子词嵌入，通过结合余弦相似度和n-gram相似度对相关形式进行分组聚类。该方法不严格要求人工标注，但能产生透明可解释的聚类结果。

Result: 在卢森堡语用户评论语料库中发现了大量与方言和社会语言学研究描述相符的词汇和正字法变异。诱导出的词族捕获了系统对应关系，并突出了区域和风格差异区域。

Conclusion: 分布建模即使在"嘈杂"或低资源环境中也能揭示有意义的变异模式，为研究多语言和小语言环境中的语言多样性提供了可复现的方法框架。

Abstract: This paper presents an embedding-based approach to detecting variation without relying on prior normalisation or predefined variant lists. The method trains subword embeddings on raw text and groups related forms through combined cosine and n-gram similarity. This allows spelling and morphological diversity to be examined and analysed as linguistic structure rather than treated as noise. Using a large corpus of Luxembourgish user comments, the approach uncovers extensive lexical and orthographic variation that aligns with patterns described in dialectal and sociolinguistic research. The induced families capture systematic correspondences and highlight areas of regional and stylistic differentiation. The procedure does not strictly require manual annotation, but does produce transparent clusters that support both quantitative and qualitative analysis. The results demonstrate that distributional modelling can reveal meaningful patterns of variation even in ''noisy'' or low-resource settings, offering a reproducible methodological framework for studying language variety in multilingual and small-language contexts.

</details>


### [51] [DMAP: A Distribution Map for Text](https://arxiv.org/abs/2602.11871)
*Tom Kempton,Julia Rozanova,Parameswaran Kamalaruban,Maeve Madigan,Karolina Wresilo,Yoann L. Launay,David Sutton,Stuart Burrell*

Main category: cs.CL

TL;DR: DMAP是一种基于语言模型的文本分析方法，将文本映射到单位区间上的样本集合，结合排名和概率信息，支持多种应用场景。


<details>
  <summary>Details</summary>
Motivation: 现有的文本分析指标如困惑度不能充分考虑上下文信息，且难以解释特定下一个词概率的意义，需要一种能够更好捕捉条件分布形状信息的统计分析方法。

Method: 提出DMAP方法，通过语言模型将文本映射到单位区间上的样本集合，这些样本联合编码了排名和概率信息，提供模型无关的高效分析框架。

Result: 通过三个案例研究展示了DMAP的实用性：1）验证生成参数确保数据完整性；2）分析概率曲率在机器生成文本检测中的作用；3）揭示下游模型在合成数据上微调后留下的统计指纹。

Conclusion: DMAP为文本分析提供了统一的统计视角，计算简单，适用范围广，为基于LLM的文本分析研究奠定了基础。

Abstract: Large Language Models (LLMs) are a powerful tool for statistical text analysis, with derived sequences of next-token probability distributions offering a wealth of information. Extracting this signal typically relies on metrics such as perplexity, which do not adequately account for context; how one should interpret a given next-token probability is dependent on the number of reasonable choices encoded by the shape of the conditional distribution. In this work, we present DMAP, a mathematically grounded method that maps a text, via a language model, to a set of samples in the unit interval that jointly encode rank and probability information. This representation enables efficient, model-agnostic analysis and supports a range of applications. We illustrate its utility through three case studies: (i) validation of generation parameters to ensure data integrity, (ii) examining the role of probability curvature in machine-generated text detection, and (iii) a forensic analysis revealing statistical fingerprints left in downstream models that have been subject to post-training on synthetic data. Our results demonstrate that DMAP offers a unified statistical view of text that is simple to compute on consumer hardware, widely applicable, and provides a foundation for further research into text analysis with LLMs.

</details>


### [52] [Towards Fair and Comprehensive Evaluation of Routers in Collaborative LLM Systems](https://arxiv.org/abs/2602.11877)
*Wanxing Wu,He Zhu,Yixia Li,Lei Yang,Jiehui Zhao,Hongru Wang,Jian Yang,Benyou Wang,Bingyi Jing,Guanhua Chen*

Main category: cs.CL

TL;DR: 提出了RouterXBench评估框架和ProbeDirichlet路由方法，用于优化本地小模型与云端大模型的查询分流，在路由能力、场景对齐和跨域鲁棒性方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM部署面临成本与隐私约束，需要本地小模型与云端大模型协同工作，但现有路由评估方法不系统，缺乏场景特定要求和分布外鲁棒性考虑。

Method: 1) 提出RouterXBench评估框架，包含路由能力、场景对齐、跨域鲁棒性三个维度；2) 利用内部隐藏状态而非输出概率或外部嵌入来捕捉模型不确定性；3) 提出ProbeDirichlet轻量级路由器，通过可学习的狄利克雷分布聚合跨层隐藏状态，采用概率训练。

Result: ProbeDirichlet在路由能力和高精度场景中分别相对最佳基线提升16.68%和18.86%，在不同模型家族、模型规模、异构任务和智能体工作流中表现一致。

Conclusion: RouterXBench为LLM路由提供了系统评估框架，ProbeDirichlet方法在捕获模型不确定性方面优于现有方法，能够有效提升混合模型部署的性能和鲁棒性。

Abstract: Large language models (LLMs) have achieved success, but cost and privacy constraints necessitate deploying smaller models locally while offloading complex queries to cloud-based models. Existing router evaluations are unsystematic, overlooking scenario-specific requirements and out-of-distribution robustness. We propose RouterXBench, a principled evaluation framework with three dimensions: router ability, scenario alignment, and cross-domain robustness. Unlike prior work that relies on output probabilities or external embeddings, we utilize internal hidden states that capture model uncertainty before answer generation. We introduce ProbeDirichlet, a lightweight router that aggregates cross-layer hidden states via learnable Dirichlet distributions with probabilistic training. Trained on multi-domain data, it generalizes robustly across in-domain and out-of-distribution scenarios. Our results show ProbeDirichlet achieves 16.68% and 18.86% relative improvements over the best baselines in router ability and high-accuracy scenarios, with consistent performance across model families, model scales, heterogeneous tasks, and agentic workflows.

</details>


### [53] [LLM-based Triplet Extraction from Financial Reports](https://arxiv.org/abs/2602.11886)
*Dante Wesslund,Ville Stenström,Pontus Linde,Alexander Holmberg*

Main category: cs.CL

TL;DR: 本文提出一个半自动化三元组提取流程，使用本体驱动的代理指标评估企业财务报告知识图谱构建，比较手动与自动本体方法，并开发混合验证策略显著减少幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 企业财务报告是构建知识图谱的宝贵结构化知识来源，但该领域缺乏标注的真实数据，使得评估变得困难。

Method: 采用半自动化流水线进行主谓宾三元组提取，使用本体一致性（Ontology Conformance）和忠实性（Faithfulness）作为代理评估指标；比较静态手动构建本体与全自动文档特定本体归纳方法；提出结合正则表达式匹配和LLM作为裁判的混合验证策略。

Result: 自动归纳的本体在所有配置中达到100%模式一致性，消除了手动方法中的本体漂移问题；混合验证策略将明显的主语幻觉率从65.2%降低到1.6%；发现了主语和宾语幻觉之间的系统性不对称，归因于财务文本中的被动结构和省略代理。

Conclusion: 本体驱动的代理指标在没有真实标注数据的情况下是有效的评估方法；自动本体归纳优于手动工程方法；混合验证策略能显著减少幻觉；财务文本的语言特征导致主语和宾语幻觉的不对称模式。

Abstract: Corporate financial reports are a valuable source of structured knowledge for Knowledge Graph construction, but the lack of annotated ground truth in this domain makes evaluation difficult. We present a semi-automated pipeline for Subject-Predicate-Object triplet extraction that uses ontology-driven proxy metrics, specifically Ontology Conformance and Faithfulness, instead of ground-truth-based evaluation. We compare a static, manually engineered ontology against a fully automated, document-specific ontology induction approach across different LLMs and two corporate annual reports. The automatically induced ontology achieves 100% schema conformance in all configurations, eliminating the ontology drift observed with the manual approach. We also propose a hybrid verification strategy that combines regex matching with an LLM-as-a-judge check, reducing apparent subject hallucination rates from 65.2% to 1.6% by filtering false positives caused by coreference resolution. Finally, we identify a systematic asymmetry between subject and object hallucinations, which we attribute to passive constructions and omitted agents in financial prose.

</details>


### [54] [Benchmark Illusion: Disagreement among LLMs and Its Scientific Consequences](https://arxiv.org/abs/2602.11898)
*Eddie Yang,Dashun Wang*

Main category: cs.CL

TL;DR: 论文揭示基准测试的准确性收敛可能掩盖模型间的深层认知分歧，当这些模型用于科学研究时，模型选择会成为影响结果可复现性的隐藏变量。


<details>
  <summary>Details</summary>
Motivation: 基准测试是衡量大语言模型进展的基础，但作者发现表面上的准确性收敛可能掩盖了模型之间深层的认知差异，这种差异可能会影响科学研究中使用LLM进行数据标注和推理的结果可靠性。

Method: 研究使用两个主要推理基准测试（MMLU-Pro和GPQA），分析不同LLM在达到相似准确性时的回答分歧情况，并通过重新分析已发表的教育学和政治科学研究，评估模型选择对研究结果的影响。

Result: 研究发现：1）准确性相当的LLM在16-66%的项目上存在分歧，顶级前沿模型之间也有16-38%的分歧；2）在教育学和政治科学的重新分析中，切换标注模型可使估计的处理效应变化超过80%，有时甚至改变效应的符号。

Conclusion: 研究揭示了"基准测试幻觉"现象——相等的准确性可能掩盖了模型间的深层分歧，模型选择成为科学研究可复现性的一个隐藏但关键变量，这对依赖LLM进行科学推断的研究提出了重要警示。

Abstract: Benchmarks underpin how progress in large language models (LLMs) is measured and trusted. Yet our analyses reveal that apparent convergence in benchmark accuracy can conceal deep epistemic divergence. Using two major reasoning benchmarks - MMLU-Pro and GPQA - we show that LLMs achieving comparable accuracy still disagree on 16-66% of items, and 16-38% among top-performing frontier models. These discrepancies suggest distinct error profiles for different LLMs. When such models are used for scientific data annotation and inference, their hidden disagreements propagate into research results: in re-analyses of published studies in education and political science, switching the annotation model can change estimated treatment effects by more than 80%, and in some cases reverses their sign. Together, these findings illustrate a benchmark illusion, where equal accuracy may conceal disagreement, with model choice becoming a hidden yet consequential variable for scientific reproducibility.

</details>


### [55] [AdaptEvolve: Improving Efficiency of Evolutionary AI Agents through Adaptive Model Selection](https://arxiv.org/abs/2602.11931)
*Pretam Ray,Pratik Prabhanjan Brahma,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

TL;DR: AdaptEvolve是一种自适应LLM选择方法，在进化顺序细化框架中利用内在生成置信度动态选择LLM，平衡计算效率与推理能力，平均减少37.9%推理成本同时保持97.5%的静态大模型基线精度。


<details>
  <summary>Details</summary>
Motivation: 进化代理系统在推理过程中反复调用大型语言模型，加剧了计算效率与推理能力之间的权衡。现有路由策略通常依赖静态启发式方法或外部控制器，未能明确考虑模型不确定性。

Method: 提出了AdaptEvolve方法：在进化顺序细化框架中，利用内在生成置信度来估计实时可解性，实现自适应的LLM选择。该方法通过置信度驱动选择，动态选择足够能力处理当前生成步骤的LLM。

Result: 实证结果表明，置信度驱动选择产生了有利的帕累托前沿，在多个基准测试中平均减少37.9%的总推理成本，同时保留了静态大模型基线97.5%的上限精度。

Conclusion: AdaptEvolve通过动态LLM选择有效平衡了进化代理系统中的计算效率与推理能力权衡，为多LLM进化细化提供了一种实用的自适应路由策略。

Abstract: Evolutionary agentic systems intensify the trade-off between computational efficiency and reasoning capability by repeatedly invoking large language models (LLMs) during inference. This setting raises a central question: how can an agent dynamically select an LLM that is sufficiently capable for the current generation step while remaining computationally efficient? While model cascades offer a practical mechanism for balancing this trade-off, existing routing strategies typically rely on static heuristics or external controllers and do not explicitly account for model uncertainty. We introduce AdaptEvolve: Adaptive LLM Selection for Multi-LLM Evolutionary Refinement within an evolutionary sequential refinement framework that leverages intrinsic generation confidence to estimate real-time solvability. Empirical results show that confidence-driven selection yields a favourable Pareto frontier, reducing total inference cost by an average of 37.9% across benchmarks while retaining 97.5% of the upper-bound accuracy of static large-model baselines. Our code is available at https://github.com/raypretam/adaptive_llm_selection.

</details>


### [56] [Cross-Modal Robustness Transfer (CMRT): Training Robust Speech Translation Models Using Adversarial Text](https://arxiv.org/abs/2602.11933)
*Abderrahmane Issam,Yusuf Can Semerci,Jan Scholtes,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 提出跨模态鲁棒性迁移框架，无需对抗语音数据即可提升端到端语音翻译模型对形态变化的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有端到端语音翻译模型主要基于"干净"数据集评估，忽略了现实世界中形态变化（如非母语或方言语音的屈折变化）带来的挑战，模型对此类变化高度脆弱

Method: 提出跨模态鲁棒性迁移框架，将文本模态的对抗鲁棒性迁移到语音模态，无需在训练中使用对抗语音数据

Result: 在四个语言对上的实验表明，该方法平均提升超过3个BLEU点的对抗鲁棒性

Conclusion: 该方法为无需生成对抗语音数据的鲁棒端到端语音翻译建立了新基准

Abstract: End-to-End Speech Translation (E2E-ST) has seen significant advancements, yet current models are primarily benchmarked on curated, "clean" datasets. This overlooks critical real-world challenges, such as morphological robustness to inflectional variations common in non-native or dialectal speech. In this work, we adapt a text-based adversarial attack targeting inflectional morphology to the speech domain and demonstrate that state-of-the-art E2E-ST models are highly vulnerable it. While adversarial training effectively mitigates such risks in text-based tasks, generating high-quality adversarial speech data remains computationally expensive and technically challenging. To address this, we propose Cross-Modal Robustness Transfer (CMRT), a framework that transfers adversarial robustness from the text modality to the speech modality. Our method eliminates the requirement for adversarial speech data during training. Extensive experiments across four language pairs demonstrate that CMRT improves adversarial robustness by an average of more than 3 BLEU points, establishing a new baseline for robust E2E-ST without the overhead of generating adversarial speech.

</details>


### [57] [Who is the richest club in the championship? Detecting and Rewriting Underspecified Questions Improve QA Performance](https://arxiv.org/abs/2602.11938)
*Yunchong Huang,Gianni Barlacchi,Sandro Pezzelle*

Main category: cs.CL

TL;DR: 研究发现问答基准中大量问题存在"未充分说明"问题，这是LLMs在标准QA测试中表现不佳的重要原因，而非模型本身能力限制。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在定义明确的问题上表现良好，但标准问答基准测试的解决程度仍然很低。研究者认为这种差距部分是由于"未充分说明的问题"造成的——这些问题的解释在没有额外上下文的情况下无法唯一确定。

Method: 1) 引入基于LLM的分类器来识别未充分说明的问题；2) 将该分类器应用于多个广泛使用的QA数据集进行分析；3) 进行受控重写实验，将未充分说明的问题重写为完全说明的变体，同时保持标准答案不变。

Result: 1) 在多个QA数据集中，16%到超过50%的基准问题被识别为未充分说明；2) LLMs在这些未充分说明的问题上表现显著更差；3) 当问题被重写为完全说明的变体后，QA性能一致提升，表明许多明显的QA失败源于问题未充分说明而非模型限制。

Conclusion: 未充分说明问题是QA评估中的重要混淆因素，强调了在基准设计中需要更加关注问题的清晰度。许多LLMs的QA失败实际上是由于问题表述不明确造成的，而非模型本身的能力缺陷。

Abstract: Large language models (LLMs) perform well on well-posed questions, yet standard question-answering (QA) benchmarks remain far from solved. We argue that this gap is partly due to underspecified questions - queries whose interpretation cannot be uniquely determined without additional context. To test this hypothesis, we introduce an LLM-based classifier to identify underspecified questions and apply it to several widely used QA datasets, finding that 16% to over 50% of benchmark questions are underspecified and that LLMs perform significantly worse on them. To isolate the effect of underspecification, we conduct a controlled rewriting experiment that serves as an upper-bound analysis, rewriting underspecified questions into fully specified variants while holding gold answers fixed. QA performance consistently improves under this setting, indicating that many apparent QA failures stem from question underspecification rather than model limitations. Our findings highlight underspecification as an important confound in QA evaluation and motivate greater attention to question clarity in benchmark design.

</details>


### [58] [Do Large Language Models Adapt to Language Variation across Socioeconomic Status?](https://arxiv.org/abs/2602.11939)
*Elisa Bassignana,Mike Zhang,Dirk Hovy,Amanda Cercas Curry*

Main category: cs.CL

TL;DR: LLMs对不同社会经济地位社群的语体适应能力有限，倾向于模仿上层社会语言风格，可能加剧语言不平等和社会分层。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在不同社会语境中的语体适应能力，因为这些模型越来越多地介入人际沟通，如果无法适应多样化的语言风格，可能会固化刻板印象、边缘化语言规范与模型差异较大的社群，从而加剧社会分层。

Method: 从Reddit和YouTube收集按社会经济地位分层的新数据集，用四个LLM基于该语料库的不完整文本生成补全，将LLM生成内容与原始内容在94个社会语言学指标（包括句法、修辞和词汇特征）上进行比较。

Result: LLMs仅在小程度上根据社会经济地位调整语体风格，常表现为近似或夸张模仿，且更倾向于模仿上层社会经济地位的语言风格。

Conclusion: LLMs存在放大语言等级风险，对其在基于代理的社会模拟、调查实验以及依赖语言风格作为社会信号的研究中的有效性提出质疑。

Abstract: Humans adjust their linguistic style to the audience they are addressing. However, the extent to which LLMs adapt to different social contexts is largely unknown. As these models increasingly mediate human-to-human communication, their failure to adapt to diverse styles can perpetuate stereotypes and marginalize communities whose linguistic norms are less closely mirrored by the models, thereby reinforcing social stratification. We study the extent to which LLMs integrate into social media communication across different socioeconomic status (SES) communities. We collect a novel dataset from Reddit and YouTube, stratified by SES. We prompt four LLMs with incomplete text from that corpus and compare the LLM-generated completions to the originals along 94 sociolinguistic metrics, including syntactic, rhetorical, and lexical features. LLMs modulate their style with respect to SES to only a minor extent, often resulting in approximation or caricature, and tend to emulate the style of upper SES more effectively. Our findings (1) show how LLMs risk amplifying linguistic hierarchies and (2) call into question their validity for agent-based social simulation, survey experiments, and any research relying on language style as a social signal.

</details>


### [59] [Scaling Model and Data for Multilingual Machine Translation with Open Large Language Models](https://arxiv.org/abs/2602.11961)
*Yuzhe Shang,Pengzhi Gao,Wei Liu,Jian Luan,Jinsong Su*

Main category: cs.CL

TL;DR: 基于Gemma3模型家族开发的MiLMMT-46在46种语言上实现了顶级多语言翻译性能，超越了多个SOTA模型，并与Google Translate、Gemini 3 Pro等专有系统表现相当。


<details>
  <summary>Details</summary>
Motivation: 近年来开源大语言模型在多语言能力方面不断改进，但需要研究如何通过持续预训练和指令微调来适配多语言机器翻译任务，并探索模型规模和数据规模对性能的影响。

Method: 基于Gemma3模型家族，通过持续预训练和指令微调来适配多语言机器翻译，开发了MiLMMT-46模型，并在46种语言上进行评估。

Result: MiLMMT-46在46种语言上实现了顶级翻译性能，持续超越Seed-X、HY-MT-1.5、TranslateGemma等最新SOTA模型，并与Google Translate、Gemini 3 Pro等专有系统表现相当。

Conclusion: 研究表明通过适当的模型规模和数据规模调整，开源大语言模型可以有效适配多语言机器翻译任务，实现与专有系统竞争的翻译质量。

Abstract: Open large language models (LLMs) have demonstrated improving multilingual capabilities in recent years. In this paper, we present a study of open LLMs for multilingual machine translation (MT) across a range of languages, and investigate the effects of model scaling and data scaling when adapting open LLMs to multilingual MT through continual pretraining and instruction finetuning. Based on the Gemma3 model family, we develop MiLMMT-46, which achieves top-tier multilingual translation performance across 46 languages. Extensive experiments show that MiLMMT-46 consistently outperforms recent state-of-the-art (SOTA) models, including Seed-X, HY-MT-1.5, and TranslateGemma, and achieves competitive performance with strong proprietary systems such as Google Translate and Gemini 3 Pro.

</details>


### [60] [DHPLT: large-scale multilingual diachronic corpora and word representations for semantic change modelling](https://arxiv.org/abs/2602.11968)
*Mariia Fedorova,Andrey Kutuzov,Khonzoda Umarova*

Main category: cs.CL

TL;DR: DHPLT是一个包含41种语言的历时语料库开放集合，基于HPLT网络爬取数据，利用网页时间戳作为文档创建时间信号，覆盖三个时间段，提供预计算的词嵌入和词汇替换，填补多语言历时语义变化建模资源空白。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏多语言历时语料库用于语义变化建模，现有资源主要局限于少数高资源语言，限制了该领域的研究发展。

Method: 基于HPLT网络爬取数据集，利用网页时间戳作为文档创建时间的近似信号，构建覆盖41种语言的历时语料库，包含三个时间段（2011-2015、2020-2021、2024至今），每个时间段每种语言包含100万文档。

Result: 创建了DHPLT开放资源集合，提供预计算的词类型和词符嵌入以及目标词的词汇替换，同时允许其他研究人员使用相同数据集定义自己的目标词。

Conclusion: DHPLT填补了多语言历时语义变化建模的资源空白，为该领域的新实验设置开辟了道路，所有资源已公开可用。

Abstract: In this resource paper, we present DHPLT, an open collection of diachronic corpora in 41 diverse languages. DHPLT is based on the web-crawled HPLT datasets; we use web crawl timestamps as the approximate signal of document creation time. The collection covers three time periods: 2011-2015, 2020-2021 and 2024-present (1 million documents per time period for each language). We additionally provide pre-computed word type and token embeddings and lexical substitutions for our chosen target words, while at the same time leaving it open for the other researchers to come up with their own target words using the same datasets. DHPLT aims at filling in the current lack of multilingual diachronic corpora for semantic change modelling (beyond a dozen of high-resource languages). It opens the way for a variety of new experimental setups in this field. All the resources described in this paper are available at https://data.hplt-project.org/three/diachronic/, sorted by language.

</details>


### [61] [Automatic Simplification of Common Vulnerabilities and Exposures Descriptions](https://arxiv.org/abs/2602.11982)
*Varpu Vehomäki,Kimmo K. Kaski*

Main category: cs.CL

TL;DR: 本研究探讨利用大语言模型(LLMs)自动简化网络安全文本（特别是CVE描述），发现现成LLMs虽能让文本表面更简单，但难以保持原意准确性。


<details>
  <summary>Details</summary>
Motivation: 网络安全信息对非专业人士理解困难，而自动文本简化(ATS)在医疗、科学等领域已有研究，但在快速变化且复杂的网络安全领域尚未探索。本研究旨在填补这一空白。

Method: 创建网络安全ATS基线，构建包含40个CVE描述的测试数据集，通过两轮网络安全专家调查进行评估，分析现成大语言模型的简化效果。

Result: 研究发现现成大语言模型能让文本看起来更简单，但在保持原意方面存在困难，即简化过程中容易损失或改变技术细节的准确性。

Conclusion: 虽然LLMs在网络安全文本简化方面有潜力，但需要进一步研究解决语义保持问题，才能在实际应用中可靠使用。

Abstract: Understanding cyber security is increasingly important for individuals and organizations. However, a lot of information related to cyber security can be difficult to understand to those not familiar with the topic. In this study, we focus on investigating how large language models (LLMs) could be utilized in automatic text simplification (ATS) of Common Vulnerability and Exposure (CVE) descriptions. Automatic text simplification has been studied in several contexts, such as medical, scientific, and news texts, but it has not yet been studied to simplify texts in the rapidly changing and complex domain of cyber security. We created a baseline for cyber security ATS and a test dataset of 40 CVE descriptions, evaluated by two groups of cyber security experts in two survey rounds. We have found that while out-of-the box LLMs can make the text appear simpler, they struggle with meaning preservation. Code and data are available at https://version.aalto.fi/gitlab/vehomav1/simplification\_nmi.

</details>


### [62] [LaCy: What Small Language Models Can and Should Learn is Not Just a Question of Loss](https://arxiv.org/abs/2602.12005)
*Szilvia Ujváry,Louis Béthune,Pierre Ablin,João Monteiro,Marco Cuturi,Michael Kirchhof*

Main category: cs.CL

TL;DR: LaCy是一种新的预训练方法，通过语法解析器帮助小语言模型识别哪些token应该学习预测，哪些应该委托给大模型，以提高事实准确性。


<details>
  <summary>Details</summary>
Motivation: 小语言模型（SLMs）由于参数规模有限，在预训练时无法容纳所有世界知识，容易产生事实错误。虽然可以通过访问外部资源（如大模型、文档、数据库）来缓解，但需要解决一个基本问题：哪些token应该由SLM学习预测，哪些应该通过<CALL> token委托给外部资源。

Method: 提出LaCy预训练方法，使用spaCy语法解析器增强损失信号，帮助SLM决定哪些token应该学习预测，哪些应该委托。该方法基于token选择哲学，不仅考虑损失值，还考虑token是否为真实可接受的文档延续。

Result: 实验表明，LaCy模型成功学会了哪些token应该预测，哪些应该委托求助。在与大模型的级联生成中，获得了更高的FactScore，并且优于Rho或LLM-judge训练的SLMs，同时更简单、更便宜。

Conclusion: LaCy方法有效解决了小语言模型在知识容量有限情况下的token选择问题，通过智能委托机制提高了生成的事实准确性，为SLMs的实际应用提供了实用解决方案。

Abstract: Language models have consistently grown to compress more world knowledge into their parameters, but the knowledge that can be pretrained into them is upper-bounded by their parameter size. Especially the capacity of Small Language Models (SLMs) is limited, leading to factually incorrect generations. This problem is often mitigated by giving the SLM access to an outside source: the ability to query a larger model, documents, or a database. Under this setting, we study the fundamental question of \emph{which tokens an SLM can and should learn} during pretraining, versus \emph{which ones it should delegate} via a \texttt{<CALL>} token. We find that this is not simply a question of loss: although the loss is predictive of whether a predicted token mismatches the ground-truth, some tokens are \emph{acceptable} in that they are truthful alternative continuations of a pretraining document, and should not trigger a \texttt{<CALL>} even if their loss is high. We find that a spaCy grammar parser can help augment the loss signal to decide which tokens the SLM should learn to delegate to prevent factual errors and which are safe to learn and predict even under high losses. We propose LaCy, a novel pretraining method based on this token selection philosophy. Our experiments demonstrate that LaCy models successfully learn which tokens to predict and where to delegate for help. This results in higher FactScores when generating in a cascade with a bigger model and outperforms Rho or LLM-judge trained SLMs, while being simpler and cheaper.

</details>


### [63] [Disentangling Ambiguity from Instability in Large Language Models: A Clinical Text-to-SQL Case Study](https://arxiv.org/abs/2602.12015)
*Angelo Ziletti,Leonardo D'Ambrosi*

Main category: cs.CL

TL;DR: CLUES框架将临床文本到SQL的不确定性分解为歧义性和不稳定性两个部分，通过二分语义图矩阵的Schur补计算不稳定性分数，改善错误预测并支持针对性干预。


<details>
  <summary>Details</summary>
Motivation: 在临床文本到SQL任务中，需要区分输出多样性的两种不同原因：输入歧义性（应触发澄清）和模型不稳定性（应触发人工审查），现有方法无法提供这种诊断性分解。

Method: 提出CLUES框架，将文本到SQL建模为两阶段过程（解释→答案），通过二分语义图矩阵的Schur补计算不稳定性分数，将语义不确定性分解为歧义性分数和不稳定性分数。

Result: 在AmbigQA/SituatedQA（黄金解释）和临床文本到SQL基准测试中，CLUES在失败预测上优于最先进的Kernel Language Entropy；在部署设置中保持竞争力，同时提供单一分数无法获得的诊断分解；高歧义/高不稳定性区域覆盖25%的查询但包含51%的错误，实现高效分诊。

Conclusion: CLUES框架通过将语义不确定性分解为歧义性和不稳定性，为临床文本到SQL系统提供了有效的错误诊断工具，能够将不确定性机制映射到针对性干预措施（歧义性进行查询优化，不稳定性进行模型改进），提高部署效率。

Abstract: Deploying large language models for clinical Text-to-SQL requires distinguishing two qualitatively different causes of output diversity: (i) input ambiguity that should trigger clarification, and (ii) model instability that should trigger human review. We propose CLUES, a framework that models Text-to-SQL as a two-stage process (interpretations --> answers) and decomposes semantic uncertainty into an ambiguity score and an instability score. The instability score is computed via the Schur complement of a bipartite semantic graph matrix. Across AmbigQA/SituatedQA (gold interpretations) and a clinical Text-to-SQL benchmark (known interpretations), CLUES improves failure prediction over state-of-the-art Kernel Language Entropy. In deployment settings, it remains competitive while providing a diagnostic decomposition unavailable from a single score. The resulting uncertainty regimes map to targeted interventions - query refinement for ambiguity, model improvement for instability. The high-ambiguity/high-instability regime contains 51% of errors while covering 25% of queries, enabling efficient triage.

</details>


### [64] [Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models](https://arxiv.org/abs/2602.12036)
*Xin Xu,Clive Bai,Kai Yang,Tianhao Chen,Yangkun Chen,Weijie Liu,Hao Chen,Yang Wang,Saiyong Yang,Can Yang*

Main category: cs.CL

TL;DR: Composition-RL通过将多个问题组合成新的可验证问题来更有效地利用有限的训练数据，提高推理能力


<details>
  <summary>Details</summary>
Motivation: 大规模可验证提示是RLVR成功的关键，但包含许多无信息示例且扩展成本高。随着训练进行，通过率为1的简单提示越来越多，减少了有效数据量。

Method: 提出Composition-RL方法，自动将多个问题组合成新的可验证问题，并使用这些组合提示进行RL训练。还提出了课程变体，逐步增加组合深度。

Result: 在4B到30B不同模型规模上的实验表明，Composition-RL一致优于在原始数据集上训练的RL。课程变体能进一步提升性能，并能实现更有效的跨领域RL。

Conclusion: Composition-RL是一种简单有效的方法，能更好地利用有限的可验证提示，提升推理能力，特别是在处理通过率为1的简单提示时表现优异。

Abstract: Large-scale verifiable prompts underpin the success of Reinforcement Learning with Verifiable Rewards (RLVR), but they contain many uninformative examples and are costly to expand further. Recent studies focus on better exploiting limited training data by prioritizing hard prompts whose rollout pass rate is 0. However, easy prompts with a pass rate of 1 also become increasingly prevalent as training progresses, thereby reducing the effective data size. To mitigate this, we propose Composition-RL, a simple yet useful approach for better utilizing limited verifiable prompts targeting pass-rate-1 prompts. More specifically, Composition-RL automatically composes multiple problems into a new verifiable question and uses these compositional prompts for RL training. Extensive experiments across model sizes from 4B to 30B show that Composition-RL consistently improves reasoning capability over RL trained on the original dataset. Performance can be further boosted with a curriculum variant of Composition-RL that gradually increases compositional depth over training. Additionally, Composition-RL enables more effective cross-domain RL by composing prompts drawn from different domains. Codes, datasets, and models are available at https://github.com/XinXU-USTC/Composition-RL.

</details>


### [65] [DeepSight: An All-in-One LM Safety Toolkit](https://arxiv.org/abs/2602.12092)
*Bo Zhang,Jiaxuan Guo,Lijun Li,Dongrui Liu,Sujin Chen,Guanxu Chen,Zhijie Zheng,Qihao Lin,Lewen Yan,Chen Qian,Yijin Zhou,Yuyao Wu,Shaoxiong Guo,Tianyi Du,Jingyi Yang,Xuhao Hu,Ziqi Miao,Xiaoya Lu,Jing Shao,Xia Hu*

Main category: cs.CL

TL;DR: DeepSight是一个开源项目，通过集成安全评估（DeepSafe）和诊断（DeepScan）工具包，实现了从黑盒到白盒的大型模型安全评估范式转变。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）和多模态大语言模型（MLLMs）的安全工作流中，评估、诊断和对齐通常由分离的工具处理，存在以下问题：安全评估只能定位外部行为风险而无法找出内部根本原因；安全诊断往往脱离具体风险场景，停留在可解释层面；安全对齐缺乏对内部机制变化的专门解释，可能降低模型的一般能力。

Method: 提出DeepSight开源项目，包含DeepSafe评估工具包和DeepScan诊断工具包。通过统一任务和数据协议，在两个阶段之间建立连接，将安全评估从黑盒转变为白盒洞察。

Result: DeepSight是第一个支持前沿AI风险评估以及联合安全评估和诊断的开源工具包。该项目具有低成本、可复现、高效和高可扩展性等特点。

Conclusion: DeepSight通过集成评估和诊断，系统性地解决了当前大模型安全流程中的分离问题，实现了从外部行为风险定位到内部根本原因分析的安全评估范式转变。

Abstract: As the development of Large Models (LMs) progresses rapidly, their safety is also a priority. In current Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) safety workflow, evaluation, diagnosis, and alignment are often handled by separate tools. Specifically, safety evaluation can only locate external behavioral risks but cannot figure out internal root causes. Meanwhile, safety diagnosis often drifts from concrete risk scenarios and remains at the explainable level. In this way, safety alignment lack dedicated explanations of changes in internal mechanisms, potentially degrading general capabilities. To systematically address these issues, we propose an open-source project, namely DeepSight, to practice a new safety evaluation-diagnosis integrated paradigm. DeepSight is low-cost, reproducible, efficient, and highly scalable large-scale model safety evaluation project consisting of a evaluation toolkit DeepSafe and a diagnosis toolkit DeepScan. By unifying task and data protocols, we build a connection between the two stages and transform safety evaluation from black-box to white-box insight. Besides, DeepSight is the first open source toolkit that support the frontier AI risk evaluation and joint safety evaluation and diagnosis.

</details>


### [66] [P-GenRM: Personalized Generative Reward Model with Test-time User-based Scaling](https://arxiv.org/abs/2602.12116)
*Pinyi Zhang,Ting-En Lin,Yuchuan Wu,Jingyang Chen,Zongqi Wang,Hua Yang,Ze Xu,Fei Huang,Kai Zhang,Yongbin Li*

Main category: cs.CL

TL;DR: P-GenRM：首个个性化生成奖励模型，通过测试时用户级缩放机制解决个性化大语言模型对齐中奖励信号获取的挑战


<details>
  <summary>Details</summary>
Motivation: 现有个性化奖励模型存在两个主要问题：1）将多样化的场景特定偏好过度简化为少量固定评估原则；2）对反馈有限的未见用户泛化能力差。需要更准确、用户特定的奖励信号来支持开放场景下的个性化大语言模型对齐。

Method: 提出P-GenRM模型，将偏好信号转化为结构化评估链，推导跨场景的自适应角色和评分标准。通过用户原型聚类和双粒度缩放机制：个体层面自适应缩放聚合每个用户的评分方案；原型层面整合相似用户偏好。引入测试时用户级缩放增强个性化对齐。

Result: 在广泛使用的个性化奖励模型基准上达到最先进水平，平均提升2.31%，并在分布外数据集上表现出强泛化能力。测试时用户级缩放额外提供3%的性能提升，展示了更强的个性化对齐能力和测试时可扩展性。

Conclusion: P-GenRM通过结构化评估链、用户原型聚类和双粒度缩放机制，有效解决了现有个性化奖励模型的局限性，为开放场景下的大语言模型个性化对齐提供了更准确、可泛化的奖励信号获取方案。

Abstract: Personalized alignment of large language models seeks to adapt responses to individual user preferences, typically via reinforcement learning. A key challenge is obtaining accurate, user-specific reward signals in open-ended scenarios. Existing personalized reward models face two persistent limitations: (1) oversimplifying diverse, scenario-specific preferences into a small, fixed set of evaluation principles, and (2) struggling with generalization to new users with limited feedback. To this end, we propose P-GenRM, the first Personalized Generative Reward Model with test-time user-based scaling. P-GenRM transforms preference signals into structured evaluation chains that derive adaptive personas and scoring rubrics across various scenarios. It further clusters users into User Prototypes and introduces a dual-granularity scaling mechanism: at the individual level, it adaptively scales and aggregates each user's scoring scheme; at the prototype level, it incorporates preferences from similar users. This design mitigates noise in inferred preferences and enhances generalization to unseen users through prototype-based transfer. Empirical results show that P-GenRM achieves state-of-the-art results on widely-used personalized reward model benchmarks, with an average improvement of 2.31%, and demonstrates strong generalization on an out-of-distribution dataset. Notably, Test-time User-based scaling provides an additional 3% boost, demonstrating stronger personalized alignment with test-time scalability.

</details>


### [67] [A Rule-based Computational Model for Gaidhlig Morphology](https://arxiv.org/abs/2602.12132)
*Peter J Barclay*

Main category: cs.CL

TL;DR: 使用Wiktionary数据构建盖尔语形态学规则模型，解决低资源语言训练数据不足问题，支持教学工具和解析器开发。


<details>
  <summary>Details</summary>
Motivation: 当前流行的神经模型需要大量训练数据，而低资源语言（如盖尔语）缺乏足够数据。需要开发能够有效利用有限样本数据的方法来支持这些语言的持续活力。

Method: 1. 使用Wiktionary数据构建基于规则的盖尔语形态学模型
2. 研究使用SQL查询不同词汇模式的出现情况
3. 开发声明式规则库，使Python工具能够推导盖尔语单词的屈折形式

Result: 1. 提出了一个基于规则的盖尔语形态学模型原型
2. 展示了SQL在查询词汇模式中的应用
3. 开发了能够推导盖尔语单词屈折形式的Python工具
4. 该功能可用于支持教学工具和规则依存解析器等高级工具

Conclusion: 基于规则的系统能够有效利用有限的样本数据，提供更好的可解释性，并为教学设计提供有用见解。这种方法通过将Wiktionary数据适应新用例，增加了现有数据的价值，为低资源语言支持提供了实用途径。

Abstract: Language models and software tools are essential to support the continuing vitality of lesser-used languages; however, currently popular neural models require considerable data for training, which normally is not available for such low-resource languages. This paper describes work-in-progress to construct a rule-based model of Gaidhlig morphology using data from Wiktionary, arguing that rule-based systems effectively leverage limited sample data, support greater interpretability, and provide insights useful in the design of teaching materials. The use of SQL for querying the occurrence of different lexical patterns is investigated, and a declarative rule-base is presented that allows Python utilities to derive inflected forms of Gaidhlig words. This functionality could be used to support educational tools that teach or explain language patterns, for example, or to support higher level tools such as rule-based dependency parsers. This approach adds value to the data already present in Wiktionary by adapting it to new use-cases.

</details>


### [68] [WavBench: Benchmarking Reasoning, Colloquialism, and Paralinguistics for End-to-End Spoken Dialogue Models](https://arxiv.org/abs/2602.12135)
*Yangzhuo Li,Shengpeng Ji,Yifu Chen,Tianle Liang,Haorong Ying,Yule Wang,Junbo Li,Jun Fang,Zhou Zhao*

Main category: cs.CL

TL;DR: WavBench是一个全面的语音对话基准测试，专门针对现实世界对话的复杂性设计，包含三个子集：Pro（增强推理能力）、Basic（口语化表达）和Acoustic（副语言能力），填补了现有评估忽视音频特性和认知深度的空白。


<details>
  <summary>Details</summary>
Motivation: 当前语音对话模型的评估主要遵循文本生成标准，忽视了副语言特征（如语调、节奏）和口语化表达，同时也缺乏对现代智能体所需认知深度的评估。现有基准无法充分评估现实世界对话的复杂性。

Method: 提出WavBench基准测试，采用三部分框架：1) Pro子集-设计更具挑战性的任务来测试增强推理模型；2) Basic子集-建立口语化表达的新标准，强调"可听性"而非书面准确性；3) Acoustic子集-评估在真实场景中的副语言理解、生成和隐式对话能力。

Result: 通过评估五个最先进的模型，WavBench为复杂问题解决、口语化表达和副语言保真度的交叉领域提供了关键见解。基准数据集和评估工具包已公开可用。

Conclusion: WavBench填补了语音对话评估的重要空白，为开发更强大的语音对话模型提供了指导，推动了该领域向更真实、更复杂的对话能力发展。

Abstract: With the rapid integration of advanced reasoning capabilities into spoken dialogue models, the field urgently demands benchmarks that transcend simple interactions to address real-world complexity. However, current evaluations predominantly adhere to text-generation standards, overlooking the unique audio-centric characteristics of paralinguistics and colloquialisms, alongside the cognitive depth required by modern agents. To bridge this gap, we introduce WavBench, a comprehensive benchmark designed to evaluate realistic conversational abilities where prior works fall short. Uniquely, WavBench establishes a tripartite framework: 1) Pro subset, designed to rigorously challenge reasoning-enhanced models with significantly increased difficulty; 2) Basic subset, defining a novel standard for spoken colloquialism that prioritizes "listenability" through natural vocabulary, linguistic fluency, and interactive rapport, rather than rigid written accuracy; and 3) Acoustic subset, covering explicit understanding, generation, and implicit dialogue to rigorously evaluate comprehensive paralinguistic capabilities within authentic real-world scenarios. Through evaluating five state-of-the-art models, WavBench offers critical insights into the intersection of complex problem-solving, colloquial delivery, and paralinguistic fidelity, guiding the evolution of robust spoken dialogue models. The benchmark dataset and evaluation toolkit are available at https://naruto-2024.github.io/wavbench.github.io/.

</details>


### [69] [CitiLink-Minutes: A Multilayer Annotated Dataset of Municipal Meeting Minutes](https://arxiv.org/abs/2602.12137)
*Ricardo Campos,Ana Filipa Pacheco,Ana Luísa Fernandes,Inês Cantante,Rute Rebouças,Luís Filipe Cunha,José Miguel Isidro,José Pedro Evans,Miguel Marques,Rodrigo Batista,Evelin Amorim,Alípio Jorge,Nuno Guimarães,Sérgio Nunes,António Leal,Purificação Silvano*

Main category: cs.CL

TL;DR: 本文介绍了CitiLink-Minutes数据集，这是一个包含120份葡萄牙市政会议记录的多层标注数据集，用于促进市政决策信息检索和自然语言处理研究。


<details>
  <summary>Details</summary>
Motivation: 市政会议记录作为官方决策文件对公民生活有直接影响，但由于缺乏标注数据集，在信息检索和自然语言处理领域研究不足，限制了计算模型的发展。

Method: 收集了6个葡萄牙城市的120份市政会议记录，进行多层标注：包括元数据、讨论主题和投票结果，由两名训练有素的标注员手动标注，并由经验丰富的语言学家审核。

Result: 创建了包含超过100万词汇的去标识化数据集，包含超过38,000个标注，遵循FAIR原则发布，并提供了元数据提取、主题分类和投票标签的基线结果。

Conclusion: CitiLink-Minutes数据集填补了市政会议记录研究的数据空白，展示了在NLP和IR任务中的潜力，同时促进了市政决策的透明访问。

Abstract: City councils play a crucial role in local governance, directly influencing citizens' daily lives through decisions made during municipal meetings. These deliberations are formally documented in meeting minutes, which serve as official records of discussions, decisions, and voting outcomes. Despite their importance, municipal meeting records have received little attention in Information Retrieval (IR) and Natural Language Processing (NLP), largely due to the lack of annotated datasets, which ultimately limit the development of computational models. To address this gap, we introduce CitiLink-Minutes, a multilayer dataset of 120 European Portuguese municipal meeting minutes from six municipalities. Unlike prior annotated datasets of parliamentary or video records, CitiLink-Minutes provides multilayer annotations and structured linkage of official written minutes. The dataset contains over one million tokens, with all personal identifiers de-identified. Each minute was manually annotated by two trained annotators and curated by an experienced linguist across three complementary dimensions: (1) metadata, (2) subjects of discussion, and (3) voting outcomes, totaling over 38,000 individual annotations. Released under FAIR principles and accompanied by baseline results on metadata extraction, topic classification, and vote labeling, CitiLink-Minutes demonstrates its potential for downstream NLP and IR tasks, while promoting transparent access to municipal decisions.

</details>


### [70] [dVoting: Fast Voting for dLLMs](https://arxiv.org/abs/2602.12153)
*Sicheng Feng,Zigeng Chen,Xinyin Ma,Gongfan Fang,Xinchao Wang*

Main category: cs.CL

TL;DR: dVoting是一种无需训练的快速投票技术，利用扩散大语言模型的任意位置生成能力，通过迭代采样、一致性分析、投票重生成来提升推理性能，在多个基准测试中显著提升效果。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（dLLMs）相比自回归模型具有并行生成优势，但推理能力仍有提升空间。研究者观察到对于同一提示的多样本生成中，大多数token预测一致，而性能差异主要由少数变化token决定，这启发了通过投票机制提升推理能力的方法。

Method: dVoting采用迭代精炼过程：1）对同一提示进行多次采样；2）通过一致性分析识别不确定token；3）利用dLLMs的任意位置生成能力，通过投票机制重新生成这些token；4）重复此过程直到收敛。整个过程无需额外训练。

Result: 在多个基准测试中，dVoting显著提升了性能：GSM8K提升6.22%-7.66%，MATH500提升4.40%-7.20%，ARC-C提升3.16%-14.84%，MMLU提升4.83%-5.74%。该方法仅需可接受的计算开销。

Conclusion: dVoting展示了扩散大语言模型在推理任务中的巨大潜力，通过利用其任意位置生成特性和投票机制，无需训练即可显著提升性能，为并行测试时扩展提供了有效解决方案。

Abstract: Diffusion Large Language Models (dLLMs) represent a new paradigm beyond autoregressive modeling, offering competitive performance while naturally enabling a flexible decoding process. Specifically, dLLMs can generate tokens at arbitrary positions in parallel, endowing them with significant potential for parallel test-time scaling, which was previously constrained by severe inefficiency in autoregressive modeling. In this work, we introduce dVoting, a fast voting technique that boosts reasoning capability without training, with only an acceptable extra computational overhead. dVoting is motivated by the observation that, across multiple samples for the same prompt, token predictions remain largely consistent, whereas performance is determined by a small subset of tokens exhibiting cross-sample variability. Leveraging the arbitrary-position generation capability of dLLMs, dVoting performs iterative refinement by sampling, identifying uncertain tokens via consistency analysis, regenerating them through voting, and repeating this process until convergence. Extensive evaluations demonstrate that dVoting consistently improves performance across various benchmarks. It achieves gains of 6.22%-7.66% on GSM8K, 4.40%-7.20% on MATH500, 3.16%-14.84% on ARC-C, and 4.83%-5.74% on MMLU. Our code is available at https://github.com/fscdc/dVoting

</details>


### [71] [Query-focused and Memory-aware Reranker for Long Context Processing](https://arxiv.org/abs/2602.12192)
*Yuqing Li,Jiangnan Li,Mo Yu,Guoxuan Ding,Zheng Lin,Weiping Wang,Jie Zhou*

Main category: cs.CL

TL;DR: 基于大语言模型中检索头的分析，提出一种利用选定注意力头分数估计查询-段落相关性的重排序框架，实现轻量高效的列表式排序，无需Likert级监督即可训练。


<details>
  <summary>Details</summary>
Motivation: 现有检索头分析表明大语言模型中的注意力机制包含相关性信息，但缺乏能利用整个候选列表整体信息的轻量级列表式重排序方法，且传统方法需要Likert级监督数据。

Method: 训练模型使用选定注意力头的注意力分数来估计查询与段落的相关性，构建列表式重排序框架，支持任意检索数据集训练，无需Likert级监督，仅需小规模模型（如4B参数）。

Result: 方法在多个领域（包括Wikipedia和长叙事数据集）超越现有最先进的点式和列表式重排序器，在LoCoMo基准测试中（评估对话理解和记忆使用能力）创下新纪录，支持灵活扩展（如上下文增强和中层注意力头训练）。

Conclusion: 提出的轻量级重排序框架通过利用大语言模型注意力头中的相关性信息，实现了高效且灵活的列表式排序，无需Likert级监督，在多个领域和任务中表现出色，具有实际应用价值。

Abstract: Built upon the existing analysis of retrieval heads in large language models, we propose an alternative reranking framework that trains models to estimate passage-query relevance using the attention scores of selected heads. This approach provides a listwise solution that leverages holistic information within the entire candidate shortlist during ranking. At the same time, it naturally produces continuous relevance scores, enabling training on arbitrary retrieval datasets without requiring Likert-scale supervision. Our framework is lightweight and effective, requiring only small-scale models (e.g., 4B parameters) to achieve strong performance. Extensive experiments demonstrate that our method outperforms existing state-of-the-art pointwise and listwise rerankers across multiple domains, including Wikipedia and long narrative datasets. It further establishes a new state-of-the-art on the LoCoMo benchmark that assesses the capabilities of dialogue understanding and memory usage. We further demonstrate that our framework supports flexible extensions. For example, augmenting candidate passages with contextual information further improves ranking accuracy, while training attention heads from middle layers enhances efficiency without sacrificing performance.

</details>


### [72] [Visual Reasoning Benchmark: Evaluating Multimodal LLMs on Classroom-Authentic Visual Problems from Primary Education](https://arxiv.org/abs/2602.12196)
*Mohamed Huti,Alasdair Mackintosh,Amy Waldock,Dominic Andrews,Maxime Lelièvre,Moritz Boos,Tobias Murray,Paul Atherton,Robin A. A. Ince,Oliver G. B. Garrod*

Main category: cs.CL

TL;DR: 本文提出了视觉推理基准（VRB），这是一个基于赞比亚和印度小学考试题构建的701道视觉问题数据集，用于评估多模态大语言模型在真实课堂环境中的视觉推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管AI模型在文本推理方面取得了最先进的成果，但它们在空间和关系结构推理方面的能力仍然是一个关键瓶颈。特别是在小学数学教育中，视觉推理至关重要，而现有模型在这方面存在不足，可能对课堂使用产生负面影响。

Method: 从赞比亚和印度的小学考试中收集了701道问题，构建了视觉推理基准（VRB）。该基准采用未经编辑、文本最少的图像，覆盖类比推理、模式完成、空间匹配等任务，旨在测试模型在真实教育环境中的能力。

Result: 研究发现模型能力存在"锯齿状边界"：在计数和缩放等静态技能上表现较好，但在折叠、反射和旋转等动态操作上存在明显的"空间天花板"。这些弱点可能导致错误评分、虚假脚手架和强化学生误解等风险。

Conclusion: VRB等教育导向的基准对于确定课堂中使用的多模态工具的功能边界至关重要。模型在视觉推理方面的局限性表明它们目前还不适合在课堂环境中直接应用，需要进一步改进。

Abstract: AI models have achieved state-of-the-art results in textual reasoning; however, their ability to reason over spatial and relational structures remains a critical bottleneck -- particularly in early-grade maths, which relies heavily on visuals. This paper introduces the visual reasoning benchmark (VRB), a novel dataset designed to evaluate Multimodal Large Language Models (MLLMs) on their ability to solve authentic visual problems from classrooms. This benchmark is built on a set of 701 questions sourced from primary school examinations in Zambia and India, which cover a range of tasks such as reasoning by analogy, pattern completion, and spatial matching. We outline the methodology and development of the benchmark which intentionally uses unedited, minimal-text images to test if models can meet realistic needs of primary education. Our findings reveal a ``jagged frontier'' of capability where models demonstrate better proficiency in static skills such as counting and scaling, but reach a distinct ``spatial ceiling'' when faced with dynamic operations like folding, reflection, and rotation. These weaknesses pose a risk for classroom use on visual reasoning problems, with the potential for incorrect marking, false scaffolding, and reinforcing student misconceptions. Consequently, education-focused benchmarks like the VRB are essential for determining the functional boundaries of multimodal tools used in classrooms.

</details>


### [73] [ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images](https://arxiv.org/abs/2602.12203)
*Mathieu Sibue,Andres Muñoz Garza,Samuel Mensah,Pranav Shetty,Zhiqiang Ma,Xiaomo Liu,Manuela Veloso*

Main category: cs.CL

TL;DR: ExStrucTiny是一个新的文档图像结构化信息提取基准数据集，统一了关键实体提取、关系提取和视觉问答任务，用于评估视觉语言模型在多样化文档类型和灵活模式下的细粒度提取能力。


<details>
  <summary>Details</summary>
Motivation: 现有文档理解基准在全面、细粒度的结构化提取方面存在不足，现有数据集局限于狭窄的实体本体、简单查询或同质文档类型，缺乏对多样化文档类型和灵活模式的适应性提取能力评估。

Method: 通过结合人工和合成人工验证样本的新颖流程构建ExStrucTiny基准数据集，涵盖更多样化的文档类型和提取场景，并对开放和封闭视觉语言模型在该基准上进行分析。

Result: 研究揭示了视觉语言模型在模式适应、查询不完整规范和答案定位等方面的挑战，为改进通用模型在文档结构化信息提取方面的能力提供了基础。

Conclusion: ExStrucTiny基准为改进通用视觉语言模型在文档结构化信息提取方面的能力提供了重要基础，有望推动该领域的发展。

Abstract: Enterprise documents, such as forms and reports, embed critical information for downstream applications like data archiving, automated workflows, and analytics. Although generalist Vision Language Models (VLMs) perform well on established document understanding benchmarks, their ability to conduct holistic, fine-grained structured extraction across diverse document types and flexible schemas is not well studied. Existing Key Entity Extraction (KEE), Relation Extraction (RE), and Visual Question Answering (VQA) datasets are limited by narrow entity ontologies, simple queries, or homogeneous document types, often overlooking the need for adaptable and structured extraction. To address these gaps, we introduce ExStrucTiny, a new benchmark dataset for structured Information Extraction (IE) from document images, unifying aspects of KEE, RE, and VQA. Built through a novel pipeline combining manual and synthetic human-validated samples, ExStrucTiny covers more varied document types and extraction scenarios. We analyze open and closed VLMs on this benchmark, highlighting challenges such as schema adaptation, query under-specification, and answer localization. We hope our work provides a bedrock for improving generalist models for structured IE in documents.

</details>


### [74] [Detecting Overflow in Compressed Token Representations for Retrieval-Augmented Generation](https://arxiv.org/abs/2602.12235)
*Julia Belikova,Danila Rozhevskii,Dennis Svirin,Konstantin Polev,Alexander Panchenko*

Main category: cs.CL

TL;DR: 该论文研究软压缩架构中的"token溢出"问题，提出一种检测方法，发现结合查询信息的轻量级分类器能有效识别压缩导致的性能下降。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在资源受限环境中的长上下文处理效率是重要挑战，软压缩架构通过用学习到的压缩token替换长序列来扩展有效上下文长度，但压缩极限以及何时压缩会擦除任务相关内容仍未被充分探索。

Method: 定义"token溢出"为压缩表示不再包含足够信息回答查询的情况，提出检测方法。在xRAG软压缩设置中，使用查询无关的饱和统计分离压缩和未压缩token表示，同时开发轻量级探测分类器分析查询和上下文xRAG表示。

Result: 查询无关的饱和统计能可靠分离压缩和未压缩token表示，但溢出检测能力有限。结合查询信息的轻量级分类器在HotpotQA、SQuADv2和TriviaQA数据集上平均达到0.72 AUC-ROC，显著提升检测性能。

Conclusion: 该研究从查询无关诊断发展到查询感知检测器，实现了低成本的预LLM门控机制来减轻压缩引起的错误，为高效长上下文处理提供了实用工具。

Abstract: Efficient long-context processing remains a crucial challenge for contemporary large language models (LLMs), especially in resource-constrained environments. Soft compression architectures promise to extend effective context length by replacing long token sequences with smaller sets of learned compressed tokens. Yet, the limits of compressibility -- and when compression begins to erase task-relevant content -- remain underexplored. In this paper, we define \emph{token overflow} as a regime in which compressed representations no longer contain sufficient information to answer a given query, and propose a methodology to characterize and detect it. In the xRAG soft-compression setting, we find that query-agnostic saturation statistics reliably separate compressed from uncompressed token representations, providing a practical tool for identifying compressed tokens but showing limited overflow detection capability. Lightweight probing classifiers over both query and context xRAG representations detect overflow with 0.72 AUC-ROC on average on HotpotQA, SQuADv2, and TriviaQA datasets, demonstrating that incorporating query information improves detection performance. These results advance from query-independent diagnostics to query-aware detectors, enabling low-cost pre-LLM gating to mitigate compression-induced errors.

</details>


### [75] [Moonshine v2: Ergodic Streaming Encoder ASR for Latency-Critical Speech Applications](https://arxiv.org/abs/2602.12241)
*Manjunath Kudlur,Evan King,James Wang,Pete Warden*

Main category: cs.CL

TL;DR: Moonshine v2是一种采用滑动窗口自注意力的流式编码器ASR模型，在边缘设备上实现了低延迟、高精度的语音识别。


<details>
  <summary>Details</summary>
Motivation: 延迟敏感的语音应用（如实时转录、语音命令）需要在资源受限的边缘设备上实现低首次令牌时间（TTFT）和高转录精度。传统全注意力Transformer编码器虽然精度高，但具有二次复杂度，导致TTFT随语音长度线性增长，不适合流式应用。

Method: 引入Moonshine v2模型，采用滑动窗口自注意力机制替代全注意力，实现有界的低延迟推理，同时保持强大的局部上下文能力。

Result: 在标准基准测试中取得了最先进的词错误率，精度与尺寸大6倍的模型相当，同时运行速度显著更快。证明了精心设计的局部注意力能以更小的尺寸和延迟成本达到全注意力的精度水平。

Conclusion: 通过滑动窗口自注意力机制，Moonshine v2在边缘设备上实现了低延迟、高精度的流式ASR，为交互式语音界面在边缘设备上的应用开辟了新可能性。

Abstract: Latency-critical speech applications (e.g., live transcription, voice commands, and real-time translation) demand low time-to-first-token (TTFT) and high transcription accuracy, particularly on resource-constrained edge devices. Full-attention Transformer encoders remain a strong accuracy baseline for automatic speech recognition (ASR) because every frame can directly attend to every other frame, which resolves otherwise locally ambiguous acoustics using distant lexical context. However, this global dependency incurs quadratic complexity in sequence length, inducing an inherent "encode-the-whole-utterance" latency profile. For streaming use cases, this causes TTFT to grow linearly with utterance length as the encoder must process the entire prefix before any decoder token can be emitted. To better meet the needs of on-device, streaming ASR use cases we introduce Moonshine v2, an ergodic streaming-encoder ASR model that employs sliding-window self-attention to achieve bounded, low-latency inference while preserving strong local context. Our models achieve state of the art word error rates across standard benchmarks, attaining accuracy on-par with models 6x their size while running significantly faster. These results demonstrate that carefully designed local attention is competitive with the accuracy of full attention at a fraction of the size and latency cost, opening new possibilities for interactive speech interfaces on edge devices.

</details>


### [76] [A technical curriculum on language-oriented artificial intelligence in translation and specialised communication](https://arxiv.org/abs/2602.12251)
*Ralph Krüger*

Main category: cs.CL

TL;DR: 该论文提出了一个面向语言与翻译行业的语言导向AI技术课程，旨在提升从业者的领域特定AI素养，核心内容包括向量嵌入、神经网络基础、分词和Transformer架构。


<details>
  <summary>Details</summary>
Motivation: 在AI驱动的语言与翻译行业中，从业者需要具备领域特定的技术AI素养，以发展计算思维、算法意识和算法能动性，从而增强在AI工作环境中的数字韧性。

Method: 设计了一个聚焦四个核心模块的技术课程：1) 向量嵌入，2) 神经网络技术基础，3) 分词技术，4) Transformer神经网络。该课程在科隆应用技术大学翻译与多语言传播研究所的AI相关硕士课程中进行了教学实践测试。

Result: 课程的教学适宜性测试表明其教学效果良好，但参与者反馈显示课程需要更高层次的教学支架支持（如讲师指导），以创造最佳学习条件。

Conclusion: 该语言导向AI技术课程能有效培养语言与翻译行业从业者的技术AI素养，但需要整合到有适当教学支持的结构化教学环境中，才能最大化其教育效果。

Abstract: This paper presents a technical curriculum on language-oriented artificial intelligence (AI) in the language and translation (L&T) industry. The curriculum aims to foster domain-specific technical AI literacy among stakeholders in the fields of translation and specialised communication by exposing them to the conceptual and technical/algorithmic foundations of modern language-oriented AI in an accessible way. The core curriculum focuses on 1) vector embeddings, 2) the technical foundations of neural networks, 3) tokenization and 4) transformer neural networks. It is intended to help users develop computational thinking as well as algorithmic awareness and algorithmic agency, ultimately contributing to their digital resilience in AI-driven work environments. The didactic suitability of the curriculum was tested in an AI-focused MA course at the Institute of Translation and Multilingual Communication at TH Koeln. Results suggest the didactic effectiveness of the curriculum, but participant feedback indicates that it should be embedded into higher-level didactic scaffolding - e.g., in the form of lecturer support - in order to enable optimal learning conditions.

</details>


### [77] [T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization](https://arxiv.org/abs/2602.12262)
*Tunyu Zhang,Xinxi Zhang,Ligong Han,Haizhou Shi,Xiaoxiao He,Zhuowei Li,Hao Wang,Kai Xu,Akash Srivastava,Hao Wang,Vladimir Pavlovic,Dimitris N. Metaxas*

Main category: cs.CL

TL;DR: 提出了轨迹自蒸馏框架，通过蒸馏模型自身的生成轨迹来改进DLLMs的少步解码性能，显著减少了与全步解码的差距。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型理论上可以通过并行解码实现快速文本生成，但实际推理效率受限于需要大量细化步骤。如果激进地减少步数，会导致生成质量显著下降。

Method: 提出轨迹自蒸馏框架，采用直接判别优化（DDO）方法，这是一种反向KL目标，促进模式寻求蒸馏，鼓励学生模型专注于教师模型的高概率模式。

Result: 在多个基准测试中，该方法始终优于强大的少步基线方法和标准训练方法，特别是在严格的步数预算下。虽然全步解码仍然更优，但显著缩小了差距。

Conclusion: 该方法为实用的少步DLLMs建立了坚实基础，代码已开源。

Abstract: Diffusion large language models (DLLMs) have the potential to enable fast text generation by decoding multiple tokens in parallel. However, in practice, their inference efficiency is constrained by the need for many refinement steps, while aggressively reducing the number of steps leads to a substantial degradation in generation quality. To alleviate this, we propose a trajectory self-distillation framework that improves few-step decoding by distilling the model's own generative trajectories. We incorporate Direct Discriminative Optimization (DDO), a reverse-KL objective that promotes mode-seeking distillation and encourages the student to concentrate on high-probability teacher modes. Across benchmarks, our approach consistently outperforms strong few-step baselines and standard training under tight step budgets. Although full-step decoding remains superior, we substantially narrow the gap, establishing a strong foundation towards practical few-step DLLMs. The source code is available at https://github.com/Tyrion58/T3D.

</details>


### [78] [On-Policy Context Distillation for Language Models](https://arxiv.org/abs/2602.12275)
*Tianzhu Ye,Li Dong,Xun Wu,Shaohan Huang,Furu Wei*

Main category: cs.CL

TL;DR: 提出了一种名为OPCD的新框架，将上下文蒸馏与在线策略蒸馏相结合，通过最小化反向KL散度来训练学生模型内部化上下文知识。


<details>
  <summary>Details</summary>
Motivation: 当前上下文蒸馏方法虽然能让语言模型将上下文知识参数化，但缺乏系统性的训练框架来有效整合在线策略蒸馏的优势。

Method: 提出On-Policy Context Distillation (OPCD)框架，训练学生模型基于自身生成的轨迹，同时最小化与上下文条件教师模型的反向Kullback-Leibler散度。

Result: 在数学推理、文本游戏和领域特定任务中，OPCD持续优于基线方法，实现更高任务准确率并更好保持分布外能力。同时支持跨尺寸蒸馏，让小模型从大模型中内化经验知识。

Conclusion: OPCD为上下文蒸馏提供了有效的在线策略训练框架，在经验知识蒸馏和系统提示蒸馏等应用中表现出色，为知识内化和模型压缩提供了新途径。

Abstract: Context distillation enables language models to internalize in-context knowledge into their parameters. In our work, we propose On-Policy Context Distillation (OPCD), a framework that bridges on-policy distillation with context distillation by training a student model on its own generated trajectories while minimizing reverse Kullback-Leibler divergence against a context-conditioned teacher. We demonstrate the effectiveness of OPCD on two important applications: experiential knowledge distillation, where models extract and consolidate transferable knowledge from their historical solution traces, and system prompt distillation, where models internalize beneficial behaviors encoded in optimized prompts. Across mathematical reasoning, text-based games, and domain-specific tasks, OPCD consistently outperforms baseline methods, achieving higher task accuracy while better preserving out-of-distribution capabilities. We further show that OPCD enables effective cross-size distillation, where smaller student models can internalize experiential knowledge from larger teachers.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [79] [MTFM: A Scalable and Alignment-free Foundation Model for Industrial Recommendation in Meituan](https://arxiv.org/abs/2602.11235)
*Xin Song,Zhilin Guan,Ruidong Han,Binghao Tang,Tianwen Chen,Bing Li,Zihao Li,Han Zhang,Fei Jiang,Chaolin Xie,Chi Ma,Chunyang Jiang,Chunzhen Jing,Dengxuan Li,Fengyi Li,Lei Yu,Mengyao Sun,Pu Wang,Qing Wang,Rui Fan,Shangyu Chen,Shifeng Du,Siyuan Bai,Wei Lin,Wentao Zhu,Zhou Han,Zhuo Chen,Zikang Xu*

Main category: cs.IR

TL;DR: MTFM（美团推荐基础模型）是一个基于Transformer的框架，通过将跨域数据转换为异构token、采用多场景用户级样本聚合、分组查询注意力等创新方法，解决了传统跨域和多场景推荐系统资源消耗大、输入对齐要求严格的问题。


<details>
  <summary>Details</summary>
Motivation: 工业推荐系统通常涉及多个场景，但现有的跨域推荐（CDR）和多场景推荐（MSR）方法需要大量资源和严格的输入对齐，限制了系统的可扩展性。

Method: 1. 将跨域数据转换为异构token，以对齐自由的方式捕获多场景知识；2. 引入多场景用户级样本聚合，减少实例总数以提高训练吞吐量；3. 集成分组查询注意力（Grouped-Query Attention）和定制化的混合目标注意力（Hybrid Target Attention）以降低内存使用和计算复杂度；4. 实施系统级优化，如内核融合和消除CPU-GPU阻塞。

Result: 离线和在线实验验证了MTFM的有效性，表明通过扩展模型容量和多场景训练数据可以实现显著的性能提升。

Conclusion: MTFM通过创新的对齐自由架构和系统优化，有效解决了多场景推荐系统的资源消耗和扩展性问题，为工业推荐系统提供了高效可扩展的解决方案。

Abstract: Industrial recommendation systems typically involve multiple scenarios, yet existing cross-domain (CDR) and multi-scenario (MSR) methods often require prohibitive resources and strict input alignment, limiting their extensibility. We propose MTFM (Meituan Foundation Model for Recommendation), a transformer-based framework that addresses these challenges. Instead of pre-aligning inputs, MTFM transforms cross-domain data into heterogeneous tokens, capturing multi-scenario knowledge in an alignment-free manner. To enhance efficiency, we first introduce a multi-scenario user-level sample aggregation that significantly enhances training throughput by reducing the total number of instances. We further integrate Grouped-Query Attention and a customized Hybrid Target Attention to minimize memory usage and computational complexity. Furthermore, we implement various system-level optimizations, such as kernel fusion and the elimination of CPU-GPU blocking, to further enhance both training and inference throughput. Offline and online experiments validate the effectiveness of MTFM, demonstrating that significant performance gains are achieved by scaling both model capacity and multi-scenario training data.

</details>


### [80] [From Noise to Order: Learning to Rank via Denoising Diffusion](https://arxiv.org/abs/2602.11453)
*Sajad Ebrahimi,Bhaskar Mitra,Negar Arabzadeh,Ye Yuan,Haolun Wu,Fattane Zarrinkalam,Ebrahim Bagheri*

Main category: cs.IR

TL;DR: 提出DiffusionRank，一种基于去噪扩散的深度生成式学习排序方法，替代传统的判别式方法，通过建模特征向量和相关性标签的联合分布来提升排序鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统学习排序方法局限于判别式机器学习，仅建模文档相关性概率。作者认为在生成式设置下，能够解释完整数据分布的候选解能产生更鲁棒的排序模型。

Method: 提出DiffusionRank，扩展现有的表格数据去噪扩散生成模型TabDiff，创建生成式等价于经典判别式点对点和点对点学习排序目标。

Result: 实验结果表明，DiffusionRank模型相比其判别式对应方法有显著改进。

Conclusion: 这项工作为未来研究指明了丰富方向，展示了如何利用深度生成建模方法（如扩散模型）来改进信息检索中的学习排序。

Abstract: In information retrieval (IR), learning-to-rank (LTR) methods have traditionally limited themselves to discriminative machine learning approaches that model the probability of the document being relevant to the query given some feature representation of the query-document pair. In this work, we propose an alternative denoising diffusion-based deep generative approach to LTR that instead models the full joint distribution over feature vectors and relevance labels. While in the discriminative setting, an over-parameterized ranking model may find different ways to fit the training data, we hypothesize that candidate solutions that can explain the full data distribution under the generative setting produce more robust ranking models. With this motivation, we propose DiffusionRank that extends TabDiff, an existing denoising diffusion-based generative model for tabular datasets, to create generative equivalents of classical discriminative pointwise and pairwise LTR objectives. Our empirical results demonstrate significant improvements from DiffusionRank models over their discriminative counterparts. Our work points to a rich space for future research exploration on how we can leverage ongoing advancements in deep generative modeling approaches, such as diffusion, for learning-to-rank in IR.

</details>


### [81] [KuaiSearch: A Large-Scale E-Commerce Search Dataset for Recall, Ranking, and Relevance](https://arxiv.org/abs/2602.11518)
*Yupeng Li,Ben Chen,Mingyue Cheng,Zhiding Liu,Xuxin Zhang,Chenyi Lei,Wenwu Ou*

Main category: cs.IR

TL;DR: KuaiSearch是当前最大的电子商务搜索数据集，基于快手平台真实用户搜索交互构建，覆盖冷启动用户和长尾商品，并系统覆盖召回、排序和相关性判断三个搜索关键阶段。


<details>
  <summary>Details</summary>
Motivation: 现实电商搜索面临查询高度模糊、商品文本噪声大且语义顺序弱、用户偏好多样等挑战，而现有数据集存在查询人为构建、过滤冷启动用户和长尾商品、文本匿名化、仅覆盖单一搜索阶段等问题，限制了基于大语言模型的电商搜索研究。

Method: 基于快手平台真实用户搜索交互构建KuaiSearch数据集，保留真实用户查询和自然语言商品文本，覆盖冷启动用户和长尾商品，并系统覆盖召回、排序和相关性判断三个搜索关键阶段。

Result: KuaiSearch成为当前最大的电商搜索数据集，通过多角度（商品、用户、查询）综合分析，并在多个代表性搜索任务上建立基准实验，证明其为现实世界电商搜索研究提供了有价值的基础。

Conclusion: KuaiSearch解决了现有电商搜索数据集的局限性，为基于大语言模型的电商搜索研究提供了高质量、真实、全面的数据集基础，有助于推动该领域的发展。

Abstract: E-commerce search serves as a central interface, connecting user demands with massive product inventories and plays a vital role in our daily lives. However, in real-world applications, it faces challenges, including highly ambiguous queries, noisy product texts with weak semantic order, and diverse user preferences, all of which make it difficult to accurately capture user intent and fine-grained product semantics. In recent years, significant advances in large language models (LLMs) for semantic representation and contextual reasoning have created new opportunities to address these challenges. Nevertheless, existing e-commerce search datasets still suffer from notable limitations: queries are often heuristically constructed, cold-start users and long-tail products are filtered out, query and product texts are anonymized, and most datasets cover only a single stage of the search pipeline. Collectively, these issues constrain research on LLM-based e-commerce search. To address these challenges, we construct and release KuaiSearch. To the best of our knowledge, it is the largest e-commerce search dataset currently available. KuaiSearch is built upon real user search interactions from the Kuaishou platform, preserving authentic user queries and natural-language product texts, covering cold-start users and long-tail products, and systematically spanning three key stages of the search pipeline: recall, ranking, and relevance judgment. We conduct a comprehensive analysis of KuaiSearch from multiple perspectives, including products, users, and queries, and establish benchmark experiments across several representative search tasks. Experimental results demonstrate that KuaiSearch provides a valuable foundation for research on real-world e-commerce search.

</details>


### [82] [LASER: An Efficient Target-Aware Segmented Attention Framework for End-to-End Long Sequence Modeling](https://arxiv.org/abs/2602.11562)
*Tianhe Lin,Ziwei Xiong,Baoyuan Ou,Yingjie Qin,Lai Xu,Xiaocheng Zhong,Yao Hu,Zhiyong Wang,Tao Zhou,Yubin Xu,Di Wu*

Main category: cs.IR

TL;DR: LASER是一个用于超长用户行为序列建模的优化框架，通过系统优化和算法创新解决了工业推荐系统中的延迟瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 工业推荐系统中建模超长用户行为序列面临严格的"延迟墙"约束，包括检索海量用户历史的高I/O延迟和标准注意力机制的二次计算复杂度瓶颈。

Method: 提出LASER框架，包含两方面创新：1) SeqVault系统架构，采用混合DRAM-SSD索引策略降低检索延迟；2) Segmented Target Attention机制，通过sigmoid门控过滤噪声项，配合GSTA模块捕获跨片段依赖。

Result: SeqVault将检索延迟降低50%，CPU使用降低75%；在线A/B测试中，LASER在1亿日活用户上实现ADVV提升2.36%，收入提升2.08%，显著超越现有方法。

Conclusion: LASER成功解决了超长序列建模的延迟瓶颈，证明了其在工业推荐系统中的可扩展性和商业价值，为实时处理用户全生命周期行为提供了有效解决方案。

Abstract: Modeling ultra-long user behavior sequences is pivotal for capturing evolving and lifelong interests in modern recommendation systems. However, deploying such models in real-time industrial environments faces a strict "Latency Wall", constrained by two distinct bottlenecks: the high I/O latency of retrieving massive user histories and the quadratic computational complexity of standard attention mechanisms. To break these bottlenecks, we present LASER, a full-stack optimization framework developed and deployed at Xiaohongshu (RedNote). Our approach tackles the challenges through two complementary innovations: (1) System efficiency: We introduce SeqVault, a unified schema-aware serving infrastructure for long user histories. By implementing a hybrid DRAM-SSD indexing strategy, SeqVault reduces retrieval latency by 50% and CPU usage by 75%, ensuring millisecond-level access to full real-time and life-cycle user histories. (2) Algorithmic efficiency: We propose a Segmented Target Attention (STA) mechanism to address the computational overhead. Motivated by the inherent sparsity of user interests, STA employs a sigmoid-based gating strategy that acts as a silence mechanism to filter out noisy items. Subsequently, a lightweight Global Stacked Target Attention (GSTA) module refines these compressed segments to capture cross-segment dependencies without incurring high computational costs. This design performs effective sequence compression, reducing the complexity of long-sequence modeling while preserving critical signals. Extensive offline evaluations demonstrate that LASER consistently outperforms state-of-the-art baselines. In large-scale online A/B testing serving over 100 million daily active users, LASER achieved a 2.36% lift in ADVV and a 2.08% lift in revenue, demonstrating its scalability and significant commercial impact.

</details>


### [83] [Analytical Search](https://arxiv.org/abs/2602.11581)
*Yiteng Tu,Shuo Miao,Weihang Su,Yiqun Liu,Qingyao Ai*

Main category: cs.IR

TL;DR: 提出"分析性搜索"作为新兴搜索范式，旨在满足跨领域分析性信息需求，通过证据驱动、流程导向的工作流解决现有检索范式在端到端问题解决和可验证性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有信息检索范式（基于相关性的文档排序或RAG）难以满足分析性信息需求（如趋势分析、因果影响评估）的端到端要求，要么侧重信息查找而非问题解决，要么将一切视为简单问答，缺乏对推理、证据使用和可验证性的控制。

Method: 提出分析性搜索作为独立范式，将其重构为证据驱动、流程导向的分析工作流，明确建模分析意图、检索证据进行融合，并通过结构化多步推理产生可验证结论。提出统一系统框架，整合查询理解、召回导向检索、推理感知融合和自适应验证。

Result: 提出了分析性搜索的概念框架和系统设计，将其定位为区别于现有范式的新兴搜索范式，并讨论了构建分析性搜索引擎的潜在研究方向。

Conclusion: 分析性搜索具有重要的概念意义和实践价值，呼吁学界业界共同努力开发下一代支持分析性信息需求的搜索引擎。

Abstract: Analytical information needs, such as trend analysis and causal impact assessment, are prevalent across various domains including law, finance, science, and much more. However, existing information retrieval paradigms, whether based on relevance-oriented document ranking or retrieval-augmented generation (RAG) with large language models (LLMs), often struggle to meet the end-to-end requirements of such tasks at the corpus scale. They either emphasize information finding rather than end-to-end problem solving, or simply treat everything as naive question answering, offering limited control over reasoning, evidence usage, and verifiability. As a result, they struggle to support analytical queries that have diverse utility concepts and high accountability requirements.
  In this paper, we propose analytical search as a distinct and emerging search paradigm designed to fulfill these analytical information needs. Analytical search reframes search as an evidence-governed, process-oriented analytical workflow that explicitly models analytical intent, retrieves evidence for fusion, and produces verifiable conclusions through structured, multi-step inference. We position analytical search in contrast to existing paradigms, and present a unified system framework that integrates query understanding, recall-oriented retrieval, reasoning-aware fusion, and adaptive verification. We also discuss potential research directions for the construction of analytical search engines. In this way, we highlight the conceptual significance and practical importance of analytical search and call on efforts toward the next generation of search engines that support analytical information needs.

</details>


### [84] [Recurrent Preference Memory for Efficient Long-Sequence Generative Recommendation](https://arxiv.org/abs/2602.11605)
*Yixiao Chen,Yuan Wang,Yue Liu,Qiyao Wang,Ke Cheng,Xin Xu,Juntong Yan,Shuojin Yang,Menghao Guo,Jun Zhang,Huan Yu,Jie Jiang*

Main category: cs.IR

TL;DR: Rec2PM框架通过将长用户交互历史压缩为紧凑的偏好记忆令牌，解决了生成式推荐模型在处理终身序列时的计算成本和噪声积累问题。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐模型通常使用全注意力机制建模用户行为，但在处理终身序列时面临计算成本过高和随机交互导致的噪声积累问题。

Method: 提出Rec2PM框架，采用自参考教师强制策略：利用历史全局视图生成参考记忆作为监督目标，实现并行化循环更新；将记忆表示为令牌嵌入而非大量KV缓存。

Result: 在大规模基准测试中，Rec2PM显著降低推理延迟和内存占用，同时比全序列模型获得更优的准确性；偏好记忆起到去噪信息瓶颈作用。

Conclusion: Rec2PM通过偏好记忆压缩实现了高效且准确的终身序列建模，为生成式推荐系统提供了可扩展的解决方案。

Abstract: Generative recommendation (GenRec) models typically model user behavior via full attention, but scaling to lifelong sequences is hindered by prohibitive computational costs and noise accumulation from stochastic interactions. To address these challenges, we introduce Rec2PM, a framework that compresses long user interaction histories into compact Preference Memory tokens. Unlike traditional recurrent methods that suffer from serial training, Rec2PM employs a novel self-referential teacher-forcing strategy: it leverages a global view of the history to generate reference memories, which serve as supervision targets for parallelized recurrent updates. This allows for fully parallel training while maintaining the capability for iterative updates during inference. Additionally, by representing memory as token embeddings rather than extensive KV caches, Rec2PM achieves extreme storage efficiency. Experiments on large-scale benchmarks show that Rec2PM significantly reduces inference latency and memory footprint while achieving superior accuracy compared to full-sequence models. Analysis reveals that the Preference Memory functions as a denoising Information Bottleneck, effectively filtering interaction noise to capture robust long-term interests.

</details>


### [85] [Evolutionary Router Feature Generation for Zero-Shot Graph Anomaly Detection with Mixture-of-Experts](https://arxiv.org/abs/2602.11622)
*Haiyang Jiang,Tong Chen,Xinyi Gao,Guansong Pang,Quoc Viet Hung Nguyen,Hongzhi Yin*

Main category: cs.IR

TL;DR: EvoFG：基于进化特征生成和记忆增强路由器的零样本图异常检测MoE框架


<details>
  <summary>Details</summary>
Motivation: 现有单GNN方法难以建模跨图的异构结构、特征和异常模式，而MoE架构虽能集成多种GNN专家，但在零样本GAD中面临分布偏移导致的路由挑战：节点语义跨图差异大，基于特征的路由易产生偏置分配；异常图分布差异显著，现有路由器难以捕捉领域不变的路由原则。

Method: 提出EvoFG框架：1）进化特征生成方案，通过LLM生成器和Shapley引导评估迭代构建和选择信息丰富的结构特征；2）记忆增强路由器，设计不变学习目标以捕捉分布偏移下的可迁移路由模式。

Result: 在六个基准数据集上的大量实验表明，EvoFG始终优于最先进的基线方法，实现了强大且稳定的零样本GAD性能。

Conclusion: EvoFG通过进化特征生成和记忆增强路由器有效解决了MoE在零样本图异常检测中的路由挑战，显著提升了跨图分布的泛化能力。

Abstract: Zero-shot graph anomaly detection (GAD) has attracted increasing attention recent years, yet the heterogeneity of graph structures, features, and anomaly patterns across graphs make existing single GNN methods insufficiently expressive to model diverse anomaly mechanisms. In this regard, Mixture-of-experts (MoE) architectures provide a promising paradigm by integrating diverse GNN experts with complementary inductive biases, yet their effectiveness in zero-shot GAD is severely constrained by distribution shifts, leading to two key routing challenges. First, nodes often carry vastly different semantics across graphs, and straightforwardly performing routing based on their features is prone to generating biased or suboptimal expert assignments. Second, as anomalous graphs often exhibit pronounced distributional discrepancies, existing router designs fall short in capturing domain-invariant routing principles that generalize beyond the training graphs. To address these challenges, we propose a novel MoE framework with evolutionary router feature generation (EvoFG) for zero-shot GAD. To enhance MoE routing, we propose an evolutionary feature generation scheme that iteratively constructs and selects informative structural features via an LLM-based generator and Shapley-guided evaluation. Moreover, a memory-enhanced router with an invariant learning objective is designed to capture transferable routing patterns under distribution shifts. Extensive experiments on six benchmarks show that EvoFG consistently outperforms state-of-the-art baselines, achieving strong and stable zero-shot GAD performance.

</details>


### [86] [IntTravel: A Real-World Dataset and Generative Framework for Integrated Multi-Task Travel Recommendation](https://arxiv.org/abs/2602.11664)
*Huimin Yan,Longfei Xu,Junjie Sun,Zheng Liu,Wei Luo,Kaikui Liu,Xiangxiang Chu*

Main category: cs.IR

TL;DR: IntTravel是一个大规模集成旅行推荐数据集和生成式多任务推荐框架，解决了当前POI推荐中忽视出发时间、交通方式和途中需求的问题，在真实部署中取得了显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 当前的下一个兴趣点(POI)推荐研究存在两个主要问题：1）数据集碎片化，仅关注"去哪里"，忽视了出发时间、交通方式和途中需求等完整旅程要素；2）数据集规模有限，阻碍了准确性能评估。

Method: 提出了IntTravel大规模公开数据集（41亿次交互、1.63亿用户、730万POI），并构建了一个端到端的仅解码器生成式多任务推荐框架，通过信息保留、选择和分解来平衡任务协作与专业化差异。

Result: 1）在IntTravel数据集和非旅行基准测试中都取得了最先进的性能；2）在Amap平台成功部署，服务数亿用户，点击率提升1.09%；3）数据集和框架已开源。

Conclusion: IntTravel填补了集成旅行推荐领域的大规模公开数据集空白，其生成式多任务框架能够全面考虑旅程的多个维度，在实际应用中取得了显著效果，为未来研究提供了重要基础。

Abstract: Next Point of Interest (POI) recommendation is essential for modern mobility and location-based services. To provide a smooth user experience, models must understand several components of a journey holistically: "when to depart", "how to travel", "where to go", and "what needs arise via the route". However, current research is limited by fragmented datasets that focus merely on next POI recommendation ("where to go"), neglecting the departure time, travel mode, and situational requirements along the journey. Furthermore, the limited scale of these datasets impedes accurate evaluation of performance. To bridge this gap, we introduce IntTravel, the first large-scale public dataset for integrated travel recommendation, including 4.1 billion interactions from 163 million users with 7.3 million POIs. Built upon this dataset, we introduce an end-to-end, decoder-only generative framework for multi-task recommendation. It incorporates information preservation, selection, and factorization to balance task collaboration with specialized differentiation, yielding substantial performance gains. The framework's generalizability is highlighted by its state-of-the-art performance across both IntTravel dataset and an additional non-travel benchmark. IntTravel has been successfully deployed on Amap serving hundreds of millions of users, leading to a 1.09% increase in CTR. IntTravel is available at https://github.com/AMAP-ML/IntTravel.

</details>


### [87] [EpicCBR: Item-Relation-Enhanced Dual-Scenario Contrastive Learning for Cold-Start Bundle Recommendation](https://arxiv.org/abs/2602.11680)
*Yihang Li,Zhuo Liu,Wei Wei*

Main category: cs.IR

TL;DR: EpicCBR是一个用于冷启动捆绑推荐的多视图对比学习框架，通过挖掘项目关系和流行度特征来提升新捆绑包的推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有捆绑推荐模型主要依赖已观察到的用户-捆绑交互，难以处理不断出现的新捆绑包。这些方法通常将每个捆绑包视为独立实例，未能充分利用用户-项目和捆绑-项目关系中的流行项目信息。

Method: 提出了EpicCBR框架：1）通过挖掘项目关系构建用户画像，识别可能对捆绑包感兴趣的用户；2）基于流行度的方法，通过历史捆绑信息和用户偏好来表征新捆绑包特征；3）多视图图对比学习框架，整合冷启动和热启动场景以确保模型泛化能力。

Result: 在三个流行基准测试上的广泛实验表明，EpicCBR大幅优于现有最先进方法（提升高达387%），充分证明了该方法在冷启动场景中的优越性。

Conclusion: EpicCBR通过多视图对比学习框架有效解决了捆绑推荐中的冷启动问题，通过挖掘项目关系和流行度特征显著提升了新捆绑包的推荐性能。

Abstract: Bundle recommendation aims to recommend a set of items to users for overall consumption. Existing bundle recommendation models primarily depend on observed user-bundle interactions, limiting exploration of newly-emerged bundles that are constantly created. It pose a critical representation challenge for current bundle methods, as they usually treat each bundle as an independent instance, while neglecting to fully leverage the user-item (UI) and bundle-item (BI) relations over popular items. To alleviate it, in this paper we propose a multi-view contrastive learning framework for cold-start bundle recommendation, named EpicCBR. Specifically, it precisely mine and utilize the item relations to construct user profiles, identifying users likely to engage with bundles. Additionally, a popularity-based method that characterizes the features of new bundles through historical bundle information and user preferences is proposed. To build a framework that demonstrates robustness in both cold-start and warm-start scenarios, a multi-view graph contrastive learning framework capable of integrating these diverse scenarios is introduced to ensure the model's generalization capability. Extensive experiments conducted on three popular benchmarks showed that EpicCBR outperforms state-of-the-art by a large margin (up to 387%), sufficiently demonstrating the superiority of the proposed method in cold-start scenario. The code and dataset can be found in the GitHub repository: https://github.com/alexlovecoding/EpicCBR.

</details>


### [88] [Uncertainty-aware Generative Recommendation](https://arxiv.org/abs/2602.11719)
*Chenxiao Fan,Chongming Gao,Yaxin Gong,Haoyan Liu,Fuli Feng,Xiangnan He*

Main category: cs.IR

TL;DR: UGR是一个不确定性感知的生成式推荐框架，通过加权奖励、难度感知优化和置信度对齐机制，解决了现有方法忽视模型内在生成置信度的问题，提升了推荐性能和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐中的偏好优化方法通常依赖二元结果正确性，存在"不确定性盲视"的系统性局限。这种局限表现为忽视模型内在生成置信度、忽略样本学习难度差异、缺乏显式置信度表达，导致训练不稳定和决策风险不可量化。

Method: 提出不确定性感知生成式推荐(UGR)框架，包含三个协同机制：1)不确定性加权奖励，惩罚高置信度错误；2)难度感知优化动态，防止过早收敛；3)显式置信度对齐，赋予模型置信度表达能力。

Result: 大量实验表明，UGR不仅实现了优越的推荐性能，而且从根本上稳定了训练过程，避免了标准方法中常见的性能下降。学习到的置信度还能支持可靠的下游风险感知应用。

Conclusion: UGR通过将不确定性作为自适应优化的关键信号，成功解决了生成式推荐中的不确定性盲视问题，为推荐系统提供了更稳定、可解释且风险感知的能力。

Abstract: Generative Recommendation has emerged as a transformative paradigm, reformulating recommendation as an end-to-end autoregressive sequence generation task. Despite its promise, existing preference optimization methods typically rely on binary outcome correctness, suffering from a systemic limitation we term uncertainty blindness. This issue manifests in the neglect of the model's intrinsic generation confidence, the variation in sample learning difficulty, and the lack of explicit confidence expression, directly leading to unstable training dynamics and unquantifiable decision risks. In this paper, we propose Uncertainty-aware Generative Recommendation (UGR), a unified framework that leverages uncertainty as a critical signal for adaptive optimization. UGR synergizes three mechanisms: (1) an uncertainty-weighted reward to penalize confident errors; (2) difficulty-aware optimization dynamics to prevent premature convergence; and (3) explicit confidence alignment to empower the model with confidence expression capabilities. Extensive experiments demonstrate that UGR not only yields superior recommendation performance but also fundamentally stabilizes training, preventing the performance degradation often observed in standard methods. Furthermore, the learned confidence enables reliable downstream risk-aware applications.

</details>


### [89] [ULTRA:Urdu Language Transformer-based Recommendation Architecture](https://arxiv.org/abs/2602.11836)
*Alishbah Bashir,Fatima Qaiser,Ijaz Hussain*

Main category: cs.IR

TL;DR: ULTRA：基于Urdu语言Transformer的自适应语义推荐框架，通过双嵌入架构和查询长度感知路由机制，显著提升低资源语言新闻推荐的相关性。


<details>
  <summary>Details</summary>
Motivation: Urdu作为低资源语言，缺乏有效的语义内容推荐系统，现有方法主要依赖词汇匹配或语言无关技术，难以捕捉语义意图，在不同查询长度和信息需求下表现不佳，导致推荐相关性降低和适应性不足。

Method: 提出ULTRA框架，采用双嵌入架构和查询长度感知路由机制，通过阈值驱动决策过程将用户查询路由到专门优化的语义管道：短查询使用标题/标题级表示，长查询使用全文/文档级表示，结合基于Transformer的嵌入和优化池化策略实现上下文感知相似性搜索。

Result: 在大规模Urdu新闻语料上的实验表明，该架构在不同查询类型下持续提升推荐相关性，与单管道基线相比，精度提升超过90%，证明了查询自适应语义对齐在低资源语言中的有效性。

Conclusion: ULTRA被确立为一种稳健且可泛化的内容推荐架构，为低资源语言环境下的语义检索系统提供了实用的设计见解，解决了Urdu等低资源语言个性化新闻检索中的语义理解挑战。

Abstract: Urdu, as a low-resource language, lacks effective semantic content recommendation systems, particularly in the domain of personalized news retrieval. Existing approaches largely rely on lexical matching or language-agnostic techniques, which struggle to capture semantic intent and perform poorly under varying query lengths and information needs. This limitation results in reduced relevance and adaptability in Urdu content recommendation. We propose ULTRA (Urdu Language Transformer-based Recommendation Architecture),an adaptive semantic recommendation framework designed to address these challenges. ULTRA introduces a dual-embedding architecture with a query-length aware routing mechanism that dynamically distinguishes between short, intent-focused queries and longer, context-rich queries. Based on a threshold-driven decision process, user queries are routed to specialized semantic pipelines optimized for either title/headline-level or full-content/document level representations, ensuring appropriate semantic granularity during retrieval. The proposed system leverages transformer-based embeddings and optimized pooling strategies to move beyond surface-level keyword matching and enable context-aware similarity search. Extensive experiments conducted on a large-scale Urdu news corpus demonstrate that the proposed architecture consistently improves recommendation relevance across diverse query types. Results show gains in precision above 90% compared to single-pipeline baselines, highlighting the effectiveness of query-adaptive semantic alignment for low-resource languages. The findings establish ULTRA as a robust and generalizable content recommendation architecture, offering practical design insights for semantic retrieval systems in low-resource language settings.

</details>


### [90] [Improving Neural Retrieval with Attribution-Guided Query Rewriting](https://arxiv.org/abs/2602.11841)
*Moncef Garouani,Josiane Mothe*

Main category: cs.IR

TL;DR: 提出一种基于归因引导的查询重写方法，通过梯度归因识别查询中的薄弱或误导性部分，利用LLM进行针对性重写，显著提升检索效果


<details>
  <summary>Details</summary>
Motivation: 神经检索器虽然有效但很脆弱，对于不明确或模糊的查询容易误判，即使存在相关文档。现有方法（如LLM查询重写、事后可解释性分析）只部分解决了这一问题。

Method: 提出归因引导的查询重写方法：1) 计算检索器的梯度归因分数，识别查询中薄弱或误导性的token；2) 将这些分数作为软指导构建结构化提示，引导LLM澄清这些部分同时保持查询意图

Result: 在BEIR数据集上的评估显示，该方法相比强基线一致提升了检索效果，对于隐含或模糊的信息需求提升更为显著

Conclusion: 通过将检索器的归因反馈融入查询重写过程，形成了有效的查询优化闭环，显著改善了神经检索器对模糊查询的鲁棒性

Abstract: Neural retrievers are effective but brittle: underspecified or ambiguous queries can misdirect ranking even when relevant documents exist. Existing approaches address this brittleness only partially: LLMs rewrite queries without retriever feedback, and explainability methods identify misleading tokens but are used for post-hoc analysis. We close this loop and propose an attribution-guided query rewriting method that uses token-level explanations to guide query rewriting. For each query, we compute gradient-based token attributions from the retriever and then use these scores as soft guidance in a structured prompt to an LLM that clarifies weak or misleading query components while preserving intent. Evaluated on BEIR collections, the resulting rewrites consistently improve retrieval effectiveness over strong baselines, with larger gains for implicit or ambiguous information needs.

</details>


### [91] [Efficient Crawling for Scalable Web Data Acquisition (Extended Version)](https://arxiv.org/abs/2602.11874)
*Antoine Gauquier,Ioana Manolescu,Pierre Senellart*

Main category: cs.IR

TL;DR: 本文提出了一种基于强化学习的聚焦网络爬虫算法SB-CLASSIFIER，用于高效检索网站中的统计数据集，只需爬取网站的小部分内容就能获取大部分目标资源。


<details>
  <summary>Details</summary>
Motivation: 新闻事实核查、社会经济研究等需要高质量统计数据集的领域面临一个核心问题：大规模检索统计数据集往往很困难、效率低下甚至不可能，这取决于这些数据集在网上的发布方式。为了改善开放统计数据可访问性，需要一种能够从给定网站高效、可扩展地检索尽可能多目标资源的方法。

Method: 本文首先证明了最优解决此问题是不可行的，然后提出了一种基于强化学习（具体是sleeping bandits）的方法。开发了SB-CLASSIFIER爬虫，该爬虫能够高效学习哪些超链接会导向包含许多目标资源的页面，基于这些链接在其所在网页中的路径信息。

Result: 在包含数百万网页的网站上进行实验表明，该爬虫具有高度效率，仅爬取网站的一小部分内容就能获取网站大部分的目标资源。

Conclusion: SB-CLASSIFIER提供了一种有效且可扩展的解决方案，能够显著改善开放统计数据集的网络可访问性，通过智能选择爬取路径来最大化目标资源的获取，同时最小化爬取成本。

Abstract: Journalistic fact-checking, as well as social or economic research, require analyzing high-quality statistics datasets (SDs, in short). However, retrieving SD corpora at scale may be hard, inefficient, or impossible, depending on how they are published online. To improve open statistics data accessibility, we present a focused Web crawling algorithm that retrieves as many targets, i.e., resources of certain types, as possible, from a given website, in an efficient and scalable way, by crawling (much) less than the full website. We show that optimally solving this problem is intractable, and propose an approach based on reinforcement learning, namely using sleeping bandits. We propose SB-CLASSIFIER, a crawler that efficiently learns which hyperlinks lead to pages that link to many targets, based on the paths leading to the links in their enclosing webpages. Our experiments on websites with millions of webpages show that our crawler is highly efficient, delivering high fractions of a site's targets while crawling only a small part.

</details>


### [92] [IncompeBench: A Permissively Licensed, Fine-Grained Benchmark for Music Information Retrieval](https://arxiv.org/abs/2602.11941)
*Benjamin Clavié,Atoof Shakir,Jonah Turner,Sean Lee,Aamir Shakir,Makoto P. Kato*

Main category: cs.IR

TL;DR: IncompeBench：一个用于音乐信息检索的高质量基准测试，包含1574个音乐片段、500个多样化查询和超过12.5万条相关性标注。


<details>
  <summary>Details</summary>
Motivation: 当前音乐信息检索领域缺乏高质量基准测试来评估检索性能，现有基准在质量、多样性和标注一致性方面存在不足。

Method: 通过多阶段标注流程创建基准：收集1574个高质量、许可友好的音乐片段，设计500个多样化查询，采用人工标注生成超过12.5万条相关性判断，确保高标注者一致性。

Result: 创建了IncompeBench基准测试，包含严格和宽松两个版本的数据集，已在Hugging Face和GitHub上公开可用。

Conclusion: IncompeBench填补了音乐信息检索领域高质量基准测试的空白，为评估和比较不同音乐检索系统提供了可靠标准。

Abstract: Multimodal Information Retrieval has made significant progress in recent years, leveraging the increasingly strong multimodal abilities of deep pre-trained models to represent information across modalities. Music Information Retrieval (MIR), in particular, has considerably increased in quality, with neural representations of music even making its way into everyday life products. However, there is a lack of high-quality benchmarks for evaluating music retrieval performance. To address this issue, we introduce \textbf{IncompeBench}, a carefully annotated benchmark comprising $1,574$ permissively licensed, high-quality music snippets, $500$ diverse queries, and over $125,000$ individual relevance judgements. These annotations were created through the use of a multi-stage pipeline, resulting in high agreement between human annotators and the generated data. The resulting datasets are publicly available at https://huggingface.co/datasets/mixedbread-ai/incompebench-strict and https://huggingface.co/datasets/mixedbread-ai/incompebench-lenient with the prompts available at https://github.com/mixedbread-ai/incompebench-programs.

</details>


### [93] [Compress, Cross and Scale: Multi-Level Compression Cross Networks for Efficient Scaling in Recommender Systems](https://arxiv.org/abs/2602.12041)
*Heng Yu,Xiangjun Zhou,Jie Xia,Heng Zhao,Anxin Wu,Yu Zhao,Dongying Kong*

Main category: cs.IR

TL;DR: 提出MLCC结构化特征交互架构，通过分层压缩和动态组合高效捕获高阶特征依赖，并扩展为多通道MC-MLCC，在保持计算效率的同时显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统中的交互模块难以同时实现强大的交互能力、高计算效率和良好可扩展性，导致在生产约束下模型扩展的投资回报率有限。

Method: 提出MLCC架构，通过分层压缩和动态组合组织特征交叉；进一步引入MC-MLCC多通道扩展，将特征交互分解到并行子空间，实现高效水平扩展。

Result: 在三个公开基准和大型工业数据集上，提出的模型比DLRM基线提升高达0.52 AUC，同时减少26倍参数和FLOPs；通道扩展比传统嵌入膨胀效率更高；在线A/B测试验证了实际效果。

Conclusion: MLCC和MC-MLCC能高效捕获高阶特征交互，在严格延迟和资源约束下表现出优越的性能和可扩展性，已在Bilibili广告系统中广泛应用。

Abstract: Modeling high-order feature interactions efficiently is a central challenge in click-through rate and conversion rate prediction. Modern industrial recommender systems are predominantly built upon deep learning recommendation models, where the interaction backbone plays a critical role in determining both predictive performance and system efficiency. However, existing interaction modules often struggle to simultaneously achieve strong interaction capacity, high computational efficiency, and good scalability, resulting in limited ROI when models are scaled under strict production constraints. In this work, we propose MLCC, a structured feature interaction architecture that organizes feature crosses through hierarchical compression and dynamic composition, which can efficiently capture high-order feature dependencies while maintaining favorable computational complexity. We further introduce MC-MLCC, a Multi-Channel extension that decomposes feature interactions into parallel subspaces, enabling efficient horizontal scaling with improved representation capacity and significantly reduced parameter growth. Extensive experiments on three public benchmarks and a large-scale industrial dataset show that our proposed models consistently outperform strong DLRM-style baselines by up to 0.52 AUC, while reducing model parameters and FLOPs by up to 26$\times$ under comparable performance. Comprehensive scaling analyses demonstrate stable and predictable scaling behavior across embedding dimension, head number, and channel count, with channel-based scaling achieving substantially better efficiency than conventional embedding inflation. Finally, online A/B testing on a real-world advertising platform validates the practical effectiveness of our approach, which has been widely adopted in Bilibili advertising system under strict latency and resource constraints.

</details>


### [94] [Towards Personalized Bangla Book Recommendation: A Large-Scale Multi-Entity Book Graph Dataset](https://arxiv.org/abs/2602.12129)
*Rahin Arefin Ahmed,Md. Anik Chowdhury,Sakil Ahmed Sheikh Reza,Devnil Bhattacharjee,Muhammad Abdullah Adnan,Nafis Sadeq*

Main category: cs.IR

TL;DR: 提出了RokomariBG数据集，这是一个针对孟加拉语书籍推荐的大规模多实体异构图数据集，包含超过12万本书和6万用户，并进行了系统基准测试。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语文学领域的个性化书籍推荐研究受到缺乏结构化、大规模公开数据集的限制，特别是在低资源语言环境下。

Method: 构建了包含书籍、用户、作者、类别、出版商和评论等多种实体的异构图数据集，通过八种关系类型连接。采用系统基准测试方法，评估了包括协同过滤、矩阵分解、内容推荐、图神经网络、混合模型和神经检索架构在内的多种推荐模型。

Result: 数据集包含127,302本书、63,723用户、16,601作者等丰富实体。基准测试显示神经检索模型表现最佳（NDCG@10 = 0.204），表明利用多关系结构和文本侧信息的重要性。

Conclusion: 该工作为孟加拉语书籍推荐研究建立了基础基准和公开资源，支持可重复评估和在低资源文化领域的未来研究，数据集和代码已公开。

Abstract: Personalized book recommendation in Bangla literature has been constrained by the lack of structured, large-scale, and publicly available datasets. This work introduces RokomariBG, a large-scale, multi-entity heterogeneous book graph dataset designed to support research on personalized recommendation in a low-resource language setting. The dataset comprises 127,302 books, 63,723 users, 16,601 authors, 1,515 categories, 2,757 publishers, and 209,602 reviews, connected through eight relation types and organized as a comprehensive knowledge graph.
  To demonstrate the utility of the dataset, we provide a systematic benchmarking study on the Top-N recommendation task, evaluating a diverse set of representative recommendation models, including classical collaborative filtering methods, matrix factorization models, content-based approaches, graph neural networks, a hybrid matrix factorization model with side information, and a neural two-tower retrieval architecture. The benchmarking results highlight the importance of leveraging multi-relational structure and textual side information, with neural retrieval models achieving the strongest performance (NDCG@10 = 0.204). Overall, this work establishes a foundational benchmark and a publicly available resource for Bangla book recommendation research, enabling reproducible evaluation and future studies on recommendation in low-resource cultural domains. The dataset and code are publicly available at https://github.com/backlashblitz/Bangla-Book-Recommendation-Dataset

</details>


### [95] [SAGEO Arena: A Realistic Environment for Evaluating Search-Augmented Generative Engine Optimization](https://arxiv.org/abs/2602.12187)
*Sunghwan Kim,Wooseok Jeong,Serin Kim,Sangam Lee,Dongha Lee*

Main category: cs.IR

TL;DR: 提出了SAGEO Arena——首个支持端到端可见性评估的搜索增强生成引擎优化基准环境，解决了现有基准缺乏真实检索和结构化信息的问题。


<details>
  <summary>Details</summary>
Motivation: 搜索增强生成引擎（SAGE）已成为信息获取的新范式，但现有的SAGEO（搜索增强生成引擎优化）评估环境存在严重不足：缺乏端到端可见性评估、忽视检索和重排序过程、丢弃网页结构化信息，无法支持全面研究。

Method: 构建SAGEO Arena基准环境，整合包含丰富结构化信息的大规模网页文档语料库，实现完整的生成搜索流程，支持同时针对搜索优化（SEO）和生成优化（GEO）的联合优化研究。

Result: 研究发现：现有方法在真实条件下大多不实用，甚至可能损害检索和重排序性能；结构化信息有助于缓解这些限制；有效的SAGEO需要针对每个流程阶段进行定制化优化。

Conclusion: SAGEO Arena为超越简化设置的真实SAGEO评估和优化铺平了道路，揭示了结构化信息的重要性以及分阶段优化策略的必要性。

Abstract: Search-Augmented Generative Engines (SAGE) have emerged as a new paradigm for information access, bridging web-scale retrieval with generative capabilities to deliver synthesized answers. This shift has fundamentally reshaped how web content gains exposure online, giving rise to Search-Augmented Generative Engine Optimization (SAGEO), the practice of optimizing web documents to improve their visibility in AI-generated responses. Despite growing interest, no evaluation environment currently supports comprehensive investigation of SAGEO. Specifically, existing benchmarks lack end-to-end visibility evaluation of optimization strategies, operating on pre-determined candidate documents that abstract away retrieval and reranking preceding generation. Moreover, existing benchmarks discard structural information (e.g., schema markup) present in real web documents, overlooking the rich signals that search systems actively leverage in practice. Motivated by these gaps, we introduce SAGEO Arena, a realistic and reproducible environment for stage-level SAGEO analysis. Our objective is to jointly target search-oriented optimization (SEO) and generation-centric optimization (GEO). To achieve this, we integrate a full generative search pipeline over a large-scale corpus of web documents with rich structural information. Our findings reveal that existing approaches remain largely impractical under realistic conditions and often degrade performance in retrieval and reranking. We also find that structural information helps mitigate these limitations, and that effective SAGEO requires tailoring optimization to each pipeline stage. Overall, our benchmark paves the way for realistic SAGEO evaluation and optimization beyond simplified settings.

</details>


### [96] [AttentionRetriever: Attention Layers are Secretly Long Document Retrievers](https://arxiv.org/abs/2602.12278)
*David Jiahao Fu,Lam Thanh Do,Jiayu Li,Kevin Chen-Chuan Chang*

Main category: cs.IR

TL;DR: AttentionRetriever是一个基于注意力机制和实体检索的新型长文档检索模型，能够显著超越现有检索模型在长文档检索任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的检索模型不是为长文档检索设计的，无法解决长文档检索中的关键挑战，包括上下文感知、因果依赖和检索范围等问题。

Method: 提出AttentionRetriever模型，利用注意力机制和基于实体的检索来构建长文档的上下文感知嵌入，并确定检索范围。

Result: 通过大量实验发现，AttentionRetriever在长文档检索数据集上能够大幅超越现有检索模型，同时保持与密集检索模型相当的效率。

Conclusion: AttentionRetriever是一种有效的长文档检索解决方案，能够解决现有检索模型在长文档检索中的关键挑战，在性能和效率方面都表现出色。

Abstract: Retrieval augmented generation (RAG) has been widely adopted to help Large Language Models (LLMs) to process tasks involving long documents. However, existing retrieval models are not designed for long document retrieval and fail to address several key challenges of long document retrieval, including context-awareness, causal dependence, and scope of retrieval. In this paper, we proposed AttentionRetriever, a novel long document retrieval model that leverages attention mechanism and entity-based retrieval to build context-aware embeddings for long document and determine the scope of retrieval. With extensive experiments, we found AttentionRetriever is able to outperform existing retrieval models on long document retrieval datasets by a large margin while remaining as efficient as dense retrieval models.

</details>
