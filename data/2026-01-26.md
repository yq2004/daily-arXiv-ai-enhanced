<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 53]
- [cs.IR](#cs.IR) [Total: 7]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [ChiEngMixBench: Evaluating Large Language Models on Spontaneous and Natural Chinese-English Code-Mixed Generation](https://arxiv.org/abs/2601.16217)
*Qingyan Yang,Tongxi Wang,Yunsheng Luo*

Main category: cs.CL

TL;DR: 该论文提出了首个评估中英代码混合能力的基准测试ChiEngMixBench，通过自发性和自然性两个维度评估模型在真实社区上下文中的代码混合表现，并发现了与矩阵语言框架理论一致的术语分层策略。


<details>
  <summary>Details</summary>
Motivation: 当前研究通常将代码混合简化为翻译或可转换性问题，难以评估模型的语言切换行为是否符合上下文和人类惯例。需要建立能够评估模型在真实社区语境下代码混合能力的基准测试。

Method: 提出了ChiEngMixBench基准测试，采用通用的构建流程，支持跨领域和双语对的可扩展数据集开发。将代码混合定义为认知对齐问题，通过自发性和自然性两个互补信号来评估。

Result: 实验评估表明，提出的度量标准能够系统地区分不同模型的代码混合性能。此外，发现了隐含出现的术语分层策略，这一现象与矩阵语言框架理论一致，表明多语言大语言模型与人类通信之间存在结构化的认知对齐。

Conclusion: ChiEngMixBench是首个在真实社区上下文中评估代码混合能力的基准测试，不仅能够有效评估模型性能，还揭示了多语言大语言模型与人类通信之间的结构化认知对齐模式，为理解模型的语言切换行为提供了新的视角。

Abstract: Code-mixing is increasingly prevalent in interactions between humans and large language models, yet existing work often reduces it to a translation or convertibility problem, making it difficult to assess whether a model's switching behavior is context-appropriate and aligned with human conventions. We introduce ChiEngMixBench, the first benchmark designed to evaluate code-mixing ability in authentic community contexts, built upon a general construction pipeline that enables scalable dataset development across domains and bilingual pairs. ChiEngMixBench formulates code-mixing as a cognitive alignment problem, characterized by two complementary signals: Spontaneity and Naturalness. Empirical evaluation shows that our metrics can systematically distinguish code-mixing performance across models. Beyond benchmarking, we further uncover an implicitly emergent Terminology Layering Strategy, a phenomenon consistent with the Matrix Language Frame (MLF) theory, indicating structured cognitive alignment between multilingual large language models and human communication.

</details>


### [2] [M3Kang: Evaluating Multilingual Multimodal Mathematical Reasoning in Vision-Language Models](https://arxiv.org/abs/2601.16218)
*Aleix Torres-Camps,Nathaniel Mitrani Hadida,Víctor Conchello Vendrell,Àlex Batlle Casellas,Arnau Padrés Masdemont,Jordi Ros-Giralt*

Main category: cs.CL

TL;DR: M3Kang是首个大规模多语言多模态数学推理数据集，源自全球最大的数学竞赛袋鼠数学竞赛，包含1,747道按年级难度组织的选择题，翻译成108种语言，部分题目包含解题必需的图表。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在推理能力方面表现出色，但其在多语言数学推理方面的性能仍未得到充分探索，特别是与人类表现相比存在差距。现有研究缺乏大规模、多语言、多模态的数学推理数据集来评估模型性能。

Method: 基于袋鼠数学竞赛构建M3Kang数据集，包含1,747道独特的选择题，按年级难度组织，翻译成108种文化多样性的语言，部分题目包含图表。使用该数据集对开源和闭源的最先进模型进行广泛基准测试，并与68,000多名学生的表现数据进行对比分析。

Result: 研究发现：1）尽管有最新进展，模型在基础数学和基于图表的推理方面仍然困难；2）性能随语言存在和模型规模扩展，但不随年级水平扩展；3）多语言技术可以有效扩展到多模态设置，相比基线方法有显著改进。

Conclusion: M3Kang数据集填补了多语言多模态数学推理评估的空白，揭示了当前模型在数学推理方面的局限性，并展示了多语言技术在多模态场景中的可扩展性。数据集、框架和代码库已开源，为未来研究提供了重要资源。

Abstract: Despite state-of-the-art vision-language models (VLMs) have demonstrated strong reasoning capabilities, their performance in multilingual mathematical reasoning remains underexplored, particularly when compared to human performance. To bridge this gap, we introduce M3Kang, the first massively multilingual, multimodal mathematical reasoning dataset for VLMs. It is derived from the Kangaroo Math Competition, the world's largest mathematics contest, which annually engages over six million participants under the age of 18 across more than 90 countries. M3Kang includes 1,747 unique multiple-choice problems organized by grade-level difficulty, with translations into 108 culturally diverse languages, some of them including diagrams essential for solving them. Using this dataset, we conduct extensive benchmarking on both closed- and open-source SOTA models. We observe that, despite recent advances, models still struggle with basic math and diagram-based reasoning, with performance scaling with language presence and model size, but not with grade level. We also find that multilingual techniques can be effectively extended to the multimodal setting, resulting in significant improvements over baseline approaches. Our analysis also incorporates performance data from over 68,000 students, enabling direct comparison with human performance. We are open-sourcing M3Kang, including the English-only subset M2Kang, along with the framework and codebase used to construct the dataset.

</details>


### [3] [Domain Specific Specialization in Low-Resource Settings: The Efficacy of Offline Response-Based Knowledge Distillation in Large Language Models](https://arxiv.org/abs/2601.16219)
*Erdem Aslan,Pakize Erdoğmuş*

Main category: cs.CL

TL;DR: 本文提出了一种离线响应式知识蒸馏方法，在有限硬件资源下开发高精度专业助手，验证了数据质量和结构对齐比数据量更重要的LIMA假设。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在通用任务上表现出色，但在处理领域特定或机构知识时容易出现幻觉，这些知识往往未包含在其预训练数据中。

Method: 采用离线响应式知识蒸馏方法，评估三种数据策略：通用领域适应（15,000行）、非结构化知识注入（2,000行）和由教师模型生成的上下文感知合成数据集（500行）。使用Unsloth库优化Qwen-2.5-7B学生模型，将NVIDIA A100 GPU内存需求从40GB降低到16GB。

Result: 实验结果表明，较大的非结构化数据集存在持续幻觉问题，而500行的上下文感知数据集达到了96.7%的准确率，并具备强大的拒绝能力。

Conclusion: 这些发现验证了LIMA假设，表明在低资源设置下，数据质量和结构对齐比数据量对领域适应更为关键。

Abstract: Large Language Models (LLMs) excel in general tasks but often struggle with hallucinations when handling domain-specific or institutional knowledge absent from their pre-training. We present an offline response-based knowledge distillation method that develops high-accuracy specialized assistants under constrained hardware resources. We evaluate three distinct data strategies: general domain adaptation (15,000 lines), unstructured knowledge injection (2,000 lines), and a context-aware synthetic dataset (500 lines) generated by a teacher model. To minimize computational costs, we utilize the Unsloth library to optimize the Qwen-2.5-7B student model, reducing NVIDIA A100 GPU memory requirements from 40 GB to 16 GB. Experimental results demonstrate that while larger unstructured datasets suffer from persistent hallucinations, the 500-line context-aware dataset achieves a 96.7% accuracy rate and robust rejection capability. These findings validate the LIMA hypothesis, showing that data quality and structural alignment are more critical than quantity for domain adaptation in low-resource settings.

</details>


### [4] [Towards Latent Diffusion Suitable For Text](https://arxiv.org/abs/2601.16220)
*Nesta Midavaine,Christian A. Naesseth,Grigory Bartosh*

Main category: cs.CL

TL;DR: Neural Flow Diffusion Models (NFDM)将连续扩散模型扩展应用于离散状态空间的语言生成，大幅减少了与自回归模型的似然差距，同时保持与先前潜扩散模型相当的采样质量。


<details>
  <summary>Details</summary>
Motivation: 语言扩散模型旨在提高采样速度和连贯性，超越自回归大语言模型。现有连续扩散模型难以直接应用于离散状态空间的语言建模。

Method: 提出Neural Flow Diffusion Models (NFDM)，扩展NFDM框架，使连续扩散模型能够直接应用于离散状态空间。模型从数据中学习多元前向过程，确保前向过程和生成轨迹适合语言建模。

Result: NFDM大幅减少了与同规模自回归模型的似然差距，同时实现了与先前潜扩散模型相当的采样质量。

Conclusion: NFDM为语言生成提供了一种有效的扩散模型方法，在保持高质量采样的同时，显著缩小了与自回归模型在似然性能上的差距。

Abstract: Language diffusion models aim to improve sampling speed and coherence over autoregressive LLMs. We introduce Neural Flow Diffusion Models for language generation, an extension of NFDM that enables the straightforward application of continuous diffusion models to discrete state spaces. NFDM learns a multivariate forward process from the data, ensuring that the forward process and generative trajectory are a good fit for language modeling. Our model substantially reduces the likelihood gap with autoregressive models of the same size, while achieving sample quality comparable to that of previous latent diffusion models.

</details>


### [5] [Limits of n-gram Style Control for LLMs via Logit-Space Injection](https://arxiv.org/abs/2601.16224)
*Sami-ul Ahmed*

Main category: cs.CL

TL;DR: 通过向冻结LLM的logit空间注入n-gram风格先验，在解码时进行轻量级风格控制。虽然可调，但仅在特定参数下有效，整体表现不如提示工程和LoRA微调。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM个性化方法（如提示工程和LoRA微调）存在局限性：提示难以捕捉复杂的写作风格，而LoRA需要大量计算资源和训练。因此需要探索更轻量级的替代方案。

Method: 在风格鲜明的语料库（如《堂吉诃德》、CNN/DailyMail新闻标题、arXiv摘要）上训练n-gram模型，构建1-3元语法先验。在生成时，将匹配当前上下文的n-gram风格对数概率加权后添加到LLM的logits中，通过控制参数λ调节强度。

Result: 在TinyLlama-1.1B上，仅发现一个狭窄的有效区域（《堂吉诃德》语料，λ=0.1），风格困惑度改善24.7%，基础模型困惑度改善51.4%。对于多作者语料（如新闻和论文），即使很小的λ值也会导致风格和流畅性变差，较大λ值会导致文本崩溃。该方法整体表现不如提示工程和LoRA。

Conclusion: Logit空间注入n-gram风格先验虽然能实现轻量级、可调的风格控制，但非常脆弱：仅在低λ值的狭窄范围内有效，且始终不如提示工程和LoRA等传统方法。

Abstract: Large language models (LLMs) are typically personalized via prompt engineering or parameter-efficient fine-tuning such as LoRA. However, writing style can be difficult to distill into a single prompt, and LoRA fine-tuning requires computationally intensive training and infrastructure. We investigate a possible lightweight alternative: steering a frozen LLM with n-gram style priors injected in logit space at decoding time. We train an n-gram model on stylistically distinct corpora -- including Don Quixote, CNN/DailyMail news headlines, and arXiv abstracts -- constructing an interpolated 1-to-3-gram prior over next-token probabilities. During generation we modify the LLM's logits by adding a weighted sum of style log-probabilities from each n-gram order that matches the current context, scaled by a control parameter lambda in [0, 1].
  We sweep lambda and style corpora and report style perplexity under the n-gram model, base-model perplexity as a proxy for fluency, Jensen-Shannon (JS) divergence between the original and steered token distributions, and token-overlap statistics. On TinyLlama-1.1B we identify a single narrow regime (for the Don Quixote corpus at lambda=0.1) where style perplexity improves by 24.7% and base-model perplexity improves by 51.4% relative to the frozen model. Outside this regime, and for multi-author corpora such as CNN/DailyMail and arXiv abstracts, even small nonzero lambda values generally result in worse style and fluency, and larger lambda values lead to collapse with extreme perplexities and incoherent text. Logit-space injection of n-gram style priors provides lightweight, tunable style control, but it is fragile: it operates effectively only within a narrow range of low lambda values and is consistently outperformed by prompting and LoRA.

</details>


### [6] [GameTalk: Training LLMs for Strategic Conversation](https://arxiv.org/abs/2601.16276)
*Victor Conchello Vendrell,Max Ruiz Luyten,Mihaela van der Schaar*

Main category: cs.CL

TL;DR: GameTalk框架通过对话式微调训练LLMs在多智能体环境中进行战略决策，显著提升长期目标优化能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在多智能体战略决策中面临挑战，特别是在需要长期协调和谈判的对话场景中。现有研究主要关注单轮决策任务，缺乏对通过对话优化长期目标的方法探索。

Method: 提出GameTalk框架，通过适应GRPO、DPO和STaR等微调方法，将依赖整个交互过程的奖励信号纳入训练，使LLMs能够在多轮对话中优化全局目标。

Result: 在复杂度递增的游戏套件上评估，GameTalk显著优于未训练模型，尤其在奖励塑形下表现更佳，其中DPO方法获得最强劲的性能提升。

Conclusion: 对话式微调为LLMs在交互环境中进行推理、谈判和行动提供了一条有前景的技术路径，能够有效提升多智能体战略决策能力。

Abstract: Strategic decision-making in multi-agent settings is a key challenge for large language models (LLMs), particularly when coordination and negotiation must unfold over extended conversations. While recent work has explored the use of LLMs in isolated decision tasks, little attention has been given to optimizing long-term objectives through dialogue. We introduce \textbf{GameTalk}, a framework for training LLMs to make strategic decisions via multi-turn interactions. Unlike prior work that focuses on single-turn objectives or static action prediction, we train LLMs to optimize a global objective across full conversations. We achieve this by adapting fine-tuning methods like GRPO, DPO, and STaR to incorporate reward signals that depend on the entire interaction. We evaluate this approach on a suite of increasingly complex games, designed to stress different aspects of reasoning, coordination, and opponent modeling. Our results show that GameTalk significantly outperforms untrained models, especially under reward shaping, with DPO consistently yielding the strongest gains. These findings position conversational fine-tuning as a promising path for LLMs to reason, negotiate, and act in interactive environments.

</details>


### [7] [Better as Generators Than Classifiers: Leveraging LLMs and Synthetic Data for Low-Resource Multilingual Classification](https://arxiv.org/abs/2601.16278)
*Branislav Pecher,Jan Cegin,Robert Belanec,Ivan Srba,Jakub Simko,Maria Bielikova*

Main category: cs.CL

TL;DR: LLMs作为教师生成合成数据，训练小型模型在低资源语言中超越大模型性能


<details>
  <summary>Details</summary>
Motivation: 探索LLMs的合成数据生成能力是否可作为知识蒸馏形式，使小型模型在多语言任务中达到或超越大模型性能，特别是在低资源语言场景中

Method: 使用先进的多语言LLM为11种语言和4个分类任务生成合成数据集，通过微调、指令调优或作为紧凑LLM的上下文示例来训练小型模型

Result: 即使少量合成数据也能让小型模型超越大型生成器，尤其在低资源语言中表现突出

Conclusion: LLMs最佳用途是作为数据生成器（教师）而非分类器，它们产生的数据能赋能更小、更高效的多语言模型

Abstract: Large Language Models (LLMs) have demonstrated remarkable multilingual capabilities, making them promising tools in both high- and low-resource languages. One particularly valuable use case is generating synthetic samples that can be used to train smaller models in low-resource scenarios where human-labelled data is scarce. In this work, we investigate whether these synthetic data generation capabilities can serve as a form of distillation, producing smaller models that perform on par with or even better than massive LLMs across languages and tasks. To this end, we use a state-of-the-art multilingual LLM to generate synthetic datasets covering 11 languages and 4 classification tasks. These datasets are then used to train smaller models via fine-tuning or instruction tuning, or as synthetic in-context examples for compact LLMs. Our experiments show that even small amounts of synthetic data enable smaller models to outperform the large generator itself, particularly in low-resource languages. Overall, the results suggest that LLMs are best utilised as generators (teachers) rather than classifiers, producing data that empowers smaller and more efficient multilingual models.

</details>


### [8] [Generating Literature-Driven Scientific Theories at Scale](https://arxiv.org/abs/2601.16282)
*Peter Jansen,Peter Clark,Doug Downey,Daniel S. Weld*

Main category: cs.CL

TL;DR: 本文提出了一种从大规模科学文献中自动合成理论的方法，通过文献支持生成比仅依赖参数化LLM记忆更准确且能预测未来结果的理论。


<details>
  <summary>Details</summary>
Motivation: 当前自动化科学发现主要集中在实验生成代理，而理论构建等高层次科学活动系统研究不足。本文旨在解决从大量科学文献中合成包含定性和定量定律的理论的问题。

Method: 研究理论生成的大规模应用，使用13.7k篇源论文合成2.9k个理论。比较了基于文献基础与参数化知识生成，以及准确性导向与新颖性导向生成目标对理论特性的影响。

Result: 实验表明，与使用参数化LLM记忆生成相比，文献支持方法创建的理论在匹配现有证据和预测4.6k篇后续论文的未来结果方面表现显著更好。

Conclusion: 文献支持的理论生成方法在科学理论合成中优于仅依赖参数化知识的生成，为自动化高层次科学发现提供了有效途径。

Abstract: Contemporary automated scientific discovery has focused on agents for generating scientific experiments, while systems that perform higher-level scientific activities such as theory building remain underexplored. In this work, we formulate the problem of synthesizing theories consisting of qualitative and quantitative laws from large corpora of scientific literature. We study theory generation at scale, using 13.7k source papers to synthesize 2.9k theories, examining how generation using literature-grounding versus parametric knowledge, and accuracy-focused versus novelty-focused generation objectives change theory properties. Our experiments show that, compared to using parametric LLM memory for generation, our literature-supported method creates theories that are significantly better at both matching existing evidence and at predicting future results from 4.6k subsequently-written papers

</details>


### [9] [A Longitudinal, Multinational, and Multilingual Corpus of News Coverage of the Russo-Ukrainian War](https://arxiv.org/abs/2601.16309)
*Dikshya Mohanty,Taisiia Sabadyn,Jelwin Rodrigues,Chenlu Wang,Abhishek Kalugade,Ritwik Banerjee*

Main category: cs.CL

TL;DR: DNIPRO是一个包含24.6万篇新闻文章的多语言语料库，涵盖俄乌战争期间（2022年2月至2024年8月）来自5个国家11个媒体的报道，用于研究战时话语、叙事分歧和信息战。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能够系统性分析跨国战时话语的多语言语料库，特别是包含竞争性地缘政治视角的资源，限制了研究人员对叙事分歧、媒体框架和信息战的研究。

Method: 构建了包含英语、俄语和中文的24.6万篇新闻文章语料库，涵盖俄罗斯、乌克兰、美国、英国和中国等5个国家的11个媒体，包含一致的元数据和多种注释类型，并进行严格的人工评估。

Result: DNIPRO语料库成功展示了媒体如何构建竞争性现实，报道呈现出反映地缘政治利益的极化解释。通过立场检测、情感分析、主题框架和矛盾分析等实验验证了其实用性。

Conclusion: DNIPRO为理解全球信息生态系统中冲突性叙事的出现和演变提供了基础资源，不仅支持计算新闻学研究，也有助于系统性的跨国战时话语分析。

Abstract: We introduce DNIPRO, a novel longitudinal corpus of 246K news articles documenting the Russo-Ukrainian war from Feb 2022 to Aug 2024, spanning eleven media outlets across five nation states (Russia, Ukraine, U.S., U.K., and China) and three languages (English, Russian, and Mandarin Chinese). This multilingual resource features consistent and comprehensive metadata, and multiple types of annotation with rigorous human evaluations for downstream tasks relevant to systematic transnational analyses of contentious wartime discourse. DNIPRO's distinctive value lies in its inclusion of competing geopolitical perspectives, making it uniquely suited for studying narrative divergence, media framing, and information warfare. To demonstrate its utility, we include use case experiments using stance detection, sentiment analysis, topical framing, and contradiction analysis of major conflict events within the larger war. Our explorations reveal how outlets construct competing realities, with coverage exhibiting polarized interpretations that reflect geopolitical interests. Beyond supporting computational journalism research, DNIPRO provides a foundational resource for understanding how conflicting narratives emerge and evolve across global information ecosystems.

</details>


### [10] [Teaching and Evaluating LLMs to Reason About Polymer Design Related Tasks](https://arxiv.org/abs/2601.16312)
*Dikshya Mohanty,Mohammad Saqib Hasan,Syed Mostofa Monsur,Size Zheng,Benjamin Hsiao,Niranjan Balasubramanian*

Main category: cs.CL

TL;DR: PolyBench是一个包含12.5万个聚合物设计任务的大规模训练和测试基准数据集，通过知识增强推理蒸馏方法训练的小型语言模型在聚合物设计任务上超越了大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在聚合物设计领域效果不佳，因为缺乏聚合物专业知识且现有对齐模型覆盖范围不足，需要专门的聚合物设计基准和训练方法。

Method: 构建PolyBench基准数据集（基于1300万+数据点），采用知识增强推理蒸馏方法，为任务添加结构化思维链，并按简单到复杂组织任务以支持泛化测试。

Result: 在PolyBench测试集上，7B-14B参数的小型语言模型超越了同规模模型和闭源前沿LLMs，在其他聚合物基准测试中也表现出优势。

Conclusion: PolyBench基准和知识增强推理蒸馏方法有效提升了小型语言模型在聚合物设计领域的性能，为解决专业科学领域的AI应用提供了有效途径。

Abstract: Research in AI4Science has shown promise in many science applications, including polymer design. However, current LLMs prove ineffective on this problem space because: (i) most models lack polymer-specific knowledge (ii) existing aligned models lack coverage of knowledge and capabilities relevant to polymer design. Addressing this, we introduce PolyBench, a large scale training and test benchmark dataset of more than 125K polymer design related tasks, leveraging a knowledge base of 13M+ data points obtained from experimental and synthetic sources to ensure broad coverage of polymers and their properties. For effective alignment using PolyBench, we introduce a knowledge-augmented reasoning distillation method that augments this dataset with structured CoT. Furthermore, tasks in PolyBench are organized from simple to complex analytical reasoning problems, enabling generalization tests and diagnostic probes across the problem space. Experiments show that small language models (SLMs), of 7B to 14B parameters, trained on PolyBench data outperform similar sized models, and even closed source frontier LLMs on PolyBench test dataset while demonstrating gains on other polymer benchmarks as well.

</details>


### [11] [Machine-Assisted Grading of Nationwide School-Leaving Essay Exams with LLMs and Statistical NLP](https://arxiv.org/abs/2601.16314)
*Andres Karjus,Kais Allkivi,Silvia Maine,Katarin Leppik,Krister Kruusmaa,Merilin Aruvee*

Main category: cs.CL

TL;DR: 该研究验证了使用大语言模型对爱沙尼亚全国考试作文进行自动化评分的可行性，证明其能达到与人类评分员相当的水平，同时保持人类监督。


<details>
  <summary>Details</summary>
Motivation: 传统考试评分需要大量人力，特别是在全国性考试中，需要快速、一致地评估大量开放性问题回答。爱沙尼亚即将采用全电子化考试系统，需要高效的自动化评分解决方案。

Method: 使用大语言模型和统计自然语言处理技术，基于官方课程标准的评分标准，对爱沙尼亚两个全国队列的模拟考试作文数据集进行自动化评分，并与人类专家评分进行对比。

Result: 自动化评分能达到与人类评分员相当的性能，评分结果倾向于落在人类评分范围内。研究还评估了偏见、提示注入风险以及LLM作为作文写手的情况。

Conclusion: 基于原则、标准驱动、人机协作的评分流程对于高风险写作评估是可行的，即使在爱沙尼亚这样的小语种环境下也能实现全国规模的LLM辅助评估，同时保持人类监督和符合教育监管标准。

Abstract: Large language models (LLMs) enable rapid and consistent automated evaluation of open-ended exam responses, including dimensions of content and argumentation that have traditionally required human judgment. This is particularly important in cases where a large amount of exams need to be graded in a limited time frame, such as nation-wide graduation exams in various countries. Here, we examine the applicability of automated scoring on two large datasets of trial exam essays of two full national cohorts from Estonia. We operationalize the official curriculum-based rubric and compare LLM and statistical natural language processing (NLP) based assessments with human panel scores. The results show that automated scoring can achieve performance comparable to that of human raters and tends to fall within the human scoring range. We also evaluate bias, prompt injection risks, and LLMs as essay writers. These findings demonstrate that a principled, rubric-driven, human-in-the-loop scoring pipeline is viable for high-stakes writing assessment, particularly relevant for digitally advanced societies like Estonia, which is about to adapt a fully electronic examination system. Furthermore, the system produces fine-grained subscore profiles that can be used to generate systematic, personalized feedback for instruction and exam preparation. The study provides evidence that LLM-assisted assessment can be implemented at a national scale, even in a small-language context, while maintaining human oversight and compliance with emerging educational and regulatory standards.

</details>


### [12] [Better Generalizing to Unseen Concepts: An Evaluation Framework and An LLM-Based Auto-Labeled Pipeline for Biomedical Concept Recognition](https://arxiv.org/abs/2601.16711)
*Shanshan Liu,Noriki Nishida,Fei Cheng,Narumi Tokunaga,Rumana Ferdous Munne,Yuki Yamagata,Kouji Kozaki,Takehito Utsuro,Yuji Matsumoto*

Main category: cs.CL

TL;DR: 该论文针对生物医学概念识别中未见概念泛化困难的问题，提出了基于层级概念索引的评估框架和LLM自动标注数据生成方法，证明自动标注数据能有效提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 生物医学概念识别（MA-BCR）面临的主要挑战是对未见概念的泛化能力不足，这主要是由于人工标注数据的稀缺性所导致。

Method: 1. 提出基于层级概念索引的评估框架和新颖指标来衡量泛化能力；2. 探索基于LLM的自动标注数据（ALD）作为可扩展资源，并创建了任务特定的数据生成流程。

Result: 研究表明，虽然LLM生成的自动标注数据不能完全替代人工标注，但它是改善泛化能力的宝贵资源，能够为模型提供更广泛的覆盖范围和结构化知识，帮助模型接近识别未见概念的能力。

Conclusion: LLM生成的自动标注数据是提升生物医学概念识别泛化能力的有效补充资源，尽管不能完全替代人工标注，但能显著增强模型对未见概念的识别能力。

Abstract: Generalization to unseen concepts is a central challenge due to the scarcity of human annotations in Mention-agnostic Biomedical Concept Recognition (MA-BCR). This work makes two key contributions to systematically address this issue. First, we propose an evaluation framework built on hierarchical concept indices and novel metrics to measure generalization. Second, we explore LLM-based Auto-Labeled Data (ALD) as a scalable resource, creating a task-specific pipeline for its generation. Our research unequivocally shows that while LLM-generated ALD cannot fully substitute for manual annotations, it is a valuable resource for improving generalization, successfully providing models with the broader coverage and structural knowledge needed to approach recognizing unseen concepts. Code and datasets are available at https://github.com/bio-ie-tool/hi-ald.

</details>


### [13] [Regional Bias in Large Language Models](https://arxiv.org/abs/2601.16349)
*M P V S Gopinadh,Kappara Lakshmi Sindhu,Soma Sekhar Pandu Ranga Raju P,Yesaswini Swarna*

Main category: cs.CL

TL;DR: 评估十大主流大语言模型在上下文中性场景下的区域偏见，发现模型间存在显著差异，GPT-3.5偏见最严重，Claude 3.5 Sonnet最轻微。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型中的区域偏见问题，这是AI公平性和全球代表性领域的新兴关切。区域偏见可能损害LLM输出在现实世界跨文化应用中的可靠性、公平性和包容性。

Method: 使用100个精心设计的提示数据集，在上下文中性场景下探测模型在区域间的强制选择决策。引入FAZE评估框架，这是一个基于提示的评估方法，用10分制测量区域偏见程度（分数越高表示越倾向于特定区域）。评估了10个主流LLM：GPT-3.5、GPT-4o、Gemini 1.5 Flash、Gemini 1.0 Pro、Claude 3 Opus、Claude 3.5 Sonnet、Llama 3、Gemma 7B、Mistral 7B和Vicuna-13B。

Result: 实验结果显示不同模型的偏见水平存在显著差异。GPT-3.5表现出最高的偏见分数（9.5分），而Claude 3.5 Sonnet得分最低（2.5分）。这表明区域偏见确实存在且在不同模型中程度不同。

Conclusion: 区域偏见可能显著损害大语言模型在现实世界跨文化应用中的可靠性、公平性和包容性。这项工作通过强调包容性评估框架和系统性方法在识别和减轻语言模型地理偏见方面的重要性，为AI公平性研究做出了贡献。

Abstract: This study investigates regional bias in large language models (LLMs), an emerging concern in AI fairness and global representation. We evaluate ten prominent LLMs: GPT-3.5, GPT-4o, Gemini 1.5 Flash, Gemini 1.0 Pro, Claude 3 Opus, Claude 3.5 Sonnet, Llama 3, Gemma 7B, Mistral 7B, and Vicuna-13B using a dataset of 100 carefully designed prompts that probe forced-choice decisions between regions under contextually neutral scenarios. We introduce FAZE, a prompt-based evaluation framework that measures regional bias on a 10-point scale, where higher scores indicate a stronger tendency to favor specific regions. Experimental results reveal substantial variation in bias levels across models, with GPT-3.5 exhibiting the highest bias score (9.5) and Claude 3.5 Sonnet scoring the lowest (2.5). These findings indicate that regional bias can meaningfully undermine the reliability, fairness, and inclusivity of LLM outputs in real-world, cross-cultural applications. This work contributes to AI fairness research by highlighting the importance of inclusive evaluation frameworks and systematic approaches for identifying and mitigating geographic biases in language models.

</details>


### [14] [Identity, Cooperation and Framing Effects within Groups of Real and Simulated Humans](https://arxiv.org/abs/2601.16355)
*Suhong Moon,Minwoo Kang,Joseph Suh,Mustafa Safdari,John Canny*

Main category: cs.CL

TL;DR: 该研究探讨了如何使用大语言模型模拟人类在社交困境游戏中的行为，通过深度绑定基础模型与丰富背景故事来提高身份行为模拟的保真度。


<details>
  <summary>Details</summary>
Motivation: 人类行为既取决于理性思考，也受身份认同和情境因素的影响。现有研究主要通过"引导"（弱绑定）聊天模型来模拟人物角色，但这种方法在忠实复制基于身份的行为方面存在局限性。

Method: 采用深度绑定基础模型与扩展背景故事的方法，使用丰富的叙事身份上下文来条件化基础语言模型，并通过指令调优模型检查一致性。同时建模时间（研究年份）、问题框架和参与者群体效应等情境因素。

Result: 相比人类研究，模拟保真度得到提升。LLMs能够有效建模各种情境因素，包括时间效应、问题框架和参与者群体差异。

Conclusion: 大语言模型能够探索影响人类研究但常被实验描述忽略的细节因素，这有助于提高实验复现的准确性，为人类行为模拟提供了更强大的工具。

Abstract: Humans act via a nuanced process that depends both on rational deliberation and also on identity and contextual factors. In this work, we study how large language models (LLMs) can simulate human action in the context of social dilemma games. While prior work has focused on "steering" (weak binding) of chat models to simulate personas, we analyze here how deep binding of base models with extended backstories leads to more faithful replication of identity-based behaviors. Our study has these findings: simulation fidelity vs human studies is improved by conditioning base LMs with rich context of narrative identities and checking consistency using instruction-tuned models. We show that LLMs can also model contextual factors such as time (year that a study was performed), question framing, and participant pool effects. LLMs, therefore, allow us to explore the details that affect human studies but which are often omitted from experiment descriptions, and which hamper accurate replication.

</details>


### [15] [PolyAgent: Large Language Model Agent for Polymer Design](https://arxiv.org/abs/2601.16376)
*Vani Nigam,Achuth Chandrasekhar,Amir Barati Farimani*

Main category: cs.CL

TL;DR: 基于LLM推理的闭环聚合物结构-性能预测框架，集成了性能预测、性能导向的聚合物结构生成和结构修改功能，旨在加速实验室研究人员的早期聚合物发现。


<details>
  <summary>Details</summary>
Motivation: 聚合物发现通常需要漫长试错过程，消耗大量时间和资源。现有机器学习模型虽然能加速科学发现，但实验室研究人员由于基础设施限制难以访问代码和模型来提取单个结构和性能信息。

Method: 开发了一个集成在终端中的闭环聚合物结构-性能预测框架，利用LLM推理能力提供性能预测、性能导向的聚合物结构生成和结构修改功能。使用SMILES序列，并通过合成可及性分数和合成复杂性分数（SC Score）引导，确保生成的聚合物结构尽可能接近可合成的单体级结构。

Result: 该框架解决了为实验室研究人员生成新型聚合物结构的挑战，为聚合物研究提供了计算洞察。通过LLM推理和合成可及性引导，实现了更接近实际合成可行性的聚合物结构生成。

Conclusion: 提出的闭环框架通过LLM推理和合成可及性约束，使实验室研究人员能够更有效地进行早期聚合物发现，解决了现有模型可访问性不足的问题，加速了聚合物材料研究进程。

Abstract: On-demand Polymer discovery is essential for various industries, ranging from biomedical to reinforcement materials. Experiments with polymers have a long trial-and-error process, leading to long procedures and extensive resources. For these processes, machine learning has accelerated scientific discovery at the property prediction and latent space search fronts. However, laboratory researchers cannot readily access codes and these models to extract individual structures and properties due to infrastructure limitations. We present a closed-loop polymer structure-property predictor integrated in a terminal for early-stage polymer discovery. The framework is powered by LLM reasoning to provide users with property prediction, property-guided polymer structure generation, and structure modification capabilities. The SMILES sequences are guided by the synthetic accessibility score and the synthetic complexity score (SC Score) to ensure that polymer generation is as close as possible to synthetically accessible monomer-level structures. This framework addresses the challenge of generating novel polymer structures for laboratory researchers, thereby providing computational insights into polymer research.

</details>


### [16] [Cross-Lingual Activation Steering for Multilingual Language Models](https://arxiv.org/abs/2601.16390)
*Rhitabrat Pokharel,Ameeta Agrawal,Tanay Nagar*

Main category: cs.CL

TL;DR: 提出无需训练、推理时干预的跨语言激活引导方法CLAS，通过选择性调节神经元激活来改善非主导语言性能，同时保持高资源语言表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然具备多语言能力，但主导语言与非主导语言之间存在显著性能差距。先前研究将此归因于多语言表示中共享神经元和语言特定神经元之间的不平衡。

Method: 提出Cross-Lingual Activation Steering (CLAS)方法，这是一种无需训练、在推理时进行的干预技术，通过选择性调节神经元激活来改善语言表现。

Result: 在分类和生成基准测试中，CLAS分别实现了平均2.3%（准确率）和3.4%（F1分数）的提升，同时保持了高资源语言的性能。研究发现有效迁移通过功能分歧而非严格对齐实现，性能提升与语言簇分离度增加相关。

Conclusion: 有针对性的激活引导可以在不修改模型权重的情况下，解锁现有模型中潜在的多语言能力，为改善语言模型的多语言性能提供了新的途径。

Abstract: Large language models exhibit strong multilingual capabilities, yet significant performance gaps persist between dominant and non-dominant languages. Prior work attributes this gap to imbalances between shared and language-specific neurons in multilingual representations. We propose Cross-Lingual Activation Steering (CLAS), a training-free inference-time intervention that selectively modulates neuron activations. We evaluate CLAS on classification and generation benchmarks, achieving average improvements of 2.3% (Acc.) and 3.4% (F1) respectively, while maintaining high-resource language performance. We discover that effective transfer operates through functional divergence rather than strict alignment; performance gains correlate with increased language cluster separation. Our results demonstrate that targeted activation steering can unlock latent multilingual capacity in existing models without modification to model weights.

</details>


### [17] [Cite-While-You-Generate: Training-Free Evidence Attribution for Multimodal Clinical Summarization](https://arxiv.org/abs/2601.16397)
*Qianqi Yan,Huy Nguyen,Sumana Srivatsa,Hari Bandi,Xin Eric Wang,Krishnaram Kenthapadi*

Main category: cs.CL

TL;DR: 提出无需训练的生成时多模态源归因框架，利用解码器注意力直接引用支持文本或图像，在临床对话和放射报告领域优于嵌入和自归因基线。


<details>
  <summary>Details</summary>
Motivation: 可信的临床摘要不仅需要流畅生成，还需要透明度说明每个陈述的来源。现有方法存在后处理或需要重新训练的限制。

Method: 提出训练免费框架，利用解码器注意力进行生成时源归因。引入两种多模态归因策略：原始图像模式（直接使用图像补丁注意力）和标题作为跨度模式（用生成标题替代图像实现纯文本对齐）。

Result: 在临床医患对话（CliConSummation）和放射报告（MIMIC-CXR）评估中，方法持续优于嵌入基线和自归因基线，提高文本级和多模态归因准确率（如比嵌入基线F1提高15%）。标题归因在保持轻量实用的同时达到与原始图像注意力竞争的性能。

Conclusion: 注意力引导的归因是迈向可解释和可部署临床摘要系统的有前景步骤，为多模态临床摘要提供了有效的源归因解决方案。

Abstract: Trustworthy clinical summarization requires not only fluent generation but also transparency about where each statement comes from. We propose a training-free framework for generation-time source attribution that leverages decoder attentions to directly cite supporting text spans or images, overcoming the limitations of post-hoc or retraining-based methods. We introduce two strategies for multimodal attribution: a raw image mode, which directly uses image patch attentions, and a caption-as-span mode, which substitutes images with generated captions to enable purely text-based alignment. Evaluations on two representative domains: clinician-patient dialogues (CliConSummation) and radiology reports (MIMIC-CXR), show that our approach consistently outperforms embedding-based and self-attribution baselines, improving both text-level and multimodal attribution accuracy (e.g., +15% F1 over embedding baselines). Caption-based attribution achieves competitive performance with raw-image attention while being more lightweight and practical. These findings highlight attention-guided attribution as a promising step toward interpretable and deployable clinical summarization systems.

</details>


### [18] [Clarify or Answer: Reinforcement Learning for Agentic VQA with Context Under-specification](https://arxiv.org/abs/2601.16400)
*Zongwan Cao,Bingbing Wen,Lucy Lu Wang*

Main category: cs.CL

TL;DR: CoA是一个在视觉问答中处理模糊问题的框架，通过判断是否需要澄清、生成澄清问题并整合回答来提高VQA准确性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的视觉问答常常依赖上下文信息，图像-问题对可能信息不足，导致模型做出自信但错误的预测。需要一种能够识别模糊性并主动寻求澄清的机制。

Method: CoA框架包含两个模块：1)判断是否需要澄清；2)如果需要则生成单个聚焦的澄清问题，然后整合回答生成最终答案。使用GRPO-CR强化学习方法优化澄清问题生成，通过多个奖励信号鼓励生成结构良好、聚焦、非平凡且能解决模糊性的问题。

Result: 在三个视觉语言模型和三个数据集上的实验表明，CoA在模块和系统层面都取得了持续改进，端到端VQA准确率比基于提示的基线平均提高了+15.3分（83%）。

Conclusion: CoA框架通过主动澄清模糊性，显著提高了现实世界视觉问答的准确性和可靠性，为解决上下文依赖的VQA问题提供了一种有效方法。

Abstract: Real-world visual question answering (VQA) is often context-dependent: an image-question pair may be under-specified, such that the correct answer depends on external information that is not observable in the image. In such cases, directly answering can lead to confident but incorrect predictions. We propose CoA(Clarify-or-Answer), an ask-or-answer agent that separately models the decision to ask or answer, and what to ask if needed. CoA first determines whether clarification is necessary; if so, it asks a single focused question and then incorporates the response to produce the final answer. We introduce CONTEXTCLARIFY with a set of ambiguous VQA questions and the contrast set that is non-ambiguous. We further introduce GRPO-CR (Clarification Reasoning), a reinforcement learning approach that optimizes clarification question generation with multiple reward signals encouraging well-formed, focused, non-trivial questions that resolve ambiguity. Across three VLLMs and three datasets, CoA achieves consistent improvements at both the module and system levels, improving end-to-end VQA accuracy by an average of +15.3 points (83%) over prompting-based baselines

</details>


### [19] [Jacobian Scopes: token-level causal attributions in LLMs](https://arxiv.org/abs/2601.16407)
*Toni J. B. Liu,Baran Zadeoğlu,Nicolas Boullé,Raphaël Sarfati,Christopher J. Earls*

Main category: cs.CL

TL;DR: 该论文提出了Jacobian Scopes方法，一种基于梯度的token级因果归因技术，用于解释LLM的预测机制。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型(LLM)基于上下文中的语义描述和上下文示例进行下一个token预测，但由于现代架构中大量层和注意力头的存在，难以阐明哪些先前token对特定预测影响最大。

Method: 提出了Jacobian Scopes方法，通过分析最终隐藏状态相对于输入的线性化关系来量化输入token对模型预测的影响。引入了三种变体：Semantic Scope（针对特定logit的敏感性）、Fisher Scope（针对完整预测分布）和Temperature Scope（针对模型置信度/逆温度）。

Result: 通过指令理解、翻译和上下文学习(ICL)等案例研究，发现了有趣的现象，例如Jacobian Scopes可以揭示隐含的政治偏见。该方法也为最近讨论的上下文时间序列预测机制提供了见解。

Conclusion: Jacobian Scopes方法能够有效解释LLM的预测机制，揭示了输入token对模型决策的具体影响，为理解模型内部工作机制提供了有价值的工具。

Abstract: Large language models (LLMs) make next-token predictions based on clues present in their context, such as semantic descriptions and in-context examples. Yet, elucidating which prior tokens most strongly influence a given prediction remains challenging due to the proliferation of layers and attention heads in modern architectures. We propose Jacobian Scopes, a suite of gradient-based, token-level causal attribution methods for interpreting LLM predictions. By analyzing the linearized relations of final hidden state with respect to inputs, Jacobian Scopes quantify how input tokens influence a model's prediction. We introduce three variants - Semantic, Fisher, and Temperature Scopes - which respectively target sensitivity of specific logits, the full predictive distribution, and model confidence (inverse temperature). Through case studies spanning instruction understanding, translation and in-context learning (ICL), we uncover interesting findings, such as when Jacobian Scopes point to implicit political biases. We believe that our proposed methods also shed light on recently debated mechanisms underlying in-context time-series forecasting. Our code and interactive demonstrations are publicly available at https://github.com/AntonioLiu97/JacobianScopes.

</details>


### [20] [Learning Domain Knowledge in Multimodal Large Language Models through Reinforcement Fine-Tuning](https://arxiv.org/abs/2601.16419)
*Qinglong Cao,Yuntian Chen,Chao Ma,Xiaokang Yang*

Main category: cs.CL

TL;DR: 当前多模态大语言模型在专业领域（如遥感、医学影像）表现有限，研究发现仅通过文本指令注入领域知识效果甚微，需要优化层面的知识整合。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在通用领域表现出色，但在遥感、医学等专业领域的性能有限。研究发现仅通过输入层面的领域知识注入（如文本指令、提示词）效果不佳，即使明确提供领域知识也难以改善性能。

Method: 提出强化微调框架，将领域知识直接整合到学习目标中。将领域知识编码为领域感知的约束和奖励信号，在输出空间塑造模型行为，而非仅作为描述性信息。

Result: 在遥感和医学领域的多个数据集上进行广泛实验，均取得显著性能提升，在多模态领域任务上达到最先进水平。

Conclusion: 研究强调了优化层面领域知识整合的必要性，揭示了当前多模态大语言模型中文本领域条件化的根本局限性。领域知识必须通过优化目标整合，而非仅通过输入层注入。

Abstract: Multimodal large language models (MLLMs) have shown remarkable capabilities in multimodal perception and understanding tasks. However, their effectiveness in specialized domains, such as remote sensing and medical imaging, remains limited. A natural approach to domain adaptation is to inject domain knowledge through textual instructions, prompts, or auxiliary captions. Surprisingly, we find that such input-level domain knowledge injection yields little to no improvement on scientific multimodal tasks, even when the domain knowledge is explicitly provided. This observation suggests that current MLLMs fail to internalize domain-specific priors through language alone, and that domain knowledge must be integrated at the optimization level. Motivated by this insight, we propose a reinforcement fine-tuning framework that incorporates domain knowledge directly into the learning objective. Instead of treating domain knowledge as descriptive information, we encode it as domain-informed constraints and reward signals, shaping the model's behavior in the output space. Extensive experiments across multiple datasets in remote sensing and medical domains consistently demonstrate good performance gains, achieving state-of-the-art results on multimodal domain tasks. Our results highlight the necessity of optimization-level domain knowledge integration and reveal a fundamental limitation of textual domain conditioning in current MLLMs.

</details>


### [21] [Exploring the Effects of Alignment on Numerical Bias in Large Language Models](https://arxiv.org/abs/2601.16444)
*Ayako Sato,Hwichan Kim,Zhousi Chen,Masato Mita,Mamoru Komachi*

Main category: cs.CL

TL;DR: 研究发现LLM作为评估者时存在数值偏见问题，这种偏见源于对齐过程，通过分数范围调整可以有效缓解偏见


<details>
  <summary>Details</summary>
Motivation: 虽然LLM作为评估者在许多任务中有效，但评估者LLM表现出数值偏见，某些评分出现频率过高，降低了评估性能。研究旨在探究这种偏见的成因

Method: 1. 假设数值偏见源于对齐过程（指令微调和偏好调优）；2. 比较对齐前后的LLM输出；3. 探索缓解策略：温度缩放、分布校准、分数范围调整

Result: 1. 对齐确实增加了数值偏见；2. 在三种缓解策略中，分数范围调整在减少偏见和提升性能方面最有效，但仍属启发式方法

Conclusion: 数值偏见问题源于LLM的对齐过程，需要进一步研究最优分数范围选择和更稳健的缓解策略

Abstract: ``LLM-as-a-judge,'' which utilizes large language models (LLMs) as evaluators, has proven effective in many evaluation tasks. However, evaluator LLMs exhibit numerical bias, a phenomenon where certain evaluation scores are generated disproportionately often, leading reduced evaluation performance. This study investigates the cause of this bias. Given that most evaluator LLMs are aligned through instruction tuning and preference tuning, and that prior research suggests alignment reduces output diversity, we hypothesize that numerical bias arises from alignment. To test this, we compare outputs from pre- and post-alignment LLMs, and observe that alignment indeed increases numerical bias. We also explore mitigation strategies for post-alignment LLMs, including temperature scaling, distribution calibration, and score range adjustment. Among these, score range adjustment is most effective in reducing bias and improving performance, though still heuristic. Our findings highlight the need for further work on optimal score range selection and more robust mitigation strategies.

</details>


### [22] [Mixing Expert Knowledge: Bring Human Thoughts Back To the Game of Go](https://arxiv.org/abs/2601.16447)
*Yichuan Ma,Linyang Li,Yongkang Chen,Peiji Li,Jiasheng Ye,Qipeng Guo,Dahua Lin,Kai Chen*

Main category: cs.CL

TL;DR: 本文提出了LoGos模型，通过混合微调和强化学习将LLM的通用推理能力与围棋领域专业知识结合，使LLM在保持通用推理能力的同时达到人类专业水平的围棋水平。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在数学和编程等通用推理任务上表现出色，但在围棋等专业领域表现不佳，远低于AlphaGo等专业AI系统，限制了LLM在专业领域的应用。

Method: 采用混合微调方法：1）使用结构化围棋专业知识和通用长链思维推理数据进行冷启动微调；2）通过强化学习将围棋专业知识与通用推理能力结合。

Result: LoGos模型在保持优秀通用推理能力的同时，能以自然语言进行围棋对弈，表现出有效的策略推理和准确的落子预测，达到人类专业棋手水平，远超现有LLM。

Conclusion: 该研究成功弥合了LLM通用推理能力与专业领域知识之间的鸿沟，为将通用LLM推理能力应用于专业领域提供了见解，并发布了首个大规模围棋训练数据集、评估基准和达到人类专业水平的LLM。

Abstract: Large language models (LLMs) have demonstrated exceptional performance in reasoning tasks such as mathematics and coding, matching or surpassing human capabilities. However, these impressive reasoning abilities face significant challenges in specialized domains. Taking Go as an example, although AlphaGo has established the high performance ceiling of AI systems in Go, mainstream LLMs still struggle to reach even beginner-level proficiency, let alone perform natural language reasoning. This performance gap between general-purpose LLMs and domain experts is significantly limiting the application of LLMs on a wider range of domain-specific tasks. In this work, we aim to bridge the divide between LLMs' general reasoning capabilities and expert knowledge in domain-specific tasks. We perform mixed fine-tuning with structured Go expertise and general long Chain-of-Thought (CoT) reasoning data as a cold start, followed by reinforcement learning to integrate expert knowledge in Go with general reasoning capabilities. Through this methodology, we present \textbf{LoGos}, a powerful LLM that not only maintains outstanding general reasoning abilities, but also conducts Go gameplay in natural language, demonstrating effective strategic reasoning and accurate next-move prediction. LoGos achieves performance comparable to human professional players, substantially surpassing all existing LLMs. Through this work, we aim to contribute insights on applying general LLM reasoning capabilities to specialized domains. We will release the first large-scale Go dataset for LLM training, the first LLM Go evaluation benchmark, and the first general LLM that reaches human professional-level performance in Go at: https://github.com/Entarochuan/LoGos.

</details>


### [23] [Graph-Anchored Knowledge Indexing for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.16462)
*Zhenghao Liu,Mingyan Wu,Xinze Li,Yukun Yan,Shuo Wang,Cheng Yang,Minghe Yu,Zheni Zeng,Maosong Sun*

Main category: cs.CL

TL;DR: GraphAnchor提出了一种基于图锚定的知识索引方法，将图结构从静态知识表示重构为动态演化的知识索引，以改善多跳问答中的检索增强生成效果。


<details>
  <summary>Details</summary>
Motivation: 尽管检索增强生成（RAG）已成为减少大语言模型幻觉的主要范式，但现有RAG系统在有效整合和解释分散在嘈杂文档中的关键证据方面仍面临重大挑战。

Method: GraphAnchor是一种图锚定知识索引方法，在迭代检索过程中增量更新图结构，锚定重要实体和关系，形成结构化索引来指导LLM评估知识充分性和制定后续子查询，最终联合利用所有检索文档和演化完成的图来生成答案。

Result: 在四个多跳问答基准测试上的实验表明GraphAnchor有效，并揭示该方法能够调节LLM的注意力，使其更有效地关联检索文档中分布的关键信息。

Conclusion: GraphAnchor通过将图结构重新概念化为动态演化的知识索引，为RAG系统提供了一种有效的方法来整合分散的关键证据，改善多跳问答性能。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a dominant paradigm for mitigating hallucinations in Large Language Models (LLMs) by incorporating external knowledge. Nevertheless, effectively integrating and interpreting key evidence scattered across noisy documents remains a critical challenge for existing RAG systems. In this paper, we propose GraphAnchor, a novel Graph-Anchored Knowledge Indexing approach that reconceptualizes graph structures from static knowledge representations into active, evolving knowledge indices. GraphAnchor incrementally updates a graph during iterative retrieval to anchor salient entities and relations, yielding a structured index that guides the LLM in evaluating knowledge sufficiency and formulating subsequent subqueries. The final answer is generated by jointly leveraging all retrieved documents and the final evolved graph. Experiments on four multi-hop question answering benchmarks demonstrate the effectiveness of GraphAnchor, and reveal that GraphAnchor modulates the LLM's attention to more effectively associate key information distributed in retrieved documents. All code and data are available at https://github.com/NEUIR/GraphAnchor.

</details>


### [24] [Persona Jailbreaking in Large Language Models](https://arxiv.org/abs/2601.16466)
*Jivnesh Sandhan,Fei Cheng,Tushar Sandhan,Yugo Murawaki*

Main category: cs.CL

TL;DR: PHISH框架通过历史对话中的隐式引导实现黑盒LLM人格编辑，暴露了LLM在对抗性交互中人格稳定性的新漏洞。


<details>
  <summary>Details</summary>
Motivation: LLM在教育、心理健康等高敏感领域部署时，需要稳定的人格特性以保证可靠性。现有研究关注叙事或角色扮演任务，忽视了对抗性对话历史单独重塑LLM人格的可能性，黑盒人格操纵问题尚未探索。

Method: 提出PHISH（Persona Hijacking via Implicit Steering in History）框架，在仅推理的黑盒设置下，通过在用户查询中嵌入语义线索来逐步诱导反向人格。定义了量化攻击成功的指标。

Result: 在3个基准测试和8个LLM上，PHISH可预测地改变人格，触发相关特性的连带变化，在多轮对话中效果更强。在高风险领域（心理健康、辅导、客服）能可靠操纵人格，经人类和LLM-as-Judge评估验证。对推理基准性能影响小，总体效用基本保持。

Conclusion: 当前防护措施在持续攻击下仍然脆弱，暴露了LLM人格的新漏洞，凸显了需要上下文弹性人格的必要性。

Abstract: Large Language Models (LLMs) are increasingly deployed in domains such as education, mental health and customer support, where stable and consistent personas are critical for reliability. Yet, existing studies focus on narrative or role-playing tasks and overlook how adversarial conversational history alone can reshape induced personas. Black-box persona manipulation remains unexplored, raising concerns for robustness in realistic interactions. In response, we introduce the task of persona editing, which adversarially steers LLM traits through user-side inputs under a black-box, inference-only setting. To this end, we propose PHISH (Persona Hijacking via Implicit Steering in History), the first framework to expose a new vulnerability in LLM safety that embeds semantically loaded cues into user queries to gradually induce reverse personas. We also define a metric to quantify attack success. Across 3 benchmarks and 8 LLMs, PHISH predictably shifts personas, triggers collateral changes in correlated traits, and exhibits stronger effects in multi-turn settings. In high-risk domains mental health, tutoring, and customer support, PHISH reliably manipulates personas, validated by both human and LLM-as-Judge evaluations. Importantly, PHISH causes only a small reduction in reasoning benchmark performance, leaving overall utility largely intact while still enabling significant persona manipulation. While current guardrails offer partial protection, they remain brittle under sustained attack. Our findings expose new vulnerabilities in personas and highlight the need for context-resilient persona in LLMs. Our codebase and dataset is available at: https://github.com/Jivnesh/PHISH

</details>


### [25] [DeepEra: A Deep Evidence Reranking Agent for Scientific Retrieval-Augmented Generated Question Answering](https://arxiv.org/abs/2601.16478)
*Haotian Chen,Qingqing Long,Siyu Pu,Xiao Luo,Wei Ju,Meng Xiao,Yuanchun Zhou,Jianghua Zhao,Xuezhi Wang*

Main category: cs.CL

TL;DR: 本文提出DeepEra系统，通过深度证据重排序解决科学问答中语义相似但逻辑无关的SSLI问题，并构建了SciRAG-SSLI数据集进行系统评估。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成(RAG)方法在科学问答中存在严重问题：容易检索到语义相似但逻辑无关的段落，这会降低事实可靠性并加剧幻觉现象。

Method: 提出Deep Evidence Reranking Agent (DeepEra)，整合逐步推理能力，超越表层语义对候选段落进行更精确评估。同时构建SciRAG-SSLI大规模数据集，包含约30万科学问答实例，涵盖10个学科，结合自然检索上下文和系统生成的干扰项。

Result: 综合评估表明，该方法相比领先的重排序方法取得了更优的检索性能。这是首次全面研究和实证验证两阶段RAG框架中不可忽视的SSLI问题。

Conclusion: DeepEra通过深度推理有效解决了科学问答中语义相似但逻辑无关的挑战，构建的SciRAG-SSLI数据集为系统评估提供了基准，显著提升了检索增强生成在科学领域的可靠性。

Abstract: With the rapid growth of scientific literature, scientific question answering (SciQA) has become increasingly critical for exploring and utilizing scientific knowledge. Retrieval-Augmented Generation (RAG) enhances LLMs by incorporating knowledge from external sources, thereby providing credible evidence for scientific question answering. But existing retrieval and reranking methods remain vulnerable to passages that are semantically similar but logically irrelevant, often reducing factual reliability and amplifying hallucinations.To address this challenge, we propose a Deep Evidence Reranking Agent (DeepEra) that integrates step-by-step reasoning, enabling more precise evaluation of candidate passages beyond surface-level semantics. To support systematic evaluation, we construct SciRAG-SSLI (Scientific RAG - Semantically Similar but Logically Irrelevant), a large-scale dataset comprising about 300K SciQA instances across 10 subjects, constructed from 10M scientific corpus. The dataset combines naturally retrieved contexts with systematically generated distractors to test logical robustness and factual grounding. Comprehensive evaluations confirm that our approach achieves superior retrieval performance compared to leading rerankers. To our knowledge, this work is the first to comprehensively study and empirically validate innegligible SSLI issues in two-stage RAG frameworks.

</details>


### [26] [TL-GRPO: Turn-Level RL for Reasoning-Guided Iterative Optimization](https://arxiv.org/abs/2601.16480)
*Peiji Li,Linyang Li,Handa Sun,Wenjin Mai,Yongkang Chen,Xiaozhe Li,Yue Shen,Yichuan Ma,Yiliu Sun,Jiaxi Cao,Zhishu He,Bo Wang,Xiaoqing Zheng,Zhaori Bi,Xipeng Qiu,Qipeng Guo,Kai Chen,Dahua Lin*

Main category: cs.CL

TL;DR: TL-GRPO是一种轻量级RL算法，针对迭代优化任务进行轮级优化，在模拟电路尺寸优化任务中超越标准GRPO和贝叶斯优化方法。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法无法在迭代优化任务中进行细粒度的轮级优化，而黑盒优化方法会丢弃先验知识和推理能力。迭代优化任务的特点是智能体在多轮中与相同的底层环境状态交互，轨迹价值由最佳轮级奖励而非累积回报决定。

Method: 提出了Turn-Level GRPO (TL-GRPO)，一种轻量级RL算法，通过轮级分组采样实现细粒度优化。该方法专门针对迭代优化任务设计，其中智能体在多轮中与相同环境状态交互。

Result: TL-GRPO在模拟电路尺寸优化任务中优于标准GRPO和贝叶斯优化方法。使用TL-GRPO训练的30B模型在相同模拟预算下实现了最先进的性能，展示了强大的泛化能力和实际效用。

Conclusion: TL-GRPO成功解决了迭代优化任务中的轮级优化挑战，为大语言模型在需要多次交互和领域专业知识的复杂科学优化任务中提供了有效的强化学习框架。

Abstract: Large language models have demonstrated strong reasoning capabilities in complex tasks through tool integration, which is typically framed as a Markov Decision Process and optimized with trajectory-level RL algorithms such as GRPO. However, a common class of reasoning tasks, iterative optimization, presents distinct challenges: the agent interacts with the same underlying environment state across turns, and the value of a trajectory is determined by the best turn-level reward rather than cumulative returns. Existing GRPO-based methods cannot perform fine-grained, turn-level optimization in such settings, while black-box optimization methods discard prior knowledge and reasoning capabilities. To address this gap, we propose Turn-Level GRPO (TL-GRPO), a lightweight RL algorithm that performs turn-level group sampling for fine-grained optimization. We evaluate TL-GRPO on analog circuit sizing (ACS), a challenging scientific optimization task requiring multiple simulations and domain expertise. Results show that TL-GRPO outperforms standard GRPO and Bayesian optimization methods across various specifications. Furthermore, our 30B model trained with TL-GRPO achieves state-of-the-art performance on ACS tasks under same simulation budget, demonstrating both strong generalization and practical utility.

</details>


### [27] [Timely Machine: Awareness of Time Makes Test-Time Scaling Agentic](https://arxiv.org/abs/2601.16486)
*Yichuan Ma,Linyang Li,Yongkang chen,Peiji Li,Xiaozhe Li,Qipeng Guo,Dahua Lin,Kai Chen*

Main category: cs.CL

TL;DR: Timely Machine重新定义测试时间为实际耗时，提出Timely-Eval基准和Timely-RL方法，使LLM在工具调用场景中根据时间预算动态调整策略。


<details>
  <summary>Details</summary>
Motivation: 传统基于生成长度的测试时间定义在工具调用场景中失效，因为工具延迟使推理时间与生成长度解耦。需要新的方法来处理智能体场景中的时间预算约束。

Method: 1) 提出Timely Machine框架，将测试时间重新定义为实际耗时；2) 创建Timely-Eval基准，涵盖高频工具调用、低频工具调用和时间约束推理三种场景；3) 开发Timely-RL方法，通过强化学习增强时间规划能力。

Result: 研究发现：1) 小模型在快速反馈场景中表现更好，大模型在高延迟场景中占优；2) 现有模型无法根据时间预算调整推理策略；3) Timely-RL能显著提升时间预算意识和性能，在Timely-Eval基准上持续改进表现。

Conclusion: 该工作为智能体时代的测试时间缩放提供了新视角，强调在实际耗时约束下动态调整策略的重要性，提出的Timely-RL方法能有效增强LLM的时间规划能力。

Abstract: As large language models (LLMs) increasingly tackle complex reasoning tasks, test-time scaling has become critical for enhancing capabilities. However, in agentic scenarios with frequent tool calls, the traditional generation-length-based definition breaks down: tool latency decouples inference time from generation length. We propose Timely Machine, redefining test-time as wall-clock time, where models dynamically adjust strategies based on time budgets. We introduce Timely-Eval, a benchmark spanning high-frequency tool calls, low-frequency tool calls, and time-constrained reasoning. By varying tool latency, we find smaller models excel with fast feedback through more interactions, while larger models dominate high-latency settings via superior interaction quality. Moreover, existing models fail to adapt reasoning to time budgets. We propose Timely-RL to address this gap. After cold-start supervised fine-tuning, we use reinforcement learning to enhance temporal planning. Timely-RL improves time budget awareness and consistently boosts performance across Timely-Eval. We hope our work offers a new perspective on test-time scaling for the agentic era.

</details>


### [28] [MRAG: Benchmarking Retrieval-Augmented Generation for Bio-medicine](https://arxiv.org/abs/2601.16503)
*Wei Zhu*

Main category: cs.CL

TL;DR: MRAG 是一个医学领域的检索增强生成基准测试，涵盖中英文任务，包含 MRAG-Bench 数据集和 MRAG-Toolkit 工具包，用于系统评估 RAG 在医学 QA 中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前医学领域缺乏全面的检索增强生成（RAG）评估基准，阻碍了对 RAG 在科学和临床 QA 系统中应用的系统研究。

Method: 构建 MRAG 基准测试，覆盖中英文多种医学任务，基于 Wikipedia 和 PubMed 构建语料库，并开发 MRAG-Toolkit 工具包来系统探索不同 RAG 组件。

Result: 实验表明：(a) RAG 提高了 LLM 在 MRAG 任务中的可靠性；(b) RAG 性能受检索方法、模型大小和提示策略影响；(c) RAG 改善了有用性和推理质量，但可能略微降低长问题回答的可读性。

Conclusion: MRAG 基准填补了医学 RAG 评估的空白，为学术界和工业界提供了系统评估工具，并揭示了 RAG 在医学 QA 中的优势与局限。

Abstract: While Retrieval-Augmented Generation (RAG) has been swiftly adopted in scientific and clinical QA systems, a comprehensive evaluation benchmark in the medical domain is lacking. To address this gap, we introduce the Medical Retrieval-Augmented Generation (MRAG) benchmark, covering various tasks in English and Chinese languages, and building a corpus with Wikipedia and Pubmed. Additionally, we develop the MRAG-Toolkit, facilitating systematic exploration of different RAG components. Our experiments reveal that: (a) RAG enhances LLM reliability across MRAG tasks. (b) the performance of RAG systems is influenced by retrieval approaches, model sizes, and prompting strategies. (c) While RAG improves usefulness and reasoning quality, LLM responses may become slightly less readable for long-form questions. We will release the MRAG-Bench's dataset and toolkit with CCBY-4.0 license upon acceptance, to facilitate applications from both academia and industry.

</details>


### [29] [LOGICAL-COMMONSENSEQA: A Benchmark for Logical Commonsense Reasoning](https://arxiv.org/abs/2601.16504)
*Obed Junias,Maria Leonor Pacheco*

Main category: cs.CL

TL;DR: LOGICAL-COMMONSENSEQA是一个重新构建常识推理为逻辑组合的基准测试，使用AND、OR、NEITHER/NOR等运算符评估语句对的联合合理性。


<details>
  <summary>Details</summary>
Motivation: 现有常识推理基准大多依赖单标签评估，掩盖了多个解释是否联合合理、互斥或联合不合理的问题，无法全面评估模型的组合推理能力。

Method: 引入LOGICAL-COMMONSENSEQA基准，将常识推理重新构建为对原子陈述对使用合理性级别运算符（AND、OR、NEITHER/NOR）的逻辑组合。在零样本、少样本和思维链提示下评估指令调优、推理专业化和微调模型。

Result: 模型在合取推理上表现合理，在析取推理上表现中等，但在基于否定的问题上性能急剧下降。该基准揭示了基本推理局限性。

Conclusion: LOGICAL-COMMONSENSEQA暴露了当前模型在组合常识推理方面的根本限制，为推进该领域提供了受控框架。

Abstract: Commonsense reasoning often involves evaluating multiple plausible interpretations rather than selecting a single atomic answer, yet most benchmarks rely on single-label evaluation, obscuring whether statements are jointly plausible, mutually exclusive, or jointly implausible. We introduce LOGICAL-COMMONSENSEQA, a benchmark that re-frames commonsense reasoning as logical composition over pairs of atomic statements using plausibility-level operators (AND, OR, NEITHER/NOR). Evaluating instruction-tuned, reasoning-specialized, and fine-tuned models under zero-shot, few-shot, and chain-of-thought prompting, we find that while models perform reasonably on conjunctive and moderately on disjunctive reasoning, performance degrades sharply on negation-based questions. LOGICAL-COMMONSENSEQA exposes fundamental reasoning limitations and provides a controlled framework for advancing compositional commonsense reasoning.

</details>


### [30] [Is Length Really A Liability? An Evaluation of Multi-turn LLM Conversations using BoolQ](https://arxiv.org/abs/2601.16508)
*Karl Neergaard,Le Qiu,Emmanuele Chersoni*

Main category: cs.CL

TL;DR: 研究发现对话长度影响LLM回答真实性，单轮测试无法捕捉模型在真实对话中的漏洞


<details>
  <summary>Details</summary>
Motivation: 当前LLM基准测试主要采用单轮评估，无法捕捉真实对话中发生的危害。研究旨在探索对话长度是否影响LLM回答的真实性

Method: 使用BoolQ数据集，在不同对话长度和支架条件下评估三个不同LLM的性能

Result: 发现了模型特定的漏洞，这些漏洞在单轮测试中不可见。观察到长度依赖和支架特定的效应，表明静态评估存在根本局限性

Conclusion: 部署相关的漏洞只能在多轮对话设置中被发现，凸显了当前静态评估方法的不足

Abstract: Single-prompt evaluations dominate current LLM benchmarking, yet they fail to capture the conversational dynamics where real-world harm occurs. In this study, we examined whether conversation length affects response veracity by evaluating LLM performance on the BoolQ dataset under varying length and scaffolding conditions. Our results across three distinct LLMs revealed model-specific vulnerabilities that are invisible under single-turn testing. The length-dependent and scaffold-specific effects we observed demonstrate a fundamental limitation of static evaluations, as deployment-relevant vulnerabilities could only be spotted in a multi-turn conversational setting.

</details>


### [31] [SearchLLM: Detecting LLM Paraphrased Text by Measuring the Similarity with Regeneration of the Candidate Source via Search Engine](https://arxiv.org/abs/2601.16512)
*Hoang-Quoc Nguyen-Son,Minh-Son Dao,Koji Zettsu*

Main category: cs.CL

TL;DR: SearchLLM是一种通过搜索引擎查找原始文本来源来检测LLM改写文本的新方法，能够增强现有检测器的性能并防止改写攻击。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的普及，用户常用LLM改写文本以提升质量，但这可能导致原始意图的丢失或扭曲。传统检测方法对LLM生成的类人文本效果不佳，特别是当改写文本与原始内容高度相似时。

Method: SearchLLM利用搜索引擎能力定位潜在的原始文本来源，通过分析输入文本与候选来源的再生版本之间的相似性来识别LLM改写内容。该方法设计为代理层，可与现有检测器无缝集成。

Result: 实验结果表明，SearchLLM在不同LLM上都能持续提升现有检测器在检测高度模仿原始内容的LLM改写文本时的准确性，并能帮助检测器防止改写攻击。

Conclusion: SearchLLM通过结合搜索引擎查找原始来源的方法，有效解决了LLM改写文本检测的挑战，为现有检测器提供了性能增强的有效途径。

Abstract: With the advent of large language models (LLMs), it has become common practice for users to draft text and utilize LLMs to enhance its quality through paraphrasing. However, this process can sometimes result in the loss or distortion of the original intended meaning. Due to the human-like quality of LLM-generated text, traditional detection methods often fail, particularly when text is paraphrased to closely mimic original content. In response to these challenges, we propose a novel approach named SearchLLM, designed to identify LLM-paraphrased text by leveraging search engine capabilities to locate potential original text sources. By analyzing similarities between the input and regenerated versions of candidate sources, SearchLLM effectively distinguishes LLM-paraphrased content. SearchLLM is designed as a proxy layer, allowing seamless integration with existing detectors to enhance their performance. Experimental results across various LLMs demonstrate that SearchLLM consistently enhances the accuracy of recent detectors in detecting LLM-paraphrased text that closely mimics original content. Furthermore, SearchLLM also helps the detectors prevent paraphrasing attacks.

</details>


### [32] [Curate-Train-Refine: A Closed-Loop Agentic Framework for Zero Shot Classification](https://arxiv.org/abs/2601.16530)
*Gaurav Maheshwari,Kevin El Haddad*

Main category: cs.CL

TL;DR: 利用LLM动态生成监督数据训练轻量级文本分类器，通过迭代代理循环提升数据质量，在四个基准测试中优于标准零/少样本基线。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和高容量编码器虽然提升了零样本和少样本分类性能，但其推理成本和延迟限制了实际部署，需要寻找更高效的解决方案。

Method: 采用迭代代理循环方法：LLM首先策划训练数据，分析模型成功与失败案例，然后针对观察到的错误合成针对性示例，通过闭环生成和评估过程逐步提升数据质量并适应下游分类器和任务。

Result: 在四个广泛使用的基准测试中，该方法持续优于标准的零样本和少样本基线方法。

Conclusion: LLM可以有效地作为数据策划者，在不部署大型模型的情况下实现准确高效的分类，避免了大规模模型部署的操作成本。

Abstract: Large language models (LLMs) and high-capacity encoders have advanced zero and few-shot classification, but their inference cost and latency limit practical deployment. We propose training lightweight text classifiers using dynamically generated supervision from an LLM. Our method employs an iterative, agentic loop in which the LLM curates training data, analyzes model successes and failures, and synthesizes targeted examples to address observed errors. This closed-loop generation and evaluation process progressively improves data quality and adapts it to the downstream classifier and task. Across four widely used benchmarks, our approach consistently outperforms standard zero and few-shot baselines. These results indicate that LLMs can serve effectively as data curators, enabling accurate and efficient classification without the operational cost of large-model deployment.

</details>


### [33] [Retrieve-Refine-Calibrate: A Framework for Complex Claim Fact-Checking](https://arxiv.org/abs/2601.16555)
*Mingwei Sun,Qianlong Wang,Ruifeng Xu*

Main category: cs.CL

TL;DR: 提出基于大语言模型的检索-精炼-校准框架，通过识别实体、精炼证据和校准验证过程，提高事实核查准确性。


<details>
  <summary>Details</summary>
Motivation: 现有事实核查方法通常采用分解范式，将声明分解为子声明单独验证。但这种范式可能因引入不相关实体或证据而产生噪声，从而降低验证准确性。

Method: 提出检索-精炼-校准框架：1) 识别声明中的实体并检索相关证据；2) 基于声明精炼检索到的证据以减少不相关信息；3) 通过重新评估低置信度预测来校准验证过程。

Result: 在两个流行的事实核查数据集（HOVER和FEVEROUS-S）上的实验表明，该框架相比竞争基线实现了更优越的性能。

Conclusion: 该研究提出的RRC框架有效解决了分解范式中的噪声问题，通过证据精炼和验证校准提高了事实核查的准确性。

Abstract: Fact-checking aims to verify the truthfulness of a claim based on the retrieved evidence. Existing methods typically follow a decomposition paradigm, in which a claim is broken down into sub-claims that are individually verified. However, the decomposition paradigm may introduce noise to the verification process due to irrelevant entities or evidence, ultimately degrading verification accuracy. To address this problem, we propose a Retrieve-Refine-Calibrate (RRC) framework based on large language models (LLMs). Specifically, the framework first identifies the entities mentioned in the claim and retrieves evidence relevant to them. Then, it refines the retrieved evidence based on the claim to reduce irrelevant information. Finally, it calibrates the verification process by re-evaluating low-confidence predictions. Experiments on two popular fact-checking datasets (HOVER and FEVEROUS-S) demonstrate that our framework achieves superior performance compared with competitive baselines.

</details>


### [34] [Attention-MoA: Enhancing Mixture-of-Agents via Inter-Agent Semantic Attention and Deep Residual Synthesis](https://arxiv.org/abs/2601.16596)
*Jianyu Wen,Yang Wei,Xiongxi Yu,Changxuan Xiao,Ke Zeng*

Main category: cs.CL

TL;DR: Attention-MoA是一个基于注意力机制的新型混合代理框架，通过代理间语义注意力促进深度交互，配合层间残差模块和自适应早停机制，显著提升集体智能效果。


<details>
  <summary>Details</summary>
Motivation: 现有混合代理框架虽然引入了动态路由和残差连接，但未能促进代理间的深度语义交互，限制了系统纠正幻觉和优化逻辑的能力。

Method: 提出Attention-MoA框架，核心是代理间语义注意力机制，配合层间残差模块和自适应早停机制，缓解深层信息退化并提高计算效率。

Result: 在AlpacaEval 2.0上获得91.15%的长度控制胜率，MT-Bench得分8.83，在FLASK的12项能力中主导10项。开源小模型组合甚至超越Claude-4.5-Sonnet和GPT-4.1等大型专有模型。

Conclusion: Attention-MoA通过重新定义代理间协作机制，实现了更有效的集体智能，使开源小模型能够超越大型专有模型，为大语言模型推理时协作提供了新范式。

Abstract: As the development of Large Language Models (LLMs) shifts from parameter scaling to inference-time collaboration, the Mixture-of-Agents (MoA) framework has emerged as a general paradigm to harness collective intelligence by layering diverse models. While recent MoA variants have introduced dynamic routing and residual connections to improve efficiency, these methods often fail to facilitate deep semantic interaction between agents, limiting the system's ability to actively correct hallucinations and refine logic. In this paper, we introduce Attention-MoA, a novel MoA-based framework that redefines collaboration through Inter-agent Semantic Attention. Complemented by an Inter-layer Residual Module with Adaptive Early Stopping Mechanism, our architecture mitigates information degradation in deep layers while improving computational efficiency. Extensive evaluations across AlpacaEval 2.0, MT-Bench, and FLASK demonstrate that Attention-MoA significantly outperforms state-of-the-art baselines, achieving a 91.15% Length-Controlled Win Rate on AlpacaEval 2.0 and dominating in 10 out of 12 capabilities on FLASK. Notably, Attention-MoA enables an ensemble of small open-source models to outperform massive proprietary models like Claude-4.5-Sonnet and GPT-4.1, achieving an MT-Bench score of 8.83 and an AlpacaEval 2.0 LC Win Rate of 77.36%.

</details>


### [35] [AuroraEdge-V-2B: A Faster And Stronger Edge Visual Large Language Model](https://arxiv.org/abs/2601.16615)
*Xiang Chen*

Main category: cs.CL

TL;DR: 提出AuroraEdge-V-2B，这是一个专为边缘部署设计的紧凑、鲁棒、高速的视觉大语言模型，通过压缩融合方法提高推理效率，在保持高性能的同时实现更快的运行速度和更低的计算成本。


<details>
  <summary>Details</summary>
Motivation: 随着多模态技术的发展，视觉大语言模型（VLLMs）逐渐应用于工业生产，但现有的VLLMs存在特定领域性能不如定制深度学习模型、参数量大、计算资源需求高、推理速度慢等缺点，限制了其在边缘环境中的实时应用。

Method: 提出了AuroraEdge-V-2B模型，这是一个仅有20亿参数的紧凑型VLLM，专门针对边缘部署设计。同时提出了一种压缩融合方法来提高推理效率，通过显著减少解码过程中的视觉标记数量，将推理时的浮点运算减少一半。

Result: AuroraEdge-V-2B在9个基准测试中获得了比同等参数规模模型（如Qwen2-VL-2B、Qwen2.5-VL-3B、InternVL-2.5-2B）更高的分数，同时具有更快的推理速度和更低的计算成本。

Conclusion: AuroraEdge-V-2B通过压缩融合技术和紧凑设计，成功解决了现有VLLMs在边缘部署中的资源消耗大、推理速度慢的问题，为工业应用中的实时视觉理解任务提供了高效可行的解决方案。

Abstract: Recently, due to the advancement of multimodal technology, people are attempting to use visual large language models (VLLMs) in industrial production. Many deep learning models (DLMs) deployed in the production environment are gradually being replaced by VLLMs. Compared with DLMs, VLLMs have some advantages in industrial applications: (1) Their strong generalization ability enables them to perform well across a wide range of tasks. (2) They are flexible and can deal with unfamiliar samples through context learning quickly. However, VLLMs also have obvious drawbacks: (1) VLLMs do not perform as well as custom-developed DLMs in specific domains. (2) The number of parameters in VLLMs is generally quite large, and their deployment requires substantial computational resources. (3) VLLMs generally operate much slower than DLMs, making real-time response challenging to achieve. To better utilize VLLMs in industrial applications, we introduce AuroraEdge-V-2B in this work, a compact, robust, and high-speed VLLM designed for edge deployment. To make the model run faster, we also propose a compression-fusion method to improve inference efficiency. AuroraEdge-V-2B has the following notable features: (1) Easy deployment and faster: It has only 2B parameters and is highly suitable for edge deployment, offering better real-time performance. (2) Fewer visual tokens and cheaper: It significantly reduces the number of visual tokens in the decoding process, thereby reducing the floating-point operations by half during inference and making it cheaper to use. (3) Strong performance: It gets a higher score on 9 benchmarks than models with the same number of parameter (e.g., Qwen2-VL-2B, Qwen2.5-VL-3B, InternVL-2.5-2B).

</details>


### [36] [PROST-LLM: Progressively Enhancing the Speech-to-Speech Translation Capability in LLMs](https://arxiv.org/abs/2601.16618)
*Jing Xu,Jiaqi Wang,Daxin Tan,Xiao Chen*

Main category: cs.CL

TL;DR: PROST-LLM通过渐进式训练方法增强大语言模型的语音到语音翻译能力，包括三任务学习、链式模态、自采样反向翻译和偏好优化等技术。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在语音到语音翻译任务中的应用尚未充分探索，且受到数据稀缺的限制，需要开发有效的训练方法来提升其S2ST能力。

Method: 1. 使用CVSS语料库微调LLM，采用三任务学习和链式模态方法提升初始性能；2. 通过自采样和反向翻译生成偏好对，无需人工评估；3. 使用偏好对进行偏好优化进一步强化模型能力。

Result: 大量实验证实PROST-LLM能有效提升LLM的语音到语音翻译能力。

Conclusion: PROST-LLM通过渐进式训练策略成功解决了LLM在S2ST任务中的数据稀缺问题，为LLM在语音翻译领域的应用提供了有效解决方案。

Abstract: Although Large Language Models (LLMs) excel in many tasks, their application to Speech-to-Speech Translation (S2ST) is underexplored and hindered by data scarcity. To bridge this gap, we propose PROST-LLM (PROgressive Speech-to-speech Translation) to enhance the S2ST capabilities in LLMs progressively. First, we fine-tune the LLMs with the CVSS corpus, employing designed tri-task learning and chain of modality methods to boost the initial performance. Then, leveraging the fine-tuned model, we generate preference pairs through self-sampling and back-translation without human evaluation. Finally, these preference pairs are used for preference optimization to enhance the model's S2ST capability further. Extensive experiments confirm the effectiveness of our proposed PROST-LLM in improving the S2ST capability of LLMs.

</details>


### [37] [How Does Personalized Memory Shape LLM Behavior? Benchmarking Rational Preference Utilization in Personalized Assistants](https://arxiv.org/abs/2601.16621)
*Xueyang Feng,Weinan Gan,Xu Chen,Quanyu Dai,Yong Liu*

Main category: cs.CL

TL;DR: 该研究提出了RPEval基准和RP-Reasoner方法，用于评估和解决LLM助手在个性化记忆使用中的非理性个人化问题。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM助手通过记忆机制记录用户偏好能够提供更个性化的响应，但无关的个性化记忆会干扰LLM对用户意图的理解，影响用户体验。

Method: 1. 开发RPEval基准，包含个性化意图推理数据集和多粒度评估协议；2. 提出RP-Reasoner方法，将记忆利用视为语用推理过程，选择性整合个性化信息。

Result: 1. RPEval揭示了现有LLM中普遍存在的非理性个人化现象；2. RP-Reasoner在RPEval上显著优于精心设计的基线方法；3. 在大规模商业个性化助手中解决了80%的不良案例。

Conclusion: 语用推理能够有效缓解LLM助手中的非理性个人化问题，提升个性化服务质量。RPEval基准和RP-Reasoner方法为解决这一挑战提供了有效工具。

Abstract: Large language model (LLM)-powered assistants have recently integrated memory mechanisms that record user preferences, leading to more personalized and user-aligned responses. However, irrelevant personalized memories are often introduced into the context, interfering with the LLM's intent understanding. To comprehensively investigate the dual effects of personalization, we develop RPEval, a benchmark comprising a personalized intent reasoning dataset and a multi-granularity evaluation protocol. RPEval reveals the widespread phenomenon of irrational personalization in existing LLMs and, through error pattern analysis, illustrates its negative impact on user experience. Finally, we introduce RP-Reasoner, which treats memory utilization as a pragmatic reasoning process, enabling the selective integration of personalized information. Experimental results demonstrate that our method significantly outperforms carefully designed baselines on RPEval, and resolves 80% of the bad cases observed in a large-scale commercial personalized assistant, highlighting the potential of pragmatic reasoning to mitigate irrational personalization. Our benchmark is publicly available at https://github.com/XueyangFeng/RPEval.

</details>


### [38] [MultiLexNorm++: A Unified Benchmark and a Generative Model for Lexical Normalization for Asian Languages](https://arxiv.org/abs/2601.16623)
*Weerayut Buaphet,Thanh-Nhi Nguyen,Risa Kondo,Tomoyuki Kajiwara,Yumin Kim,Jimin Lee,Hwanhee Lee,Holy Lovenia,Peerat Limkonchotiwat,Sarana Nutanong,Rob Van der Goot*

Main category: cs.CL

TL;DR: 扩展MultiLexNorm基准至5种亚洲语言，提出基于大语言模型的新架构，实现更稳健的词汇规范化性能


<details>
  <summary>Details</summary>
Motivation: 现有词汇规范化基准MultiLexNorm主要覆盖印欧语系拉丁文字语言，缺乏对亚洲语言多样性的覆盖，需要扩展基准以评估模型在更广泛语言环境下的表现

Method: 1. 扩展MultiLexNorm基准，新增5种来自不同语系的亚洲语言（使用4种不同文字）
2. 提出基于大语言模型的新架构用于词汇规范化任务
3. 评估先前最先进模型在新语言上的表现
4. 分析剩余错误，探讨未来研究方向

Result: 1. 先前最先进模型在新亚洲语言上表现下降
2. 提出的基于大语言模型的新架构展现出更稳健的性能
3. 通过错误分析揭示了该任务的未来研究方向

Conclusion: 扩展MultiLexNorm基准至亚洲语言有助于推动词汇规范化研究的多样性和包容性，基于大语言模型的方法在跨语言任务中表现更稳健，但仍需进一步研究解决剩余挑战

Abstract: Social media data has been of interest to Natural Language Processing (NLP) practitioners for over a decade, because of its richness in information, but also challenges for automatic processing. Since language use is more informal, spontaneous, and adheres to many different sociolects, the performance of NLP models often deteriorates. One solution to this problem is to transform data to a standard variant before processing it, which is also called lexical normalization. There has been a wide variety of benchmarks and models proposed for this task. The MultiLexNorm benchmark proposed to unify these efforts, but it consists almost solely of languages from the Indo-European language family in the Latin script. Hence, we propose an extension to MultiLexNorm, which covers 5 Asian languages from different language families in 4 different scripts. We show that the previous state-of-the-art model performs worse on the new languages and propose a new architecture based on Large Language Models (LLMs), which shows more robust performance. Finally, we analyze remaining errors, revealing future directions for this task.

</details>


### [39] [Typologically Informed Parameter Aggregation](https://arxiv.org/abs/2601.16629)
*Stef Accou,Wessel Poelman*

Main category: cs.CL

TL;DR: TIPA是一种无需训练的方法，通过基于类型学相似性聚合现有适配器来构建代理语言适配器，实现零样本跨语言迁移。


<details>
  <summary>Details</summary>
Motivation: 大规模多语言模型虽然支持跨语言泛化，但在低资源和未见语言上表现不佳。基于适配器的微调虽然参数高效，但大规模训练语言特定适配器成本高昂。

Method: TIPA是一种无需训练的方法，通过基于类型学相似性加权聚合现有适配器来构建代理语言适配器，并集成到MAD-X框架中。

Result: 在5个NLP任务和230多种语言上评估，TIPA始终优于或匹配基线方法（如仅英语微调或选择类型学最接近的语言适配器），对于缺乏专用适配器的语言提升最大。

Conclusion: 类型学信息聚合为无需训练的语言特定模块提供了可行的替代方案，证明了其有效性。

Abstract: Massively multilingual language models enable cross-lingual generalization but underperform on low-resource and unseen languages. While adapter-based fine-tuning offers a parameter-efficient solution, training language-specific adapters at scale remains costly. We introduce Typologically Informed Parameter Aggregation (TIPA), a training-free method that constructs proxy language adapters by aggregating existing ones, weighted by typological similarity. Integrated into the MAD-X framework, these proxies enable zero-shot cross-lingual transfer without additional training. We evaluate TIPA on five NLP tasks and over 230 languages. TIPA consistently outperforms or matches baselines such as English-only fine-tuning or selecting the typologically closest language adapter. We see the largest gains for languages lacking dedicated adapters. Our results demonstrate that typologically informed aggregation provides a viable alternative to language-specific modules without any training needed.

</details>


### [40] [Sycophancy Hides Linearly in the Attention Heads](https://arxiv.org/abs/2601.16644)
*Rifo Genadi,Munachiso Nwadike,Nurdaulet Mukhituly,Hilal Alquabeh,Tatsuya Hiraoka,Kentaro Inui*

Main category: cs.CL

TL;DR: 研究发现通过注意力机制中的线性探针可以有效识别和缓解AI模型中的"奉承"行为，特别是在中间层注意力头中效果最佳。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解AI模型中的"奉承"行为（sycophancy）如何在线性表示中体现，特别是在多头部注意力激活中，并探索通过线性干预来缓解这种行为的可能性。

Method: 使用线性探针在残差流、多层感知机和注意力层中训练，分析奉承信号的线性可分性。基于TruthfulQA数据集，在注意力层中发现稀疏的中间层注意力头对奉承行为最敏感，并通过注意力模式分析验证。

Result: 1）正确-错误奉承信号在多头部注意力激活中最线性可分；2）训练在TruthfulQA上的探针能有效迁移到其他事实QA基准；3）发现的"奉承"方向与之前识别的"真实"方向重叠有限，表明事实准确性和抗顺从性来自相关但不同的机制；4）有影响力的注意力头过度关注用户怀疑表达，导致奉承转变。

Conclusion: 奉承行为可以通过简单、有针对性的线性干预来缓解，这些干预利用了注意力激活的内部几何结构，特别是在中间层注意力头中的稀疏子集。

Abstract: We find that correct-to-incorrect sycophancy signals are most linearly separable within multi-head attention activations. Motivated by the linear representation hypothesis, we train linear probes across the residual stream, multilayer perceptron (MLP), and attention layers to analyze where these signals emerge. Although separability appears in the residual stream and MLPs, steering using these probes is most effective in a sparse subset of middle-layer attention heads. Using TruthfulQA as the base dataset, we find that probes trained on it transfer effectively to other factual QA benchmarks. Furthermore, comparing our discovered direction to previously identified "truthful" directions reveals limited overlap, suggesting that factual accuracy, and deference resistance, arise from related but distinct mechanisms. Attention-pattern analysis further indicates that the influential heads attend disproportionately to expressions of user doubt, contributing to sycophantic shifts. Overall, these findings suggest that sycophancy can be mitigated through simple, targeted linear interventions that exploit the internal geometry of attention activations.

</details>


### [41] [Select or Project? Evaluating Lower-dimensional Vectors for LLM Training Data Explanations](https://arxiv.org/abs/2601.16651)
*Lukas Hinterleitner,Loris Schoenegger,Benjamin Roth*

Main category: cs.CL

TL;DR: 梯度方法用于大语言模型实例解释受限于高维梯度，研究发现贪心选择架构子集比全梯度或随机投影更有效。


<details>
  <summary>Details</summary>
Motivation: 基于梯度的大语言模型实例解释方法面临梯度维度巨大的挑战，实践中通常随意选择参数子集进行计算，缺乏系统评估和理论依据。

Method: 研究比较两种降维策略：选择架构感知的小规模组件子集 vs 将完整梯度投影到低维空间。使用新基准测试，通过贪心算法选择组件子集。

Result: 贪心选择的组件子集在检索任务中比完整梯度或随机投影能更有效地捕捉训练数据影响信息，且计算效率高于随机投影方法。

Conclusion: 针对性的组件选择是提高大模型实例解释计算可行性的实用策略，架构感知的贪心选择优于随机投影和完整梯度分析。

Abstract: Gradient-based methods for instance-based explanation for large language models (LLMs) are hindered by the immense dimensionality of model gradients. In practice, influence estimation is restricted to a subset of model parameters to make computation tractable, but this subset is often chosen ad hoc and rarely justified by systematic evaluation. This paper investigates if it is better to create low-dimensional representations by selecting a small, architecturally informed subset of model components or by projecting the full gradients into a lower-dimensional space. Using a novel benchmark, we show that a greedily selected subset of components captures the information about training data influence needed for a retrieval task more effectively than either the full gradient or random projection. We further find that this approach is more computationally efficient than random projection, demonstrating that targeted component selection is a practical strategy for making instance-based explanations of large models more computationally feasible.

</details>


### [42] [PLawBench: A Rubric-Based Benchmark for Evaluating LLMs in Real-World Legal Practice](https://arxiv.org/abs/2601.16669)
*Yuzhen Shi,Huanghai Liu,Yiran Hu,Gaojie Song,Xinran Xu,Yubo Ma,Tianyi Tang,Li Zhang,Qingjing Chen,Di Feng,Wenbo Lv,Weiheng Wu,Kexin Yang,Sen Yang,Wei Wang,Rongyao Shi,Yuanyang Qiu,Yuemeng Qi,Jingwen Zhang,Xiaoyu Sui,Yifan Chen,Yi Zhang,An Yang,Bowen Yu,Dayiheng Liu,Junyang Lin,Weixing Shen,Bing Zhao,Charles L. A. Clarke,Hu Wei*

Main category: cs.CL

TL;DR: PLawBench是一个评估大语言模型在实际法律实践场景中能力的基准测试，包含850个问题覆盖13个法律场景，使用细粒度评估标准，发现当前LLMs在法律推理方面存在显著局限。


<details>
  <summary>Details</summary>
Motivation: 现有法律基准测试过于简化，无法捕捉真实法律实践的模糊性、复杂性和推理需求，且采用粗粒度的单维度指标，未能明确评估细粒度法律推理能力。

Method: 基于真实法律工作流程设计PLawBench，包含三个任务类别：公共法律咨询、实际案例分析、法律文件生成。包含850个问题，覆盖13个实际法律场景，每个问题配有专家设计的评估标准，共约12,500个评估项。使用基于LLM的评估器（与人类专家判断对齐）评估10个最先进的LLM。

Result: 实验结果显示，没有LLM在PLawBench上表现强劲，揭示了当前LLMs在细粒度法律推理能力方面存在显著局限。

Conclusion: PLawBench为法律LLMs的未来评估和发展指明了重要方向，表明当前模型在实际法律实践场景中的能力仍有限。

Abstract: As large language models (LLMs) are increasingly applied to legal domain-specific tasks, evaluating their ability to perform legal work in real-world settings has become essential. However, existing legal benchmarks rely on simplified and highly standardized tasks, failing to capture the ambiguity, complexity, and reasoning demands of real legal practice. Moreover, prior evaluations often adopt coarse, single-dimensional metrics and do not explicitly assess fine-grained legal reasoning. To address these limitations, we introduce PLawBench, a Practical Law Benchmark designed to evaluate LLMs in realistic legal practice scenarios. Grounded in real-world legal workflows, PLawBench models the core processes of legal practitioners through three task categories: public legal consultation, practical case analysis, and legal document generation. These tasks assess a model's ability to identify legal issues and key facts, perform structured legal reasoning, and generate legally coherent documents. PLawBench comprises 850 questions across 13 practical legal scenarios, with each question accompanied by expert-designed evaluation rubrics, resulting in approximately 12,500 rubric items for fine-grained assessment. Using an LLM-based evaluator aligned with human expert judgments, we evaluate 10 state-of-the-art LLMs. Experimental results show that none achieves strong performance on PLawBench, revealing substantial limitations in the fine-grained legal reasoning capabilities of current LLMs and highlighting important directions for future evaluation and development of legal LLMs. Data is available at: https://github.com/skylenage/PLawbench.

</details>


### [43] [EMemBench: Interactive Benchmarking of Episodic Memory for VLM Agents](https://arxiv.org/abs/2601.16690)
*Xinze Li,Ziyue Zhu,Siyuan Liu,Yubo Ma,Yuhang Zang,Yixin Cao,Aixin Sun*

Main category: cs.CL

TL;DR: EMemBench是一个用于评估智能体长期记忆的程序化基准测试，通过交互式游戏生成个性化问题，涵盖文本和视觉环境，验证智能体在多维度记忆技能上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估智能体长期记忆能力的系统性基准测试。现有方法通常使用固定问题集，无法覆盖智能体个性化轨迹中的记忆需求，特别是在复杂交互环境中对多维度记忆技能的评估不足。

Method: 1. 通过交互式游戏生成个性化问题，基于智能体自身轨迹而非固定问题集
2. 涵盖文本和视觉两种游戏环境
3. 使用模板从底层游戏信号计算可验证的真实答案
4. 控制问题的可回答性和平衡覆盖多种记忆技能：单跳/多跳回忆、归纳、时间、空间、逻辑和对抗性推理

Result: 1. 在15个文本游戏和多个视觉种子上的测试结果显示，智能体表现远未饱和
2. 归纳和空间推理是持续存在的瓶颈，在视觉环境中尤为明显
3. 持久记忆对开放骨干网络在文本游戏中带来明显收益，但对VLM智能体的改进不够一致
4. 人类研究证实了EMemBench的难度，表明视觉基础的场景记忆仍是开放挑战

Conclusion: EMemBench为评估智能体长期记忆提供了一个全面且具有挑战性的基准测试。结果表明当前智能体在复杂记忆任务上仍有显著不足，特别是在视觉环境中。该基准揭示了记忆系统设计中的关键挑战，为未来研究提供了重要方向。

Abstract: We introduce EMemBench, a programmatic benchmark for evaluating long-term memory of agents through interactive games. Rather than using a fixed set of questions, EMemBench generates questions from each agent's own trajectory, covering both text and visual game environments. Each template computes verifiable ground truth from underlying game signals, with controlled answerability and balanced coverage over memory skills: single/multi-hop recall, induction, temporal, spatial, logical, and adversarial. We evaluate memory agents with strong LMs/VLMs as backbones, using in-context prompting as baselines. Across 15 text games and multiple visual seeds, results are far from saturated: induction and spatial reasoning are persistent bottlenecks, especially in visual setting. Persistent memory yields clear gains for open backbones on text games, but improvements are less consistent for VLM agents, suggesting that visually grounded episodic memory remains an open challenge. A human study further confirms the difficulty of EMemBench.

</details>


### [44] [Mitigating Bias in Automated Grading Systems for ESL Learners: A Contrastive Learning Approach](https://arxiv.org/abs/2601.16724)
*Kevin Fan,Eric Yun*

Main category: cs.CL

TL;DR: 本研究通过对比学习匹配论文对的方法，有效减少了自动作文评分系统对高熟练度英语学习者的评分偏见，将评分差距降低了39.9%。


<details>
  <summary>Details</summary>
Motivation: 随着自动作文评分系统在教育高风险场景中的广泛应用，针对英语学习者的算法偏见问题日益突出。当前基于Transformer的回归模型主要基于母语者语料训练，容易学习到表层语言特征与作文质量之间的虚假相关性，导致对英语学习者的不公平评分。

Method: 首先对微调的DeBERTa-v3模型在ASAP 2.0和ELLIPSE数据集上进行偏见研究，发现高熟练度英语学习者作文比同等质量母语作文得分低10.3%。为缓解此问题，提出基于对比学习的匹配论文对方法，构建了17,161对匹配论文数据集，使用三元组边界损失微调模型，使英语学习者和母语写作的潜在表示对齐。

Result: 该方法将高熟练度评分差距减少了39.9%，从10.3%降至6.2%，同时保持了0.76的二次加权Kappa值。后验语言分析表明，模型成功解耦了句子复杂性和语法错误，避免了对有效二语句法结构的惩罚。

Conclusion: 对比学习匹配论文对方法能有效减少自动作文评分系统对英语学习者的算法偏见，改善评分公平性，同时保持评分准确性。该方法有助于区分语言习得特征与写作质量，为构建更公平的教育评估系统提供了有效途径。

Abstract: As Automated Essay Scoring (AES) systems are increasingly used in high-stakes educational settings, concerns regarding algorithmic bias against English as a Second Language (ESL) learners have increased. Current Transformer-based regression models trained primarily on native-speaker corpora often learn spurious correlations between surface-level L2 linguistic features and essay quality. In this study, we conduct a bias study of a fine-tuned DeBERTa-v3 model using the ASAP 2.0 and ELLIPSE datasets, revealing a constrained score scaling for high-proficiency ESL writing where high-proficiency ESL essays receive scores 10.3% lower than Native speaker essays of identical human-rated quality. To mitigate this, we propose applying contrastive learning with a triplet construction strategy: Contrastive Learning with Matched Essay Pairs. We constructed a dataset of 17,161 matched essay pairs and fine-tuned the model using Triplet Margin Loss to align the latent representations of ESL and Native writing. Our approach reduced the high-proficiency scoring disparity by 39.9% (to a 6.2% gap) while maintaining a Quadratic Weighted Kappa (QWK) of 0.76. Post-hoc linguistic analysis suggests the model successfully disentangled sentence complexity from grammatical error, preventing the penalization of valid L2 syntactic structures.

</details>


### [45] [Standardizing Longitudinal Radiology Report Evaluation via Large Language Model Annotation](https://arxiv.org/abs/2601.16753)
*Xinyi Wang,Grazziela Figueredo,Ruizhe Li,Xin Chen*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型（LLM）的自动化流水线，用于标注放射学报告中的纵向信息，解决了现有方法劳动密集、领域特定且难以适应的问题。


<details>
  <summary>Details</summary>
Motivation: 现有放射学报告生成方法难以验证其捕获纵向信息的性能，缺乏能够一致标注真实文本和模型生成文本中时间变化的工具。传统标注方法劳动密集、依赖特定领域词典和规则，要么过于复杂难以适应，要么过于简单遗漏重要信息。

Method: 提出基于LLM的自动化标注流水线，包含两个任务：1）识别包含相关信息的句子；2）提取疾病进展信息。评估了五种主流LLM在500份人工标注报告上的表现，基于效率和性能选择了Qwen2.5-32B模型，用于标注MIMIC-CXR数据集的95,169份报告。

Result: LLM标注方法在纵向信息检测和疾病跟踪任务上分别获得11.3%和5.3%的F1分数提升。使用Qwen2.5-32B标注的数据集为标准化的基准评估提供了支持，评估了七种最先进的报告生成模型。

Conclusion: 基于LLM的标注方法为放射学报告中的纵向信息标注提供了有效解决方案，优于现有方法，并为报告生成模型的评估提供了标准化基准。

Abstract: Longitudinal information in radiology reports refers to the sequential tracking of findings across multiple examinations over time, which is crucial for monitoring disease progression and guiding clinical decisions. Many recent automated radiology report generation methods are designed to capture longitudinal information; however, validating their performance is challenging. There is no proper tool to consistently label temporal changes in both ground-truth and model-generated texts for meaningful comparisons. Existing annotation methods are typically labor-intensive, relying on the use of manual lexicons and rules. Complex rules are closed-source, domain specific and hard to adapt, whereas overly simple ones tend to miss essential specialised information. Large language models (LLMs) offer a promising annotation alternative, as they are capable of capturing nuanced linguistic patterns and semantic similarities without extensive manual intervention. They also adapt well to new contexts. In this study, we therefore propose an LLM-based pipeline to automatically annotate longitudinal information in radiology reports. The pipeline first identifies sentences containing relevant information and then extracts the progression of diseases. We evaluate and compare five mainstream LLMs on these two tasks using 500 manually annotated reports. Considering both efficiency and performance, Qwen2.5-32B was subsequently selected and used to annotate another 95,169 reports from the public MIMIC-CXR dataset. Our Qwen2.5-32B-annotated dataset provided us with a standardized benchmark for evaluating report generation models. Using this new benchmark, we assessed seven state-of-the-art report generation models. Our LLM-based annotation method outperforms existing annotation solutions, achieving 11.3\% and 5.3\% higher F1-scores for longitudinal information detection and disease tracking, respectively.

</details>


### [46] [Do LLM hallucination detectors suffer from low-resource effect?](https://arxiv.org/abs/2601.16766)
*Debtanu Datta,Mohan Kishore Chilukuri,Yash Kumar,Saptarshi Ghosh,Muhammad Bilal Zafar*

Main category: cs.CL

TL;DR: 大语言模型在低资源语言中任务准确率大幅下降，但幻觉检测器的准确率下降幅度远小于任务准确率下降，表明LLM在低资源语言中仍能编码不确定性信号。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型的两个普遍失败模式：幻觉（产生错误信息）和低资源效应（在低资源语言中性能显著下降）。探讨幻觉检测器是否也受低资源效应影响。

Method: 在三个领域（事实回忆、STEM、人文学科）的五个任务上进行实验，使用四个大语言模型和三个幻觉检测器，比较英语和低资源语言（如孟加拉语）的性能差异。

Result: 实验显示：1）低资源语言中的任务准确率相比英语大幅下降；2）但幻觉检测器的准确率下降幅度远小于任务准确率下降（通常是几倍差异）；3）检测器在语言内部（包括非英语）和多语言设置中表现稳健，但在没有目标语言监督的跨语言设置中表现不佳。

Conclusion: 即使在低资源语言中，大语言模型的内部机制仍能编码关于其不确定性的信号。幻觉检测器在语言内部和多语言设置中具有鲁棒性，但在无监督的跨语言迁移中表现不佳。

Abstract: LLMs, while outperforming humans in a wide range of tasks, can still fail in unanticipated ways. We focus on two pervasive failure modes: (i) hallucinations, where models produce incorrect information about the world, and (ii) the low-resource effect, where the models show impressive performance in high-resource languages like English but the performance degrades significantly in low-resource languages like Bengali. We study the intersection of these issues and ask: do hallucination detectors suffer from the low-resource effect? We conduct experiments on five tasks across three domains (factual recall, STEM, and Humanities). Experiments with four LLMs and three hallucination detectors reveal a curious finding: As expected, the task accuracies in low-resource languages experience large drops (compared to English). However, the drop in detectors' accuracy is often several times smaller than the drop in task accuracy. Our findings suggest that even in low-resource languages, the internal mechanisms of LLMs might encode signals about their uncertainty. Further, the detectors are robust within language (even for non-English) and in multilingual setups, but not in cross-lingual settings without in-language supervision.

</details>


### [47] [Persuasion Tokens for Editing Factual Knowledge in LLMs](https://arxiv.org/abs/2601.16781)
*Paul Youssef,Jörg Schlötterer,Christin Seifert*

Main category: cs.CL

TL;DR: P-Tokens：一种无需事实特定演示的上下文知识编辑新方法，通过训练特殊令牌来模拟IKE演示效果，实现高效的知识更新。


<details>
  <summary>Details</summary>
Motivation: 现有的上下文知识编辑（IKE）方法依赖于冗长、事实特定的演示，这些演示创建成本高且消耗大量上下文窗口空间，限制了其实际应用性。

Method: 提出说服令牌（P-Tokens）——训练特殊令牌来复制IKE演示的效果，使模型能够在无需事实特定演示的情况下进行知识编辑。

Result: 在两个编辑数据集和三个LLM上的评估显示，P-Tokens性能与IKE相当甚至更优，对干扰物具有鲁棒性，增加P-Tokens数量可提升性能。

Conclusion: P-Tokens解决了IKE的关键限制，为LLM编辑提供了更实用和可扩展的替代方案。

Abstract: In-context knowledge editing (IKE) is a promising technique for updating Large Language Models (LLMs) with new information. However, IKE relies on lengthy, fact-specific demonstrations which are costly to create and consume significant context window space. In this paper, we introduce persuasion tokens (P-Tokens) -- special tokens trained to replicate the effect of IKE demonstrations, enabling efficient knowledge editing without requiring fact-specific demonstrations. We evaluate P-Tokens across two editing datasets and three LLMs, demonstrating performance comparable to, and often exceeding, IKE. We further find that editing performance is robust to distractors with small negative effects to neighboring facts, and that increasing the number of P-Tokens improves performance. Our work addresses key limitations of IKE and provides a more practical and scalable alternative for editing LLMs.

</details>


### [48] [Large Language Models as Automatic Annotators and Annotation Adjudicators for Fine-Grained Opinion Analysis](https://arxiv.org/abs/2601.16800)
*Gaurav Negi,MA Waskow,Paul Buitelaar*

Main category: cs.CL

TL;DR: 本文探索使用LLMs作为细粒度意见分析的自动标注器，通过声明式标注流程减少人工标注成本，并在ASTE和ACOS任务上验证了可行性。


<details>
  <summary>Details</summary>
Motivation: 细粒度意见分析需要大量人工标注数据，成本高昂且难以覆盖多领域应用。研究者希望利用LLMs作为自动标注器来解决领域特定标注数据不足的问题。

Method: 采用声明式标注流程，减少手动提示工程的变异性。提出新颖的方法让LLM裁决多个标签并生成最终标注。在不同规模的模型上测试了ASTE和ACOS分析任务。

Result: LLMs能够作为有效的自动标注器和裁决器，在个体LLM标注器之间实现了高标注一致性，显著降低了创建细粒度意见标注数据集的成本和人力需求。

Conclusion: LLMs可以作为细粒度意见分析的自动标注器，通过声明式标注流程和标签裁决方法，能够高效生成高质量的标注数据，解决领域特定标注数据短缺问题。

Abstract: Fine-grained opinion analysis of text provides a detailed understanding of expressed sentiments, including the addressed entity. Although this level of detail is sound, it requires considerable human effort and substantial cost to annotate opinions in datasets for training models, especially across diverse domains and real-world applications. We explore the feasibility of LLMs as automatic annotators for fine-grained opinion analysis, addressing the shortage of domain-specific labelled datasets. In this work, we use a declarative annotation pipeline. This approach reduces the variability of manual prompt engineering when using LLMs to identify fine-grained opinion spans in text. We also present a novel methodology for an LLM to adjudicate multiple labels and produce final annotations. After trialling the pipeline with models of different sizes for the Aspect Sentiment Triplet Extraction (ASTE) and Aspect-Category-Opinion-Sentiment (ACOS) analysis tasks, we show that LLMs can serve as automatic annotators and adjudicators, achieving high Inter-Annotator Agreement across individual LLM-based annotators. This reduces the cost and human effort needed to create these fine-grained opinion-annotated datasets.

</details>


### [49] [SoS: Analysis of Surface over Semantics in Multilingual Text-To-Image Generation](https://arxiv.org/abs/2601.16803)
*Carolin Holtermann,Florian Schneider,Anne Lauscher*

Main category: cs.CL

TL;DR: 本文首次系统分析了文本到图像（T2I）模型中的"表面优先于语义"（SoS）倾向，发现多数模型在非英语提示下会产生文化刻板印象的视觉描绘。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明T2I模型对不同输入语言高度敏感，面对非英语提示时往往产生文化刻板印象的描绘，优先考虑表面形式而非提示语义。目前缺乏对这种SoS行为的全面分析。

Method: 创建了涵盖171种文化身份的提示词集，翻译成14种语言，并用7个T2I模型生成图像。引入新的度量方法来量化SoS倾向，并分析这些倾向在视觉上的表现方式。

Result: 除一个模型外，所有模型至少在两种语言中表现出强烈的表面层面倾向，这种效应在T2I文本编码器的各层中逐渐增强。这些表面倾向经常与文化刻板印象的视觉描绘相关。

Conclusion: T2I模型普遍存在SoS倾向，这种倾向在非英语提示下尤为明显，导致文化刻板印象的视觉输出。研究强调了改进T2I模型跨语言处理能力的重要性。

Abstract: Text-to-image (T2I) models are increasingly employed by users worldwide. However, prior research has pointed to the high sensitivity of T2I towards particular input languages - when faced with languages other than English (i.e., different surface forms of the same prompt), T2I models often produce culturally stereotypical depictions, prioritizing the surface over the prompt's semantics. Yet a comprehensive analysis of this behavior, which we dub Surface-over-Semantics (SoS), is missing. We present the first analysis of T2I models' SoS tendencies. To this end, we create a set of prompts covering 171 cultural identities, translated into 14 languages, and use it to prompt seven T2I models. To quantify SoS tendencies across models, languages, and cultures, we introduce a novel measure and analyze how the tendencies we identify manifest visually. We show that all but one model exhibit strong surface-level tendency in at least two languages, with this effect intensifying across the layers of T2I text encoders. Moreover, these surface tendencies frequently correlate with stereotypical visual depictions.

</details>


### [50] [Trapped in the past? Disentangling fluid and crystallized intelligence of large language models using chess](https://arxiv.org/abs/2601.16823)
*Leonard S. Pleiss,Maximilian Schiffer,Robert K. von Weizsäcker*

Main category: cs.CL

TL;DR: 这篇论文使用国际象棋作为测试平台，揭示了大语言模型在记忆（晶体智力）和推理（流体智力）能力上的差异，发现模型在需要流体智力的任务上表现显著下降，特别是在分布外任务上表现接近随机水平。


<details>
  <summary>Details</summary>
Motivation: 大语言模型展现出卓越能力，但尚不清楚这些能力在多大程度上反映了复杂的记忆（晶体智力）或推理能力（流体智力）。研究旨在通过可控环境区分这两种智力形式。

Method: 使用国际象棋作为测试平台，利用游戏结构和可扩展的引擎评估，构建了不同训练语料接近度的位置分类体系。系统评估了多个GPT模型在不同推理强度下的表现。

Result: 发现明显的性能梯度：随着流体智力需求增加，性能持续下降。在分布外任务中，性能崩溃至随机水平。较新模型虽有改进，但在训练分布外任务上的进展显著放缓。推理增强的推理能提高性能，但其边际效益随分布接近度而降低。

Conclusion: 当前架构在系统性泛化方面仍然有限，表明需要超越规模扩展的机制来实现稳健的流体智力。

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities, yet it remains unclear to what extent these reflect sophisticated recall (crystallized intelligence) or reasoning ability (fluid intelligence). We introduce chess as a controlled testbed for disentangling these faculties. Leveraging the game's structure and scalable engine evaluations, we construct a taxonomy of positions varying in training corpus proximity--ranging from common states solvable by memorization to novel ones requiring first-principles reasoning. We systematically evaluate multiple GPT generations under varying reasoning intensities. Our analysis reveals a clear gradient: performance consistently degrades as fluid intelligence demands increase. Notably, in out-of-distribution tasks, performance collapses to random levels. While newer models improve, progress slows significantly for tasks outside the training distribution. Furthermore, while reasoning-augmented inference improves performance, its marginal benefit per token decreases with distributional proximity. These results suggest current architectures remain limited in systematic generalization, highlighting the need for mechanisms beyond scale to achieve robust fluid intelligence.

</details>


### [51] [LLM-Based Adversarial Persuasion Attacks on Fact-Checking Systems](https://arxiv.org/abs/2601.16890)
*João A. Leite,Olesya Razuvayevskaya,Kalina Bontcheva,Carolina Scarton*

Main category: cs.CL

TL;DR: 本文提出了一种针对自动事实核查系统的新型说服性对抗攻击方法，通过使用LLM重新表述虚假声明来规避检测


<details>
  <summary>Details</summary>
Motivation: 现有的对抗攻击框架主要依赖注入噪声或改变语义，但尚未利用说服技术在虚假信息传播中的重要潜力。说服技术被广泛用于操纵受众，具有对抗自动事实核查系统的潜在威胁。

Method: 使用生成式LLM将虚假声明重新表述，应用15种说服技术（分为6个类别），采用解耦评估策略分别研究说服对声明验证和证据检索的影响。

Result: 在FEVER和FEVEROUS基准测试上的实验表明，说服攻击能显著降低验证性能和证据检索效果，识别出说服技术是一类有效的对抗攻击手段。

Conclusion: 说服技术构成了对自动事实核查系统的强大对抗攻击类别，凸显了开发更鲁棒的事实核查系统的必要性。

Abstract: Automated fact-checking (AFC) systems are susceptible to adversarial attacks, enabling false claims to evade detection. Existing adversarial frameworks typically rely on injecting noise or altering semantics, yet no existing framework exploits the adversarial potential of persuasion techniques, which are widely used in disinformation campaigns to manipulate audiences. In this paper, we introduce a novel class of persuasive adversarial attacks on AFCs by employing a generative LLM to rephrase claims using persuasion techniques. Considering 15 techniques grouped into 6 categories, we study the effects of persuasion on both claim verification and evidence retrieval using a decoupled evaluation strategy. Experiments on the FEVER and FEVEROUS benchmarks show that persuasion attacks can substantially degrade both verification performance and evidence retrieval. Our analysis identifies persuasion techniques as a potent class of adversarial attacks, highlighting the need for more robust AFC systems.

</details>


### [52] [Information Representation Fairness in Long-Document Embeddings: The Peculiar Interaction of Positional and Language Bias](https://arxiv.org/abs/2601.16934)
*Elias Schuhmacher,Andrianos Michail,Juri Opitz,Rico Sennrich,Simon Clematide*

Main category: cs.CL

TL;DR: 本文针对嵌入模型在长文档搜索中的位置和语言偏见问题，提出了基于排列的评估框架和注意力校准方法，以提高文档各部分的可发现性。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的嵌入模型在长文档和多段落的搜索中存在系统性偏见：早期段落和高资源语言（如英语）被过度表征，而后期段落和低资源语言则被边缘化，这影响了文档各部分的可发现性。

Method: 1. 引入基于排列的评估框架来量化嵌入模型的反射偏见；2. 分析发现位置偏见源于池化标记嵌入中前向加载的注意力分布；3. 提出推理时的注意力校准方法，重新分配文档位置的注意力，使注意力分布更均匀。

Result: 评估框架揭示了嵌入模型存在显著的位置和语言偏见；注意力校准方法能够有效提高后期段落的可发现性，使文档各部分在嵌入表示中得到更均衡的反映。

Conclusion: 本文提出的评估框架和注意力校准方法有助于识别和缓解嵌入模型中的系统性偏见，从而提高长文档搜索的公平性和可发现性，特别是在处理多语言和多段落文档时。

Abstract: To be discoverable in an embedding-based search process, each part of a document should be reflected in its embedding representation. To quantify any potential reflection biases, we introduce a permutation-based evaluation framework. With this, we observe that state-of-the-art embedding models exhibit systematic positional and language biases when documents are longer and consist of multiple segments. Specifically, early segments and segments in higher-resource languages like English are over-represented, while later segments and segments in lower-resource languages are marginalized. In our further analysis, we find that the positional bias stems from front-loaded attention distributions in pooling-token embeddings, where early tokens receive more attention. To mitigate this issue, we introduce an inference-time attention calibration method that redistributes attention more evenly across document positions, increasing discoverabiltiy of later segments. Our evaluation framework and attention calibration is available at https://github.com/impresso/fair-sentence-transformers

</details>


### [53] [Strategies for Span Labeling with Large Language Models](https://arxiv.org/abs/2601.16946)
*Danil Semin,Ondřej Dušek,Zdeněk Kasner*

Main category: cs.CL

TL;DR: 本文针对大语言模型在文本分析任务中缺乏显式引用输入特定部分机制的问题，提出了LogitMatch这一新的约束解码方法，并通过实验验证了其优于其他策略的效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型越来越多地用于命名实体识别、错误检测等文本分析任务，但与编码器模型不同，生成式架构缺乏显式引用输入特定部分的机制。这导致了各种临时性的跨度标注提示策略，且结果往往不一致。

Method: 首先将现有策略分为三类：标注输入文本、索引跨度的数值位置、匹配跨度内容。针对内容匹配的局限性，提出了LogitMatch这一新的约束解码方法，强制模型输出与有效输入跨度对齐。

Result: 在四个不同任务上评估所有方法后发现：标注方法仍然是稳健的基线，但LogitMatch通过消除跨度匹配问题，在某些设置中优于竞争性的基于匹配的方法，并且超越了其他策略。

Conclusion: LogitMatch作为一种约束解码方法，有效解决了大语言模型在跨度标注任务中的局限性，为文本分析任务提供了更可靠的方法。

Abstract: Large language models (LLMs) are increasingly used for text analysis tasks, such as named entity recognition or error detection. Unlike encoder-based models, however, generative architectures lack an explicit mechanism to refer to specific parts of their input. This leads to a variety of ad-hoc prompting strategies for span labeling, often with inconsistent results. In this paper, we categorize these strategies into three families: tagging the input text, indexing numerical positions of spans, and matching span content. To address the limitations of content matching, we introduce LogitMatch, a new constrained decoding method that forces the model's output to align with valid input spans. We evaluate all methods across four diverse tasks. We find that while tagging remains a robust baseline, LogitMatch improves upon competitive matching-based methods by eliminating span matching issues and outperforms other strategies in some setups.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [54] [LLM-based Semantic Search for Conversational Queries in E-commerce](https://arxiv.org/abs/2601.16492)
*Emad Siddiqui,Venkatesh Terikuti,Xuan Lu*

Main category: cs.IR

TL;DR: 提出基于LLM的语义搜索框架，通过领域特定嵌入和结构化过滤器结合来捕捉会话查询的用户意图，使用合成数据微调嵌入模型和生成模型，结合相似性检索和约束过滤，在真实数据集上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统电商平台的搜索系统通常针对关键词查询进行优化，难以处理日益复杂的会话式用户查询。需要开发能够有效捕捉会话查询中用户意图的搜索框架。

Method: 1. 提出基于LLM的语义搜索框架，结合领域特定嵌入和结构化过滤器
2. 使用LLM生成合成数据来解决标注数据有限的问题
3. 微调两个模型：嵌入模型（将语义相似产品在表示空间中靠近）和生成模型（将自然语言查询转换为结构化约束）
4. 结合相似性检索和约束过滤进行搜索

Result: 在真实世界数据集上，与基线方法相比，该框架在各种设置下都实现了较强的精确率和召回率。

Conclusion: 该LLM-based语义搜索框架能够有效处理会话式查询，通过合成数据生成和模型微调解决了数据稀缺问题，结合相似性检索和约束过滤的方法在电商搜索场景中表现优异。

Abstract: Conversational user queries are increasingly challenging traditional e-commerce platforms, whose search systems are typically optimized for keyword-based queries. We present an LLM-based semantic search framework that effectively captures user intent from conversational queries by combining domain-specific embeddings with structured filters. To address the challenge of limited labeled data, we generate synthetic data using LLMs to guide the fine-tuning of two models: an embedding model that positions semantically similar products close together in the representation space, and a generative model for converting natural language queries into structured constraints. By combining similarity-based retrieval with constraint-based filtering, our framework achieves strong precision and recall across various settings compared to baseline approaches on a real-world dataset.

</details>


### [55] [PRISM: Purified Representation and Integrated Semantic Modeling for Generative Sequential Recommendation](https://arxiv.org/abs/2601.16556)
*Dengzhao Fang,Jingtong Gao,Yu Li,Xiangyu Zhao,Yi Chang*

Main category: cs.IR

TL;DR: PRISM是一个新的生成式序列推荐框架，通过纯净表示和集成语义建模解决现有方法在语义标记化和生成过程中的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前生成式序列推荐（GSR）面临两个关键限制：1）不纯净和不稳定的语义标记化，量化方法受交互噪声和码本塌陷影响，导致语义ID区分度模糊；2）有信息损失和弱结构化的生成，仅依赖粗粒度离散标记会引入信息损失并忽视物品的层次逻辑。

Method: 提出PRISM框架，包含两个核心组件：1）纯净语义量化器，通过自适应协同去噪和层次语义锚定机制构建鲁棒码本；2）集成语义推荐器，通过动态语义集成机制整合细粒度语义，并通过语义结构对齐目标增强逻辑有效性。

Result: PRISM在四个真实世界数据集上持续优于最先进的基线方法，在高稀疏性场景下表现出显著的性能提升。

Conclusion: PRISM通过解决语义标记化和生成过程中的关键限制，为生成式序列推荐提供了更有效的框架，特别是在数据稀疏情况下表现优异。

Abstract: Generative Sequential Recommendation (GSR) has emerged as a promising paradigm, reframing recommendation as an autoregressive sequence generation task over discrete Semantic IDs (SIDs), typically derived via codebook-based quantization. Despite its great potential in unifying retrieval and ranking, existing GSR frameworks still face two critical limitations: (1) impure and unstable semantic tokenization, where quantization methods struggle with interaction noise and codebook collapse, resulting in SIDs with ambiguous discrimination; and (2) lossy and weakly structured generation, where reliance solely on coarse-grained discrete tokens inevitably introduces information loss and neglects items' hierarchical logic. To address these issues, we propose a novel generative recommendation framework, PRISM, with Purified Representation and Integrated Semantic Modeling. Specifically, to ensure high-quality tokenization, we design a Purified Semantic Quantizer that constructs a robust codebook via adaptive collaborative denoising and hierarchical semantic anchoring mechanisms. To compensate for information loss during quantization, we further propose an Integrated Semantic Recommender, which incorporates a dynamic semantic integration mechanism to integrate fine-grained semantics and enforces logical validity through a semantic structure alignment objective. PRISM consistently outperforms state-of-the-art baselines across four real-world datasets, demonstrating substantial performance gains, particularly in high-sparsity scenarios.

</details>


### [56] [LLM-powered Real-time Patent Citation Recommendation for Financial Technologies](https://arxiv.org/abs/2601.16775)
*Tianang Deng,Yu Deng,Tianchen Gao,Yonghong Hu,Rui Pan*

Main category: cs.IR

TL;DR: 本文提出了一种面向快速变化的金融专利语料库的实时专利引文推荐框架，采用LLM嵌入、近似最近邻搜索和增量索引策略，显著提升了推荐准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 金融创新加速导致专利申请活动激增，使及时全面的现有技术发现变得困难。现有专利检索和引文推荐方法通常依赖静态索引或定期重新训练，难以在快速变化的动态环境中有效运作。

Method: 提出三阶段推荐流水线：1) 使用LLM嵌入表示专利摘要的语义内容；2) 应用高效的近似最近邻搜索构建可管理的候选集；3) 通过语义相似度对候选进行排序生成top-k引文推荐。采用基于HNSW图的增量索引策略，支持新专利的实时添加而无需重建整个索引。

Result: 在2000-2024年CNIPA授权的428,843项金融专利数据集上验证，增量更新策略相比重建式索引显著提高召回率并大幅降低计算成本。该方法持续优于传统文本基准和替代最近邻检索方法。

Conclusion: 所提出的实时专利引文推荐框架有效解决了金融专利动态环境下的现有技术发现问题，通过增量索引和语义相似度匹配，为快速变化的专利语料库提供了高效、准确的引文推荐解决方案。

Abstract: Rapid financial innovation has been accompanied by a sharp increase in patenting activity, making timely and comprehensive prior-art discovery more difficult. This problem is especially evident in financial technologies, where innovations develop quickly, patent collections grow continuously, and citation recommendation systems must be updated as new applications arrive. Existing patent retrieval and citation recommendation methods typically rely on static indexes or periodic retraining, which limits their ability to operate effectively in such dynamic settings. In this study, we propose a real-time patent citation recommendation framework designed for large and fast-changing financial patent corpora. Using a dataset of 428,843 financial patents granted by the China National Intellectual Property Administration (CNIPA) between 2000 and 2024, we build a three-stage recommendation pipeline. The pipeline uses large language model (LLM) embeddings to represent the semantic content of patent abstracts, applies efficient approximate nearest-neighbor search to construct a manageable candidate set, and ranks candidates by semantic similarity to produce top-k citation recommendations. In addition to improving recommendation accuracy, the proposed framework directly addresses the dynamic nature of patent systems. By using an incremental indexing strategy based on hierarchical navigable small-world (HNSW) graphs, newly issued patents can be added without rebuilding the entire index. A rolling day-by-day update experiment shows that incremental updating improves recall while substantially reducing computational cost compared with rebuild-based indexing. The proposed method also consistently outperforms traditional text-based baselines and alternative nearest-neighbor retrieval approaches.

</details>


### [57] [PI2I: A Personalized Item-Based Collaborative Filtering Retrieval Framework](https://arxiv.org/abs/2601.16815)
*Shaoqing Wang,Yingcai Ma,Kairui Fu,Ziyang Wang,Dunxian Huang,Yuliang Yan,Jian Wu*

Main category: cs.IR

TL;DR: 提出了PI2I框架，一个两阶段个性化检索系统，通过优化截断策略和引入交互式评分模型，显著提升了推荐系统的个性化能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如协同过滤和双塔模型）存在均匀截断策略和用户-物品交叉不足的问题，难以捕捉复杂的用户-物品交互，限制了推荐系统的个性化效果。

Method: PI2I采用两阶段框架：1)索引构建阶段放宽截断阈值以最大化命中率；2)个性化检索阶段引入交互式评分模型替代内积计算，并基于触发-目标关系构建负样本。

Result: 在大规模真实数据集上离线实验优于传统协同过滤方法，与双塔模型相当；在淘宝"猜你喜欢"部署后，线上交易率提升1.05%；同时发布了包含1.3亿真实交互的数据集。

Conclusion: PI2I框架通过创新的两阶段设计和交互式评分模型，有效解决了传统推荐系统的局限性，显著提升了推荐质量，并为研究社区提供了有价值的基准数据集。

Abstract: Efficiently selecting relevant content from vast candidate pools is a critical challenge in modern recommender systems. Traditional methods, such as item-to-item collaborative filtering (CF) and two-tower models, often fall short in capturing the complex user-item interactions due to uniform truncation strategies and overdue user-item crossing. To address these limitations, we propose Personalized Item-to-Item (PI2I), a novel two-stage retrieval framework that enhances the personalization capabilities of CF. In the first Indexer Building Stage (IBS), we optimize the retrieval pool by relaxing truncation thresholds to maximize Hit Rate, thereby temporarily retaining more items users might be interested in. In the second Personalized Retrieval Stage (PRS), we introduce an interactive scoring model to overcome the limitations of inner product calculations, allowing for richer modeling of intricate user-item interactions. Additionally, we construct negative samples based on the trigger-target (item-to-item) relationship, ensuring consistency between offline training and online inference. Offline experiments on large-scale real-world datasets demonstrate that PI2I outperforms traditional CF methods and rivals Two-Tower models. Deployed in the "Guess You Like" section on Taobao, PI2I achieved a 1.05% increase in online transaction rates. In addition, we have released a large-scale recommendation dataset collected from Taobao, containing 130 million real-world user interactions used in the experiments of this paper. The dataset is publicly available at https://huggingface.co/datasets/PI2I/PI2I, which could serve as a valuable benchmark for the research community.

</details>


### [58] [Navigating the Shift: A Comparative Analysis of Web Search and Generative AI Response Generation](https://arxiv.org/abs/2601.16858)
*Mahe Chen,Xiaoxuan Wang,Kaiwen Chen,Nick Koudas*

Main category: cs.IR

TL;DR: 该研究通过大规模实证分析揭示了谷歌搜索与生成式AI服务在搜索结果上的本质差异，包括信息来源、时效性、查询意图等多个维度，并探讨了LLM预训练知识库对实时网络搜索的影响，为新兴的答案引擎优化(AEO)领域提供了重要见解。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正在成为主要信息来源，这带来了从传统网络搜索到AI生成答案的范式转变。需要量化理解谷歌搜索与领先生成式AI服务在结果上的根本差异，以揭示这两种信息生态系统的不同机制。

Method: 采用大规模实证研究方法，从多个维度分析谷歌搜索与生成式AI服务的差异：1) 咨询的源域；2) 域类型（如赚取媒体vs自有媒体、社交媒体）；3) 查询意图；4) 信息新鲜度。同时研究LLM预训练作为关键因素如何塑造这些差异，分析其内在知识库如何与实时网络搜索交互和影响。

Result: 研究发现AI生成的答案与网络搜索结果在多个维度上存在显著差异：1) 咨询的源域不同；2) 域类型分布有本质区别；3) 查询意图的满足方式不同；4) 信息新鲜度存在差异。LLM预训练知识库是塑造这些差异的关键因素，当启用实时网络搜索时，这种内在知识库会与外部信息交互并产生影响。

Conclusion: 研究揭示了谷歌搜索与生成式AI服务这两种信息生态系统的不同机制，为新兴的答案引擎优化(AEO)领域提供了关键观察，并与传统的搜索引擎优化(SEO)形成了鲜明对比。这些发现对于理解AI时代信息检索系统的演变具有重要意义。

Abstract: The rise of generative AI as a primary information source presents a paradigm shift from traditional web search. This paper presents a large-scale empirical study quantifying the fundamental differences between the results returned by Google Search and leading generative AI services. We analyze multiple dimensions, demonstrating that AI-generated answers and web search results diverge significantly in their consulted source domains, the typology of these domains (e.g., earned media vs. owned, social), query intent and the freshness of the information provided. We then investigate the role of LLM pre-training as a key factor shaping these differences, analyzing how this intrinsic knowledge base interacts with and influences real-time web search when enabled. Our findings reveal the distinct mechanics of these two information ecosystems, leading to critical observations on the emergent field of Answer Engine Optimization (AEO) and its contrast with traditional Search Engine Optimization (SEO).

</details>


### [59] [From Atom to Community: Structured and Evolving Agent Memory for User Behavior Modeling](https://arxiv.org/abs/2601.16872)
*Yuxin Liao,Le Wu,Min Hou,Yu Wang,Han Wu,Meng Wang*

Main category: cs.IR

TL;DR: STEAM是一个新颖的LLM智能体记忆框架，通过结构化分解用户偏好、跨用户社区组织和自适应演化机制，显著提升了推荐系统在准确性、模拟保真度和多样性方面的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体的记忆机制在处理非文本用户行为时存在三个主要问题：1）单一非结构化摘要导致多面兴趣混淆；2）简单覆盖更新导致偏好演化时遗忘；3）稀疏个体交互缺乏协同信号利用。

Method: STEAM框架包含三个核心组件：1）将偏好分解为原子记忆单元，每个单元捕捉一个独立的兴趣维度并与观察到的行为显式链接；2）跨用户组织相似记忆形成社区，并生成原型记忆进行信号传播；3）自适应演化机制，包括记忆精炼的巩固过程和捕捉新兴兴趣的形成过程。

Result: 在三个真实世界数据集上的实验表明，STEAM在推荐准确性、模拟保真度和多样性方面显著优于最先进的基线方法。

Conclusion: STEAM通过重新构想智能体记忆的组织和更新方式，有效解决了现有记忆机制在建模非文本用户行为时的局限性，为个性化推荐系统提供了更强大的用户偏好表示框架。

Abstract: User behavior modeling lies at the heart of personalized applications like recommender systems. With LLM-based agents, user preference representation has evolved from latent embeddings to semantic memory. While existing memory mechanisms show promise in textual dialogues, modeling non-textual behaviors remains challenging, as preferences must be inferred from implicit signals like clicks without ground truth supervision. Current approaches rely on a single unstructured summary, updated through simple overwriting. However, this is suboptimal: users exhibit multi-faceted interests that get conflated, preferences evolve yet naive overwriting causes forgetting, and sparse individual interactions necessitate collaborative signals. We present STEAM (\textit{\textbf{ST}ructured and \textbf{E}volving \textbf{A}gent \textbf{M}emory}), a novel framework that reimagines how agent memory is organized and updated. STEAM decomposes preferences into atomic memory units, each capturing a distinct interest dimension with explicit links to observed behaviors. To exploit collaborative patterns, STEAM organizes similar memories across users into communities and generates prototype memories for signal propagation. The framework further incorporates adaptive evolution mechanisms, including consolidation for refining memories and formation for capturing emerging interests. Experiments on three real-world datasets demonstrate that STEAM substantially outperforms state-of-the-art baselines in recommendation accuracy, simulation fidelity, and diversity.

</details>


### [60] [Explaining Group Recommendations via Counterfactuals](https://arxiv.org/abs/2601.16882)
*Maria Stratigi,Nikos Bikakis*

Main category: cs.IR

TL;DR: 该论文提出了一个群体反事实解释框架，通过移除特定历史交互来揭示群体推荐的变化，帮助理解推荐决策


<details>
  <summary>Details</summary>
Motivation: 群体推荐系统通常缺乏透明度，现有解释方法主要针对个体，难以处理群体中多个偏好的复杂交互，导致群体成员不确定推荐的原因

Method: 提出了群体反事实解释框架，形式化了群体反事实解释的概念，引入了针对群体的效用和公平性度量，设计了启发式算法（如基于帕累托的过滤和增长-剪枝策略）来高效发现解释

Result: 在MovieLens和Amazon数据集上的实验显示明确的权衡：低成本方法产生更大但公平性较差的解释，而其他方法以更高成本产生更简洁和平衡的结果。帕累托过滤启发式算法在稀疏设置中表现出显著的效率提升

Conclusion: 群体反事实解释框架能够有效提高群体推荐系统的透明度，帮助群体成员理解推荐决策，同时揭示了不同方法在成本、解释大小和公平性之间的权衡关系

Abstract: Group recommender systems help users make collective choices but often lack transparency, leaving group members uncertain about why items are suggested. Existing explanation methods focus on individuals, offering limited support for groups where multiple preferences interact. In this paper, we propose a framework for group counterfactual explanations, which reveal how removing specific past interactions would change a group recommendation. We formalize this concept, introduce utility and fairness measures tailored to groups, and design heuristic algorithms, such as Pareto-based filtering and grow-and-prune strategies, for efficient explanation discovery. Experiments on MovieLens and Amazon datasets show clear trade-offs: low-cost methods produce larger, less fair explanations, while other approaches yield concise and balanced results at higher cost. Furthermore, the Pareto-filtering heuristic demonstrates significant efficiency improvements in sparse settings.

</details>
