{"id": "2512.16033", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.16033", "abs": "https://arxiv.org/abs/2512.16033", "authors": ["Qihao Wang", "Pritom Saha Akash", "Varvara Kollia", "Kevin Chen-Chuan Chang", "Biwei Jiang", "Vadim Von Brzeski"], "title": "On Recommending Category: A Cascading Approach", "comment": null, "summary": "Recommendation plays a key role in e-commerce, enhancing user experience and boosting commercial success. Existing works mainly focus on recommending a set of items, but online e-commerce platforms have recently begun to pay attention to exploring users' potential interests at the category level. Category-level recommendation allows e-commerce platforms to promote users' engagements by expanding their interests to different types of items. In addition, it complements item-level recommendations when the latter becomes extremely challenging for users with little-known information and past interactions. Furthermore, it facilitates item-level recommendations in existing works. The predicted category, which is called intention in those works, aids the exploration of item-level preference. However, such category-level preference prediction has mostly been accomplished through applying item-level models. Some key differences between item-level recommendations and category-level recommendations are ignored in such a simplistic adaptation. In this paper, we propose a cascading category recommender (CCRec) model with a variational autoencoder (VAE) to encode item-level information to perform category-level recommendations. Experiments show the advantages of this model over methods designed for item-level recommendations."}
{"id": "2512.16236", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16236", "abs": "https://arxiv.org/abs/2512.16236", "authors": ["Tejul Pandit", "Sakshi Mahendru", "Meet Raval", "Dhvani Upadhyay"], "title": "The Evolution of Reranking Models in Information Retrieval: From Heuristic Methods to Large Language Models", "comment": "15 pages, 1 figure, Accepted in CLNLP'25", "summary": "Reranking is a critical stage in contemporary information retrieval (IR) systems, improving the relevance of the user-presented final results by honing initial candidate sets. This paper is a thorough guide to examine the changing reranker landscape and offer a clear view of the advancements made in reranking methods. We present a comprehensive survey of reranking models employed in IR, particularly within modern Retrieval Augmented Generation (RAG) pipelines, where retrieved documents notably influence output quality.\n  We embark on a chronological journey through the historical trajectory of reranking techniques, starting with foundational approaches, before exploring the wide range of sophisticated neural network architectures such as cross-encoders, sequence-generation models like T5, and Graph Neural Networks (GNNs) utilized for structural information. Recognizing the computational cost of advancing neural rerankers, we analyze techniques for enhancing efficiency, notably knowledge distillation for creating competitive, lighter alternatives. Furthermore, we map the emerging territory of integrating Large Language Models (LLMs) in reranking, examining novel prompting strategies and fine-tuning tactics. This survey seeks to elucidate the fundamental ideas, relative effectiveness, computational features, and real-world trade-offs of various reranking strategies. The survey provides a structured synthesis of the diverse reranking paradigms, highlighting their underlying principles and comparative strengths and weaknesses."}
{"id": "2512.16348", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.16348", "abs": "https://arxiv.org/abs/2512.16348", "authors": ["Shayan Azizi", "Norihiro Okui", "Masataka Nakahara", "Ayumu Kubota", "Hassan Habibi Gharakheili"], "title": "From Flows to Functions: Macroscopic Behavioral Fingerprinting of IoT Devices via Network Services", "comment": "10 pages, 3 figures, 1 table, and 1 algorithm", "summary": "Identifying devices such as cameras, printers, voice assistants, or health monitoring sensors, collectively known as the Internet of Things (IoT), within a network is a critical operational task, particularly to manage the cyber risks they introduce. While behavioral fingerprinting based on network traffic analysis has shown promise, most existing approaches rely on machine learning (ML) techniques applied to fine-grained features of short-lived traffic units (packets and/or flows). These methods tend to be computationally expensive, sensitive to traffic measurement errors, and often produce opaque inferences. In this paper, we propose a macroscopic, lightweight, and explainable alternative to behavioral fingerprinting focusing on the network services (e.g., TCP/80, UDP/53) that IoT devices use to perform their intended functions over extended periods. Our contributions are threefold. (1) We demonstrate that IoT devices exhibit stable and distinguishable patterns in their use of network services over a period of time. We formalize the notion of service-level fingerprints and derive a generalized method to represent network behaviors using a configurable granularity parameter. (2) We develop a procedure to extract service-level fingerprints, apply it to traffic from 13 consumer IoT device types in a lab testbed, and evaluate the resulting representations in terms of their convergence and recurrence properties. (3) We validate the efficacy of service-level fingerprints for device identification in closed-set and open-set scenarios. Our findings are based on a large dataset comprising about 10 million IPFIX flow records collected over a 1.5-year period."}
{"id": "2512.16425", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16425", "abs": "https://arxiv.org/abs/2512.16425", "authors": ["Allard Oelen", "Mohamad Yaser Jaradeh", "Sören Auer"], "title": "Introducing ORKG ASK: an AI-driven Scholarly Literature Search and Exploration System Taking a Neuro-Symbolic Approach", "comment": null, "summary": "As the volume of published scholarly literature continues to grow, finding relevant literature becomes increasingly difficult. With the rise of generative Artificial Intelligence (AI), and particularly Large Language Models (LLMs), new possibilities emerge to find and explore literature. We introduce ASK (Assistant for Scientific Knowledge), an AI-driven scholarly literature search and exploration system that follows a neuro-symbolic approach. ASK aims to provide active support to researchers in finding relevant scholarly literature by leveraging vector search, LLMs, and knowledge graphs. The system allows users to input research questions in natural language and retrieve relevant articles. ASK automatically extracts key information and generates answers to research questions using a Retrieval-Augmented Generation (RAG) approach. We present an evaluation of ASK, assessing the system's usability and usefulness. Findings indicate that the system is user-friendly and users are generally satisfied while using the system."}
{"id": "2512.15907", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.15907", "abs": "https://arxiv.org/abs/2512.15907", "authors": ["Tejas Anvekar", "Juhna Park", "Aparna Garimella", "Vivek Gupta"], "title": "TabReX : Tabular Referenceless eXplainable Evaluation", "comment": null, "summary": "Evaluating the quality of tables generated by large language models (LLMs) remains an open challenge: existing metrics either flatten tables into text, ignoring structure, or rely on fixed references that limit generalization. We present TabReX, a reference-less, property-driven framework for evaluating tabular generation via graph-based reasoning. TabReX converts both source text and generated tables into canonical knowledge graphs, aligns them through an LLM-guided matching process, and computes interpretable, rubric-aware scores that quantify structural and factual fidelity. The resulting metric provides controllable trade-offs between sensitivity and specificity, yielding human-aligned judgments and cell-level error traces. To systematically asses metric robustness, we introduce TabReX-Bench, a large-scale benchmark spanning six domains and twelve planner-driven perturbation types across three difficulty tiers. Empirical results show that TabReX achieves the highest correlation with expert rankings, remains stable under harder perturbations, and enables fine-grained model-vs-prompt analysis establishing a new paradigm for trustworthy, explainable evaluation of structured generation systems."}
{"id": "2512.16576", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.16576", "abs": "https://arxiv.org/abs/2512.16576", "authors": ["Xufeng Liang", "Zhida Qin", "Chong Zhang", "Tianyu Huang", "Gangyi Ding"], "title": "InfoDCL: Informative Noise Enhanced Diffusion Based Contrastive Learning", "comment": null, "summary": "Contrastive learning has demonstrated promising potential in recommender systems. Existing methods typically construct sparser views by randomly perturbing the original interaction graph, as they have no idea about the authentic user preferences. Owing to the sparse nature of recommendation data, this paradigm can only capture insufficient semantic information. To address the issue, we propose InfoDCL, a novel diffusion-based contrastive learning framework for recommendation. Rather than injecting randomly sampled Gaussian noise, we employ a single-step diffusion process that integrates noise with auxiliary semantic information to generate signals and feed them to the standard diffusion process to generate authentic user preferences as contrastive views. Besides, based on a comprehensive analysis of the mutual influence between generation and preference learning in InfoDCL, we build a collaborative training objective strategy to transform the interference between them into mutual collaboration. Additionally, we employ multiple GCN layers only during inference stage to incorporate higher-order co-occurrence information while maintaining training efficiency. Extensive experiments on five real-world datasets demonstrate that InfoDCL significantly outperforms state-of-the-art methods. Our InfoDCL offers an effective solution for enhancing recommendation performance and suggests a novel paradigm for applying diffusion method in contrastive learning frameworks."}
{"id": "2512.15925", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2512.15925", "abs": "https://arxiv.org/abs/2512.15925", "authors": ["Joel Mire", "Maria Antoniak", "Steven R. Wilson", "Zexin Ma", "Achyutarama R. Ganti", "Andrew Piper", "Maarten Sap"], "title": "Social Story Frames: Contextual Reasoning about Narrative Intent and Reception", "comment": "Presented at IC2S2 2025; Under Review (ARR Oct 2025)", "summary": "Reading stories evokes rich interpretive, affective, and evaluative responses, such as inferences about narrative intent or judgments about characters. Yet, computational models of reader response are limited, preventing nuanced analyses. To address this gap, we introduce SocialStoryFrames, a formalism for distilling plausible inferences about reader response, such as perceived author intent, explanatory and predictive reasoning, affective responses, and value judgments, using conversational context and a taxonomy grounded in narrative theory, linguistic pragmatics, and psychology. We develop two models, SSF-Generator and SSF-Classifier, validated through human surveys (N=382 participants) and expert annotations, respectively. We conduct pilot analyses to showcase the utility of the formalism for studying storytelling at scale. Specifically, applying our models to SSF-Corpus, a curated dataset of 6,140 social media stories from diverse contexts, we characterize the frequency and interdependence of storytelling intents, and we compare and contrast narrative practices (and their diversity) across communities. By linking fine-grained, context-sensitive modeling with a generic taxonomy of reader responses, SocialStoryFrames enable new research into storytelling in online communities."}
{"id": "2512.16661", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16661", "abs": "https://arxiv.org/abs/2512.16661", "authors": ["Jacob Reiss", "Shikshya Shiwakoti", "Samuel Goldsmith", "Ujjwal Pandit"], "title": "Microsoft Academic Graph Information Retrieval for Research Recommendation and Assistance", "comment": "5 pages, 3 figures", "summary": "In today's information-driven world, access to scientific publications has become increasingly easy. At the same time, filtering through the massive volume of available research has become more challenging than ever. Graph Neural Networks (GNNs) and graph attention mechanisms have shown strong effectiveness in searching large-scale information databases, particularly when combined with modern large language models. In this paper, we propose an Attention-Based Subgraph Retriever, a GNN-as-retriever model that applies attention-based pruning to extract a refined subgraph, which is then passed to a large language model for advanced knowledge reasoning."}
{"id": "2512.15959", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15959", "abs": "https://arxiv.org/abs/2512.15959", "authors": ["Armağan Amcalar", "Eyup Cinar"], "title": "BRAID: Bounded Reasoning for Autonomous Inference and Decisions", "comment": null, "summary": "Large Language Models (LLMs) exhibit nonlinear relationships between performance, cost, and token usage. This paper presents a quantitative study on structured prompting using BRAID (Bounded Reasoning for Au tonomous Inference and Decisions) across multiple GPT model tiers, eval uated on the AdvancedIF, GSM-Hard, and the SCALE MultiChallenge benchmark datasets. BRAID introduces a bounded reasoning framework using Mermaid-based instruction graphs that enable models to reason struc turally rather than through unbounded natural-language token expansion. We show that structured machine-readable prompts substantially increase reasoning accuracy and cost efficiency for agents in production systems. The findings establish BRAID as an effective and scalable technique for optimizing inference efficiency in autonomous agent systems. All datasets and detailed result logs are available at https://benchmark.openserv.ai."}
{"id": "2512.16795", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.16795", "abs": "https://arxiv.org/abs/2512.16795", "authors": ["Shubham Mishra", "Samyek Jain", "Gorang Mehrishi", "Shiv Tiwari", "Harsh Sharma", "Pratik Narang", "Dhruv Kumar"], "title": "From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs", "comment": "Under Review", "summary": "Retrieval-Augmented Generation (RAG) grounds large language models (LLMs) in external evidence, but fails when retrieved sources conflict or contain outdated or subjective information. Prior work address these issues independently but lack unified reasoning supervision. We propose a reasoning-trace-augmented RAG framework that adds structured, interpretable reasoning across three stages : (1) document-level adjudication, (2) conflict analysis, and (3) grounded synthesis, producing citation-linked answers or justified refusals. A Conflict-Aware Trust-Score (CATS) pipeline is introduced which evaluates groundedness, factual correctness, refusal accuracy, and conflict-behavior alignment using an LLM-as-a-Judge. Our 539-query reasoning dataset and evaluation pipeline establish a foundation for conflict-aware, interpretable RAG systems. Experimental results demonstrate substantial gains over baselines, most notably with Qwen, where Supervised Fine-Tuning improved End-to-End answer correctness from 0.069 to 0.883 and behavioral adherence from 0.074 to 0.722."}
{"id": "2512.16034", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16034", "abs": "https://arxiv.org/abs/2512.16034", "authors": ["Kieran Henderson", "Kian Omoomi", "Vasudha Varadarajan", "Allison Lahnala", "Charles Welch"], "title": "Examining the Utility of Self-disclosure Types for Modeling Annotators of Social Norms", "comment": null, "summary": "Recent work has explored the use of personal information in the form of persona sentences or self-disclosures to improve modeling of individual characteristics and prediction of annotator labels for subjective tasks. The volume of personal information has historically been restricted and thus little exploration has gone into understanding what kind of information is most informative for predicting annotator labels. In this work, we categorize self-disclosure sentences and use them to build annotator models for predicting judgments of social norms. We perform several ablations and analyses to examine the impact of the type of information on our ability to predict annotation patterns. We find that demographics are more impactful than attitudes, relationships, and experiences. Generally, theory-based approaches worked better than automatic clusters. Contrary to previous work, only a small number of related comments are needed. Lastly, having a more diverse sample of annotator self-disclosures leads to the best performance."}
{"id": "2512.16041", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16041", "abs": "https://arxiv.org/abs/2512.16041", "authors": ["Yuanning Feng", "Sinan Wang", "Zhengxiang Cheng", "Yao Wan", "Dongping Chen"], "title": "Are We on the Right Way to Assessing LLM-as-a-Judge?", "comment": null, "summary": "LLM-as-a-Judge has been widely adopted as an evaluation method and served as supervised rewards in model training. However, existing benchmarks for LLM-as-a-Judge are mainly relying on human-annotated ground truth, which introduces human bias that undermines the assessment of reliability and imposes scalability constraints. To overcome these limitations, we introduce Sage, a novel evaluation suite that assesses the quality of LLM judges without necessitating any human annotation. Inspired by axioms of rational choice theory, Sage introduces two new lenses for measuring LLM-as-a-Judge: local self-consistency (pair-wise preference stability) and global logical consistency (transitivity across a full set of preferences). We curate a dataset of 650 questions by combining structured benchmark problems with real-world user queries. Our experiments demonstrate both the stability of our metrics and their high correlation with supervised benchmarks like LLMBar and RewardBench2, confirming Sage's reliability as an evaluation suite for the robustness and accuracy of LLM-as-a-Judge. Based on Sage, we reveal that current state-of-the-art LLMs exhibit significant reliability problems when acting as judges in both scoring and pairwise settings; even the top-performing models, Gemini-2.5-Pro and GPT-5, fail to maintain consistent preferences in nearly a quarter of difficult cases. We attribute this to a new phenomenon called situational preference, which explains why explicit rubrics or criteria can help the model judge consistently across answer pairs. Our further analysis shows that finetuned LLM-as-a-Judge is a feasible method to boost performance, and the panel-based judge as well as deep reasoning can enhance the judging consistency. We also find substantial inconsistency in human judgments, which indicates that human annotation may not be a reliable gold standard."}
{"id": "2512.16125", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16125", "abs": "https://arxiv.org/abs/2512.16125", "authors": ["Daniela N. Rim", "Heeyoul Choi"], "title": "Convolutional Lie Operator for Sentence Classification", "comment": "Proceedings of the 2024 8th International Conference on Natural Language Processing and Information Retrieval", "summary": "Traditional Convolutional Neural Networks have been successful in capturing local, position-invariant features in text, but their capacity to model complex transformation within language can be further explored. In this work, we explore a novel approach by integrating Lie Convolutions into Convolutional-based sentence classifiers, inspired by the ability of Lie group operations to capture complex, non-Euclidean symmetries. Our proposed models SCLie and DPCLie empirically outperform traditional Convolutional-based sentence classifiers, suggesting that Lie-based models relatively improve the accuracy by capturing transformations not commonly associated with language. Our findings motivate more exploration of new paradigms in language modeling."}
{"id": "2512.16145", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16145", "abs": "https://arxiv.org/abs/2512.16145", "authors": ["Pengyu Wang", "Shuchang Ye", "Usman Naseem", "Jinman Kim"], "title": "MRG-R1: Reinforcement Learning for Clinically Aligned Medical Report Generation", "comment": "12 pages", "summary": "Medical report generation (MRG) aims to automatically derive radiology-style reports from medical images to aid in clinical decision-making. However, existing methods often generate text that mimics the linguistic style of radiologists but fails to guarantee clinical correctness, because they are trained on token-level objectives which focus on word-choice and sentence structure rather than actual medical accuracy. We propose a semantic-driven reinforcement learning (SRL) method for medical report generation, adopted on a large vision-language model (LVLM). SRL adopts Group Relative Policy Optimization (GRPO) to encourage clinical-correctness-guided learning beyond imitation of language style. Specifically, we optimise a report-level reward: a margin-based cosine similarity (MCCS) computed between key radiological findings extracted from generated and reference reports, thereby directly aligning clinical-label agreement and improving semantic correctness. A lightweight reasoning format constraint further guides the model to generate structured \"thinking report\" outputs. We evaluate Medical Report Generation with Sematic-driven Reinforment Learning (MRG-R1), on two datasets: IU X-Ray and MIMIC-CXR using clinical efficacy (CE) metrics. MRG-R1 achieves state-of-the-art performance with CE-F1 51.88 on IU X-Ray and 40.39 on MIMIC-CXR. We found that the label-semantic reinforcement is better than conventional token-level supervision. These results indicate that optimizing a clinically grounded, report-level reward rather than token overlap,meaningfully improves clinical correctness. This work is a prior to explore semantic-reinforcement in supervising medical correctness in medical Large vision-language model(Med-LVLM) training."}
{"id": "2512.16147", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16147", "abs": "https://arxiv.org/abs/2512.16147", "authors": ["Yash Bhaskar", "Sankalp Bahad", "Parameswari Krishnamurthy"], "title": "Decoding Fake Narratives in Spreading Hateful Stories: A Dual-Head RoBERTa Model with Multi-Task Learning", "comment": "Accepted Paper, Anthology ID: 2024.icon-fauxhate.3, 4 pages, 1 figure, 1 table", "summary": "Social media platforms, while enabling global connectivity, have become hubs for the rapid spread of harmful content, including hate speech and fake narratives \\cite{davidson2017automated, shu2017fake}. The Faux-Hate shared task focuses on detecting a specific phenomenon: the generation of hate speech driven by fake narratives, termed Faux-Hate. Participants are challenged to identify such instances in code-mixed Hindi-English social media text. This paper describes our system developed for the shared task, addressing two primary sub-tasks: (a) Binary Faux-Hate detection, involving fake and hate speech classification, and (b) Target and Severity prediction, categorizing the intended target and severity of hateful content. Our approach combines advanced natural language processing techniques with domain-specific pretraining to enhance performance across both tasks. The system achieved competitive results, demonstrating the efficacy of leveraging multi-task learning for this complex problem."}
{"id": "2512.16183", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.16183", "abs": "https://arxiv.org/abs/2512.16183", "authors": ["Mengfan Shen", "Kangqi Song", "Xindi Wang", "Wei Jia", "Tao Wang", "Ziqiang Han"], "title": "A Domain-Adapted Pipeline for Structured Information Extraction from Police Incident Announcements on Social Media", "comment": "41 pages,3figures and 9 tables", "summary": "Structured information extraction from police incident announcements is crucial for timely and accurate data processing, yet presents considerable challenges due to the variability and informal nature of textual sources such as social media posts. To address these challenges, we developed a domain-adapted extraction pipeline that leverages targeted prompt engineering with parameter-efficient fine-tuning of the Qwen2.5-7B model using Low-Rank Adaptation (LoRA). This approach enables the model to handle noisy, heterogeneous text while reliably extracting 15 key fields, including location, event characteristics, and impact assessment, from a high-quality, manually annotated dataset of 4,933 instances derived from 27,822 police briefing posts on Chinese Weibo (2019-2020). Experimental results demonstrated that LoRA-based fine-tuning significantly improved performance over both the base and instruction-tuned models, achieving an accuracy exceeding 98.36% for mortality detection and Exact Match Rates of 95.31% for fatality counts and 95.54% for province-level location extraction. The proposed pipeline thus provides a validated and efficient solution for multi-task structured information extraction in specialized domains, offering a practical framework for transforming unstructured text into reliable structured data in social science research."}
{"id": "2512.16189", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16189", "abs": "https://arxiv.org/abs/2512.16189", "authors": ["Musarrat Zeba", "Abdullah Al Mamun", "Kishoar Jahan Tithee", "Debopom Sutradhar", "Mohaimenul Azam Khan Raiaan", "Saddam Mukta", "Reem E. Mohamed", "Md Rafiqul Islam", "Yakub Sebastian", "Mukhtar Hussain", "Sami Azam"], "title": "Mitigating Hallucinations in Healthcare LLMs with Granular Fact-Checking and Domain-Specific Adaptation", "comment": null, "summary": "In healthcare, it is essential for any LLM-generated output to be reliable and accurate, particularly in cases involving decision-making and patient safety. However, the outputs are often unreliable in such critical areas due to the risk of hallucinated outputs from the LLMs. To address this issue, we propose a fact-checking module that operates independently of any LLM, along with a domain-specific summarization model designed to minimize hallucination rates. Our model is fine-tuned using Low-Rank Adaptation (LoRa) on the MIMIC III dataset and is paired with the fact-checking module, which uses numerical tests for correctness and logical checks at a granular level through discrete logic in natural language processing (NLP) to validate facts against electronic health records (EHRs). We trained the LLM model on the full MIMIC-III dataset. For evaluation of the fact-checking module, we sampled 104 summaries, extracted them into 3,786 propositions, and used these as facts. The fact-checking module achieves a precision of 0.8904, a recall of 0.8234, and an F1-score of 0.8556. Additionally, the LLM summary model achieves a ROUGE-1 score of 0.5797 and a BERTScore of 0.9120 for summary quality."}
{"id": "2512.16227", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16227", "abs": "https://arxiv.org/abs/2512.16227", "authors": ["Qizhou Chen", "Chengyu Wang", "Taolin Zhang", "Xiaofeng He"], "title": "An Information-Theoretic Framework for Robust Large Language Model Editing", "comment": null, "summary": "Large Language Models (LLMs) have become indispensable tools in science, technology, and society, enabling transformative advances across diverse fields. However, errors or outdated information within these models can undermine their accuracy and restrict their safe deployment. Developing efficient strategies for updating model knowledge without the expense and disruption of full retraining remains a critical challenge. Current model editing techniques frequently struggle to generalize corrections beyond narrow domains, leading to unintended consequences and limiting their practical impact. Here, we introduce a novel framework for editing LLMs, grounded in information bottleneck theory. This approach precisely compresses and isolates the essential information required for generalizable knowledge correction while minimizing disruption to unrelated model behaviors. Building upon this foundation, we present the Information Bottleneck Knowledge Editor (IBKE), which leverages compact latent representations to guide gradient-based updates, enabling robust and broadly applicable model editing. We validate IBKE's effectiveness across multiple LLM architectures and standard benchmark tasks, demonstrating state-of-the-art accuracy and improved generality and specificity of edits. These findings establish a theoretically principled and practical paradigm for open-domain knowledge editing, advancing the utility and trustworthiness of LLMs in real-world applications."}
{"id": "2512.16229", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16229", "abs": "https://arxiv.org/abs/2512.16229", "authors": ["Chenkai Xu", "Yijie Jin", "Jiajun Li", "Yi Tu", "Guoping Long", "Dandan Tu", "Tianqi Hou", "Junchi Yan", "Zhijie Deng"], "title": "LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding", "comment": null, "summary": "Diffusion Large Language Models (dLLMs) have demonstrated significant potential for high-speed inference. However, current confidence-driven decoding strategies are constrained by limited parallelism, typically achieving only 1--3 tokens per forward pass (TPF). In this work, we identify that the degree of parallelism during dLLM inference is highly sensitive to the Token Filling Order (TFO). Then, we introduce Lookahead PArallel Decoding LoPA, a training-free, plug-and-play algorithm, to identify a superior TFO and hence accelerate inference. LoPA concurrently explores distinct candidate TFOs via parallel branches, and selects the one with the highest potential for future parallelism based on branch confidence. We apply LoPA to the state-of-the-art D2F model and observe a substantial enhancement in decoding efficiency. Notably, LoPA increases the TPF of D2F-Dream to 10.1 on the GSM8K while maintaining performance superior to the Dream baseline. Furthermore, to facilitate this unprecedented degree of parallelism, we develop a specialized multi-device inference system featuring Branch Parallelism (BP), which achieves a single-sample throughput of 1073.9 tokens per second under multi-GPU deployment. The code is available at https://github.com/zhijie-group/LoPA."}
{"id": "2512.16248", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16248", "abs": "https://arxiv.org/abs/2512.16248", "authors": ["Qingguo Hu", "Zhenghao Lin", "Ziyue Yang", "Yucheng Ding", "Xiao Liu", "Yuting Jiang", "Ruizhe Wang", "Tianyu Chen", "Zhongxin Guo", "Yifan Xiong", "Rui Gao", "Lei Qu", "Jinsong Su", "Peng Cheng", "Yeyun Gong"], "title": "Sigma-Moe-Tiny Technical Report", "comment": null, "summary": "Mixture-of-Experts (MoE) has emerged as a promising paradigm for foundation models due to its efficient and powerful scalability. In this work, we present Sigma-MoE-Tiny, an MoE language model that achieves the highest sparsity compared to existing open-source models. Sigma-MoE-Tiny employs fine-grained expert segmentation with up to 96 experts per layer, while activating only one expert for each token, resulting in 20B total parameters with just 0.5B activated. The major challenge introduced by such extreme sparsity lies in expert load balancing. We find that the widely-used load balancing loss tends to become ineffective in the lower layers under this setting. To address this issue, we propose a progressive sparsification schedule aiming to balance expert utilization and training stability. Sigma-MoE-Tiny is pre-trained on a diverse and high-quality corpus, followed by post-training to further unlock its capabilities. The entire training process remains remarkably stable, with no occurrence of irrecoverable loss spikes. Comprehensive evaluations reveal that, despite activating only 0.5B parameters, Sigma-MoE-Tiny achieves top-tier performance among counterparts of comparable or significantly larger scale. In addition, we provide an in-depth discussion of load balancing in highly sparse MoE models, offering insights for advancing sparsity in future MoE architectures.\n  Project page: https://qghuxmu.github.io/Sigma-MoE-Tiny\n  Code: https://github.com/microsoft/ltp-megatron-lm"}
{"id": "2512.16287", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16287", "abs": "https://arxiv.org/abs/2512.16287", "authors": ["Yehor Tereshchenko", "Mika Hämäläinen", "Svitlana Myroniuk"], "title": "Evaluating OpenAI GPT Models for Translation of Endangered Uralic Languages: A Comparison of Reasoning and Non-Reasoning Architectures", "comment": "IWCLUL 2025", "summary": "The evaluation of Large Language Models (LLMs) for translation tasks has primarily focused on high-resource languages, leaving a significant gap in understanding their performance on low-resource and endangered languages. This study presents a comprehensive comparison of OpenAI's GPT models, specifically examining the differences between reasoning and non-reasoning architectures for translating between Finnish and four low-resource Uralic languages: Komi-Zyrian, Moksha, Erzya, and Udmurt. Using a parallel corpus of literary texts, we evaluate model willingness to attempt translation through refusal rate analysis across different model architectures. Our findings reveal significant performance variations between reasoning and non-reasoning models, with reasoning models showing 16 percentage points lower refusal rates. The results provide valuable insights for researchers and practitioners working with Uralic languages and contribute to the broader understanding of reasoning model capabilities for endangered language preservation."}
{"id": "2512.16323", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16323", "abs": "https://arxiv.org/abs/2512.16323", "authors": ["Hiroyuki Deguchi", "Katsuki Chousa", "Yusuke Sakai"], "title": "Hacking Neural Evaluation Metrics with Single Hub Text", "comment": null, "summary": "Strongly human-correlated evaluation metrics serve as an essential compass for the development and improvement of generation models and must be highly reliable and robust. Recent embedding-based neural text evaluation metrics, such as COMET for translation tasks, are widely used in both research and development fields. However, there is no guarantee that they yield reliable evaluation results due to the black-box nature of neural networks. To raise concerns about the reliability and safety of such metrics, we propose a method for finding a single adversarial text in the discrete space that is consistently evaluated as high-quality, regardless of the test cases, to identify the vulnerabilities in evaluation metrics. The single hub text found with our method achieved 79.1 COMET% and 67.8 COMET% in the WMT'24 English-to-Japanese (En--Ja) and English-to-German (En--De) translation tasks, respectively, outperforming translations generated individually for each source sentence by using M2M100, a general translation model. Furthermore, we also confirmed that the hub text found with our method generalizes across multiple language pairs such as Ja--En and De--En."}
{"id": "2512.16378", "categories": ["cs.CL", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.16378", "abs": "https://arxiv.org/abs/2512.16378", "authors": ["Sara Papi", "Javier Garcia Gilabert", "Zachary Hopton", "Vilém Zouhar", "Carlos Escolano", "Gerard I. Gállego", "Jorge Iranzo-Sánchez", "Ahrii Kim", "Dominik Macháček", "Patricia Schmidtova", "Maike Züfle"], "title": "Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs", "comment": "Project available at https://github.com/sarapapi/hearing2translate", "summary": "As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation."}
{"id": "2512.16401", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16401", "abs": "https://arxiv.org/abs/2512.16401", "authors": ["Darshil Chauhan", "Adityasinh Solanki", "Vansh Patel", "Kanav Kapoor", "Ritvik Jain", "Aditya Bansal", "Dhruv Kumar", "Prateek Narang"], "title": "Bridging the Reality Gap: Efficient Adaptation of ASR systems for Challenging Low-Resource Domains", "comment": null, "summary": "Automatic Speech Recognition (ASR) holds immense potential to streamline clinical documentation, such as digitizing handwritten prescriptions and reports, thereby increasing patient throughput and reducing costs in resource-constrained sectors like rural healthcare. However, realizing this utility is currently obstructed by significant technical barriers: strict data privacy constraints, limited computational resources, and severe acoustic domain shifts. We quantify this gap by showing that a robust multilingual model (IndicWav2Vec) degrades to a stark 40.94% Word Error Rate (WER) when deployed on real-world clinical audio (Gram Vaani), rendering it unusable for practical applications. To address these challenges and bring ASR closer to deployment, we propose an efficient, privacy-preserving adaptation framework. We employ Low-Rank Adaptation (LoRA) to enable continual learning from incoming data streams directly on edge devices, ensuring patient data confidentiality. Our strategy yields a 17.1% relative improvement in WER on the target domain. Furthermore, by integrating multi-domain experience replay, we reduce catastrophic forgetting by 47% compared to naive adaptation. These results demonstrate a viable pathway for building reliable, self-improving ASR systems that can operate effectively within the constraints of high-impact real-world environments."}
{"id": "2512.16530", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16530", "abs": "https://arxiv.org/abs/2512.16530", "authors": ["Primoz Kocbek", "Leon Kopitar", "Gregor Stiglic"], "title": "Plain language adaptations of biomedical text using LLMs: Comparision of evaluation metrics", "comment": "5 pages, 1 figure", "summary": "This study investigated the application of Large Language Models (LLMs) for simplifying biomedical texts to enhance health literacy. Using a public dataset, which included plain language adaptations of biomedical abstracts, we developed and evaluated several approaches, specifically a baseline approach using a prompt template, a two AI agent approach, and a fine-tuning approach. We selected OpenAI gpt-4o and gpt-4o mini models as baselines for further research. We evaluated our approaches with quantitative metrics, such as Flesch-Kincaid grade level, SMOG Index, SARI, and BERTScore, G-Eval, as well as with qualitative metric, more precisely 5-point Likert scales for simplicity, accuracy, completeness, brevity. Results showed a superior performance of gpt-4o-mini and an underperformance of FT approaches. G-Eval, a LLM based quantitative metric, showed promising results, ranking the approaches similarly as the qualitative metric."}
{"id": "2512.16541", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16541", "abs": "https://arxiv.org/abs/2512.16541", "authors": ["Primoz Kocbek", "Gregor Stiglic"], "title": "UM_FHS at the CLEF 2025 SimpleText Track: Comparing No-Context and Fine-Tune Approaches for GPT-4.1 Models in Sentence and Document-Level Text Simplification", "comment": "10 pages, 3 tables. CLEF 2025 Working Notes, 9 to 12 September 2025, Madrid, Spain", "summary": "This work describes our submission to the CLEF 2025 SimpleText track Task 1, addressing both sentenceand document-level simplification of scientific texts. The methodology centered on using the gpt-4.1, gpt-4.1mini, and gpt-4.1-nano models from OpenAI. Two distinct approaches were compared: a no-context method relying on prompt engineering and a fine-tuned (FT) method across models. The gpt-4.1-mini model with no-context demonstrated robust performance at both levels of simplification, while the fine-tuned models showed mixed results, highlighting the complexities of simplifying text at different granularities, where gpt-4.1-nano-ft performance stands out at document-level simplification in one case."}
{"id": "2512.16602", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16602", "abs": "https://arxiv.org/abs/2512.16602", "authors": ["Iker García-Ferrero", "David Montero", "Roman Orus"], "title": "Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics", "comment": null, "summary": "We introduce Refusal Steering, an inference-time method to exercise fine-grained control over Large Language Models refusal behaviour on politically sensitive topics without retraining. We replace fragile pattern-based refusal detection with an LLM-as-a-judge that assigns refusal confidence scores and we propose a ridge-regularized variant to compute steering vectors that better isolate the refusal--compliance direction. On Qwen3-Next-80B-A3B-Thinking, our method removes the refusal behaviour of the model around politically sensitive topics while maintaining safety on JailbreakBench and near-baseline performance on general benchmarks. The approach generalizes across 4B and 80B models and can also induce targeted refusals when desired. We analize the steering vectors and show that refusal signals concentrate in deeper layers of the transformer and are distributed across many dimensions. Together, these results demonstrate that activation steering can remove political refusal behaviour while retaining safety alignment for harmful content, offering a practical path to controllable, transparent moderation at inference time."}
{"id": "2512.16649", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16649", "abs": "https://arxiv.org/abs/2512.16649", "authors": ["Bingxiang He", "Zekai Qu", "Zeyuan Liu", "Yinghao Chen", "Yuxin Zuo", "Cheng Qian", "Kaiyan Zhang", "Weize Chen", "Chaojun Xiao", "Ganqu Cui", "Ning Ding", "Zhiyuan Liu"], "title": "JustRL: Scaling a 1.5B LLM with a Simple RL Recipe", "comment": "12 pages, 3 figures", "summary": "Recent advances in reinforcement learning for large language models have converged on increasing complexity: multi-stage training pipelines, dynamic hyperparameter schedules, and curriculum learning strategies. This raises a fundamental question: \\textbf{Is this complexity necessary?} We present \\textbf{JustRL}, a minimal approach using single-stage training with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B reasoning models (54.9\\% and 64.3\\% average accuracy across nine mathematical benchmarks) while using 2$\\times$ less compute than sophisticated approaches. The same hyperparameters transfer across both models without tuning, and training exhibits smooth, monotonic improvement over 4,000+ steps without the collapses or plateaus that typically motivate interventions. Critically, ablations reveal that adding ``standard tricks'' like explicit length penalties and robust verifiers may degrade performance by collapsing exploration. These results suggest that the field may be adding complexity to solve problems that disappear with a stable, scaled-up baseline. We release our models and code to establish a simple, validated baseline for the community."}
{"id": "2512.16770", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16770", "abs": "https://arxiv.org/abs/2512.16770", "authors": ["William English", "Chase Walker", "Dominic Simon", "Rickard Ewetz"], "title": "GinSign: Grounding Natural Language Into System Signatures for Temporal Logic Translation", "comment": null, "summary": "Natural language (NL) to temporal logic (TL) translation enables engineers to specify, verify, and enforce system behaviors without manually crafting formal specifications-an essential capability for building trustworthy autonomous systems. While existing NL-to-TL translation frameworks have demonstrated encouraging initial results, these systems either explicitly assume access to accurate atom grounding or suffer from low grounded translation accuracy. In this paper, we propose a framework for Grounding Natural Language Into System Signatures for Temporal Logic translation called GinSign. The framework introduces a grounding model that learns the abstract task of mapping NL spans onto a given system signature: given a lifted NL specification and a system signature $\\mathcal{S}$, the classifier must assign each lifted atomic proposition to an element of the set of signature-defined atoms $\\mathcal{P}$. We decompose the grounding task hierarchically- first predicting predicate labels, then selecting the appropriately typed constant arguments. Decomposing this task from a free-form generation problem into a structured classification problem permits the use of smaller masked language models and eliminates the reliance on expensive LLMs. Experiments across multiple domains show that frameworks which omit grounding tend to produce syntactically correct lifted LTL that is semantically nonequivalent to grounded target expressions, whereas our framework supports downstream model checking and achieves grounded logical-equivalence scores of $95.5\\%$, a $1.4\\times$ improvement over SOTA."}
{"id": "2512.16795", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.16795", "abs": "https://arxiv.org/abs/2512.16795", "authors": ["Shubham Mishra", "Samyek Jain", "Gorang Mehrishi", "Shiv Tiwari", "Harsh Sharma", "Pratik Narang", "Dhruv Kumar"], "title": "From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs", "comment": "Under Review", "summary": "Retrieval-Augmented Generation (RAG) grounds large language models (LLMs) in external evidence, but fails when retrieved sources conflict or contain outdated or subjective information. Prior work address these issues independently but lack unified reasoning supervision. We propose a reasoning-trace-augmented RAG framework that adds structured, interpretable reasoning across three stages : (1) document-level adjudication, (2) conflict analysis, and (3) grounded synthesis, producing citation-linked answers or justified refusals. A Conflict-Aware Trust-Score (CATS) pipeline is introduced which evaluates groundedness, factual correctness, refusal accuracy, and conflict-behavior alignment using an LLM-as-a-Judge. Our 539-query reasoning dataset and evaluation pipeline establish a foundation for conflict-aware, interpretable RAG systems. Experimental results demonstrate substantial gains over baselines, most notably with Qwen, where Supervised Fine-Tuning improved End-to-End answer correctness from 0.069 to 0.883 and behavioral adherence from 0.074 to 0.722."}
{"id": "2512.16802", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16802", "abs": "https://arxiv.org/abs/2512.16802", "authors": ["Primož Kocbek", "Azra Frkatović-Hodžić", "Dora Lalić", "Vivian Hui", "Gordan Lauc", "Gregor Štiglic"], "title": "Exploration of Augmentation Strategies in Multi-modal Retrieval-Augmented Generation for the Biomedical Domain: A Case Study Evaluating Question Answering in Glycobiology", "comment": "Will be published in IEEE BigData 2025 proceedings. Contains 10 pages, 1 figure, 5 tables", "summary": "Multi-modal retrieval-augmented generation (MM-RAG) promises grounded biomedical QA, but it is unclear when to (i) convert figures/tables into text versus (ii) use optical character recognition (OCR)-free visual retrieval that returns page images and leaves interpretation to the generator. We study this trade-off in glycobiology, a visually dense domain. We built a benchmark of 120 multiple-choice questions (MCQs) from 25 papers, stratified by retrieval difficulty (easy text, medium figures/tables, hard cross-evidence). We implemented four augmentations-None, Text RAG, Multi-modal conversion, and late-interaction visual retrieval (ColPali)-using Docling parsing and Qdrant indexing. We evaluated mid-size open-source and frontier proprietary models (e.g., Gemma-3-27B-IT, GPT-4o family). Additional testing used the GPT-5 family and multiple visual retrievers (ColPali/ColQwen/ColFlor). Accuracy with Agresti-Coull 95% confidence intervals (CIs) was computed over 5 runs per configuration. With Gemma-3-27B-IT, Text and Multi-modal augmentation outperformed OCR-free retrieval (0.722-0.740 vs. 0.510 average accuracy). With GPT-4o, Multi-modal achieved 0.808, with Text 0.782 and ColPali 0.745 close behind; within-model differences were small. In follow-on experiments with the GPT-5 family, the best results with ColPali and ColFlor improved by ~2% to 0.828 in both cases. In general, across the GPT-5 family, ColPali, ColQwen, and ColFlor were statistically indistinguishable. GPT-5-nano trailed larger GPT-5 variants by roughly 8-10%. Pipeline choice is capacity-dependent: converting visuals to text lowers the reader burden and is more reliable for mid-size models, whereas OCR-free visual retrieval becomes competitive under frontier models. Among retrievers, ColFlor offers parity with heavier options at a smaller footprint, making it an efficient default when strong generators are available."}
